{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import myFunctions2 as my\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data \n",
    "into feature matrix X, class labels y, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = my.load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "Here we deal with problematic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can work with less samples, it saves time.\n",
    "#print(tX.shape)\n",
    "#N = 100000\n",
    "#tX = tX[0:N]\n",
    "#y = y[0:N]\n",
    "#print(tX.shape)\n",
    "\n",
    "\n",
    "# indices of each class\n",
    "#idx_red = np.where(y == 1)[0]\n",
    "#idx_blue = np.where(y == -1)[0]\n",
    "\n",
    "#for d in range(1,tX.shape[1]):\n",
    "#    f, axarr = plt.subplots(2, sharex=True)\n",
    "#    axarr[0].hist(tX[idx_red,d], 50, normed=1, facecolor='red', alpha=0.5)\n",
    "#    axarr[1].hist(tX[idx_blue,d], 50, normed=1, facecolor='blue', alpha=0.5)\n",
    "#    plt.show()\n",
    "\n",
    "\n",
    "# look for outliers (> 40*std)\n",
    "#outliers = np.where(tX > 40)\n",
    "# here we see that our single outlier is in fact a higgs boson event...\n",
    "# should we keep it ???\n",
    "#print(outliers[0], y[outliers[0]])\n",
    "#plt.boxplot(tX)\n",
    "#np.sum(y == 1)/len(y), np.sum(y == -1)/len(y)\n",
    "\n",
    "\n",
    "#Is there samples that are identical ?\n",
    "#ncols = tX.shape[1]\n",
    "#dtype = tX.dtype.descr * ncols\n",
    "#struct = tX.view(dtype)\n",
    "\n",
    "#uniq, idx = np.unique(struct, return_index=True)\n",
    "#tX = uniq.view(tX.dtype).reshape(-1, ncols)\n",
    "\n",
    "#print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4 : Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_RR(y, x, k_indices, k, lamb):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    ## get k'th subgroup in test, others in train\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    ## regression method\n",
    "    \n",
    "    #w = my.least_squares(y_tr, tx_tr)\n",
    "    w = my.ridge_regression(y_tr, tx_tr, lamb)\n",
    "    \n",
    "    ## calculate the loss for train and test data :\n",
    "    ## - with mse, mae or rmse\n",
    "    #loss_tr = my.compute_loss(y_tr, tx_tr, w)\n",
    "    #loss_te = my.compute_loss(y_te, tx_te, w)\n",
    "    ## - with missclassification rates\n",
    "    loss_tr, loss_te = my.compute_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "    \n",
    "       \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_RR_demo(y, X):\n",
    "    ## parameters\n",
    "    seed = 22\n",
    "    k_fold = 10\n",
    "    \n",
    "    ## hyperparameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "    lambdas = np.logspace(-4, 1, 18)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = np.zeros((len(degrees),k_fold))\n",
    "    rmse_te = np.zeros((len(degrees),k_fold))\n",
    "    best_lambda = np.zeros((len(degrees),))\n",
    "    \n",
    "    # Hyperparameter 1\n",
    "    for d, degree in enumerate(degrees):\n",
    "        print(degree)\n",
    "        # Build polynomial function\n",
    "        tX = my.build_poly(X, degree)\n",
    "        \n",
    "        loss_tr = np.zeros((len(lambdas),k_fold))\n",
    "        loss_te = np.zeros((len(lambdas),k_fold))\n",
    "        \n",
    "        for l, lamb in enumerate(lambdas):\n",
    "            # Cross-validation\n",
    "            for k in range(k_fold):\n",
    "                ## Store losses\n",
    "                loss_tr[l,k], loss_te[l,k] = cross_validation_RR(y, tX, k_indices, k, lamb)\n",
    "        ## Select automatically the value of lambda that minimize the missclassification rate       \n",
    "        best_idx = np.argmin(np.mean(loss_te,axis=0))\n",
    "        best_lambda[d] = lambdas[best_idx]\n",
    "        \n",
    "        ## Store losses\n",
    "        rmse_tr[d,:] = loss_tr[best_idx,:]\n",
    "        rmse_te[d,:] = loss_te[best_idx,:]\n",
    "    \n",
    "    ## Bias-Variance graphic\n",
    "    my.bias_variance_decomposition_visualization(degrees, rmse_tr.T, rmse_te.T)   \n",
    "    print(best_lambda)\n",
    "    \n",
    "    return rmse_tr, rmse_te, degrees, best_lambda\n",
    "\n",
    "rmse_tr, rmse_te, degrees, best_lambda = cross_validation_RR_demo(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your predictions\n",
    "First, train on the all datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the hyperparameter that have been chosen\n",
    "final_degree = 10\n",
    "final_lambda = 0.00239503\n",
    "#10 0.00239503\n",
    "\n",
    "# Build the polynomial basis, perform ridge regression\n",
    "final_X = my.build_poly(tX, final_degree)\n",
    "final_weights = my.ridge_regression(y, final_X, final_lambda)\n",
    "\n",
    "# Take a look at the train error\n",
    "y_pred = my.predict_labels(final_weights, final_X)\n",
    "loss = len(np.nonzero(y_pred-y)[0])/len(y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download test data\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, X_test, ids_test = my.load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform test data as it has been done for training : cleaning and polynomial basis\n",
    "tX_test, mean_xtest, std_xtest = clean_data(X_test)\n",
    "final_X_test = my.build_poly(tX_test, final_degree)\n",
    "\n",
    "# Aplly the model to the test data in order to get predictions\n",
    "OUTPUT_PATH = 'results.csv'\n",
    "y_pred = my.predict_labels(final_weights, final_X_test)\n",
    "# create submission file\n",
    "my.create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5 : Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_cross_validation(y, x, k_indices, k):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 0.01\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "    batch_size = 5\n",
    "    max_epochs = 300\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # Stochastics gradient descent\n",
    "    losses, ws = my.logistic_regression(y_tr, tx_tr, w, batch_size, max_epochs, gamma)\n",
    "    best_idx = np.argmin(losses)\n",
    "    w = ws[best_idx]\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_logic_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "        \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "\n",
    "def logistic_demo(y, tx):\n",
    "    \n",
    "    y[np.where(y == -1)[0]] = 0\n",
    "    seed = 22\n",
    "    \n",
    "    # prepare k-fold cross-validation\n",
    "    k_fold = 10\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # losses matrices\n",
    "    loss_tr = np.zeros((k_fold,))\n",
    "    loss_te = np.zeros((k_fold,))\n",
    "    \n",
    "    #tx = my.build_poly(tx,2)\n",
    "    \n",
    "    # cross-validation\n",
    "    for k in range(k_fold):\n",
    "        loss_tr[k], loss_te[k] = logistic_cross_validation(y, tx, k_indices, k)\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "loss_tr, loss_te = logistic_demo(y, tX)\n",
    "print(np.mean(loss_tr), np.mean(loss_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6 : Logistic penalized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logic_cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 0.01\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "    batch_size = 5\n",
    "    max_epochs = 300\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # Stochastics gradient descent\n",
    "    losses, ws = my.reg_logistic_regression(y_tr, tx_tr, w, batch_size, max_epochs, gamma, lambda_)\n",
    "    best_idx = np.argmin(losses)\n",
    "    w = ws[best_idx]\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_logic_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "        \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx):\n",
    "    \n",
    "    y[np.where(y == -1)[0]] = 0\n",
    "    lambdas = np.logspace(-5,-1,10)\n",
    "    \n",
    "    seed = 22\n",
    "    \n",
    "    # prepare k-fold cross-validation\n",
    "    k_fold = 10\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    #tx = my.build_poly(tx,2)\n",
    "    \n",
    "    # losses matrices\n",
    "    loss_tr = np.zeros((k_fold,len(lambdas)))\n",
    "    loss_te = np.zeros((k_fold,len(lambdas)))\n",
    "    \n",
    "    for l, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        # cross-validation\n",
    "        for k in range(k_fold):\n",
    "            loss_tr[k,l], loss_te[k,l] = logic_cross_validation(y, tx, k_indices, k, lambda_)\n",
    "            \n",
    "    mse_tr = np.mean(loss_tr, axis=0)\n",
    "    mse_te = np.mean(loss_te, axis=0)\n",
    "    my.cross_validation_visualization(lambdas, mse_tr, mse_te)   \n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "loss_tr, loss_te = reg_logistic_regression(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "w = np.zeros((tX.shape[1],1))\n",
    "batch_size = 5\n",
    "max_epochs = 300\n",
    "lambda_ = np.logspace(-5,-1,10)[7]\n",
    "y[np.where(y == -1)[0]] = 0\n",
    "\n",
    "losses, ws = my.reg_logistic_regression(y, tX, w, batch_size, max_epochs, gamma, lambda_)\n",
    "best_idx = np.argmin(losses)\n",
    "w = ws[best_idx]\n",
    "print(losses[best_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = my.load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test, mean_xtest, std_xtest = clean_data(X_test)\n",
    "#final_X_test = my.build_poly(tX_test, degree)\n",
    "\n",
    "OUTPUT_PATH = 'results.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = my.predict_logic_labels(w, tX_test)\n",
    "y_pred[np.where(y_pred == 0)[0]] = -1\n",
    "my.create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
