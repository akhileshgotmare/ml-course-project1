{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import myFunctions as my\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data \n",
    "into feature matrix X, class labels y, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "Here we deal with problematic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "def standardize_badFeatures(X):\n",
    "    \n",
    "    # Function that calculate the mean and std of bad features without elements equal to -999\n",
    "    # Then, it remplaces -999 values by zeros, zeros won't influence the train of the model... \n",
    "    mean_x = np.zeros((X.shape[1],))\n",
    "    std_x = np.zeros((X.shape[1],))\n",
    "    for d in range(X.shape[1]):\n",
    "        idx = np.where(X[:,d] == -999)[0]\n",
    "        mean_x[d] = np.mean(np.delete(X[:,d], (idx)))\n",
    "        std_x[d] = np.std(np.delete(X[:,d], (idx)))\n",
    "        X[:,d] = (X[:,d]-mean_x[d])/std_x[d]\n",
    "        X[idx,d] = 0\n",
    "    return X, mean_x, std_x\n",
    "\n",
    "\n",
    "def clean_data(X):\n",
    "\n",
    "    # find indices of features that have at least one value -999, we call them \"bad\" features\n",
    "    idx_badFeatures = []\n",
    "    for d in range(X.shape[1]):\n",
    "        if sum(X[:,d] == -999) > 0:\n",
    "            idx_badFeatures.append(d)\n",
    "\n",
    "    # separate \"good\" and \"bad\" features\n",
    "    X_badFeatures = X[:,idx_badFeatures]\n",
    "    X_goodFeatures = np.delete(X,(idx_badFeatures), axis=1)\n",
    "\n",
    "    # Standardize it differently (see : standardize_badFeatures(X))\n",
    "    tX, mean_x, std_x = standardize(X_goodFeatures)\n",
    "    tX2, mean_x2, std_x2 = standardize_badFeatures(X_badFeatures)\n",
    "\n",
    "    # comment the 3 next lines if you want to work only with \"good\" features\n",
    "    tX = np.hstack((tX, tX2))\n",
    "    mean_x = np.hstack((mean_x, mean_x2))\n",
    "    std_x = np.hstack((std_x, std_x2))\n",
    "    \n",
    "    return tX, mean_x, std_x\n",
    "\n",
    "# Now tX already has ones in the first column...\n",
    "tX, mean_x, std_x = clean_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can work with less samples, it saves time.\n",
    "#print(tX.shape)\n",
    "#N = 100000\n",
    "#tX = tX[0:N]\n",
    "#y = y[0:N]\n",
    "#print(tX.shape)\n",
    "# indices of each class\n",
    "idx_red = np.where(y == 1)[0]\n",
    "idx_blue = np.where(y == -1)[0]\n",
    "\n",
    "#for d in range(1,tX.shape[1]):\n",
    "#    f, axarr = plt.subplots(2, sharex=True)\n",
    "#    axarr[0].hist(tX[idx_red,d], 50, normed=1, facecolor='red', alpha=0.5)\n",
    "#    axarr[1].hist(tX[idx_blue,d], 50, normed=1, facecolor='blue', alpha=0.5)\n",
    "#    plt.show()\n",
    "\n",
    "# look for outliers (> 40*std)\n",
    "#outliers = np.where(tX > 40)\n",
    "# here we see that our single outlier is in fact a higgs boson event...\n",
    "# should we keep it ???\n",
    "#print(outliers[0], y[outliers[0]])\n",
    "#plt.boxplot(tX)\n",
    "#np.sum(y == 1)/len(y), np.sum(y == -1)/len(y)\n",
    "\n",
    "\n",
    "#Is there samples that are identical ?\n",
    "#ncols = tX.shape[1]\n",
    "#dtype = tX.dtype.descr * ncols\n",
    "#struct = tX.view(dtype)\n",
    "\n",
    "#uniq, idx = np.unique(struct, return_index=True)\n",
    "#tX = uniq.view(tX.dtype).reshape(-1, ncols)\n",
    "\n",
    "#print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4 : Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lamb):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # regression/classification method\n",
    "    #w = my.least_squares(y_tr, tx_tr)\n",
    "    w = my.ridge_regression(y_tr, tx_tr, lamb)\n",
    "    \n",
    "    # calculate the loss for train and test data: TODO\n",
    "    #loss_tr = my.compute_loss(y_tr, tx_tr, w)\n",
    "    #loss_te = my.compute_loss(y_te, tx_te, w)\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "       \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, X):\n",
    "    # parameters\n",
    "    seed = 56\n",
    "    k_fold = 10\n",
    "    \n",
    "    # hyperparameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    lambdas = np.logspace(-4, 1, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = np.zeros((len(degrees),k_fold))\n",
    "    rmse_te = np.zeros((len(degrees),k_fold))\n",
    "    best_lambda = np.zeros((len(degrees),))\n",
    "    \n",
    "    # Hyperparameter 1\n",
    "    for d, degree in enumerate(degrees):\n",
    "        print(degree)\n",
    "        # Build polynomial function\n",
    "        tX = my.build_poly(X, degree)\n",
    "        \n",
    "        loss_tr = np.zeros((len(lambdas),k_fold))\n",
    "        loss_te = np.zeros((len(lambdas),k_fold))\n",
    "        \n",
    "        for l, lamb in enumerate(lambdas):\n",
    "        \n",
    "            # Cross-validation\n",
    "            for k in range(k_fold):\n",
    "                loss_tr[l,k], loss_te[l,k] = cross_validation(y, tX, k_indices, k, lamb)\n",
    "                \n",
    "        best_idx = np.argmin(np.mean(loss_te,axis=0))\n",
    "        best_lambda[d] = lambdas[best_idx]\n",
    "        \n",
    "        rmse_tr[d,:] = loss_tr[best_idx,:]\n",
    "        rmse_te[d,:] = loss_te[best_idx,:]\n",
    "        \n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr.T, rmse_te.T)   \n",
    "    print(best_lambda)\n",
    "    return rmse_tr, rmse_te, degrees, best_lambda\n",
    "\n",
    "rmse_tr, rmse_te, degrees, lambdas = cross_validation_demo(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your predictions\n",
    "First, train on the all datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the hyperparameter that have been chosen\n",
    "final_degree = 10\n",
    "final_lambda = 0.00239503\n",
    "\n",
    "# Build the polynomial basis, perform ridge regression\n",
    "final_X = my.build_poly(tX, final_degree)\n",
    "final_weights = my.ridge_regression(y, final_X, final_lambda)\n",
    "\n",
    "# Take a look at the train error\n",
    "y_pred = predict_labels(final_weights, final_X)\n",
    "loss = len(np.nonzero(y_pred-y)[0])/len(y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download test data\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# Transform test data as it has been done for training : cleaning and polynomial basis\n",
    "tX_test, mean_xtest, std_xtest = clean_data(X_test)\n",
    "final_X_test = my.build_poly(tX_test, final_degree)\n",
    "\n",
    "# Aplly the model to the test data in order to get predictions\n",
    "OUTPUT_PATH = 'results.csv'\n",
    "y_pred = my.predict_labels(final_weights, final_X_test)\n",
    "# create submission file\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5 : Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logic_cross_validation(y, x, k_indices, k):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 0.01\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "    batch_size = 100\n",
    "    max_epochs = 250\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # Stochastics gradient descent\n",
    "    losses, ws = my.learning_by_stochastic_newton_method(y_tr, tx_tr, w, batch_size, max_epochs, gamma)\n",
    "    best_idx = np.argmin(losses)\n",
    "    w = ws[best_idx]\n",
    "    print(losses[best_idx])\n",
    "    \n",
    "    ## Full gradient descent leads to Memory Error, That's why we use stochastics GD\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_logic_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "    print(loss_tr, loss_te)\n",
    "        \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression_newton_method_demo(y, tx):\n",
    "    \n",
    "    y[np.where(y == -1)[0]] = 0\n",
    "    seed = 74\n",
    "    \n",
    "    # prepare k-fold cross-validation\n",
    "    k_fold = 10\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # losses matrices\n",
    "    loss_tr = np.zeros((k_fold,))\n",
    "    loss_te = np.zeros((k_fold,))\n",
    "    # cross-validation\n",
    "    for k in range(k_fold):\n",
    "        print(k)\n",
    "        loss_tr[k], loss_te[k] = logic_cross_validation(y, tx, k_indices, k)\n",
    "    \n",
    "    loss_tr[k], loss_te[k] = logic_cross_validation(y, tx, k_indices, k)\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "loss_tr, loss_te = logistic_regression_newton_method_demo(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6 : Logistic penalized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import myFunctions as my\n",
    "from plots import cross_validation_visualization\n",
    "def logic_cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 0.01\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "    batch_size = 5\n",
    "    max_epochs = 300\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # Stochastics gradient descent\n",
    "    losses, ws = my.learning_by_stochastic_penalized_gradient_descent(y_tr, tx_tr, w, batch_size, max_epochs, gamma, lambda_)\n",
    "    best_idx = np.argmin(losses)\n",
    "    w = ws[best_idx]\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_logic_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "    print(loss_tr, loss_te)\n",
    "        \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression_newton_method_demo(y, tx):\n",
    "    \n",
    "    y[np.where(y == -1)[0]] = 0\n",
    "    lambdas = np.logspace(-5,-1,5)\n",
    "    \n",
    "    seed = 74\n",
    "    \n",
    "    # prepare k-fold cross-validation\n",
    "    k_fold = 10\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    #tx = my.build_poly(tx,2)\n",
    "    \n",
    "    # losses matrices\n",
    "    loss_tr = np.zeros((k_fold,len(lambdas)))\n",
    "    loss_te = np.zeros((k_fold,len(lambdas)))\n",
    "    \n",
    "    for l, lambda_ in enumerate(lambdas):\n",
    "        print(l+1)\n",
    "        \n",
    "        # cross-validation\n",
    "        for k in range(k_fold):\n",
    "            loss_tr[k,l], loss_te[k,l] = logic_cross_validation(y, tx, k_indices, k, lambda_)\n",
    "            \n",
    "    mse_tr = np.mean(loss_tr, axis=0)\n",
    "    mse_te = np.mean(loss_te, axis=0)\n",
    "    cross_validation_visualization(lambdas, mse_tr, mse_te)   \n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "\n",
    "loss_tr, loss_te = logistic_regression_newton_method_demo(y, tX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "w = np.zeros((tX.shape[1],1))\n",
    "batch_size = 10\n",
    "max_epochs = 200\n",
    "lambda_ = np.logspace(-5,-1,5)[3]\n",
    "y[np.where(y == -1)[0]] = 0\n",
    "\n",
    "losses, ws = my.learning_by_stochastic_penalized_newton_method(y, tX, w, batch_size, max_epochs, gamma, lambda_)\n",
    "best_idx = np.argmin(losses)\n",
    "w = ws[best_idx]\n",
    "print(losses[best_idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tX_test, mean_xtest, std_xtest = clean_data(X_test)\n",
    "#final_X_test = my.build_poly(tX_test, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'results.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = my.predict_logic_labels(w, tX_test)\n",
    "y_pred[np.where(y_pred == 0)[0]] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
