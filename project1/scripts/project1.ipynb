{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data \n",
    "into feature matrix X, class labels y, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "Here we deal with problematic values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "def standardize_badFeatures(X):\n",
    "    \n",
    "    # Function that calculate the mean and std of bad features without elements equal to -999\n",
    "    # Then, it remplaces -999 values by zeros, zeros won't influence the train of the model... \n",
    "    mean_x = np.zeros((X.shape[1],))\n",
    "    std_x = np.zeros((X.shape[1],))\n",
    "    for d in range(X.shape[1]):\n",
    "        idx = np.where(X[:,d] == -999)[0]\n",
    "        mean_x[d] = np.mean(np.delete(X[:,d], (idx)))\n",
    "        std_x[d] = np.std(np.delete(X[:,d], (idx)))\n",
    "        X[:,d] = (X[:,d]-mean_x[d])/std_x[d]\n",
    "        X[idx,d] = 0\n",
    "    return X, mean_x, std_x\n",
    "\n",
    "\n",
    "def clean_data(X):\n",
    "\n",
    "    # find indices of features that have at least one value -999, we call them \"bad\" features\n",
    "    idx_badFeatures = []\n",
    "    for d in range(X.shape[1]):\n",
    "        if sum(X[:,d] == -999) > 0:\n",
    "            idx_badFeatures.append(d)\n",
    "\n",
    "    # separate \"good\" and \"bad\" features\n",
    "    X_badFeatures = X[:,idx_badFeatures]\n",
    "    X_goodFeatures = np.delete(X,(idx_badFeatures), axis=1)\n",
    "\n",
    "    # Standardize it differently (see : standardize_badFeatures(X))\n",
    "    tX, mean_x, std_x = standardize(X_goodFeatures)\n",
    "    tX2, mean_x2, std_x2 = standardize_badFeatures(X_badFeatures)\n",
    "\n",
    "    # comment the 3 next lines if you want to work only with \"good\" features\n",
    "    tX = np.hstack((tX, tX2))\n",
    "    mean_x = np.hstack((mean_x, mean_x2))\n",
    "    std_x = np.hstack((std_x, std_x2))\n",
    "    \n",
    "    return tX, mean_x, std_x\n",
    "\n",
    "# Now tX already has ones in the first column...\n",
    "tX, mean_x, std_x = clean_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_red = np.where(y == 1)[0]\n",
    "idx_blue = np.where(y == -1)[0]\n",
    "\n",
    "#for d in range(1,X.shape[1]):\n",
    "#    f, axarr = plt.subplots(2, sharex=True)\n",
    "#    axarr[0].hist(X[idx_red,d], 50, normed=1, facecolor='red', alpha=0.5)\n",
    "#    axarr[1].hist(X[idx_blue,d], 50, normed=1, facecolor='blue', alpha=0.5)\n",
    "#    plt.show()\n",
    "\n",
    "# look for outliers (> 40*std)\n",
    "#outliers = np.where(tX > 40)\n",
    "# here we see that our single outlier is in fact a higgs boson event...\n",
    "# should we keep it ???\n",
    "#print(outliers[0], y[outliers[0]])\n",
    "#plt.boxplot(tX)\n",
    "#np.sum(y == 1)/len(y), np.sum(y == -1)/len(y)\n",
    "\n",
    "\n",
    "#Is there samples that are identical ?\n",
    "#ncols = tX.shape[1]\n",
    "#dtype = tX.dtype.descr * ncols\n",
    "#struct = tX.view(dtype)\n",
    "\n",
    "#uniq, idx = np.unique(struct, return_index=True)\n",
    "#tX = uniq.view(tX.dtype).reshape(-1, ncols)\n",
    "\n",
    "#print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "9\n",
      "13\n",
      "15\n",
      "[ 0.1         0.1         0.1         0.1         0.00031623]\n"
     ]
    }
   ],
   "source": [
    "import myFunctions as my\n",
    "from plots import bias_variance_decomposition_visualization\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lamb):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    tx_tr = x[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    y_tr = y[np.delete(k_indices, (k), axis=0).flatten()]\n",
    "    tx_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    \n",
    "    # regression/classification method\n",
    "    #w = my.least_squares(y_tr, tx_tr)\n",
    "    w = my.ridge_regression(y_tr, tx_tr, lamb)\n",
    "    \n",
    "    # calculate the loss for train and test data: TODO\n",
    "    #loss_tr = my.compute_loss(y_tr, tx_tr, w)\n",
    "    #loss_te = my.compute_loss(y_te, tx_te, w)\n",
    "    \n",
    "    loss_tr, loss_te = my.compute_classerror(w, tx_tr, tx_te, y_tr, y_te)\n",
    "       \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, X):\n",
    "    # parameters\n",
    "    seed = 74\n",
    "    k_fold = 10\n",
    "    \n",
    "    # hyperparameters\n",
    "    degrees = [1, 5, 9, 13, 15]\n",
    "    lambdas = np.logspace(-5, 2, 15)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = my.build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = np.zeros((len(degrees),k_fold))\n",
    "    rmse_te = np.zeros((len(degrees),k_fold))\n",
    "    best_lambda = np.zeros((len(degrees),))\n",
    "    \n",
    "    # Hyperparameter 1\n",
    "    for d, degree in enumerate(degrees):\n",
    "        print(degree)\n",
    "        # Build polynomial function\n",
    "        tX = my.build_poly(X, degree)\n",
    "        \n",
    "        loss_tr = np.zeros((len(lambdas),k_fold))\n",
    "        loss_te = np.zeros((len(lambdas),k_fold))\n",
    "        \n",
    "        for l, lamb in enumerate(lambdas):\n",
    "        \n",
    "            # Cross-validation\n",
    "            for k in range(k_fold):\n",
    "                loss_tr[l,k], loss_te[l,k] = cross_validation(y, tX, k_indices, k, lamb)\n",
    "                \n",
    "        best_idx = np.argmin(np.mean(loss_te,axis=0))\n",
    "        best_lambda[d] = lambdas[best_idx]\n",
    "        \n",
    "        rmse_tr[d,:] = loss_tr[best_idx,:]\n",
    "        rmse_te[d,:] = loss_te[best_idx,:]\n",
    "        \n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr.T, rmse_te.T)   \n",
    "    print(best_lambda)\n",
    "    return rmse_tr, rmse_te, degrees, best_lambda\n",
    "\n",
    "rmse_tr, rmse_te, degrees, lambdas = cross_validation_demo(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 9, 13, 15] [ 0.26394267  0.21328978  0.20573022  0.2023      0.25617067] [ 0.264     0.213532  0.206148  0.202756  0.257792]\n"
     ]
    }
   ],
   "source": [
    "print(degrees, np.mean(rmse_tr.T, axis=0), np.mean(rmse_te.T, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-5e86380eddec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The loss={l}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_logic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlogistic_regression_newton_method_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-5e86380eddec>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method_demo\u001b[0;34m(y, tx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_by_newton_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/INTRANET/jeanning/myfiles/PCML/ML_course-master/projects/project1/scripts/myFunctions.py\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\"\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/INTRANET/jeanning/myfiles/PCML/ML_course-master/projects/project1/scripts/myFunctions.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient, and hessian.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_logic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_logic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_logic_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/INTRANET/jeanning/myfiles/PCML/ML_course-master/projects/project1/scripts/myFunctions.py\u001b[0m in \u001b[0;36mcalculate_logic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def logistic_regression_newton_method_demo(y, tx):\n",
    "    \n",
    "    y[np.where(y == -1)[0]] = 0\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.01\n",
    "    threshold = 1e-8\n",
    "    #lambdas = np.logspace(-5,2,10)\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = my.learning_by_newton_method(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 500 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criteria\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    # visualization\n",
    "    print(\"The loss={l}\".format(l=my.calculate_logic_loss(y, tx, w)))\n",
    "\n",
    "logistic_regression_newton_method_demo(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.259052"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree = 2\n",
    "lamb = 1\n",
    "final_X = my.build_poly(tX, degree)\n",
    "w = my.ridge_regression(y, final_X, lamb)\n",
    "y_pred = predict_labels(w, final_X)\n",
    "loss = len(np.nonzero(y_pred-y)[0])/len(y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test, mean_xtest, std_xtest = clean_data(X_test)\n",
    "final_X_test = my.build_poly(tX_test, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'results.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, final_X_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
