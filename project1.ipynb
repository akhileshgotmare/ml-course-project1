{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "6.320208\n"
     ]
    }
   ],
   "source": [
    "# For entries with missing data, the value -999 is filled, therefore we try to figure out ...\n",
    "# ... how much of the data is missing\n",
    "\n",
    "count_miss_instances=np.zeros((len(y),1))\n",
    "for id in ids:\n",
    "    count_miss_instances[id-100000]=sum(tX[id-100000] == -999.0)\n",
    "print(np.median(count_miss_instances))\n",
    "print(np.mean(count_miss_instances))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above, for every instance on an average about 6 field/attribute values are missing, we further perform a feature-wise check for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  38114.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [ 177457.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [  99913.]\n",
      " [  99913.]\n",
      " [  99913.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [      0.]]\n"
     ]
    }
   ],
   "source": [
    "count_miss_features=np.zeros((tX.shape[1],1))\n",
    "for d in range(tX.shape[1]):\n",
    "    count_miss_features[d]=sum(tX[:,d] == -999.0)\n",
    "print(count_miss_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we realize that only some (although not few) of the features have missing values. Since the number of instances where these features have missing values is quite a large fraction of the data, we decide to drop these features from our data for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n",
      "[  0.   4.   5.   6.  12.  23.  24.  25.  26.  27.  28.]\n",
      "(250000, 19)\n"
     ]
    }
   ],
   "source": [
    "#Counting the number of features\n",
    "print(sum(count_miss_features > 0))\n",
    "\n",
    "count_miss_features=np.zeros((tX.shape[1],1))\n",
    "del_features=[]\n",
    "\n",
    "# We create an array del_features (since we plan to drop these features) ...\n",
    "# ... to store the index of the attributes with missing values \n",
    "for d in range(tX.shape[1]):\n",
    "    count_miss_features[d]=sum(tX[:,d] == -999.0)\n",
    "    if count_miss_features[d]>0:\n",
    "            del_features=np.r_[del_features,d]\n",
    "print(del_features)   \n",
    "    \n",
    "# The features having indices in del_features computed above are now dropped from the data ... \n",
    "# ... thus reducing the tX matrix to 19 columns (deleting 11)\n",
    "\n",
    "tX_with_missing = tX # Let's keep a copy of the old data, before cleaning it\n",
    "tX = np.delete(tX, del_features, axis=1)\n",
    "print(tX.shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying to find outliers\n",
    "#plt.scatter(tX1[:,4],tX1[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Linear regression using gradient descent: least_squares_GD (y, tx, gamma, max_iters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "\n",
    "def least_squares_GD(y,tx,gamma,max_iters):\n",
    "    \n",
    "    initial_w = np.random.randn(tx.shape[1])\n",
    "    losses, ws = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    final_w = ws[-1][:]\n",
    "    \n",
    "    return final_w, ws, losses\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    e=y-np.dot(tx,w)\n",
    "    L= ( 1/(2*len(y)) )*np.dot(e.T,e) # Least squares error - assuming the (1/2N)*(e.T*e) form\n",
    "    return L\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    e=y-np.dot(tx,w)\n",
    "    grad_L = (-1/len(y))*np.dot(tx.T,e) #Using the expression gradient of Loss = (-1/N)*(X.T*e)\n",
    "    return grad_L\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # Compute Loss and Gradient\n",
    "        L = compute_loss(y, tx, w)\n",
    "        grad_L = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma*grad_L\n",
    "        \n",
    "        loss = L\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Linear regression using stochastic gradient descent: least_squares_GD (y, tx, gamma, max_iters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \n",
    "    e=y-np.dot(tx,w)\n",
    "    grad_L = (-1/(len(y)))*(np.dot(tx.T,e))\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    \n",
    "    return grad_L\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, gamma, max_epochs):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    batch_size = 1\n",
    "    initial_w = np.random.randn(tx.shape[1])\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_epochs):\n",
    "        i=0\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            i+=1\n",
    "            if i>50000:\n",
    "                return losses, ws\n",
    "            \n",
    "            # Compute Loss and Gradient\n",
    "            L = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad_L = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        \n",
    "            # update w by gradient\n",
    "            w = w - gamma*grad_L\n",
    "            loss = L\n",
    "        \n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(np.copy(w))\n",
    "            losses.append(loss)\n",
    "    \n",
    "            print(\"Stochastic Gradient Descent({bi}): loss={l}\".format(\n",
    "              bi=i , l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent(1): loss=47827.70189854662\n",
      "Stochastic Gradient Descent(2): loss=96029.11522798453\n",
      "Stochastic Gradient Descent(3): loss=32890.94651713768\n",
      "Stochastic Gradient Descent(4): loss=15179.03299483248\n",
      "Stochastic Gradient Descent(5): loss=7342.941243544964\n",
      "Stochastic Gradient Descent(6): loss=95293.38290574131\n",
      "Stochastic Gradient Descent(7): loss=2887.765943790822\n",
      "Stochastic Gradient Descent(8): loss=3499.883237518082\n",
      "Stochastic Gradient Descent(9): loss=271.0849219754372\n",
      "Stochastic Gradient Descent(10): loss=2151.303093410485\n",
      "Stochastic Gradient Descent(11): loss=6252.060608909029\n",
      "Stochastic Gradient Descent(12): loss=10436.968299822009\n",
      "Stochastic Gradient Descent(13): loss=1321.3827851562235\n",
      "Stochastic Gradient Descent(14): loss=14029.366682362877\n",
      "Stochastic Gradient Descent(15): loss=1308.1714978033774\n",
      "Stochastic Gradient Descent(16): loss=5809.3944834963795\n",
      "Stochastic Gradient Descent(17): loss=2452.9919086223695\n",
      "Stochastic Gradient Descent(18): loss=303.08321426454\n",
      "Stochastic Gradient Descent(19): loss=3721.215540956591\n",
      "Stochastic Gradient Descent(20): loss=5927.9859346483345\n",
      "Stochastic Gradient Descent(21): loss=2104.638745778597\n",
      "Stochastic Gradient Descent(22): loss=10805.953059899037\n",
      "Stochastic Gradient Descent(23): loss=7902.620381197696\n",
      "Stochastic Gradient Descent(24): loss=15295.262941883479\n",
      "Stochastic Gradient Descent(25): loss=3239.16268155109\n",
      "Stochastic Gradient Descent(26): loss=3999.3649876711456\n",
      "Stochastic Gradient Descent(27): loss=88.57241822358445\n",
      "Stochastic Gradient Descent(28): loss=144.30413507655263\n",
      "Stochastic Gradient Descent(29): loss=3746.0136460692192\n",
      "Stochastic Gradient Descent(30): loss=7366.207247721174\n",
      "Stochastic Gradient Descent(31): loss=8646.579721868597\n",
      "Stochastic Gradient Descent(32): loss=719.4086124450653\n",
      "Stochastic Gradient Descent(33): loss=69.71387760723609\n",
      "Stochastic Gradient Descent(34): loss=1671.0673594508864\n",
      "Stochastic Gradient Descent(35): loss=7533.450630398141\n",
      "Stochastic Gradient Descent(36): loss=479.7973903850134\n",
      "Stochastic Gradient Descent(37): loss=1009.1679457470632\n",
      "Stochastic Gradient Descent(38): loss=4666.826617382333\n",
      "Stochastic Gradient Descent(39): loss=2513.73057878373\n",
      "Stochastic Gradient Descent(40): loss=18.328275547550447\n",
      "Stochastic Gradient Descent(41): loss=636.2882137818345\n",
      "Stochastic Gradient Descent(42): loss=787.9677997211655\n",
      "Stochastic Gradient Descent(43): loss=228.28782719377926\n",
      "Stochastic Gradient Descent(44): loss=665.3359564314871\n",
      "Stochastic Gradient Descent(45): loss=296.6363491056994\n",
      "Stochastic Gradient Descent(46): loss=382.3712596497951\n",
      "Stochastic Gradient Descent(47): loss=126.56386665766382\n",
      "Stochastic Gradient Descent(48): loss=501.44814195967615\n",
      "Stochastic Gradient Descent(49): loss=875.5612273965124\n",
      "Stochastic Gradient Descent(50): loss=114.2202297344831\n",
      "Stochastic Gradient Descent(51): loss=1094.5329972618113\n",
      "Stochastic Gradient Descent(52): loss=12.104502521071664\n",
      "Stochastic Gradient Descent(53): loss=11646.797312514183\n",
      "Stochastic Gradient Descent(54): loss=1019.5488139205953\n",
      "Stochastic Gradient Descent(55): loss=6396.243582013791\n",
      "Stochastic Gradient Descent(56): loss=339.6325472923482\n",
      "Stochastic Gradient Descent(57): loss=3009.685869255796\n",
      "Stochastic Gradient Descent(58): loss=2075.936780719137\n",
      "Stochastic Gradient Descent(59): loss=17.798202878197213\n",
      "Stochastic Gradient Descent(60): loss=891.700481772848\n",
      "Stochastic Gradient Descent(61): loss=23.214801774342238\n",
      "Stochastic Gradient Descent(62): loss=3.4785928608810677\n",
      "Stochastic Gradient Descent(63): loss=970.9258826050132\n",
      "Stochastic Gradient Descent(64): loss=49.73357867343519\n",
      "Stochastic Gradient Descent(65): loss=258.88872186416694\n",
      "Stochastic Gradient Descent(66): loss=22.5382522642956\n",
      "Stochastic Gradient Descent(67): loss=83.73335483180145\n",
      "Stochastic Gradient Descent(68): loss=9878.045050564213\n",
      "Stochastic Gradient Descent(69): loss=3.5748033854819248\n",
      "Stochastic Gradient Descent(70): loss=1.7715255900563345\n",
      "Stochastic Gradient Descent(71): loss=283.8633263481475\n",
      "Stochastic Gradient Descent(72): loss=356.4752969874772\n",
      "Stochastic Gradient Descent(73): loss=13085.97017962427\n",
      "Stochastic Gradient Descent(74): loss=1.3106099238336515\n",
      "Stochastic Gradient Descent(75): loss=10.533368290929923\n",
      "Stochastic Gradient Descent(76): loss=18.7711243539969\n",
      "Stochastic Gradient Descent(77): loss=0.31525916139984733\n",
      "Stochastic Gradient Descent(78): loss=31.886006645229937\n",
      "Stochastic Gradient Descent(79): loss=226.23605770119207\n",
      "Stochastic Gradient Descent(80): loss=378.62309201015694\n",
      "Stochastic Gradient Descent(81): loss=456.2119297865542\n",
      "Stochastic Gradient Descent(82): loss=752.5131403247649\n",
      "Stochastic Gradient Descent(83): loss=346.7714359625379\n",
      "Stochastic Gradient Descent(84): loss=965.0407993617533\n",
      "Stochastic Gradient Descent(85): loss=4115.620414339475\n",
      "Stochastic Gradient Descent(86): loss=1621.6296804116446\n",
      "Stochastic Gradient Descent(87): loss=2.2413504414463423\n",
      "Stochastic Gradient Descent(88): loss=616.3388787260664\n",
      "Stochastic Gradient Descent(89): loss=237.97853281681296\n",
      "Stochastic Gradient Descent(90): loss=2.9990304017312774\n",
      "Stochastic Gradient Descent(91): loss=233.09409740296113\n",
      "Stochastic Gradient Descent(92): loss=73.08367529338939\n",
      "Stochastic Gradient Descent(93): loss=4663.2468874067035\n",
      "Stochastic Gradient Descent(94): loss=85.50131779748642\n",
      "Stochastic Gradient Descent(95): loss=8.927892550571231\n",
      "Stochastic Gradient Descent(96): loss=185.66118345416845\n",
      "Stochastic Gradient Descent(97): loss=169.30488591468122\n",
      "Stochastic Gradient Descent(98): loss=1251.50799164306\n",
      "Stochastic Gradient Descent(99): loss=7074.374344629654\n",
      "Stochastic Gradient Descent(100): loss=891.5753145566251\n",
      "Stochastic Gradient Descent(101): loss=0.13758286785186646\n",
      "Stochastic Gradient Descent(102): loss=1440.8520116770314\n",
      "Stochastic Gradient Descent(103): loss=159.11489921504264\n",
      "Stochastic Gradient Descent(104): loss=21.022922944340646\n",
      "Stochastic Gradient Descent(105): loss=828.8893931260733\n",
      "Stochastic Gradient Descent(106): loss=952.6231965869486\n",
      "Stochastic Gradient Descent(107): loss=930.0642878059946\n",
      "Stochastic Gradient Descent(108): loss=6.214945224443469\n",
      "Stochastic Gradient Descent(109): loss=184.318909285197\n",
      "Stochastic Gradient Descent(110): loss=540.1617727986979\n",
      "Stochastic Gradient Descent(111): loss=3.1606187971248336\n",
      "Stochastic Gradient Descent(112): loss=270.23152872349397\n",
      "Stochastic Gradient Descent(113): loss=643.4194686939616\n",
      "Stochastic Gradient Descent(114): loss=15072.313651745886\n",
      "Stochastic Gradient Descent(115): loss=475.4692666630943\n",
      "Stochastic Gradient Descent(116): loss=237.04615361782257\n",
      "Stochastic Gradient Descent(117): loss=330.97650867186684\n",
      "Stochastic Gradient Descent(118): loss=6.649728368120439\n",
      "Stochastic Gradient Descent(119): loss=2550.203085846579\n",
      "Stochastic Gradient Descent(120): loss=153.7699922212401\n",
      "Stochastic Gradient Descent(121): loss=0.23892408940982754\n",
      "Stochastic Gradient Descent(122): loss=34.766974140973566\n",
      "Stochastic Gradient Descent(123): loss=1650.1922000081777\n",
      "Stochastic Gradient Descent(124): loss=400.8130732779849\n",
      "Stochastic Gradient Descent(125): loss=430.5111966831928\n",
      "Stochastic Gradient Descent(126): loss=767.4710025498463\n",
      "Stochastic Gradient Descent(127): loss=280.9687915729948\n",
      "Stochastic Gradient Descent(128): loss=1419.3460213009503\n",
      "Stochastic Gradient Descent(129): loss=414.5614726346104\n",
      "Stochastic Gradient Descent(130): loss=60.37314333950322\n",
      "Stochastic Gradient Descent(131): loss=345.8945276165264\n",
      "Stochastic Gradient Descent(132): loss=15.619841131464499\n",
      "Stochastic Gradient Descent(133): loss=885.795975511249\n",
      "Stochastic Gradient Descent(134): loss=4.043811281155385\n",
      "Stochastic Gradient Descent(135): loss=1266.7729838401915\n",
      "Stochastic Gradient Descent(136): loss=330.3004002410063\n",
      "Stochastic Gradient Descent(137): loss=2953.219187625741\n",
      "Stochastic Gradient Descent(138): loss=473.88731648335744\n",
      "Stochastic Gradient Descent(139): loss=1154.384208239185\n",
      "Stochastic Gradient Descent(140): loss=629.1884923405468\n",
      "Stochastic Gradient Descent(141): loss=501.3620121626658\n",
      "Stochastic Gradient Descent(142): loss=98.18809443862969\n",
      "Stochastic Gradient Descent(143): loss=1871.4038059660816\n",
      "Stochastic Gradient Descent(144): loss=157.7768081300766\n",
      "Stochastic Gradient Descent(145): loss=167.9523285239123\n",
      "Stochastic Gradient Descent(146): loss=272.88721619103757\n",
      "Stochastic Gradient Descent(147): loss=91.51665350707239\n",
      "Stochastic Gradient Descent(148): loss=0.11015601069557938\n",
      "Stochastic Gradient Descent(149): loss=389.0548253001515\n",
      "Stochastic Gradient Descent(150): loss=2.8020637228260514\n",
      "Stochastic Gradient Descent(151): loss=78.0588958960689\n",
      "Stochastic Gradient Descent(152): loss=472.0100973012921\n",
      "Stochastic Gradient Descent(153): loss=103.14677836093945\n",
      "Stochastic Gradient Descent(154): loss=36.17095982831882\n",
      "Stochastic Gradient Descent(155): loss=711.1948056061684\n",
      "Stochastic Gradient Descent(156): loss=3780.4868629619887\n",
      "Stochastic Gradient Descent(157): loss=1767.2946933004557\n",
      "Stochastic Gradient Descent(158): loss=30143.496173433126\n",
      "Stochastic Gradient Descent(159): loss=2698.6036053758426\n",
      "Stochastic Gradient Descent(160): loss=15228.28599004735\n",
      "Stochastic Gradient Descent(161): loss=123.98308836934909\n",
      "Stochastic Gradient Descent(162): loss=236.7167631559625\n",
      "Stochastic Gradient Descent(163): loss=6443.969923907227\n",
      "Stochastic Gradient Descent(164): loss=1720.1373244653882\n",
      "Stochastic Gradient Descent(165): loss=1.4538937187487437\n",
      "Stochastic Gradient Descent(166): loss=405.3784346419259\n",
      "Stochastic Gradient Descent(167): loss=232.47524413013426\n",
      "Stochastic Gradient Descent(168): loss=3131.5337806936805\n",
      "Stochastic Gradient Descent(169): loss=1036.4377483088508\n",
      "Stochastic Gradient Descent(170): loss=1024.5758954843604\n",
      "Stochastic Gradient Descent(171): loss=384.880479388766\n",
      "Stochastic Gradient Descent(172): loss=8988.067569820743\n",
      "Stochastic Gradient Descent(173): loss=1907.104035796025\n",
      "Stochastic Gradient Descent(174): loss=336.1810625557076\n",
      "Stochastic Gradient Descent(175): loss=549.6649341695972\n",
      "Stochastic Gradient Descent(176): loss=835.6219843597319\n",
      "Stochastic Gradient Descent(177): loss=3385.925672280408\n",
      "Stochastic Gradient Descent(178): loss=1118.2898280141233\n",
      "Stochastic Gradient Descent(179): loss=2206.7712773082635\n",
      "Stochastic Gradient Descent(180): loss=228.9279812397046\n",
      "Stochastic Gradient Descent(181): loss=747.727423529026\n",
      "Stochastic Gradient Descent(182): loss=2240.324072488841\n",
      "Stochastic Gradient Descent(183): loss=247.762796993024\n",
      "Stochastic Gradient Descent(184): loss=300.1684690700625\n",
      "Stochastic Gradient Descent(185): loss=3708.4462264142285\n",
      "Stochastic Gradient Descent(186): loss=62.94484820687138\n",
      "Stochastic Gradient Descent(187): loss=227.88116549348138\n",
      "Stochastic Gradient Descent(188): loss=123.97827140039473\n",
      "Stochastic Gradient Descent(189): loss=38.52851500163544\n",
      "Stochastic Gradient Descent(190): loss=510.3598781204947\n",
      "Stochastic Gradient Descent(191): loss=5.447943950582711\n",
      "Stochastic Gradient Descent(192): loss=193.2481094347673\n",
      "Stochastic Gradient Descent(193): loss=235.00124722936218\n",
      "Stochastic Gradient Descent(194): loss=1164.2797532613267\n",
      "Stochastic Gradient Descent(195): loss=1051.92219013657\n",
      "Stochastic Gradient Descent(196): loss=247.50470143372078\n",
      "Stochastic Gradient Descent(197): loss=340.3104198636676\n",
      "Stochastic Gradient Descent(198): loss=1826.696054981894\n",
      "Stochastic Gradient Descent(199): loss=143.69417073034464\n",
      "Stochastic Gradient Descent(200): loss=1176.1780887299803\n",
      "Stochastic Gradient Descent(201): loss=738.3702115332569\n",
      "Stochastic Gradient Descent(202): loss=31.930673279089426\n",
      "Stochastic Gradient Descent(203): loss=155.11320398765534\n",
      "Stochastic Gradient Descent(204): loss=75.1137565831603\n",
      "Stochastic Gradient Descent(205): loss=84.35952386656172\n",
      "Stochastic Gradient Descent(206): loss=0.00023125581437258898\n",
      "Stochastic Gradient Descent(207): loss=139.36208003922852\n",
      "Stochastic Gradient Descent(208): loss=242.48347069254808\n",
      "Stochastic Gradient Descent(209): loss=217.83472107453449\n",
      "Stochastic Gradient Descent(210): loss=117.61346732690083\n",
      "Stochastic Gradient Descent(211): loss=296.2105082775447\n",
      "Stochastic Gradient Descent(212): loss=10.327585382333698\n",
      "Stochastic Gradient Descent(213): loss=22.90029739231381\n",
      "Stochastic Gradient Descent(214): loss=800.5930446180515\n",
      "Stochastic Gradient Descent(215): loss=520.3832000692836\n",
      "Stochastic Gradient Descent(216): loss=611.951772412135\n",
      "Stochastic Gradient Descent(217): loss=5.881340570155488\n",
      "Stochastic Gradient Descent(218): loss=453.10777527537766\n",
      "Stochastic Gradient Descent(219): loss=130.2217650672992\n",
      "Stochastic Gradient Descent(220): loss=21.804469350012777\n",
      "Stochastic Gradient Descent(221): loss=1430.5738956973446\n",
      "Stochastic Gradient Descent(222): loss=188.27085248035618\n",
      "Stochastic Gradient Descent(223): loss=3152.8259201025962\n",
      "Stochastic Gradient Descent(224): loss=2.393920807806028\n",
      "Stochastic Gradient Descent(225): loss=26.69690619451084\n",
      "Stochastic Gradient Descent(226): loss=622.2148347436403\n",
      "Stochastic Gradient Descent(227): loss=130.94991428654828\n",
      "Stochastic Gradient Descent(228): loss=2868.0541493654155\n",
      "Stochastic Gradient Descent(229): loss=17.088763768449347\n",
      "Stochastic Gradient Descent(230): loss=0.12016945036884592\n",
      "Stochastic Gradient Descent(231): loss=277.9489698003402\n",
      "Stochastic Gradient Descent(232): loss=5.582751078891481\n",
      "Stochastic Gradient Descent(233): loss=418.34950294913943\n",
      "Stochastic Gradient Descent(234): loss=265.7465688840722\n",
      "Stochastic Gradient Descent(235): loss=924.7480345454193\n",
      "Stochastic Gradient Descent(236): loss=100.14122691554188\n",
      "Stochastic Gradient Descent(237): loss=424.6084819508099\n",
      "Stochastic Gradient Descent(238): loss=1396.2328062009499\n",
      "Stochastic Gradient Descent(239): loss=175.72727562749813\n",
      "Stochastic Gradient Descent(240): loss=90.8212458907838\n",
      "Stochastic Gradient Descent(241): loss=1.8844106744185551\n",
      "Stochastic Gradient Descent(242): loss=867.8504424675444\n",
      "Stochastic Gradient Descent(243): loss=1017.1019053951823\n",
      "Stochastic Gradient Descent(244): loss=1867.4613532723922\n",
      "Stochastic Gradient Descent(245): loss=520.7758682742399\n",
      "Stochastic Gradient Descent(246): loss=349.84569165315116\n",
      "Stochastic Gradient Descent(247): loss=150.05720812689364\n",
      "Stochastic Gradient Descent(248): loss=3937.3923240789272\n",
      "Stochastic Gradient Descent(249): loss=6.553905451923166\n",
      "Stochastic Gradient Descent(250): loss=2733.635253779271\n",
      "Stochastic Gradient Descent(251): loss=481.71012091695377\n",
      "Stochastic Gradient Descent(252): loss=38.2647811633737\n",
      "Stochastic Gradient Descent(253): loss=51.861827904115245\n",
      "Stochastic Gradient Descent(254): loss=377.15683410229303\n",
      "Stochastic Gradient Descent(255): loss=90.11604460500945\n",
      "Stochastic Gradient Descent(256): loss=126.14872969928841\n",
      "Stochastic Gradient Descent(257): loss=51.24102135754645\n",
      "Stochastic Gradient Descent(258): loss=104.10336200116932\n",
      "Stochastic Gradient Descent(259): loss=125.34324082064329\n",
      "Stochastic Gradient Descent(260): loss=140.26223833021538\n",
      "Stochastic Gradient Descent(261): loss=1769.0231415958033\n",
      "Stochastic Gradient Descent(262): loss=939.570954107697\n",
      "Stochastic Gradient Descent(263): loss=275.8799197091911\n",
      "Stochastic Gradient Descent(264): loss=424.84431593634355\n",
      "Stochastic Gradient Descent(265): loss=2392.2522641890646\n",
      "Stochastic Gradient Descent(266): loss=157.65328211988134\n",
      "Stochastic Gradient Descent(267): loss=11.291077642863627\n",
      "Stochastic Gradient Descent(268): loss=130.5610214197725\n",
      "Stochastic Gradient Descent(269): loss=3249.297834663455\n",
      "Stochastic Gradient Descent(270): loss=347.7459369449893\n",
      "Stochastic Gradient Descent(271): loss=4049.624752220112\n",
      "Stochastic Gradient Descent(272): loss=122.48802552314523\n",
      "Stochastic Gradient Descent(273): loss=893.1970186117592\n",
      "Stochastic Gradient Descent(274): loss=650.5788659791658\n",
      "Stochastic Gradient Descent(275): loss=487.86148607096226\n",
      "Stochastic Gradient Descent(276): loss=85.37754865832989\n",
      "Stochastic Gradient Descent(277): loss=377.76742532952386\n",
      "Stochastic Gradient Descent(278): loss=16212.722456730897\n",
      "Stochastic Gradient Descent(279): loss=35161.93906321199\n",
      "Stochastic Gradient Descent(280): loss=190.7942553803944\n",
      "Stochastic Gradient Descent(281): loss=8636.903247049859\n",
      "Stochastic Gradient Descent(282): loss=63.25787566481086\n",
      "Stochastic Gradient Descent(283): loss=293.2594845775253\n",
      "Stochastic Gradient Descent(284): loss=908.5792946936149\n",
      "Stochastic Gradient Descent(285): loss=423.52716252140715\n",
      "Stochastic Gradient Descent(286): loss=399.30467554611414\n",
      "Stochastic Gradient Descent(287): loss=35.73827576113876\n",
      "Stochastic Gradient Descent(288): loss=0.9978544171971845\n",
      "Stochastic Gradient Descent(289): loss=11.403568286601029\n",
      "Stochastic Gradient Descent(290): loss=814.7285025408653\n",
      "Stochastic Gradient Descent(291): loss=278.2623280212589\n",
      "Stochastic Gradient Descent(292): loss=15.411392059826195\n",
      "Stochastic Gradient Descent(293): loss=94.18482188461819\n",
      "Stochastic Gradient Descent(294): loss=460.00771797499056\n",
      "Stochastic Gradient Descent(295): loss=0.17588464752209879\n",
      "Stochastic Gradient Descent(296): loss=524.7377937791866\n",
      "Stochastic Gradient Descent(297): loss=83.78721021811987\n",
      "Stochastic Gradient Descent(298): loss=53.232414117419715\n",
      "Stochastic Gradient Descent(299): loss=398.00807240617013\n",
      "Stochastic Gradient Descent(300): loss=4273.378647395693\n",
      "Stochastic Gradient Descent(301): loss=579.2016802130386\n",
      "Stochastic Gradient Descent(302): loss=66.32008300677506\n",
      "Stochastic Gradient Descent(303): loss=154.27725956171307\n",
      "Stochastic Gradient Descent(304): loss=306.1592122575127\n",
      "Stochastic Gradient Descent(305): loss=6889.040425016741\n",
      "Stochastic Gradient Descent(306): loss=133.4885928375491\n",
      "Stochastic Gradient Descent(307): loss=16250.014554013624\n",
      "Stochastic Gradient Descent(308): loss=3491.526097516938\n",
      "Stochastic Gradient Descent(309): loss=2480.0154712800977\n",
      "Stochastic Gradient Descent(310): loss=1.8997806860437432\n",
      "Stochastic Gradient Descent(311): loss=16.042690702199806\n",
      "Stochastic Gradient Descent(312): loss=20.643882900973686\n",
      "Stochastic Gradient Descent(313): loss=102.26446414306899\n",
      "Stochastic Gradient Descent(314): loss=145.27559630730863\n",
      "Stochastic Gradient Descent(315): loss=695.4597441632117\n",
      "Stochastic Gradient Descent(316): loss=743.5190421763207\n",
      "Stochastic Gradient Descent(317): loss=147.22286714905655\n",
      "Stochastic Gradient Descent(318): loss=331.67069140153296\n",
      "Stochastic Gradient Descent(319): loss=43.75058292355191\n",
      "Stochastic Gradient Descent(320): loss=0.7401400502142254\n",
      "Stochastic Gradient Descent(321): loss=1725.7274103855166\n",
      "Stochastic Gradient Descent(322): loss=132.68134896022943\n",
      "Stochastic Gradient Descent(323): loss=2503.0783038330574\n",
      "Stochastic Gradient Descent(324): loss=359.5905081439497\n",
      "Stochastic Gradient Descent(325): loss=62.0870308591204\n",
      "Stochastic Gradient Descent(326): loss=248.92737739717091\n",
      "Stochastic Gradient Descent(327): loss=134.23488618434718\n",
      "Stochastic Gradient Descent(328): loss=3063.710792834627\n",
      "Stochastic Gradient Descent(329): loss=991.7217605882054\n",
      "Stochastic Gradient Descent(330): loss=546.9951828667124\n",
      "Stochastic Gradient Descent(331): loss=326.4331931516032\n",
      "Stochastic Gradient Descent(332): loss=134.8521057760966\n",
      "Stochastic Gradient Descent(333): loss=12.309195601547218\n",
      "Stochastic Gradient Descent(334): loss=125.35043387453214\n",
      "Stochastic Gradient Descent(335): loss=1377.594464900642\n",
      "Stochastic Gradient Descent(336): loss=174.14987965819788\n",
      "Stochastic Gradient Descent(337): loss=230.62579601872633\n",
      "Stochastic Gradient Descent(338): loss=293.46649490133365\n",
      "Stochastic Gradient Descent(339): loss=171.42177998118206\n",
      "Stochastic Gradient Descent(340): loss=316.8808700339909\n",
      "Stochastic Gradient Descent(341): loss=412.1718299273444\n",
      "Stochastic Gradient Descent(342): loss=88.38473088791946\n",
      "Stochastic Gradient Descent(343): loss=41.981583077812815\n",
      "Stochastic Gradient Descent(344): loss=1504.3972209892997\n",
      "Stochastic Gradient Descent(345): loss=11442.104368926019\n",
      "Stochastic Gradient Descent(346): loss=2432.054848921139\n",
      "Stochastic Gradient Descent(347): loss=107.67619779157323\n",
      "Stochastic Gradient Descent(348): loss=7903.825945067455\n",
      "Stochastic Gradient Descent(349): loss=66.93026017123601\n",
      "Stochastic Gradient Descent(350): loss=8.01641195508049\n",
      "Stochastic Gradient Descent(351): loss=0.11121602692274828\n",
      "Stochastic Gradient Descent(352): loss=879.2077972504018\n",
      "Stochastic Gradient Descent(353): loss=389.3848563263402\n",
      "Stochastic Gradient Descent(354): loss=589.7855723888773\n",
      "Stochastic Gradient Descent(355): loss=231.86496904238575\n",
      "Stochastic Gradient Descent(356): loss=1870.7900076473356\n",
      "Stochastic Gradient Descent(357): loss=6.882853547899402\n",
      "Stochastic Gradient Descent(358): loss=14.040955861939176\n",
      "Stochastic Gradient Descent(359): loss=556.9681636638658\n",
      "Stochastic Gradient Descent(360): loss=42.01200424693383\n",
      "Stochastic Gradient Descent(361): loss=174.0367495695565\n",
      "Stochastic Gradient Descent(362): loss=596.3866086064611\n",
      "Stochastic Gradient Descent(363): loss=13.570040586706059\n",
      "Stochastic Gradient Descent(364): loss=266.8901916196432\n",
      "Stochastic Gradient Descent(365): loss=27.592372268841764\n",
      "Stochastic Gradient Descent(366): loss=1082.152882512865\n",
      "Stochastic Gradient Descent(367): loss=66.87883605133926\n",
      "Stochastic Gradient Descent(368): loss=722.2722072806895\n",
      "Stochastic Gradient Descent(369): loss=483.2033253659986\n",
      "Stochastic Gradient Descent(370): loss=82.38628686760903\n",
      "Stochastic Gradient Descent(371): loss=38.011990554036956\n",
      "Stochastic Gradient Descent(372): loss=30.45601146209473\n",
      "Stochastic Gradient Descent(373): loss=364.4237512700537\n",
      "Stochastic Gradient Descent(374): loss=3223.507227939329\n",
      "Stochastic Gradient Descent(375): loss=7104.959733836627\n",
      "Stochastic Gradient Descent(376): loss=10611.716329376837\n",
      "Stochastic Gradient Descent(377): loss=146.11640572519008\n",
      "Stochastic Gradient Descent(378): loss=348.00222440744255\n",
      "Stochastic Gradient Descent(379): loss=56.89662899494516\n",
      "Stochastic Gradient Descent(380): loss=2038.3766093064885\n",
      "Stochastic Gradient Descent(381): loss=287.07914453857336\n",
      "Stochastic Gradient Descent(382): loss=3723.3047894828283\n",
      "Stochastic Gradient Descent(383): loss=344.78319193293237\n",
      "Stochastic Gradient Descent(384): loss=622.3846133305394\n",
      "Stochastic Gradient Descent(385): loss=286.12895309254026\n",
      "Stochastic Gradient Descent(386): loss=470.8885685065908\n",
      "Stochastic Gradient Descent(387): loss=55.58126321675124\n",
      "Stochastic Gradient Descent(388): loss=3798.6279104066584\n",
      "Stochastic Gradient Descent(389): loss=1581.8794510906082\n",
      "Stochastic Gradient Descent(390): loss=196.97722494142113\n",
      "Stochastic Gradient Descent(391): loss=545.1781134647424\n",
      "Stochastic Gradient Descent(392): loss=1077.6082433403471\n",
      "Stochastic Gradient Descent(393): loss=719.1804344434532\n",
      "Stochastic Gradient Descent(394): loss=112.77629134584868\n",
      "Stochastic Gradient Descent(395): loss=966.445419037432\n",
      "Stochastic Gradient Descent(396): loss=192.75559322167493\n",
      "Stochastic Gradient Descent(397): loss=1433.175886121597\n",
      "Stochastic Gradient Descent(398): loss=132.87546729872014\n",
      "Stochastic Gradient Descent(399): loss=525.2778866540923\n",
      "Stochastic Gradient Descent(400): loss=406.26414505371105\n",
      "Stochastic Gradient Descent(401): loss=18.127689513469587\n",
      "Stochastic Gradient Descent(402): loss=84.23875348682586\n",
      "Stochastic Gradient Descent(403): loss=151.0496255588904\n",
      "Stochastic Gradient Descent(404): loss=5.1267617235110725\n",
      "Stochastic Gradient Descent(405): loss=1699.4906572695327\n",
      "Stochastic Gradient Descent(406): loss=843.7737219629468\n",
      "Stochastic Gradient Descent(407): loss=790.5229632669937\n",
      "Stochastic Gradient Descent(408): loss=1098.506805542778\n",
      "Stochastic Gradient Descent(409): loss=927.1757342045281\n",
      "Stochastic Gradient Descent(410): loss=193.25339519813275\n",
      "Stochastic Gradient Descent(411): loss=923.1671869324935\n",
      "Stochastic Gradient Descent(412): loss=160.32251565845723\n",
      "Stochastic Gradient Descent(413): loss=3465.75853752544\n",
      "Stochastic Gradient Descent(414): loss=109.36219290305884\n",
      "Stochastic Gradient Descent(415): loss=274.01498257039424\n",
      "Stochastic Gradient Descent(416): loss=1728.1769546081157\n",
      "Stochastic Gradient Descent(417): loss=976.0603563738338\n",
      "Stochastic Gradient Descent(418): loss=109.9282540867098\n",
      "Stochastic Gradient Descent(419): loss=7.7526934875335405\n",
      "Stochastic Gradient Descent(420): loss=290.2542591471713\n",
      "Stochastic Gradient Descent(421): loss=0.8295274877673493\n",
      "Stochastic Gradient Descent(422): loss=318.5063395205501\n",
      "Stochastic Gradient Descent(423): loss=308.8596410485306\n",
      "Stochastic Gradient Descent(424): loss=620.4449064785614\n",
      "Stochastic Gradient Descent(425): loss=204.77048127168172\n",
      "Stochastic Gradient Descent(426): loss=1035.719793293271\n",
      "Stochastic Gradient Descent(427): loss=4160.535016670551\n",
      "Stochastic Gradient Descent(428): loss=40.7977150070435\n",
      "Stochastic Gradient Descent(429): loss=0.014015760643716842\n",
      "Stochastic Gradient Descent(430): loss=506.4095989568273\n",
      "Stochastic Gradient Descent(431): loss=2.030218163285769\n",
      "Stochastic Gradient Descent(432): loss=0.0024319913577913405\n",
      "Stochastic Gradient Descent(433): loss=65.07168038363126\n",
      "Stochastic Gradient Descent(434): loss=2564.3835310832033\n",
      "Stochastic Gradient Descent(435): loss=30.47438915664213\n",
      "Stochastic Gradient Descent(436): loss=368.5929104416619\n",
      "Stochastic Gradient Descent(437): loss=4.210955246969577\n",
      "Stochastic Gradient Descent(438): loss=17.93601735852166\n",
      "Stochastic Gradient Descent(439): loss=1022.2148181220837\n",
      "Stochastic Gradient Descent(440): loss=4264.247212869502\n",
      "Stochastic Gradient Descent(441): loss=100.558265693633\n",
      "Stochastic Gradient Descent(442): loss=299.80339919927235\n",
      "Stochastic Gradient Descent(443): loss=1230.7963625084994\n",
      "Stochastic Gradient Descent(444): loss=18.80497122706831\n",
      "Stochastic Gradient Descent(445): loss=952.7689436515897\n",
      "Stochastic Gradient Descent(446): loss=2054.77084777844\n",
      "Stochastic Gradient Descent(447): loss=6937.822403041963\n",
      "Stochastic Gradient Descent(448): loss=0.549375082966413\n",
      "Stochastic Gradient Descent(449): loss=4.785080413669356\n",
      "Stochastic Gradient Descent(450): loss=9.582041050227982\n",
      "Stochastic Gradient Descent(451): loss=224.44354204543538\n",
      "Stochastic Gradient Descent(452): loss=4218.569894190403\n",
      "Stochastic Gradient Descent(453): loss=124.53856670001737\n",
      "Stochastic Gradient Descent(454): loss=288.2168902747114\n",
      "Stochastic Gradient Descent(455): loss=189.22094727474044\n",
      "Stochastic Gradient Descent(456): loss=120.28625898098252\n",
      "Stochastic Gradient Descent(457): loss=21.75421027211254\n",
      "Stochastic Gradient Descent(458): loss=69.10582581189207\n",
      "Stochastic Gradient Descent(459): loss=0.0014438371797251483\n",
      "Stochastic Gradient Descent(460): loss=60.68831981877095\n",
      "Stochastic Gradient Descent(461): loss=220.04255113628207\n",
      "Stochastic Gradient Descent(462): loss=491.29830426411877\n",
      "Stochastic Gradient Descent(463): loss=68.83749606941376\n",
      "Stochastic Gradient Descent(464): loss=4307.5511028725205\n",
      "Stochastic Gradient Descent(465): loss=245.3800048171342\n",
      "Stochastic Gradient Descent(466): loss=10.386630855228837\n",
      "Stochastic Gradient Descent(467): loss=1993.2592517591463\n",
      "Stochastic Gradient Descent(468): loss=747.2999756921744\n",
      "Stochastic Gradient Descent(469): loss=877.4135414289088\n",
      "Stochastic Gradient Descent(470): loss=124.25621594484934\n",
      "Stochastic Gradient Descent(471): loss=459.1188412105013\n",
      "Stochastic Gradient Descent(472): loss=2652.474668437346\n",
      "Stochastic Gradient Descent(473): loss=2691.034382859235\n",
      "Stochastic Gradient Descent(474): loss=1256.0380063289451\n",
      "Stochastic Gradient Descent(475): loss=695.3067883456378\n",
      "Stochastic Gradient Descent(476): loss=10056.261453467621\n",
      "Stochastic Gradient Descent(477): loss=180.36066076564953\n",
      "Stochastic Gradient Descent(478): loss=397.24478373977377\n",
      "Stochastic Gradient Descent(479): loss=579.9491902620996\n",
      "Stochastic Gradient Descent(480): loss=23.559615506055437\n",
      "Stochastic Gradient Descent(481): loss=55.87677164773563\n",
      "Stochastic Gradient Descent(482): loss=47.200203885760324\n",
      "Stochastic Gradient Descent(483): loss=604.8888214705044\n",
      "Stochastic Gradient Descent(484): loss=6.6698842983123745\n",
      "Stochastic Gradient Descent(485): loss=650.0621317799936\n",
      "Stochastic Gradient Descent(486): loss=94.98972256138937\n",
      "Stochastic Gradient Descent(487): loss=18.268123228974705\n",
      "Stochastic Gradient Descent(488): loss=38.449450940959245\n",
      "Stochastic Gradient Descent(489): loss=122.00431483102902\n",
      "Stochastic Gradient Descent(490): loss=85.40367476278253\n",
      "Stochastic Gradient Descent(491): loss=4416.211867278878\n",
      "Stochastic Gradient Descent(492): loss=517.6272237433748\n",
      "Stochastic Gradient Descent(493): loss=39.84268736816188\n",
      "Stochastic Gradient Descent(494): loss=503.5130569039194\n",
      "Stochastic Gradient Descent(495): loss=2108.5977641098543\n",
      "Stochastic Gradient Descent(496): loss=539.0230007425494\n",
      "Stochastic Gradient Descent(497): loss=929.1181242203976\n",
      "Stochastic Gradient Descent(498): loss=901.84385855128\n",
      "Stochastic Gradient Descent(499): loss=271.97901594365703\n",
      "Stochastic Gradient Descent(500): loss=446.8135139346636\n",
      "Stochastic Gradient Descent(501): loss=82.74807524497642\n",
      "Stochastic Gradient Descent(502): loss=1906.966910035874\n",
      "Stochastic Gradient Descent(503): loss=277.6573979160292\n",
      "Stochastic Gradient Descent(504): loss=92.37583664579286\n",
      "Stochastic Gradient Descent(505): loss=713.4579368161242\n",
      "Stochastic Gradient Descent(506): loss=52.536181428944445\n",
      "Stochastic Gradient Descent(507): loss=210.99366768979075\n",
      "Stochastic Gradient Descent(508): loss=406.0548334453266\n",
      "Stochastic Gradient Descent(509): loss=945.5696879927527\n",
      "Stochastic Gradient Descent(510): loss=183.91739257703898\n",
      "Stochastic Gradient Descent(511): loss=62.54940089160861\n",
      "Stochastic Gradient Descent(512): loss=549.504423707308\n",
      "Stochastic Gradient Descent(513): loss=212.07664877542345\n",
      "Stochastic Gradient Descent(514): loss=11.626347591723517\n",
      "Stochastic Gradient Descent(515): loss=416.7864769412623\n",
      "Stochastic Gradient Descent(516): loss=15.589618138124914\n",
      "Stochastic Gradient Descent(517): loss=168.88031499215285\n",
      "Stochastic Gradient Descent(518): loss=1258.631131145389\n",
      "Stochastic Gradient Descent(519): loss=297.7919891770013\n",
      "Stochastic Gradient Descent(520): loss=274.79627734822105\n",
      "Stochastic Gradient Descent(521): loss=203.05243518083427\n",
      "Stochastic Gradient Descent(522): loss=407.5380213198588\n",
      "Stochastic Gradient Descent(523): loss=1462.0859576511477\n",
      "Stochastic Gradient Descent(524): loss=450.6505352970383\n",
      "Stochastic Gradient Descent(525): loss=3067.445778345275\n",
      "Stochastic Gradient Descent(526): loss=836.696649884908\n",
      "Stochastic Gradient Descent(527): loss=1419.3226855240111\n",
      "Stochastic Gradient Descent(528): loss=21.393206339124532\n",
      "Stochastic Gradient Descent(529): loss=67.40747392462285\n",
      "Stochastic Gradient Descent(530): loss=2.14931034103041\n",
      "Stochastic Gradient Descent(531): loss=2.2249137723280707\n",
      "Stochastic Gradient Descent(532): loss=167.1829086959762\n",
      "Stochastic Gradient Descent(533): loss=1374.315600905954\n",
      "Stochastic Gradient Descent(534): loss=40.924202604912615\n",
      "Stochastic Gradient Descent(535): loss=502.1667298516182\n",
      "Stochastic Gradient Descent(536): loss=439.44318004759646\n",
      "Stochastic Gradient Descent(537): loss=12.494177915372987\n",
      "Stochastic Gradient Descent(538): loss=40.65825826918566\n",
      "Stochastic Gradient Descent(539): loss=282.5313408226403\n",
      "Stochastic Gradient Descent(540): loss=870.4501914616742\n",
      "Stochastic Gradient Descent(541): loss=0.7394483832002213\n",
      "Stochastic Gradient Descent(542): loss=464.63952782270604\n",
      "Stochastic Gradient Descent(543): loss=9.611041156948323\n",
      "Stochastic Gradient Descent(544): loss=224.24053845090194\n",
      "Stochastic Gradient Descent(545): loss=13.099808079183699\n",
      "Stochastic Gradient Descent(546): loss=794.2757272253556\n",
      "Stochastic Gradient Descent(547): loss=32.79979110732105\n",
      "Stochastic Gradient Descent(548): loss=1977.8804290764158\n",
      "Stochastic Gradient Descent(549): loss=0.5302436167167477\n",
      "Stochastic Gradient Descent(550): loss=139.8597719555557\n",
      "Stochastic Gradient Descent(551): loss=33.5809256601977\n",
      "Stochastic Gradient Descent(552): loss=1759.4520557302076\n",
      "Stochastic Gradient Descent(553): loss=1971.9174496779108\n",
      "Stochastic Gradient Descent(554): loss=19.262379204474872\n",
      "Stochastic Gradient Descent(555): loss=207.37990199804173\n",
      "Stochastic Gradient Descent(556): loss=1272.1751515544713\n",
      "Stochastic Gradient Descent(557): loss=367.30741150091967\n",
      "Stochastic Gradient Descent(558): loss=187.13544799198124\n",
      "Stochastic Gradient Descent(559): loss=127.55521453643372\n",
      "Stochastic Gradient Descent(560): loss=27.631573928819208\n",
      "Stochastic Gradient Descent(561): loss=101.85872641533028\n",
      "Stochastic Gradient Descent(562): loss=374.20594237462734\n",
      "Stochastic Gradient Descent(563): loss=0.012720338764758483\n",
      "Stochastic Gradient Descent(564): loss=20.33847374975815\n",
      "Stochastic Gradient Descent(565): loss=8059.751282269192\n",
      "Stochastic Gradient Descent(566): loss=32.70581661237026\n",
      "Stochastic Gradient Descent(567): loss=224.27369469907524\n",
      "Stochastic Gradient Descent(568): loss=146.25259617459201\n",
      "Stochastic Gradient Descent(569): loss=883.6809580315806\n",
      "Stochastic Gradient Descent(570): loss=81.79655737689106\n",
      "Stochastic Gradient Descent(571): loss=877.3717610674381\n",
      "Stochastic Gradient Descent(572): loss=431.330584503597\n",
      "Stochastic Gradient Descent(573): loss=5156.546931160178\n",
      "Stochastic Gradient Descent(574): loss=59.26718745682741\n",
      "Stochastic Gradient Descent(575): loss=3769.516774853569\n",
      "Stochastic Gradient Descent(576): loss=384.33291508490026\n",
      "Stochastic Gradient Descent(577): loss=440.1136094020333\n",
      "Stochastic Gradient Descent(578): loss=1078.2580711803403\n",
      "Stochastic Gradient Descent(579): loss=494.0263270968416\n",
      "Stochastic Gradient Descent(580): loss=0.051156903329532134\n",
      "Stochastic Gradient Descent(581): loss=4.372088883927425\n",
      "Stochastic Gradient Descent(582): loss=4.728800827371809\n",
      "Stochastic Gradient Descent(583): loss=62.12082673191\n",
      "Stochastic Gradient Descent(584): loss=1058.1081918637294\n",
      "Stochastic Gradient Descent(585): loss=247.68235598368224\n",
      "Stochastic Gradient Descent(586): loss=122.85586463511953\n",
      "Stochastic Gradient Descent(587): loss=34.063838804476475\n",
      "Stochastic Gradient Descent(588): loss=281.55962080584305\n",
      "Stochastic Gradient Descent(589): loss=240.54984465512192\n",
      "Stochastic Gradient Descent(590): loss=1480.0023017660485\n",
      "Stochastic Gradient Descent(591): loss=38.19539565044731\n",
      "Stochastic Gradient Descent(592): loss=25.81245676912913\n",
      "Stochastic Gradient Descent(593): loss=137.4456110492949\n",
      "Stochastic Gradient Descent(594): loss=210.52200839746328\n",
      "Stochastic Gradient Descent(595): loss=103.7597665262833\n",
      "Stochastic Gradient Descent(596): loss=915.720630982386\n",
      "Stochastic Gradient Descent(597): loss=4563.692010768753\n",
      "Stochastic Gradient Descent(598): loss=31.06109815789242\n",
      "Stochastic Gradient Descent(599): loss=726.9600700131268\n",
      "Stochastic Gradient Descent(600): loss=178.55935193327628\n",
      "Stochastic Gradient Descent(601): loss=408.9024263944247\n",
      "Stochastic Gradient Descent(602): loss=433.9233731990264\n",
      "Stochastic Gradient Descent(603): loss=240.05495458693764\n",
      "Stochastic Gradient Descent(604): loss=560.2269157692391\n",
      "Stochastic Gradient Descent(605): loss=569.4167032934508\n",
      "Stochastic Gradient Descent(606): loss=1289.8902299574966\n",
      "Stochastic Gradient Descent(607): loss=2068.339475172327\n",
      "Stochastic Gradient Descent(608): loss=1418.0331888636788\n",
      "Stochastic Gradient Descent(609): loss=104.17143224195343\n",
      "Stochastic Gradient Descent(610): loss=2.1760813025414594\n",
      "Stochastic Gradient Descent(611): loss=179.7337352555995\n",
      "Stochastic Gradient Descent(612): loss=268.00997565652193\n",
      "Stochastic Gradient Descent(613): loss=4441.175943469686\n",
      "Stochastic Gradient Descent(614): loss=289.1883007827023\n",
      "Stochastic Gradient Descent(615): loss=1549.6528441904231\n",
      "Stochastic Gradient Descent(616): loss=36.12728173430945\n",
      "Stochastic Gradient Descent(617): loss=603.3621796670232\n",
      "Stochastic Gradient Descent(618): loss=2.1512844004566496\n",
      "Stochastic Gradient Descent(619): loss=131.10476677393953\n",
      "Stochastic Gradient Descent(620): loss=417.80163848684214\n",
      "Stochastic Gradient Descent(621): loss=547.4061265174637\n",
      "Stochastic Gradient Descent(622): loss=164.0833100550977\n",
      "Stochastic Gradient Descent(623): loss=342.4401386098866\n",
      "Stochastic Gradient Descent(624): loss=129.74051469021913\n",
      "Stochastic Gradient Descent(625): loss=0.47408708630410257\n",
      "Stochastic Gradient Descent(626): loss=2709.390101598677\n",
      "Stochastic Gradient Descent(627): loss=0.3244621261506931\n",
      "Stochastic Gradient Descent(628): loss=70.04805906694817\n",
      "Stochastic Gradient Descent(629): loss=1914.3754486129021\n",
      "Stochastic Gradient Descent(630): loss=516.1604258495262\n",
      "Stochastic Gradient Descent(631): loss=530.4963489972919\n",
      "Stochastic Gradient Descent(632): loss=479.6576599111042\n",
      "Stochastic Gradient Descent(633): loss=261.2364503884224\n",
      "Stochastic Gradient Descent(634): loss=0.7387380974951284\n",
      "Stochastic Gradient Descent(635): loss=5.3834758513636665\n",
      "Stochastic Gradient Descent(636): loss=191.14226308904048\n",
      "Stochastic Gradient Descent(637): loss=31.755607123970837\n",
      "Stochastic Gradient Descent(638): loss=214.553308116977\n",
      "Stochastic Gradient Descent(639): loss=184.07884785114507\n",
      "Stochastic Gradient Descent(640): loss=218.07138820325133\n",
      "Stochastic Gradient Descent(641): loss=24.61818320483774\n",
      "Stochastic Gradient Descent(642): loss=170.23819124213108\n",
      "Stochastic Gradient Descent(643): loss=341.5865083785362\n",
      "Stochastic Gradient Descent(644): loss=1185.3748774985377\n",
      "Stochastic Gradient Descent(645): loss=958.8996324405052\n",
      "Stochastic Gradient Descent(646): loss=212.49362160455698\n",
      "Stochastic Gradient Descent(647): loss=368.01145865817585\n",
      "Stochastic Gradient Descent(648): loss=2322.3827962493206\n",
      "Stochastic Gradient Descent(649): loss=566.8827410277639\n",
      "Stochastic Gradient Descent(650): loss=687.7250459239989\n",
      "Stochastic Gradient Descent(651): loss=12.616485712194216\n",
      "Stochastic Gradient Descent(652): loss=13.046592667609696\n",
      "Stochastic Gradient Descent(653): loss=182.5845285597715\n",
      "Stochastic Gradient Descent(654): loss=753.6862386917446\n",
      "Stochastic Gradient Descent(655): loss=225.2013786938645\n",
      "Stochastic Gradient Descent(656): loss=4.089840321536954\n",
      "Stochastic Gradient Descent(657): loss=980.9751683821669\n",
      "Stochastic Gradient Descent(658): loss=6253.647762785419\n",
      "Stochastic Gradient Descent(659): loss=3930.316328673485\n",
      "Stochastic Gradient Descent(660): loss=542.936347871885\n",
      "Stochastic Gradient Descent(661): loss=84.81607732714465\n",
      "Stochastic Gradient Descent(662): loss=44.785069922059606\n",
      "Stochastic Gradient Descent(663): loss=5686.208207215251\n",
      "Stochastic Gradient Descent(664): loss=11.878101998807603\n",
      "Stochastic Gradient Descent(665): loss=192.2539258069283\n",
      "Stochastic Gradient Descent(666): loss=23.44834211796844\n",
      "Stochastic Gradient Descent(667): loss=130.80164686648206\n",
      "Stochastic Gradient Descent(668): loss=87.61208369194996\n",
      "Stochastic Gradient Descent(669): loss=356.2364500740712\n",
      "Stochastic Gradient Descent(670): loss=250.73342159824523\n",
      "Stochastic Gradient Descent(671): loss=23.612942154988396\n",
      "Stochastic Gradient Descent(672): loss=6875.034671639491\n",
      "Stochastic Gradient Descent(673): loss=38828.5737247255\n",
      "Stochastic Gradient Descent(674): loss=13.978438618465525\n",
      "Stochastic Gradient Descent(675): loss=595.2419084288808\n",
      "Stochastic Gradient Descent(676): loss=204.42509843590173\n",
      "Stochastic Gradient Descent(677): loss=27.759464225162073\n",
      "Stochastic Gradient Descent(678): loss=415.9464479542969\n",
      "Stochastic Gradient Descent(679): loss=9.764933456934958\n",
      "Stochastic Gradient Descent(680): loss=210.83965893150895\n",
      "Stochastic Gradient Descent(681): loss=281.3424827754948\n",
      "Stochastic Gradient Descent(682): loss=274.567825164424\n",
      "Stochastic Gradient Descent(683): loss=4327.845447306372\n",
      "Stochastic Gradient Descent(684): loss=198.8104160592916\n",
      "Stochastic Gradient Descent(685): loss=397.23211723136234\n",
      "Stochastic Gradient Descent(686): loss=2749.454176392565\n",
      "Stochastic Gradient Descent(687): loss=108.01610745113794\n",
      "Stochastic Gradient Descent(688): loss=284.33176140888986\n",
      "Stochastic Gradient Descent(689): loss=431.18739576102706\n",
      "Stochastic Gradient Descent(690): loss=333.72603071160205\n",
      "Stochastic Gradient Descent(691): loss=92.0634177492921\n",
      "Stochastic Gradient Descent(692): loss=435.7525950017952\n",
      "Stochastic Gradient Descent(693): loss=1181.4150943701743\n",
      "Stochastic Gradient Descent(694): loss=0.08358190819072853\n",
      "Stochastic Gradient Descent(695): loss=333.3645226557095\n",
      "Stochastic Gradient Descent(696): loss=250.22183200584263\n",
      "Stochastic Gradient Descent(697): loss=133.17166740780445\n",
      "Stochastic Gradient Descent(698): loss=310.851631313567\n",
      "Stochastic Gradient Descent(699): loss=162.04802761482327\n",
      "Stochastic Gradient Descent(700): loss=61.98414275923754\n",
      "Stochastic Gradient Descent(701): loss=505.4713609096555\n",
      "Stochastic Gradient Descent(702): loss=0.03439493902102448\n",
      "Stochastic Gradient Descent(703): loss=231.342358216564\n",
      "Stochastic Gradient Descent(704): loss=246.97598006271647\n",
      "Stochastic Gradient Descent(705): loss=3.2296298422715553\n",
      "Stochastic Gradient Descent(706): loss=3182.9919207746702\n",
      "Stochastic Gradient Descent(707): loss=1012.0346421140911\n",
      "Stochastic Gradient Descent(708): loss=30.969201128361654\n",
      "Stochastic Gradient Descent(709): loss=1135.7997553559983\n",
      "Stochastic Gradient Descent(710): loss=80.37012926415635\n",
      "Stochastic Gradient Descent(711): loss=355.4462029546888\n",
      "Stochastic Gradient Descent(712): loss=662.9087480983419\n",
      "Stochastic Gradient Descent(713): loss=807.3032296756919\n",
      "Stochastic Gradient Descent(714): loss=14.74486870306347\n",
      "Stochastic Gradient Descent(715): loss=389.10690518342614\n",
      "Stochastic Gradient Descent(716): loss=59.91975162139201\n",
      "Stochastic Gradient Descent(717): loss=765.978041355303\n",
      "Stochastic Gradient Descent(718): loss=167.44412495425777\n",
      "Stochastic Gradient Descent(719): loss=138.27355912014062\n",
      "Stochastic Gradient Descent(720): loss=1188.0609482110235\n",
      "Stochastic Gradient Descent(721): loss=401.02621710952803\n",
      "Stochastic Gradient Descent(722): loss=1119.4337095006015\n",
      "Stochastic Gradient Descent(723): loss=434.24666872399723\n",
      "Stochastic Gradient Descent(724): loss=906.3355091057173\n",
      "Stochastic Gradient Descent(725): loss=69.40560028108304\n",
      "Stochastic Gradient Descent(726): loss=153.9221751040089\n",
      "Stochastic Gradient Descent(727): loss=1105.6063565265686\n",
      "Stochastic Gradient Descent(728): loss=786.8682300517727\n",
      "Stochastic Gradient Descent(729): loss=420.6340634019218\n",
      "Stochastic Gradient Descent(730): loss=221.94666621976702\n",
      "Stochastic Gradient Descent(731): loss=78.17237385513951\n",
      "Stochastic Gradient Descent(732): loss=8.182407113293866\n",
      "Stochastic Gradient Descent(733): loss=452.0535486307362\n",
      "Stochastic Gradient Descent(734): loss=40.60242717753296\n",
      "Stochastic Gradient Descent(735): loss=33.61145871146139\n",
      "Stochastic Gradient Descent(736): loss=2535.2543666606066\n",
      "Stochastic Gradient Descent(737): loss=885.76419414631\n",
      "Stochastic Gradient Descent(738): loss=298.9017714667314\n",
      "Stochastic Gradient Descent(739): loss=199.2900942802947\n",
      "Stochastic Gradient Descent(740): loss=114.77726153786672\n",
      "Stochastic Gradient Descent(741): loss=201.10394379881194\n",
      "Stochastic Gradient Descent(742): loss=550.5180973808385\n",
      "Stochastic Gradient Descent(743): loss=0.8526032072042509\n",
      "Stochastic Gradient Descent(744): loss=259.56938209060115\n",
      "Stochastic Gradient Descent(745): loss=64.46595411219702\n",
      "Stochastic Gradient Descent(746): loss=58.72213525428757\n",
      "Stochastic Gradient Descent(747): loss=0.610383259227669\n",
      "Stochastic Gradient Descent(748): loss=161.04431011907369\n",
      "Stochastic Gradient Descent(749): loss=62.532272462740174\n",
      "Stochastic Gradient Descent(750): loss=12.427759520679771\n",
      "Stochastic Gradient Descent(751): loss=156.6776024394228\n",
      "Stochastic Gradient Descent(752): loss=129.12928322749326\n",
      "Stochastic Gradient Descent(753): loss=98.99481925368191\n",
      "Stochastic Gradient Descent(754): loss=382.12750346165944\n",
      "Stochastic Gradient Descent(755): loss=150.91882777873434\n",
      "Stochastic Gradient Descent(756): loss=40.577049548381694\n",
      "Stochastic Gradient Descent(757): loss=568.5358887744223\n",
      "Stochastic Gradient Descent(758): loss=38.77353735658723\n",
      "Stochastic Gradient Descent(759): loss=1633.8277925148384\n",
      "Stochastic Gradient Descent(760): loss=14.214547625013623\n",
      "Stochastic Gradient Descent(761): loss=12.593173511191992\n",
      "Stochastic Gradient Descent(762): loss=81.21830989520328\n",
      "Stochastic Gradient Descent(763): loss=156.3725865569331\n",
      "Stochastic Gradient Descent(764): loss=69.48773336481881\n",
      "Stochastic Gradient Descent(765): loss=1106.0200562667294\n",
      "Stochastic Gradient Descent(766): loss=78.85810633895474\n",
      "Stochastic Gradient Descent(767): loss=278.80044129868355\n",
      "Stochastic Gradient Descent(768): loss=0.0432980470890996\n",
      "Stochastic Gradient Descent(769): loss=92.11708298716943\n",
      "Stochastic Gradient Descent(770): loss=181.52584350793902\n",
      "Stochastic Gradient Descent(771): loss=11.622183372327722\n",
      "Stochastic Gradient Descent(772): loss=262.91833684553126\n",
      "Stochastic Gradient Descent(773): loss=98.6224030655044\n",
      "Stochastic Gradient Descent(774): loss=171.93144439109395\n",
      "Stochastic Gradient Descent(775): loss=24.24916870849093\n",
      "Stochastic Gradient Descent(776): loss=0.2533106625557611\n",
      "Stochastic Gradient Descent(777): loss=14.759051028354856\n",
      "Stochastic Gradient Descent(778): loss=216.66209607260782\n",
      "Stochastic Gradient Descent(779): loss=2859.291564929139\n",
      "Stochastic Gradient Descent(780): loss=78.33671574651004\n",
      "Stochastic Gradient Descent(781): loss=49.83810612942171\n",
      "Stochastic Gradient Descent(782): loss=45.784802957159634\n",
      "Stochastic Gradient Descent(783): loss=30.358610940402738\n",
      "Stochastic Gradient Descent(784): loss=14.37035379432763\n",
      "Stochastic Gradient Descent(785): loss=1718.3742201806185\n",
      "Stochastic Gradient Descent(786): loss=513.9769987275057\n",
      "Stochastic Gradient Descent(787): loss=164.70787346478343\n",
      "Stochastic Gradient Descent(788): loss=4392.822063740079\n",
      "Stochastic Gradient Descent(789): loss=1223.8348109289027\n",
      "Stochastic Gradient Descent(790): loss=1202.2596766827278\n",
      "Stochastic Gradient Descent(791): loss=57.45431888517832\n",
      "Stochastic Gradient Descent(792): loss=2057.4942252047463\n",
      "Stochastic Gradient Descent(793): loss=98.55916112016926\n",
      "Stochastic Gradient Descent(794): loss=11.50931179545403\n",
      "Stochastic Gradient Descent(795): loss=142.88987778795098\n",
      "Stochastic Gradient Descent(796): loss=39.84307630311149\n",
      "Stochastic Gradient Descent(797): loss=1678.5730616372705\n",
      "Stochastic Gradient Descent(798): loss=133.20066914093172\n",
      "Stochastic Gradient Descent(799): loss=501.4561235977131\n",
      "Stochastic Gradient Descent(800): loss=251.96244289377515\n",
      "Stochastic Gradient Descent(801): loss=774.6110555112917\n",
      "Stochastic Gradient Descent(802): loss=290.63352341215733\n",
      "Stochastic Gradient Descent(803): loss=1647.520985098509\n",
      "Stochastic Gradient Descent(804): loss=15.509644091023027\n",
      "Stochastic Gradient Descent(805): loss=160.1325976209788\n",
      "Stochastic Gradient Descent(806): loss=1.9184833292185437\n",
      "Stochastic Gradient Descent(807): loss=972.2762752533828\n",
      "Stochastic Gradient Descent(808): loss=504.167941018137\n",
      "Stochastic Gradient Descent(809): loss=56.55978342253775\n",
      "Stochastic Gradient Descent(810): loss=181.15576836109847\n",
      "Stochastic Gradient Descent(811): loss=1107.78343410317\n",
      "Stochastic Gradient Descent(812): loss=869.2571713633071\n",
      "Stochastic Gradient Descent(813): loss=139.69076635692784\n",
      "Stochastic Gradient Descent(814): loss=454.99216257325065\n",
      "Stochastic Gradient Descent(815): loss=299.1638036689947\n",
      "Stochastic Gradient Descent(816): loss=595.3222722054635\n",
      "Stochastic Gradient Descent(817): loss=27.485231672365924\n",
      "Stochastic Gradient Descent(818): loss=8.67100099352373\n",
      "Stochastic Gradient Descent(819): loss=207.14761954866552\n",
      "Stochastic Gradient Descent(820): loss=39.005606372610686\n",
      "Stochastic Gradient Descent(821): loss=18.124722459119177\n",
      "Stochastic Gradient Descent(822): loss=277.5155797029245\n",
      "Stochastic Gradient Descent(823): loss=208.05214030402536\n",
      "Stochastic Gradient Descent(824): loss=599.9467811283986\n",
      "Stochastic Gradient Descent(825): loss=152.72015512323563\n",
      "Stochastic Gradient Descent(826): loss=392.4032143701299\n",
      "Stochastic Gradient Descent(827): loss=148.07256843071912\n",
      "Stochastic Gradient Descent(828): loss=830.0912163948321\n",
      "Stochastic Gradient Descent(829): loss=131.80487907485985\n",
      "Stochastic Gradient Descent(830): loss=2.7614023235928786\n",
      "Stochastic Gradient Descent(831): loss=4.467687530831678\n",
      "Stochastic Gradient Descent(832): loss=282.27068535727676\n",
      "Stochastic Gradient Descent(833): loss=359.15283702400325\n",
      "Stochastic Gradient Descent(834): loss=1222.5116112993776\n",
      "Stochastic Gradient Descent(835): loss=848.5382375506457\n",
      "Stochastic Gradient Descent(836): loss=1477.8815156985463\n",
      "Stochastic Gradient Descent(837): loss=57.12668329989849\n",
      "Stochastic Gradient Descent(838): loss=515.2965706239012\n",
      "Stochastic Gradient Descent(839): loss=323.51031596197674\n",
      "Stochastic Gradient Descent(840): loss=36.94446916414637\n",
      "Stochastic Gradient Descent(841): loss=93.71999849911063\n",
      "Stochastic Gradient Descent(842): loss=62.991208935195395\n",
      "Stochastic Gradient Descent(843): loss=393.2849652963616\n",
      "Stochastic Gradient Descent(844): loss=797.7595259165657\n",
      "Stochastic Gradient Descent(845): loss=1607.2871587728587\n",
      "Stochastic Gradient Descent(846): loss=13.284385974083731\n",
      "Stochastic Gradient Descent(847): loss=2753.24418653229\n",
      "Stochastic Gradient Descent(848): loss=208.2514005252113\n",
      "Stochastic Gradient Descent(849): loss=426.4170023993621\n",
      "Stochastic Gradient Descent(850): loss=82.10968220440847\n",
      "Stochastic Gradient Descent(851): loss=35.507873257185885\n",
      "Stochastic Gradient Descent(852): loss=4746.727142393696\n",
      "Stochastic Gradient Descent(853): loss=7529.6804600410005\n",
      "Stochastic Gradient Descent(854): loss=327.22632023523346\n",
      "Stochastic Gradient Descent(855): loss=2806.8180214114977\n",
      "Stochastic Gradient Descent(856): loss=5858.261738302317\n",
      "Stochastic Gradient Descent(857): loss=807.1262376096533\n",
      "Stochastic Gradient Descent(858): loss=416.90784083263713\n",
      "Stochastic Gradient Descent(859): loss=514.4435625932354\n",
      "Stochastic Gradient Descent(860): loss=585.5475878402597\n",
      "Stochastic Gradient Descent(861): loss=303.70227729118454\n",
      "Stochastic Gradient Descent(862): loss=41.031745402318165\n",
      "Stochastic Gradient Descent(863): loss=3022.526692469753\n",
      "Stochastic Gradient Descent(864): loss=19.874360292835274\n",
      "Stochastic Gradient Descent(865): loss=125.24962385657227\n",
      "Stochastic Gradient Descent(866): loss=99.0051677261697\n",
      "Stochastic Gradient Descent(867): loss=196.66848590637085\n",
      "Stochastic Gradient Descent(868): loss=233.0652680123858\n",
      "Stochastic Gradient Descent(869): loss=33.31478131623064\n",
      "Stochastic Gradient Descent(870): loss=3319.5445883846332\n",
      "Stochastic Gradient Descent(871): loss=3.517643977940388\n",
      "Stochastic Gradient Descent(872): loss=2549.4142795844455\n",
      "Stochastic Gradient Descent(873): loss=1122.2192598423158\n",
      "Stochastic Gradient Descent(874): loss=80.46407195005087\n",
      "Stochastic Gradient Descent(875): loss=257.32898197566107\n",
      "Stochastic Gradient Descent(876): loss=979.2825912029776\n",
      "Stochastic Gradient Descent(877): loss=23.634064527226492\n",
      "Stochastic Gradient Descent(878): loss=383.7792206360012\n",
      "Stochastic Gradient Descent(879): loss=479.1539204947391\n",
      "Stochastic Gradient Descent(880): loss=515.8444606241843\n",
      "Stochastic Gradient Descent(881): loss=20.160402774573885\n",
      "Stochastic Gradient Descent(882): loss=522.6213905430133\n",
      "Stochastic Gradient Descent(883): loss=18.850719289475762\n",
      "Stochastic Gradient Descent(884): loss=335.5706355378053\n",
      "Stochastic Gradient Descent(885): loss=23.296991015870663\n",
      "Stochastic Gradient Descent(886): loss=0.0012589644808454143\n",
      "Stochastic Gradient Descent(887): loss=9.273750845900736\n",
      "Stochastic Gradient Descent(888): loss=421.19546388116004\n",
      "Stochastic Gradient Descent(889): loss=8.057514345199223\n",
      "Stochastic Gradient Descent(890): loss=30.31061518688962\n",
      "Stochastic Gradient Descent(891): loss=1462.6078360723866\n",
      "Stochastic Gradient Descent(892): loss=15.904812698205928\n",
      "Stochastic Gradient Descent(893): loss=98.2161303242719\n",
      "Stochastic Gradient Descent(894): loss=68.47325716602992\n",
      "Stochastic Gradient Descent(895): loss=61.30421193858472\n",
      "Stochastic Gradient Descent(896): loss=5.434169826390557\n",
      "Stochastic Gradient Descent(897): loss=115.58617869844046\n",
      "Stochastic Gradient Descent(898): loss=9.589965132439453\n",
      "Stochastic Gradient Descent(899): loss=873.3303498911639\n",
      "Stochastic Gradient Descent(900): loss=2.6031235156912382\n",
      "Stochastic Gradient Descent(901): loss=0.17554840206389338\n",
      "Stochastic Gradient Descent(902): loss=6.4689805562678275\n",
      "Stochastic Gradient Descent(903): loss=18.849349396985954\n",
      "Stochastic Gradient Descent(904): loss=366.0278354018975\n",
      "Stochastic Gradient Descent(905): loss=33.50583714249392\n",
      "Stochastic Gradient Descent(906): loss=1157.38876353068\n",
      "Stochastic Gradient Descent(907): loss=3.927457772022164\n",
      "Stochastic Gradient Descent(908): loss=0.8460369972687516\n",
      "Stochastic Gradient Descent(909): loss=44.19358377495625\n",
      "Stochastic Gradient Descent(910): loss=22.811832241244648\n",
      "Stochastic Gradient Descent(911): loss=65.25699544433104\n",
      "Stochastic Gradient Descent(912): loss=0.00015366465468129997\n",
      "Stochastic Gradient Descent(913): loss=51.125370348224905\n",
      "Stochastic Gradient Descent(914): loss=169.14897462680412\n",
      "Stochastic Gradient Descent(915): loss=476.4125775939184\n",
      "Stochastic Gradient Descent(916): loss=932.4828213519999\n",
      "Stochastic Gradient Descent(917): loss=653.5005546433265\n",
      "Stochastic Gradient Descent(918): loss=167.30467441449446\n",
      "Stochastic Gradient Descent(919): loss=251.32925212920634\n",
      "Stochastic Gradient Descent(920): loss=57.00624352053997\n",
      "Stochastic Gradient Descent(921): loss=257.53119509127004\n",
      "Stochastic Gradient Descent(922): loss=100.4091178163939\n",
      "Stochastic Gradient Descent(923): loss=19.26334131287325\n",
      "Stochastic Gradient Descent(924): loss=18.199532645063172\n",
      "Stochastic Gradient Descent(925): loss=424.1558143199958\n",
      "Stochastic Gradient Descent(926): loss=0.06213441331180393\n",
      "Stochastic Gradient Descent(927): loss=139.72783300420627\n",
      "Stochastic Gradient Descent(928): loss=1530.5663912526747\n",
      "Stochastic Gradient Descent(929): loss=407.0599997524576\n",
      "Stochastic Gradient Descent(930): loss=26.624186854298365\n",
      "Stochastic Gradient Descent(931): loss=129.6433711326856\n",
      "Stochastic Gradient Descent(932): loss=1781.4300064744532\n",
      "Stochastic Gradient Descent(933): loss=369.53923574367576\n",
      "Stochastic Gradient Descent(934): loss=4053.4682962848574\n",
      "Stochastic Gradient Descent(935): loss=308.05359108299695\n",
      "Stochastic Gradient Descent(936): loss=364.128927497094\n",
      "Stochastic Gradient Descent(937): loss=310.4430165440881\n",
      "Stochastic Gradient Descent(938): loss=227.24486364143422\n",
      "Stochastic Gradient Descent(939): loss=6.6165683935860065\n",
      "Stochastic Gradient Descent(940): loss=276.00783864827565\n",
      "Stochastic Gradient Descent(941): loss=1547.835214853271\n",
      "Stochastic Gradient Descent(942): loss=294.24353462325075\n",
      "Stochastic Gradient Descent(943): loss=121.67382685247395\n",
      "Stochastic Gradient Descent(944): loss=34.96350346685243\n",
      "Stochastic Gradient Descent(945): loss=543.1462012931205\n",
      "Stochastic Gradient Descent(946): loss=110.13109341826161\n",
      "Stochastic Gradient Descent(947): loss=0.9977355396709896\n",
      "Stochastic Gradient Descent(948): loss=5.411602189867222\n",
      "Stochastic Gradient Descent(949): loss=96.89048296386389\n",
      "Stochastic Gradient Descent(950): loss=516.2747440580794\n",
      "Stochastic Gradient Descent(951): loss=992.7854856625315\n",
      "Stochastic Gradient Descent(952): loss=627.0615469417671\n",
      "Stochastic Gradient Descent(953): loss=60.11810159702508\n",
      "Stochastic Gradient Descent(954): loss=136.23373506190623\n",
      "Stochastic Gradient Descent(955): loss=1175.1044347893512\n",
      "Stochastic Gradient Descent(956): loss=76.67874617647752\n",
      "Stochastic Gradient Descent(957): loss=19.70798357105643\n",
      "Stochastic Gradient Descent(958): loss=131.7068628708553\n",
      "Stochastic Gradient Descent(959): loss=58.22023755392954\n",
      "Stochastic Gradient Descent(960): loss=199.46766556724882\n",
      "Stochastic Gradient Descent(961): loss=483.05177632510345\n",
      "Stochastic Gradient Descent(962): loss=0.04613914051283802\n",
      "Stochastic Gradient Descent(963): loss=7.9220892421055575\n",
      "Stochastic Gradient Descent(964): loss=40.79521085310462\n",
      "Stochastic Gradient Descent(965): loss=53.12124633086252\n",
      "Stochastic Gradient Descent(966): loss=990.6518786652442\n",
      "Stochastic Gradient Descent(967): loss=235.2397297701006\n",
      "Stochastic Gradient Descent(968): loss=4.148435838965564\n",
      "Stochastic Gradient Descent(969): loss=49.30646132416644\n",
      "Stochastic Gradient Descent(970): loss=102.10643277496875\n",
      "Stochastic Gradient Descent(971): loss=213.49990116012253\n",
      "Stochastic Gradient Descent(972): loss=32.496345380474565\n",
      "Stochastic Gradient Descent(973): loss=21.805604055677826\n",
      "Stochastic Gradient Descent(974): loss=419.15379190304293\n",
      "Stochastic Gradient Descent(975): loss=318.1023688437023\n",
      "Stochastic Gradient Descent(976): loss=6.654253108394238\n",
      "Stochastic Gradient Descent(977): loss=13.831877580642105\n",
      "Stochastic Gradient Descent(978): loss=253.0167574160497\n",
      "Stochastic Gradient Descent(979): loss=43.93865903759603\n",
      "Stochastic Gradient Descent(980): loss=55.24139946292196\n",
      "Stochastic Gradient Descent(981): loss=480.6072207235797\n",
      "Stochastic Gradient Descent(982): loss=47.61566971250716\n",
      "Stochastic Gradient Descent(983): loss=570.1430063213447\n",
      "Stochastic Gradient Descent(984): loss=1333.2273146524522\n",
      "Stochastic Gradient Descent(985): loss=2.715723973546972\n",
      "Stochastic Gradient Descent(986): loss=250.98039283187387\n",
      "Stochastic Gradient Descent(987): loss=614.5780550176117\n",
      "Stochastic Gradient Descent(988): loss=346.16002352317224\n",
      "Stochastic Gradient Descent(989): loss=1167.612553933091\n",
      "Stochastic Gradient Descent(990): loss=395.5903278012415\n",
      "Stochastic Gradient Descent(991): loss=2450.697600289628\n",
      "Stochastic Gradient Descent(992): loss=1050.5243150000472\n",
      "Stochastic Gradient Descent(993): loss=10.216350179519633\n",
      "Stochastic Gradient Descent(994): loss=333.8349532483514\n",
      "Stochastic Gradient Descent(995): loss=37.99582216793991\n",
      "Stochastic Gradient Descent(996): loss=318.2910444209884\n",
      "Stochastic Gradient Descent(997): loss=3.9735711068287527\n",
      "Stochastic Gradient Descent(998): loss=243.8346740137308\n",
      "Stochastic Gradient Descent(999): loss=244.25955548551102\n",
      "Stochastic Gradient Descent(1000): loss=28.81528867767675\n",
      "Stochastic Gradient Descent(1001): loss=125.43655555192309\n",
      "Stochastic Gradient Descent(1002): loss=45.28713324841683\n",
      "Stochastic Gradient Descent(1003): loss=161.29833551614115\n",
      "Stochastic Gradient Descent(1004): loss=168.9495614833587\n",
      "Stochastic Gradient Descent(1005): loss=9.486885242731955\n",
      "Stochastic Gradient Descent(1006): loss=131.91447292140907\n",
      "Stochastic Gradient Descent(1007): loss=5.136630298948989\n",
      "Stochastic Gradient Descent(1008): loss=134.92129162383202\n",
      "Stochastic Gradient Descent(1009): loss=46.043846512654056\n",
      "Stochastic Gradient Descent(1010): loss=197.6408356960995\n",
      "Stochastic Gradient Descent(1011): loss=132.0922354355846\n",
      "Stochastic Gradient Descent(1012): loss=219.77617019370456\n",
      "Stochastic Gradient Descent(1013): loss=1.4839238939198405\n",
      "Stochastic Gradient Descent(1014): loss=506.74836603417043\n",
      "Stochastic Gradient Descent(1015): loss=119.17503472348228\n",
      "Stochastic Gradient Descent(1016): loss=9.124856698336023\n",
      "Stochastic Gradient Descent(1017): loss=8.827915764436115\n",
      "Stochastic Gradient Descent(1018): loss=75.16997384441551\n",
      "Stochastic Gradient Descent(1019): loss=69.12398549020378\n",
      "Stochastic Gradient Descent(1020): loss=19.08727426651227\n",
      "Stochastic Gradient Descent(1021): loss=527.3876687781614\n",
      "Stochastic Gradient Descent(1022): loss=267.75860541732663\n",
      "Stochastic Gradient Descent(1023): loss=511.11657022305957\n",
      "Stochastic Gradient Descent(1024): loss=3.983676853462364\n",
      "Stochastic Gradient Descent(1025): loss=61.49104956781608\n",
      "Stochastic Gradient Descent(1026): loss=370.8599844086819\n",
      "Stochastic Gradient Descent(1027): loss=282.98273345548336\n",
      "Stochastic Gradient Descent(1028): loss=39.87654638245123\n",
      "Stochastic Gradient Descent(1029): loss=51.065619644825944\n",
      "Stochastic Gradient Descent(1030): loss=45.63506972473234\n",
      "Stochastic Gradient Descent(1031): loss=2809.2763351046087\n",
      "Stochastic Gradient Descent(1032): loss=89.78993353054827\n",
      "Stochastic Gradient Descent(1033): loss=112.5719147560204\n",
      "Stochastic Gradient Descent(1034): loss=1.962491800956658\n",
      "Stochastic Gradient Descent(1035): loss=251.89384521442244\n",
      "Stochastic Gradient Descent(1036): loss=945.5901053075842\n",
      "Stochastic Gradient Descent(1037): loss=592.5644065099395\n",
      "Stochastic Gradient Descent(1038): loss=4.990584010803387\n",
      "Stochastic Gradient Descent(1039): loss=5.3203612949886825\n",
      "Stochastic Gradient Descent(1040): loss=36.13625793271732\n",
      "Stochastic Gradient Descent(1041): loss=228.81630743488375\n",
      "Stochastic Gradient Descent(1042): loss=41.8978007690635\n",
      "Stochastic Gradient Descent(1043): loss=16.862858047249752\n",
      "Stochastic Gradient Descent(1044): loss=137.08909945378545\n",
      "Stochastic Gradient Descent(1045): loss=1.2239001917967134\n",
      "Stochastic Gradient Descent(1046): loss=166.82039498402025\n",
      "Stochastic Gradient Descent(1047): loss=543.701490164577\n",
      "Stochastic Gradient Descent(1048): loss=905.4364073088126\n",
      "Stochastic Gradient Descent(1049): loss=240.25488943978755\n",
      "Stochastic Gradient Descent(1050): loss=61.93915433792216\n",
      "Stochastic Gradient Descent(1051): loss=1.5622909424859193\n",
      "Stochastic Gradient Descent(1052): loss=188.8220040714495\n",
      "Stochastic Gradient Descent(1053): loss=24.09316262288147\n",
      "Stochastic Gradient Descent(1054): loss=46.34230241886851\n",
      "Stochastic Gradient Descent(1055): loss=19.74464833136336\n",
      "Stochastic Gradient Descent(1056): loss=944.9638220053771\n",
      "Stochastic Gradient Descent(1057): loss=137.20529174211356\n",
      "Stochastic Gradient Descent(1058): loss=67.48628574833812\n",
      "Stochastic Gradient Descent(1059): loss=19.82744849021875\n",
      "Stochastic Gradient Descent(1060): loss=328.30983164807566\n",
      "Stochastic Gradient Descent(1061): loss=30.438522520462552\n",
      "Stochastic Gradient Descent(1062): loss=15.226479279924504\n",
      "Stochastic Gradient Descent(1063): loss=936.2861253569165\n",
      "Stochastic Gradient Descent(1064): loss=10.484349620661355\n",
      "Stochastic Gradient Descent(1065): loss=259.1469252785493\n",
      "Stochastic Gradient Descent(1066): loss=112.55729540737883\n",
      "Stochastic Gradient Descent(1067): loss=77.40713393129134\n",
      "Stochastic Gradient Descent(1068): loss=5.400539599008508\n",
      "Stochastic Gradient Descent(1069): loss=16.4470480044447\n",
      "Stochastic Gradient Descent(1070): loss=225.95254575516498\n",
      "Stochastic Gradient Descent(1071): loss=345.4077479931927\n",
      "Stochastic Gradient Descent(1072): loss=2621.441878641146\n",
      "Stochastic Gradient Descent(1073): loss=30.21965751504702\n",
      "Stochastic Gradient Descent(1074): loss=112.89507125716905\n",
      "Stochastic Gradient Descent(1075): loss=112.28013634685077\n",
      "Stochastic Gradient Descent(1076): loss=103.02550804637622\n",
      "Stochastic Gradient Descent(1077): loss=34.13187868712437\n",
      "Stochastic Gradient Descent(1078): loss=3.646195622615062\n",
      "Stochastic Gradient Descent(1079): loss=2.839258068645131\n",
      "Stochastic Gradient Descent(1080): loss=70.06156969589674\n",
      "Stochastic Gradient Descent(1081): loss=59.64568575551192\n",
      "Stochastic Gradient Descent(1082): loss=41.403227194775475\n",
      "Stochastic Gradient Descent(1083): loss=410.61186522600264\n",
      "Stochastic Gradient Descent(1084): loss=262.1838924893138\n",
      "Stochastic Gradient Descent(1085): loss=8.236919118200198\n",
      "Stochastic Gradient Descent(1086): loss=2037.0087884406491\n",
      "Stochastic Gradient Descent(1087): loss=0.007213034456990668\n",
      "Stochastic Gradient Descent(1088): loss=297.9857136313751\n",
      "Stochastic Gradient Descent(1089): loss=7193.05937676943\n",
      "Stochastic Gradient Descent(1090): loss=1098.607712718229\n",
      "Stochastic Gradient Descent(1091): loss=742.6903635786963\n",
      "Stochastic Gradient Descent(1092): loss=6070.624704408261\n",
      "Stochastic Gradient Descent(1093): loss=164.0008272428726\n",
      "Stochastic Gradient Descent(1094): loss=1.3472877058974178\n",
      "Stochastic Gradient Descent(1095): loss=199.25049931128842\n",
      "Stochastic Gradient Descent(1096): loss=186.68563389320707\n",
      "Stochastic Gradient Descent(1097): loss=35.19681066064648\n",
      "Stochastic Gradient Descent(1098): loss=107.68883850135884\n",
      "Stochastic Gradient Descent(1099): loss=1.0401813073344195\n",
      "Stochastic Gradient Descent(1100): loss=0.11640634466490446\n",
      "Stochastic Gradient Descent(1101): loss=20.169352735082168\n",
      "Stochastic Gradient Descent(1102): loss=28.31859649081028\n",
      "Stochastic Gradient Descent(1103): loss=67.09187579850114\n",
      "Stochastic Gradient Descent(1104): loss=371.9813375203889\n",
      "Stochastic Gradient Descent(1105): loss=2415.543810680362\n",
      "Stochastic Gradient Descent(1106): loss=610.8508887485698\n",
      "Stochastic Gradient Descent(1107): loss=122.6170849915584\n",
      "Stochastic Gradient Descent(1108): loss=187.84771419187825\n",
      "Stochastic Gradient Descent(1109): loss=6.573516071120208\n",
      "Stochastic Gradient Descent(1110): loss=139.1001201110269\n",
      "Stochastic Gradient Descent(1111): loss=1.1573277419082748\n",
      "Stochastic Gradient Descent(1112): loss=501.5791721496681\n",
      "Stochastic Gradient Descent(1113): loss=10.323712615517154\n",
      "Stochastic Gradient Descent(1114): loss=555.7276270629869\n",
      "Stochastic Gradient Descent(1115): loss=127.6113632744046\n",
      "Stochastic Gradient Descent(1116): loss=32.18303413713526\n",
      "Stochastic Gradient Descent(1117): loss=269.9303290437631\n",
      "Stochastic Gradient Descent(1118): loss=6.36560363230094\n",
      "Stochastic Gradient Descent(1119): loss=43.3605901835676\n",
      "Stochastic Gradient Descent(1120): loss=263.354430510944\n",
      "Stochastic Gradient Descent(1121): loss=438.0293981376228\n",
      "Stochastic Gradient Descent(1122): loss=936.0166730580836\n",
      "Stochastic Gradient Descent(1123): loss=730.0362163045544\n",
      "Stochastic Gradient Descent(1124): loss=1068.4694000024276\n",
      "Stochastic Gradient Descent(1125): loss=314.6490043245268\n",
      "Stochastic Gradient Descent(1126): loss=88.80375697712806\n",
      "Stochastic Gradient Descent(1127): loss=25.515588426325653\n",
      "Stochastic Gradient Descent(1128): loss=26.28007822194775\n",
      "Stochastic Gradient Descent(1129): loss=12.720777075505348\n",
      "Stochastic Gradient Descent(1130): loss=6.342085347216657\n",
      "Stochastic Gradient Descent(1131): loss=223.51789045264488\n",
      "Stochastic Gradient Descent(1132): loss=244.7369871058795\n",
      "Stochastic Gradient Descent(1133): loss=2.3697718416749933\n",
      "Stochastic Gradient Descent(1134): loss=21.33310087964926\n",
      "Stochastic Gradient Descent(1135): loss=113.1669813252558\n",
      "Stochastic Gradient Descent(1136): loss=67.20922726369068\n",
      "Stochastic Gradient Descent(1137): loss=1338.3531683040499\n",
      "Stochastic Gradient Descent(1138): loss=521.9514845914833\n",
      "Stochastic Gradient Descent(1139): loss=42.91353059967863\n",
      "Stochastic Gradient Descent(1140): loss=192.98067270852533\n",
      "Stochastic Gradient Descent(1141): loss=0.06447423443688552\n",
      "Stochastic Gradient Descent(1142): loss=29.990436255119143\n",
      "Stochastic Gradient Descent(1143): loss=86.31240594016539\n",
      "Stochastic Gradient Descent(1144): loss=38.02448714406229\n",
      "Stochastic Gradient Descent(1145): loss=84.7324851005163\n",
      "Stochastic Gradient Descent(1146): loss=139.41762158362945\n",
      "Stochastic Gradient Descent(1147): loss=546.6007161353987\n",
      "Stochastic Gradient Descent(1148): loss=63.201679484429\n",
      "Stochastic Gradient Descent(1149): loss=0.6013636919425219\n",
      "Stochastic Gradient Descent(1150): loss=1302.516118653774\n",
      "Stochastic Gradient Descent(1151): loss=19.757311972477414\n",
      "Stochastic Gradient Descent(1152): loss=16.039798396952794\n",
      "Stochastic Gradient Descent(1153): loss=0.005408745261959992\n",
      "Stochastic Gradient Descent(1154): loss=144.83888702366656\n",
      "Stochastic Gradient Descent(1155): loss=0.0011386213714901605\n",
      "Stochastic Gradient Descent(1156): loss=44.34068922841111\n",
      "Stochastic Gradient Descent(1157): loss=218.65566725369032\n",
      "Stochastic Gradient Descent(1158): loss=260.0645618068998\n",
      "Stochastic Gradient Descent(1159): loss=2.4117117996007544\n",
      "Stochastic Gradient Descent(1160): loss=9.323464189241756\n",
      "Stochastic Gradient Descent(1161): loss=35.626685512449605\n",
      "Stochastic Gradient Descent(1162): loss=7.809524768213966\n",
      "Stochastic Gradient Descent(1163): loss=998.1118163698429\n",
      "Stochastic Gradient Descent(1164): loss=114.29891690413916\n",
      "Stochastic Gradient Descent(1165): loss=91.35015017262366\n",
      "Stochastic Gradient Descent(1166): loss=10.077008270822809\n",
      "Stochastic Gradient Descent(1167): loss=162.64010201164123\n",
      "Stochastic Gradient Descent(1168): loss=190.04390090308598\n",
      "Stochastic Gradient Descent(1169): loss=90.92691215597938\n",
      "Stochastic Gradient Descent(1170): loss=1606.9113781057572\n",
      "Stochastic Gradient Descent(1171): loss=4.654130028043423\n",
      "Stochastic Gradient Descent(1172): loss=117.05797996355165\n",
      "Stochastic Gradient Descent(1173): loss=16.965340975476256\n",
      "Stochastic Gradient Descent(1174): loss=254.0738386392085\n",
      "Stochastic Gradient Descent(1175): loss=1.429833332637117\n",
      "Stochastic Gradient Descent(1176): loss=110.17468747067343\n",
      "Stochastic Gradient Descent(1177): loss=143.80461335397462\n",
      "Stochastic Gradient Descent(1178): loss=30.062683530403987\n",
      "Stochastic Gradient Descent(1179): loss=62.85930932093607\n",
      "Stochastic Gradient Descent(1180): loss=751.8983719348314\n",
      "Stochastic Gradient Descent(1181): loss=57.82739451336267\n",
      "Stochastic Gradient Descent(1182): loss=30.659984450859923\n",
      "Stochastic Gradient Descent(1183): loss=548.3368426268449\n",
      "Stochastic Gradient Descent(1184): loss=32.25363083040348\n",
      "Stochastic Gradient Descent(1185): loss=0.642235665392636\n",
      "Stochastic Gradient Descent(1186): loss=163.54863785056216\n",
      "Stochastic Gradient Descent(1187): loss=38.33564092157746\n",
      "Stochastic Gradient Descent(1188): loss=219.72655138475162\n",
      "Stochastic Gradient Descent(1189): loss=43.268873749843856\n",
      "Stochastic Gradient Descent(1190): loss=1.4468073812799698\n",
      "Stochastic Gradient Descent(1191): loss=4.9478491361747885\n",
      "Stochastic Gradient Descent(1192): loss=137.8123732495633\n",
      "Stochastic Gradient Descent(1193): loss=30.693782268363034\n",
      "Stochastic Gradient Descent(1194): loss=31.012779729422547\n",
      "Stochastic Gradient Descent(1195): loss=1361.6725550161934\n",
      "Stochastic Gradient Descent(1196): loss=1550.4104029197895\n",
      "Stochastic Gradient Descent(1197): loss=827.770453179249\n",
      "Stochastic Gradient Descent(1198): loss=19.652035440294856\n",
      "Stochastic Gradient Descent(1199): loss=73.54358674024566\n",
      "Stochastic Gradient Descent(1200): loss=546.0366477674191\n",
      "Stochastic Gradient Descent(1201): loss=545.7773805354938\n",
      "Stochastic Gradient Descent(1202): loss=1488.6783727300776\n",
      "Stochastic Gradient Descent(1203): loss=3423.006111244731\n",
      "Stochastic Gradient Descent(1204): loss=126.5445862875658\n",
      "Stochastic Gradient Descent(1205): loss=3.5201329275491013\n",
      "Stochastic Gradient Descent(1206): loss=28.51354064270187\n",
      "Stochastic Gradient Descent(1207): loss=10.816791369715313\n",
      "Stochastic Gradient Descent(1208): loss=10.00471049312003\n",
      "Stochastic Gradient Descent(1209): loss=25.98871742248616\n",
      "Stochastic Gradient Descent(1210): loss=7.581789342556142\n",
      "Stochastic Gradient Descent(1211): loss=783.09965094869\n",
      "Stochastic Gradient Descent(1212): loss=1332.73496140314\n",
      "Stochastic Gradient Descent(1213): loss=94.7875720364623\n",
      "Stochastic Gradient Descent(1214): loss=67.38687873350577\n",
      "Stochastic Gradient Descent(1215): loss=7.794182245500242\n",
      "Stochastic Gradient Descent(1216): loss=49.43438110429779\n",
      "Stochastic Gradient Descent(1217): loss=196.62845507731578\n",
      "Stochastic Gradient Descent(1218): loss=104.9119930909755\n",
      "Stochastic Gradient Descent(1219): loss=120.24058926850275\n",
      "Stochastic Gradient Descent(1220): loss=16.473792420296068\n",
      "Stochastic Gradient Descent(1221): loss=826.8799173711668\n",
      "Stochastic Gradient Descent(1222): loss=46.526983258204744\n",
      "Stochastic Gradient Descent(1223): loss=68.2756598110815\n",
      "Stochastic Gradient Descent(1224): loss=2.810706439634275\n",
      "Stochastic Gradient Descent(1225): loss=0.6745876990254215\n",
      "Stochastic Gradient Descent(1226): loss=123.43909601849514\n",
      "Stochastic Gradient Descent(1227): loss=52.75800241840518\n",
      "Stochastic Gradient Descent(1228): loss=0.028438968892607636\n",
      "Stochastic Gradient Descent(1229): loss=56.062758399117484\n",
      "Stochastic Gradient Descent(1230): loss=141.02323179349926\n",
      "Stochastic Gradient Descent(1231): loss=377.34868038197027\n",
      "Stochastic Gradient Descent(1232): loss=1.625220907886186\n",
      "Stochastic Gradient Descent(1233): loss=40.21446496763057\n",
      "Stochastic Gradient Descent(1234): loss=81.16870269160869\n",
      "Stochastic Gradient Descent(1235): loss=83.38365568376933\n",
      "Stochastic Gradient Descent(1236): loss=0.36828303324886547\n",
      "Stochastic Gradient Descent(1237): loss=118.62929520652115\n",
      "Stochastic Gradient Descent(1238): loss=69.84616474282778\n",
      "Stochastic Gradient Descent(1239): loss=19.11326926545124\n",
      "Stochastic Gradient Descent(1240): loss=52.85019805273987\n",
      "Stochastic Gradient Descent(1241): loss=28.194724531671074\n",
      "Stochastic Gradient Descent(1242): loss=58.23089185863922\n",
      "Stochastic Gradient Descent(1243): loss=71.04063134752982\n",
      "Stochastic Gradient Descent(1244): loss=242.68125983291625\n",
      "Stochastic Gradient Descent(1245): loss=121.58292528814061\n",
      "Stochastic Gradient Descent(1246): loss=639.2003130273806\n",
      "Stochastic Gradient Descent(1247): loss=96.72786177638997\n",
      "Stochastic Gradient Descent(1248): loss=758.8059008869939\n",
      "Stochastic Gradient Descent(1249): loss=4.973149243337622\n",
      "Stochastic Gradient Descent(1250): loss=13.058348541422454\n",
      "Stochastic Gradient Descent(1251): loss=141.80846248737868\n",
      "Stochastic Gradient Descent(1252): loss=32.61981582466339\n",
      "Stochastic Gradient Descent(1253): loss=63.06742778532108\n",
      "Stochastic Gradient Descent(1254): loss=195.9244609599608\n",
      "Stochastic Gradient Descent(1255): loss=61.13110056325779\n",
      "Stochastic Gradient Descent(1256): loss=65.0849451365942\n",
      "Stochastic Gradient Descent(1257): loss=35.60848356087867\n",
      "Stochastic Gradient Descent(1258): loss=54.555358395478\n",
      "Stochastic Gradient Descent(1259): loss=1.2491927018590383\n",
      "Stochastic Gradient Descent(1260): loss=1632.1105182808462\n",
      "Stochastic Gradient Descent(1261): loss=3.3928348274128886\n",
      "Stochastic Gradient Descent(1262): loss=279.58721222333907\n",
      "Stochastic Gradient Descent(1263): loss=175.22295218376146\n",
      "Stochastic Gradient Descent(1264): loss=207.69076940067043\n",
      "Stochastic Gradient Descent(1265): loss=1125.6791339357903\n",
      "Stochastic Gradient Descent(1266): loss=49.365740832049056\n",
      "Stochastic Gradient Descent(1267): loss=534.1891828329709\n",
      "Stochastic Gradient Descent(1268): loss=73.9222906188996\n",
      "Stochastic Gradient Descent(1269): loss=241.08718227475487\n",
      "Stochastic Gradient Descent(1270): loss=51.25454275510372\n",
      "Stochastic Gradient Descent(1271): loss=50.325464387203056\n",
      "Stochastic Gradient Descent(1272): loss=137.0226623362633\n",
      "Stochastic Gradient Descent(1273): loss=230.1118851035656\n",
      "Stochastic Gradient Descent(1274): loss=374.9670091289424\n",
      "Stochastic Gradient Descent(1275): loss=30.906071737521096\n",
      "Stochastic Gradient Descent(1276): loss=27.87334331288648\n",
      "Stochastic Gradient Descent(1277): loss=338.42046474868874\n",
      "Stochastic Gradient Descent(1278): loss=640.8024314880053\n",
      "Stochastic Gradient Descent(1279): loss=54.04994166478772\n",
      "Stochastic Gradient Descent(1280): loss=245.64909772243132\n",
      "Stochastic Gradient Descent(1281): loss=82.1907252369795\n",
      "Stochastic Gradient Descent(1282): loss=44.630574315739246\n",
      "Stochastic Gradient Descent(1283): loss=27.7682159989441\n",
      "Stochastic Gradient Descent(1284): loss=0.013402273659577137\n",
      "Stochastic Gradient Descent(1285): loss=107.69881810113637\n",
      "Stochastic Gradient Descent(1286): loss=741.108609907793\n",
      "Stochastic Gradient Descent(1287): loss=77.92700946784797\n",
      "Stochastic Gradient Descent(1288): loss=43.58409801694395\n",
      "Stochastic Gradient Descent(1289): loss=136.46507371745886\n",
      "Stochastic Gradient Descent(1290): loss=202.22070592224162\n",
      "Stochastic Gradient Descent(1291): loss=5.292409668763554\n",
      "Stochastic Gradient Descent(1292): loss=5.690693987311898\n",
      "Stochastic Gradient Descent(1293): loss=61.49513248157929\n",
      "Stochastic Gradient Descent(1294): loss=70.5637817365556\n",
      "Stochastic Gradient Descent(1295): loss=26.940358997787072\n",
      "Stochastic Gradient Descent(1296): loss=67.11450180725451\n",
      "Stochastic Gradient Descent(1297): loss=29.14313589831455\n",
      "Stochastic Gradient Descent(1298): loss=348.6299576556655\n",
      "Stochastic Gradient Descent(1299): loss=614.604202835994\n",
      "Stochastic Gradient Descent(1300): loss=40.892526512589576\n",
      "Stochastic Gradient Descent(1301): loss=19.948914609483733\n",
      "Stochastic Gradient Descent(1302): loss=189.3663757037192\n",
      "Stochastic Gradient Descent(1303): loss=262.16741086638314\n",
      "Stochastic Gradient Descent(1304): loss=23.616460666343635\n",
      "Stochastic Gradient Descent(1305): loss=10.675854350127882\n",
      "Stochastic Gradient Descent(1306): loss=18.692354260707525\n",
      "Stochastic Gradient Descent(1307): loss=5.81743541822576\n",
      "Stochastic Gradient Descent(1308): loss=2.772035375882182\n",
      "Stochastic Gradient Descent(1309): loss=11.388112522436934\n",
      "Stochastic Gradient Descent(1310): loss=91.8251767426642\n",
      "Stochastic Gradient Descent(1311): loss=8.602577693207136\n",
      "Stochastic Gradient Descent(1312): loss=46.418823660133754\n",
      "Stochastic Gradient Descent(1313): loss=1880.8360549222741\n",
      "Stochastic Gradient Descent(1314): loss=18.85005763909709\n",
      "Stochastic Gradient Descent(1315): loss=1.403102482968557\n",
      "Stochastic Gradient Descent(1316): loss=338.4023062744181\n",
      "Stochastic Gradient Descent(1317): loss=155.21106699826882\n",
      "Stochastic Gradient Descent(1318): loss=48.290515910446025\n",
      "Stochastic Gradient Descent(1319): loss=26.5355985105968\n",
      "Stochastic Gradient Descent(1320): loss=0.003960404591038125\n",
      "Stochastic Gradient Descent(1321): loss=39.38846758806292\n",
      "Stochastic Gradient Descent(1322): loss=24.930404337254412\n",
      "Stochastic Gradient Descent(1323): loss=6.164399264274673\n",
      "Stochastic Gradient Descent(1324): loss=96.84783046841343\n",
      "Stochastic Gradient Descent(1325): loss=690.8902591927092\n",
      "Stochastic Gradient Descent(1326): loss=243.17512314951352\n",
      "Stochastic Gradient Descent(1327): loss=0.49769395870668515\n",
      "Stochastic Gradient Descent(1328): loss=125.16043916702075\n",
      "Stochastic Gradient Descent(1329): loss=0.23913926979777658\n",
      "Stochastic Gradient Descent(1330): loss=29.69405108815335\n",
      "Stochastic Gradient Descent(1331): loss=227.84665037507227\n",
      "Stochastic Gradient Descent(1332): loss=132.40571899393447\n",
      "Stochastic Gradient Descent(1333): loss=5.118290917146173\n",
      "Stochastic Gradient Descent(1334): loss=33.69707988303361\n",
      "Stochastic Gradient Descent(1335): loss=65.50489860493744\n",
      "Stochastic Gradient Descent(1336): loss=528.1708152199392\n",
      "Stochastic Gradient Descent(1337): loss=50.73329745935945\n",
      "Stochastic Gradient Descent(1338): loss=5.077635400761935\n",
      "Stochastic Gradient Descent(1339): loss=21.730644362864112\n",
      "Stochastic Gradient Descent(1340): loss=110.57883668861083\n",
      "Stochastic Gradient Descent(1341): loss=1711.7665156003777\n",
      "Stochastic Gradient Descent(1342): loss=2493.8809128542507\n",
      "Stochastic Gradient Descent(1343): loss=56.97685346727421\n",
      "Stochastic Gradient Descent(1344): loss=2.2448279142641754\n",
      "Stochastic Gradient Descent(1345): loss=61.85303333688265\n",
      "Stochastic Gradient Descent(1346): loss=28.331691610332804\n",
      "Stochastic Gradient Descent(1347): loss=19.22669418254846\n",
      "Stochastic Gradient Descent(1348): loss=0.09453347398604185\n",
      "Stochastic Gradient Descent(1349): loss=2621.032975799291\n",
      "Stochastic Gradient Descent(1350): loss=1.1019994745901212\n",
      "Stochastic Gradient Descent(1351): loss=133.43207123482014\n",
      "Stochastic Gradient Descent(1352): loss=163.08109093451583\n",
      "Stochastic Gradient Descent(1353): loss=355.7577648132968\n",
      "Stochastic Gradient Descent(1354): loss=168.88658440049204\n",
      "Stochastic Gradient Descent(1355): loss=153.1044370199998\n",
      "Stochastic Gradient Descent(1356): loss=2.438225859303703\n",
      "Stochastic Gradient Descent(1357): loss=259.4815338456753\n",
      "Stochastic Gradient Descent(1358): loss=313.58968707717116\n",
      "Stochastic Gradient Descent(1359): loss=132.2525252812679\n",
      "Stochastic Gradient Descent(1360): loss=105.68539413720208\n",
      "Stochastic Gradient Descent(1361): loss=11.466051888813002\n",
      "Stochastic Gradient Descent(1362): loss=80.48960772034904\n",
      "Stochastic Gradient Descent(1363): loss=34.683196753498265\n",
      "Stochastic Gradient Descent(1364): loss=172.49201180078208\n",
      "Stochastic Gradient Descent(1365): loss=0.004133308196233159\n",
      "Stochastic Gradient Descent(1366): loss=279.5727292599249\n",
      "Stochastic Gradient Descent(1367): loss=0.3882284285989237\n",
      "Stochastic Gradient Descent(1368): loss=2.7313118986790026\n",
      "Stochastic Gradient Descent(1369): loss=0.2042460920807247\n",
      "Stochastic Gradient Descent(1370): loss=103.72925852216076\n",
      "Stochastic Gradient Descent(1371): loss=2.7081926772173355\n",
      "Stochastic Gradient Descent(1372): loss=102.01904024120765\n",
      "Stochastic Gradient Descent(1373): loss=418.2044379354851\n",
      "Stochastic Gradient Descent(1374): loss=212.80422902024623\n",
      "Stochastic Gradient Descent(1375): loss=17.94554886786419\n",
      "Stochastic Gradient Descent(1376): loss=9.05017902662558\n",
      "Stochastic Gradient Descent(1377): loss=0.9234499090644421\n",
      "Stochastic Gradient Descent(1378): loss=18.88222207901285\n",
      "Stochastic Gradient Descent(1379): loss=2.8823742499827336\n",
      "Stochastic Gradient Descent(1380): loss=15.370915442295862\n",
      "Stochastic Gradient Descent(1381): loss=28.228470403719523\n",
      "Stochastic Gradient Descent(1382): loss=171.43806905534854\n",
      "Stochastic Gradient Descent(1383): loss=5.389469229658759\n",
      "Stochastic Gradient Descent(1384): loss=25.81704794409138\n",
      "Stochastic Gradient Descent(1385): loss=71.37795247201076\n",
      "Stochastic Gradient Descent(1386): loss=27.655510830840345\n",
      "Stochastic Gradient Descent(1387): loss=39.34715336527473\n",
      "Stochastic Gradient Descent(1388): loss=227.85650540843196\n",
      "Stochastic Gradient Descent(1389): loss=644.6142430970168\n",
      "Stochastic Gradient Descent(1390): loss=45.295002407188555\n",
      "Stochastic Gradient Descent(1391): loss=207.4965610645961\n",
      "Stochastic Gradient Descent(1392): loss=5.504939494309595\n",
      "Stochastic Gradient Descent(1393): loss=210.30940354481382\n",
      "Stochastic Gradient Descent(1394): loss=72.75547980650593\n",
      "Stochastic Gradient Descent(1395): loss=7.553479758430143\n",
      "Stochastic Gradient Descent(1396): loss=0.8621298416852194\n",
      "Stochastic Gradient Descent(1397): loss=205.86041762616608\n",
      "Stochastic Gradient Descent(1398): loss=0.006367268452279706\n",
      "Stochastic Gradient Descent(1399): loss=275.00292335735713\n",
      "Stochastic Gradient Descent(1400): loss=359.267131874966\n",
      "Stochastic Gradient Descent(1401): loss=7.182211045904528\n",
      "Stochastic Gradient Descent(1402): loss=71.1793275664326\n",
      "Stochastic Gradient Descent(1403): loss=74.82510723535943\n",
      "Stochastic Gradient Descent(1404): loss=48.46898459871235\n",
      "Stochastic Gradient Descent(1405): loss=6.953946375006734\n",
      "Stochastic Gradient Descent(1406): loss=341.5021556443214\n",
      "Stochastic Gradient Descent(1407): loss=2.1882906394391983\n",
      "Stochastic Gradient Descent(1408): loss=0.975155940636252\n",
      "Stochastic Gradient Descent(1409): loss=95.98281817627232\n",
      "Stochastic Gradient Descent(1410): loss=1935.3224065798365\n",
      "Stochastic Gradient Descent(1411): loss=750.4546780357742\n",
      "Stochastic Gradient Descent(1412): loss=6.251249917109633\n",
      "Stochastic Gradient Descent(1413): loss=235.2750926631092\n",
      "Stochastic Gradient Descent(1414): loss=105.41806190603063\n",
      "Stochastic Gradient Descent(1415): loss=2.6593138638416347\n",
      "Stochastic Gradient Descent(1416): loss=13.224233440631657\n",
      "Stochastic Gradient Descent(1417): loss=0.019070818607373547\n",
      "Stochastic Gradient Descent(1418): loss=70.66867851873316\n",
      "Stochastic Gradient Descent(1419): loss=3.72150323237503\n",
      "Stochastic Gradient Descent(1420): loss=54.050656336130196\n",
      "Stochastic Gradient Descent(1421): loss=11.522154387245333\n",
      "Stochastic Gradient Descent(1422): loss=242.62669546872618\n",
      "Stochastic Gradient Descent(1423): loss=105.7796264174019\n",
      "Stochastic Gradient Descent(1424): loss=104.38233370840034\n",
      "Stochastic Gradient Descent(1425): loss=70.59768174071257\n",
      "Stochastic Gradient Descent(1426): loss=48.05503435351897\n",
      "Stochastic Gradient Descent(1427): loss=48.29620445952642\n",
      "Stochastic Gradient Descent(1428): loss=281.83061429667436\n",
      "Stochastic Gradient Descent(1429): loss=15.639811792160375\n",
      "Stochastic Gradient Descent(1430): loss=90.09042882091623\n",
      "Stochastic Gradient Descent(1431): loss=10.920793224254641\n",
      "Stochastic Gradient Descent(1432): loss=125.2309891164567\n",
      "Stochastic Gradient Descent(1433): loss=84.7304313630134\n",
      "Stochastic Gradient Descent(1434): loss=208.2150847502112\n",
      "Stochastic Gradient Descent(1435): loss=552.5015462966235\n",
      "Stochastic Gradient Descent(1436): loss=169.79670914665272\n",
      "Stochastic Gradient Descent(1437): loss=4.069286543287588\n",
      "Stochastic Gradient Descent(1438): loss=1667.5978035799187\n",
      "Stochastic Gradient Descent(1439): loss=1116.9291951269033\n",
      "Stochastic Gradient Descent(1440): loss=74.65314930081925\n",
      "Stochastic Gradient Descent(1441): loss=9.802459086700415\n",
      "Stochastic Gradient Descent(1442): loss=22.822039652629087\n",
      "Stochastic Gradient Descent(1443): loss=78.3973907569134\n",
      "Stochastic Gradient Descent(1444): loss=0.18360893014554955\n",
      "Stochastic Gradient Descent(1445): loss=395.9246687590862\n",
      "Stochastic Gradient Descent(1446): loss=211.38699608601175\n",
      "Stochastic Gradient Descent(1447): loss=92.14914492910057\n",
      "Stochastic Gradient Descent(1448): loss=0.1774088670855424\n",
      "Stochastic Gradient Descent(1449): loss=0.15206941891400266\n",
      "Stochastic Gradient Descent(1450): loss=14.232914040633547\n",
      "Stochastic Gradient Descent(1451): loss=10.0834675849096\n",
      "Stochastic Gradient Descent(1452): loss=1702.5326299503133\n",
      "Stochastic Gradient Descent(1453): loss=1531.616708304744\n",
      "Stochastic Gradient Descent(1454): loss=76.92961083930562\n",
      "Stochastic Gradient Descent(1455): loss=45.194871768943244\n",
      "Stochastic Gradient Descent(1456): loss=158.1196479448252\n",
      "Stochastic Gradient Descent(1457): loss=4.241133351531616\n",
      "Stochastic Gradient Descent(1458): loss=312.70106927348195\n",
      "Stochastic Gradient Descent(1459): loss=82.85203191769752\n",
      "Stochastic Gradient Descent(1460): loss=260.0899001969379\n",
      "Stochastic Gradient Descent(1461): loss=538.4729503471643\n",
      "Stochastic Gradient Descent(1462): loss=69.47756883268644\n",
      "Stochastic Gradient Descent(1463): loss=110.2783109062007\n",
      "Stochastic Gradient Descent(1464): loss=208.18317730936732\n",
      "Stochastic Gradient Descent(1465): loss=125.19250072209783\n",
      "Stochastic Gradient Descent(1466): loss=28.635401719260788\n",
      "Stochastic Gradient Descent(1467): loss=249.9493412422791\n",
      "Stochastic Gradient Descent(1468): loss=343.01146364590073\n",
      "Stochastic Gradient Descent(1469): loss=44.21994813422719\n",
      "Stochastic Gradient Descent(1470): loss=49.199021847607455\n",
      "Stochastic Gradient Descent(1471): loss=7.268556286644462\n",
      "Stochastic Gradient Descent(1472): loss=9.47838293140648\n",
      "Stochastic Gradient Descent(1473): loss=763.1282839905467\n",
      "Stochastic Gradient Descent(1474): loss=1.1845648250935343\n",
      "Stochastic Gradient Descent(1475): loss=1027.8981751396764\n",
      "Stochastic Gradient Descent(1476): loss=949.2634799716096\n",
      "Stochastic Gradient Descent(1477): loss=5.872078682226639\n",
      "Stochastic Gradient Descent(1478): loss=619.9631633394013\n",
      "Stochastic Gradient Descent(1479): loss=106.4137118694893\n",
      "Stochastic Gradient Descent(1480): loss=341.24409151814683\n",
      "Stochastic Gradient Descent(1481): loss=0.31176419563508617\n",
      "Stochastic Gradient Descent(1482): loss=103.14005267848029\n",
      "Stochastic Gradient Descent(1483): loss=44.69958895477138\n",
      "Stochastic Gradient Descent(1484): loss=67.69009241919409\n",
      "Stochastic Gradient Descent(1485): loss=791.1578261715306\n",
      "Stochastic Gradient Descent(1486): loss=211.39461872342412\n",
      "Stochastic Gradient Descent(1487): loss=101.42993084636528\n",
      "Stochastic Gradient Descent(1488): loss=75.03925012595751\n",
      "Stochastic Gradient Descent(1489): loss=5999.172024115429\n",
      "Stochastic Gradient Descent(1490): loss=2202.5358225255923\n",
      "Stochastic Gradient Descent(1491): loss=2463.6652037850695\n",
      "Stochastic Gradient Descent(1492): loss=1522.6142346086622\n",
      "Stochastic Gradient Descent(1493): loss=186.89554698386704\n",
      "Stochastic Gradient Descent(1494): loss=14.54894768520754\n",
      "Stochastic Gradient Descent(1495): loss=297.5456148860414\n",
      "Stochastic Gradient Descent(1496): loss=10.818846703513596\n",
      "Stochastic Gradient Descent(1497): loss=6.622083778071738\n",
      "Stochastic Gradient Descent(1498): loss=50.49493011231678\n",
      "Stochastic Gradient Descent(1499): loss=409.84655358549844\n",
      "Stochastic Gradient Descent(1500): loss=13.684572741347402\n",
      "Stochastic Gradient Descent(1501): loss=5.827779679320661\n",
      "Stochastic Gradient Descent(1502): loss=585.0827710140062\n",
      "Stochastic Gradient Descent(1503): loss=453.9594506796981\n",
      "Stochastic Gradient Descent(1504): loss=247.54538528426787\n",
      "Stochastic Gradient Descent(1505): loss=15.694034917598932\n",
      "Stochastic Gradient Descent(1506): loss=23.030699271095962\n",
      "Stochastic Gradient Descent(1507): loss=370.415916269699\n",
      "Stochastic Gradient Descent(1508): loss=285.0937609171454\n",
      "Stochastic Gradient Descent(1509): loss=181.9148058252657\n",
      "Stochastic Gradient Descent(1510): loss=0.18990890646376588\n",
      "Stochastic Gradient Descent(1511): loss=6.350548525337677\n",
      "Stochastic Gradient Descent(1512): loss=1.3299184226570517\n",
      "Stochastic Gradient Descent(1513): loss=9.54916764358662\n",
      "Stochastic Gradient Descent(1514): loss=38.30821951768556\n",
      "Stochastic Gradient Descent(1515): loss=0.0776228016164004\n",
      "Stochastic Gradient Descent(1516): loss=15.938378902947623\n",
      "Stochastic Gradient Descent(1517): loss=46.62836655115858\n",
      "Stochastic Gradient Descent(1518): loss=5.795460119537754\n",
      "Stochastic Gradient Descent(1519): loss=1469.1489398605647\n",
      "Stochastic Gradient Descent(1520): loss=86.95391440831533\n",
      "Stochastic Gradient Descent(1521): loss=40.09876166227006\n",
      "Stochastic Gradient Descent(1522): loss=2.5549365933989545\n",
      "Stochastic Gradient Descent(1523): loss=0.5912375963641222\n",
      "Stochastic Gradient Descent(1524): loss=15.40689922503111\n",
      "Stochastic Gradient Descent(1525): loss=1349.832751567491\n",
      "Stochastic Gradient Descent(1526): loss=148.96755735101863\n",
      "Stochastic Gradient Descent(1527): loss=256.6087717339569\n",
      "Stochastic Gradient Descent(1528): loss=91.48234528323997\n",
      "Stochastic Gradient Descent(1529): loss=0.008185280505907033\n",
      "Stochastic Gradient Descent(1530): loss=104.13210766934895\n",
      "Stochastic Gradient Descent(1531): loss=7.533860518987461\n",
      "Stochastic Gradient Descent(1532): loss=39.66350248578237\n",
      "Stochastic Gradient Descent(1533): loss=98.71591541533284\n",
      "Stochastic Gradient Descent(1534): loss=150.65066427918745\n",
      "Stochastic Gradient Descent(1535): loss=6.719557959739797\n",
      "Stochastic Gradient Descent(1536): loss=1396.6276516263051\n",
      "Stochastic Gradient Descent(1537): loss=632.132388392431\n",
      "Stochastic Gradient Descent(1538): loss=871.4194403914821\n",
      "Stochastic Gradient Descent(1539): loss=229.8009404064535\n",
      "Stochastic Gradient Descent(1540): loss=74.4593635676719\n",
      "Stochastic Gradient Descent(1541): loss=109.06054983797215\n",
      "Stochastic Gradient Descent(1542): loss=13.408215597096564\n",
      "Stochastic Gradient Descent(1543): loss=106.53201272505791\n",
      "Stochastic Gradient Descent(1544): loss=35.65786369875611\n",
      "Stochastic Gradient Descent(1545): loss=47.9368714531569\n",
      "Stochastic Gradient Descent(1546): loss=0.8604768626740573\n",
      "Stochastic Gradient Descent(1547): loss=244.88898187903004\n",
      "Stochastic Gradient Descent(1548): loss=173.43478718038935\n",
      "Stochastic Gradient Descent(1549): loss=124.09434874895865\n",
      "Stochastic Gradient Descent(1550): loss=79.59195854207634\n",
      "Stochastic Gradient Descent(1551): loss=710.4628490444384\n",
      "Stochastic Gradient Descent(1552): loss=818.9391293770439\n",
      "Stochastic Gradient Descent(1553): loss=72.51783940617155\n",
      "Stochastic Gradient Descent(1554): loss=12.854865821281871\n",
      "Stochastic Gradient Descent(1555): loss=66.57597728451913\n",
      "Stochastic Gradient Descent(1556): loss=271.1977475263911\n",
      "Stochastic Gradient Descent(1557): loss=1833.5019722490595\n",
      "Stochastic Gradient Descent(1558): loss=47.86835682117443\n",
      "Stochastic Gradient Descent(1559): loss=150.54313114807528\n",
      "Stochastic Gradient Descent(1560): loss=122.67811737143728\n",
      "Stochastic Gradient Descent(1561): loss=212.7471138687552\n",
      "Stochastic Gradient Descent(1562): loss=39.29017279134991\n",
      "Stochastic Gradient Descent(1563): loss=4.415312399154316\n",
      "Stochastic Gradient Descent(1564): loss=19.533381305221603\n",
      "Stochastic Gradient Descent(1565): loss=62.857003225145036\n",
      "Stochastic Gradient Descent(1566): loss=72.83027811934417\n",
      "Stochastic Gradient Descent(1567): loss=998.1416277025975\n",
      "Stochastic Gradient Descent(1568): loss=198.99214443872887\n",
      "Stochastic Gradient Descent(1569): loss=3842.5453291607064\n",
      "Stochastic Gradient Descent(1570): loss=61.44201957378522\n",
      "Stochastic Gradient Descent(1571): loss=116.81732842888154\n",
      "Stochastic Gradient Descent(1572): loss=42.101995250071525\n",
      "Stochastic Gradient Descent(1573): loss=109.3248821848895\n",
      "Stochastic Gradient Descent(1574): loss=2.8531962993755196\n",
      "Stochastic Gradient Descent(1575): loss=4.898644831690323\n",
      "Stochastic Gradient Descent(1576): loss=0.007099617282786901\n",
      "Stochastic Gradient Descent(1577): loss=5.454811507715778\n",
      "Stochastic Gradient Descent(1578): loss=70.1199687813855\n",
      "Stochastic Gradient Descent(1579): loss=175.88595917394966\n",
      "Stochastic Gradient Descent(1580): loss=54.13476309984987\n",
      "Stochastic Gradient Descent(1581): loss=20.230940972959726\n",
      "Stochastic Gradient Descent(1582): loss=19.99894588957766\n",
      "Stochastic Gradient Descent(1583): loss=127.26584996484632\n",
      "Stochastic Gradient Descent(1584): loss=725.7274940714473\n",
      "Stochastic Gradient Descent(1585): loss=20.97213414369353\n",
      "Stochastic Gradient Descent(1586): loss=254.69432492258662\n",
      "Stochastic Gradient Descent(1587): loss=48.9124202929693\n",
      "Stochastic Gradient Descent(1588): loss=51.71831052703727\n",
      "Stochastic Gradient Descent(1589): loss=42.04704367283482\n",
      "Stochastic Gradient Descent(1590): loss=84.63679132586618\n",
      "Stochastic Gradient Descent(1591): loss=52.38766454577154\n",
      "Stochastic Gradient Descent(1592): loss=6.727918086293741\n",
      "Stochastic Gradient Descent(1593): loss=9.856153463099943\n",
      "Stochastic Gradient Descent(1594): loss=179.74727452615414\n",
      "Stochastic Gradient Descent(1595): loss=637.414956534943\n",
      "Stochastic Gradient Descent(1596): loss=281.44265844819506\n",
      "Stochastic Gradient Descent(1597): loss=1541.1480049428064\n",
      "Stochastic Gradient Descent(1598): loss=40.06927058345289\n",
      "Stochastic Gradient Descent(1599): loss=21.401299035783648\n",
      "Stochastic Gradient Descent(1600): loss=144.1081492378246\n",
      "Stochastic Gradient Descent(1601): loss=4.856001462646783\n",
      "Stochastic Gradient Descent(1602): loss=303.47206829356816\n",
      "Stochastic Gradient Descent(1603): loss=12.04121211242723\n",
      "Stochastic Gradient Descent(1604): loss=234.84568655745963\n",
      "Stochastic Gradient Descent(1605): loss=7.372916839493743\n",
      "Stochastic Gradient Descent(1606): loss=20.336171876540444\n",
      "Stochastic Gradient Descent(1607): loss=312.7424612862793\n",
      "Stochastic Gradient Descent(1608): loss=416.13968760833166\n",
      "Stochastic Gradient Descent(1609): loss=173.1341616520525\n",
      "Stochastic Gradient Descent(1610): loss=212.24424207928416\n",
      "Stochastic Gradient Descent(1611): loss=197.96685751448783\n",
      "Stochastic Gradient Descent(1612): loss=29.478365618025016\n",
      "Stochastic Gradient Descent(1613): loss=140.04555343271227\n",
      "Stochastic Gradient Descent(1614): loss=96.74545702055173\n",
      "Stochastic Gradient Descent(1615): loss=938.4779735037706\n",
      "Stochastic Gradient Descent(1616): loss=208.0267704105768\n",
      "Stochastic Gradient Descent(1617): loss=0.5793914241564393\n",
      "Stochastic Gradient Descent(1618): loss=24.239922081511274\n",
      "Stochastic Gradient Descent(1619): loss=6.8702801818554375\n",
      "Stochastic Gradient Descent(1620): loss=3.409036503548159\n",
      "Stochastic Gradient Descent(1621): loss=232.06492070771552\n",
      "Stochastic Gradient Descent(1622): loss=0.5651027405839034\n",
      "Stochastic Gradient Descent(1623): loss=18.601719152077262\n",
      "Stochastic Gradient Descent(1624): loss=176.89895073251878\n",
      "Stochastic Gradient Descent(1625): loss=31.21753337656539\n",
      "Stochastic Gradient Descent(1626): loss=0.37870337214259664\n",
      "Stochastic Gradient Descent(1627): loss=14.562366738976005\n",
      "Stochastic Gradient Descent(1628): loss=32.41480197510034\n",
      "Stochastic Gradient Descent(1629): loss=2.2161292913177357\n",
      "Stochastic Gradient Descent(1630): loss=0.5247737776087404\n",
      "Stochastic Gradient Descent(1631): loss=101.26549987097634\n",
      "Stochastic Gradient Descent(1632): loss=222.80796753994693\n",
      "Stochastic Gradient Descent(1633): loss=81.33455610436198\n",
      "Stochastic Gradient Descent(1634): loss=57.94422907741148\n",
      "Stochastic Gradient Descent(1635): loss=3.2367583813691727\n",
      "Stochastic Gradient Descent(1636): loss=0.009370964890594571\n",
      "Stochastic Gradient Descent(1637): loss=108.19376398757949\n",
      "Stochastic Gradient Descent(1638): loss=660.3998494298284\n",
      "Stochastic Gradient Descent(1639): loss=5.648059412836948\n",
      "Stochastic Gradient Descent(1640): loss=40.670977440258355\n",
      "Stochastic Gradient Descent(1641): loss=41.56377020629073\n",
      "Stochastic Gradient Descent(1642): loss=61.365251679152195\n",
      "Stochastic Gradient Descent(1643): loss=31.258522376344228\n",
      "Stochastic Gradient Descent(1644): loss=914.3165333363472\n",
      "Stochastic Gradient Descent(1645): loss=2.5091085384024363\n",
      "Stochastic Gradient Descent(1646): loss=5.493327582158796\n",
      "Stochastic Gradient Descent(1647): loss=0.055603817828156606\n",
      "Stochastic Gradient Descent(1648): loss=964.5882656718723\n",
      "Stochastic Gradient Descent(1649): loss=0.5661983268407149\n",
      "Stochastic Gradient Descent(1650): loss=6.734414816301772\n",
      "Stochastic Gradient Descent(1651): loss=380.39687334098835\n",
      "Stochastic Gradient Descent(1652): loss=127.28698574143394\n",
      "Stochastic Gradient Descent(1653): loss=268.97578134007574\n",
      "Stochastic Gradient Descent(1654): loss=13.886816119549021\n",
      "Stochastic Gradient Descent(1655): loss=7.10537899724284\n",
      "Stochastic Gradient Descent(1656): loss=77.14336692317143\n",
      "Stochastic Gradient Descent(1657): loss=139.86948787633284\n",
      "Stochastic Gradient Descent(1658): loss=24.41922751114771\n",
      "Stochastic Gradient Descent(1659): loss=291.7987078202631\n",
      "Stochastic Gradient Descent(1660): loss=350.78270812296756\n",
      "Stochastic Gradient Descent(1661): loss=91.10499687551604\n",
      "Stochastic Gradient Descent(1662): loss=156.07209737487418\n",
      "Stochastic Gradient Descent(1663): loss=38.140338428116046\n",
      "Stochastic Gradient Descent(1664): loss=11.876466681063766\n",
      "Stochastic Gradient Descent(1665): loss=15.353162761759988\n",
      "Stochastic Gradient Descent(1666): loss=19.78890446273359\n",
      "Stochastic Gradient Descent(1667): loss=12.5050168449596\n",
      "Stochastic Gradient Descent(1668): loss=12.916299863321068\n",
      "Stochastic Gradient Descent(1669): loss=78.81028652701664\n",
      "Stochastic Gradient Descent(1670): loss=46.66976828168023\n",
      "Stochastic Gradient Descent(1671): loss=23.09205983490798\n",
      "Stochastic Gradient Descent(1672): loss=582.1543209280549\n",
      "Stochastic Gradient Descent(1673): loss=295.57694355482624\n",
      "Stochastic Gradient Descent(1674): loss=93.93869425374959\n",
      "Stochastic Gradient Descent(1675): loss=7.143181968704731\n",
      "Stochastic Gradient Descent(1676): loss=248.7338806238424\n",
      "Stochastic Gradient Descent(1677): loss=19.163086653197936\n",
      "Stochastic Gradient Descent(1678): loss=231.67062328999725\n",
      "Stochastic Gradient Descent(1679): loss=32.89965390638067\n",
      "Stochastic Gradient Descent(1680): loss=107.8893350547539\n",
      "Stochastic Gradient Descent(1681): loss=28.299746611834454\n",
      "Stochastic Gradient Descent(1682): loss=3.143934715247698\n",
      "Stochastic Gradient Descent(1683): loss=5.679064214125964\n",
      "Stochastic Gradient Descent(1684): loss=65.15555556083228\n",
      "Stochastic Gradient Descent(1685): loss=310.0937571959337\n",
      "Stochastic Gradient Descent(1686): loss=166.1896137135604\n",
      "Stochastic Gradient Descent(1687): loss=94.62165263590244\n",
      "Stochastic Gradient Descent(1688): loss=43.70459983065667\n",
      "Stochastic Gradient Descent(1689): loss=8.678688351989653\n",
      "Stochastic Gradient Descent(1690): loss=97.72281441073355\n",
      "Stochastic Gradient Descent(1691): loss=264.75093779695584\n",
      "Stochastic Gradient Descent(1692): loss=10.349200328560642\n",
      "Stochastic Gradient Descent(1693): loss=0.013377644077303399\n",
      "Stochastic Gradient Descent(1694): loss=25.0980982857151\n",
      "Stochastic Gradient Descent(1695): loss=0.0015772950874624108\n",
      "Stochastic Gradient Descent(1696): loss=220.7247752089332\n",
      "Stochastic Gradient Descent(1697): loss=108.02542859827776\n",
      "Stochastic Gradient Descent(1698): loss=1390.7006959047321\n",
      "Stochastic Gradient Descent(1699): loss=0.0038513299226672726\n",
      "Stochastic Gradient Descent(1700): loss=1.3328148422238193\n",
      "Stochastic Gradient Descent(1701): loss=63.577459508758714\n",
      "Stochastic Gradient Descent(1702): loss=990.9063179560568\n",
      "Stochastic Gradient Descent(1703): loss=1438.183526994353\n",
      "Stochastic Gradient Descent(1704): loss=129.7059783358088\n",
      "Stochastic Gradient Descent(1705): loss=77.47541852888813\n",
      "Stochastic Gradient Descent(1706): loss=12.674119385346337\n",
      "Stochastic Gradient Descent(1707): loss=198.08821624315337\n",
      "Stochastic Gradient Descent(1708): loss=724.554059397761\n",
      "Stochastic Gradient Descent(1709): loss=322.68300511608766\n",
      "Stochastic Gradient Descent(1710): loss=34.51888679336727\n",
      "Stochastic Gradient Descent(1711): loss=201.90549068510413\n",
      "Stochastic Gradient Descent(1712): loss=133.74229506096708\n",
      "Stochastic Gradient Descent(1713): loss=4.132619685334801\n",
      "Stochastic Gradient Descent(1714): loss=6.034547600515326\n",
      "Stochastic Gradient Descent(1715): loss=87.10147086586103\n",
      "Stochastic Gradient Descent(1716): loss=24.982675016619833\n",
      "Stochastic Gradient Descent(1717): loss=1.8329896848710567\n",
      "Stochastic Gradient Descent(1718): loss=6.207999339130654\n",
      "Stochastic Gradient Descent(1719): loss=61.80146797913075\n",
      "Stochastic Gradient Descent(1720): loss=349.6849294408475\n",
      "Stochastic Gradient Descent(1721): loss=79.13488827914827\n",
      "Stochastic Gradient Descent(1722): loss=3.616379772660176\n",
      "Stochastic Gradient Descent(1723): loss=53.432510647794615\n",
      "Stochastic Gradient Descent(1724): loss=2.506260294857084\n",
      "Stochastic Gradient Descent(1725): loss=38.67075371489046\n",
      "Stochastic Gradient Descent(1726): loss=15.627608676142916\n",
      "Stochastic Gradient Descent(1727): loss=28.98709031987151\n",
      "Stochastic Gradient Descent(1728): loss=0.3873033785370658\n",
      "Stochastic Gradient Descent(1729): loss=0.22468617167313668\n",
      "Stochastic Gradient Descent(1730): loss=19.652630814520112\n",
      "Stochastic Gradient Descent(1731): loss=9.327129271150264\n",
      "Stochastic Gradient Descent(1732): loss=19.083785141901057\n",
      "Stochastic Gradient Descent(1733): loss=4.152716623157316\n",
      "Stochastic Gradient Descent(1734): loss=57.12384412253825\n",
      "Stochastic Gradient Descent(1735): loss=8.008157836569309\n",
      "Stochastic Gradient Descent(1736): loss=478.8588830713327\n",
      "Stochastic Gradient Descent(1737): loss=287.9162747254897\n",
      "Stochastic Gradient Descent(1738): loss=15.083256932696695\n",
      "Stochastic Gradient Descent(1739): loss=22.462309033970968\n",
      "Stochastic Gradient Descent(1740): loss=57.45600600182821\n",
      "Stochastic Gradient Descent(1741): loss=754.3249576639129\n",
      "Stochastic Gradient Descent(1742): loss=15.540549393966069\n",
      "Stochastic Gradient Descent(1743): loss=113.48548842830822\n",
      "Stochastic Gradient Descent(1744): loss=7.733823917288036\n",
      "Stochastic Gradient Descent(1745): loss=112.01551442242705\n",
      "Stochastic Gradient Descent(1746): loss=61.90367580127516\n",
      "Stochastic Gradient Descent(1747): loss=28.455568816507053\n",
      "Stochastic Gradient Descent(1748): loss=1559.9065491290755\n",
      "Stochastic Gradient Descent(1749): loss=15.859351723589844\n",
      "Stochastic Gradient Descent(1750): loss=247.854293334532\n",
      "Stochastic Gradient Descent(1751): loss=42.70357604891021\n",
      "Stochastic Gradient Descent(1752): loss=0.15818958548316098\n",
      "Stochastic Gradient Descent(1753): loss=37.30582276003338\n",
      "Stochastic Gradient Descent(1754): loss=2.354034984598137\n",
      "Stochastic Gradient Descent(1755): loss=195.0509516331438\n",
      "Stochastic Gradient Descent(1756): loss=9.014470150872993\n",
      "Stochastic Gradient Descent(1757): loss=0.00012922207501744984\n",
      "Stochastic Gradient Descent(1758): loss=0.09339533581839006\n",
      "Stochastic Gradient Descent(1759): loss=20.892546728525907\n",
      "Stochastic Gradient Descent(1760): loss=41.28826623062045\n",
      "Stochastic Gradient Descent(1761): loss=0.8693689900294613\n",
      "Stochastic Gradient Descent(1762): loss=28.85800537181393\n",
      "Stochastic Gradient Descent(1763): loss=0.8178839651416305\n",
      "Stochastic Gradient Descent(1764): loss=103.41529401833114\n",
      "Stochastic Gradient Descent(1765): loss=17.81536780094575\n",
      "Stochastic Gradient Descent(1766): loss=74.5177079401856\n",
      "Stochastic Gradient Descent(1767): loss=3.9537305734465176\n",
      "Stochastic Gradient Descent(1768): loss=22.88575971031258\n",
      "Stochastic Gradient Descent(1769): loss=12.662319371811577\n",
      "Stochastic Gradient Descent(1770): loss=113.95740299001812\n",
      "Stochastic Gradient Descent(1771): loss=73.08698546402742\n",
      "Stochastic Gradient Descent(1772): loss=258.3397972380692\n",
      "Stochastic Gradient Descent(1773): loss=12.31621927561548\n",
      "Stochastic Gradient Descent(1774): loss=157.6124290266004\n",
      "Stochastic Gradient Descent(1775): loss=121.08829368794412\n",
      "Stochastic Gradient Descent(1776): loss=0.011339900261239327\n",
      "Stochastic Gradient Descent(1777): loss=129.686124067843\n",
      "Stochastic Gradient Descent(1778): loss=73.5987787748806\n",
      "Stochastic Gradient Descent(1779): loss=34.24747216860045\n",
      "Stochastic Gradient Descent(1780): loss=21.823575339922854\n",
      "Stochastic Gradient Descent(1781): loss=264.6489992897969\n",
      "Stochastic Gradient Descent(1782): loss=128.55998196714242\n",
      "Stochastic Gradient Descent(1783): loss=2.6515833372884514\n",
      "Stochastic Gradient Descent(1784): loss=3.615120471442578\n",
      "Stochastic Gradient Descent(1785): loss=78.60171261291266\n",
      "Stochastic Gradient Descent(1786): loss=627.8932753973997\n",
      "Stochastic Gradient Descent(1787): loss=302.4001941501215\n",
      "Stochastic Gradient Descent(1788): loss=153.5241584285296\n",
      "Stochastic Gradient Descent(1789): loss=1.7881912868605794\n",
      "Stochastic Gradient Descent(1790): loss=23.61892486749264\n",
      "Stochastic Gradient Descent(1791): loss=408.45564120611783\n",
      "Stochastic Gradient Descent(1792): loss=101.13260441743321\n",
      "Stochastic Gradient Descent(1793): loss=3.988368755021558\n",
      "Stochastic Gradient Descent(1794): loss=24.686311636916827\n",
      "Stochastic Gradient Descent(1795): loss=302.3162093729348\n",
      "Stochastic Gradient Descent(1796): loss=202.6776599109927\n",
      "Stochastic Gradient Descent(1797): loss=93.30814601174967\n",
      "Stochastic Gradient Descent(1798): loss=0.1436955663609242\n",
      "Stochastic Gradient Descent(1799): loss=557.9496531757277\n",
      "Stochastic Gradient Descent(1800): loss=27.94162890083021\n",
      "Stochastic Gradient Descent(1801): loss=192.102526012938\n",
      "Stochastic Gradient Descent(1802): loss=187.6004114318313\n",
      "Stochastic Gradient Descent(1803): loss=15.3127320114213\n",
      "Stochastic Gradient Descent(1804): loss=47.70811752140192\n",
      "Stochastic Gradient Descent(1805): loss=12.046570528196188\n",
      "Stochastic Gradient Descent(1806): loss=3.136867175426196\n",
      "Stochastic Gradient Descent(1807): loss=23.825081266409775\n",
      "Stochastic Gradient Descent(1808): loss=406.0296519910279\n",
      "Stochastic Gradient Descent(1809): loss=11.309975789708886\n",
      "Stochastic Gradient Descent(1810): loss=275.1691466207066\n",
      "Stochastic Gradient Descent(1811): loss=0.03694868673151976\n",
      "Stochastic Gradient Descent(1812): loss=116.39347910946653\n",
      "Stochastic Gradient Descent(1813): loss=20.312281868080895\n",
      "Stochastic Gradient Descent(1814): loss=0.15414786887749066\n",
      "Stochastic Gradient Descent(1815): loss=6.187855619910971\n",
      "Stochastic Gradient Descent(1816): loss=1.2758414285669746\n",
      "Stochastic Gradient Descent(1817): loss=11.29383979689442\n",
      "Stochastic Gradient Descent(1818): loss=61.63089049519481\n",
      "Stochastic Gradient Descent(1819): loss=112.41756517082288\n",
      "Stochastic Gradient Descent(1820): loss=410.1630981781551\n",
      "Stochastic Gradient Descent(1821): loss=5.766641507608141\n",
      "Stochastic Gradient Descent(1822): loss=1.271053298706257\n",
      "Stochastic Gradient Descent(1823): loss=0.040292732220846374\n",
      "Stochastic Gradient Descent(1824): loss=8.620421212232834\n",
      "Stochastic Gradient Descent(1825): loss=520.133437646703\n",
      "Stochastic Gradient Descent(1826): loss=24.568267556482507\n",
      "Stochastic Gradient Descent(1827): loss=429.26416191572224\n",
      "Stochastic Gradient Descent(1828): loss=369.62312489235524\n",
      "Stochastic Gradient Descent(1829): loss=169.40370002031125\n",
      "Stochastic Gradient Descent(1830): loss=5.962384855775212\n",
      "Stochastic Gradient Descent(1831): loss=61.74294456756733\n",
      "Stochastic Gradient Descent(1832): loss=27.085891809508613\n",
      "Stochastic Gradient Descent(1833): loss=100.15800430958701\n",
      "Stochastic Gradient Descent(1834): loss=0.10086940649929275\n",
      "Stochastic Gradient Descent(1835): loss=0.4951000742333841\n",
      "Stochastic Gradient Descent(1836): loss=415.4604193871122\n",
      "Stochastic Gradient Descent(1837): loss=3.343316763517232\n",
      "Stochastic Gradient Descent(1838): loss=56.8309732124515\n",
      "Stochastic Gradient Descent(1839): loss=172.19876581816447\n",
      "Stochastic Gradient Descent(1840): loss=1.4577554942752708\n",
      "Stochastic Gradient Descent(1841): loss=83.57741796933652\n",
      "Stochastic Gradient Descent(1842): loss=78.67251655631298\n",
      "Stochastic Gradient Descent(1843): loss=103.47444596005793\n",
      "Stochastic Gradient Descent(1844): loss=121.37546004424414\n",
      "Stochastic Gradient Descent(1845): loss=17.57528387453634\n",
      "Stochastic Gradient Descent(1846): loss=60.53870310193269\n",
      "Stochastic Gradient Descent(1847): loss=61.26997849972378\n",
      "Stochastic Gradient Descent(1848): loss=0.14144883153162732\n",
      "Stochastic Gradient Descent(1849): loss=3.1910598619096553\n",
      "Stochastic Gradient Descent(1850): loss=241.58207112845318\n",
      "Stochastic Gradient Descent(1851): loss=58.17601973133043\n",
      "Stochastic Gradient Descent(1852): loss=273.86257423624943\n",
      "Stochastic Gradient Descent(1853): loss=0.0012583637997742383\n",
      "Stochastic Gradient Descent(1854): loss=105.08048752504507\n",
      "Stochastic Gradient Descent(1855): loss=7.903951851759745\n",
      "Stochastic Gradient Descent(1856): loss=18.830564124783006\n",
      "Stochastic Gradient Descent(1857): loss=41.07688101734692\n",
      "Stochastic Gradient Descent(1858): loss=193.35177226901754\n",
      "Stochastic Gradient Descent(1859): loss=31.14513539054871\n",
      "Stochastic Gradient Descent(1860): loss=25.026165437471416\n",
      "Stochastic Gradient Descent(1861): loss=1.6816508424535826e-05\n",
      "Stochastic Gradient Descent(1862): loss=0.4885946579581945\n",
      "Stochastic Gradient Descent(1863): loss=0.11799790049930003\n",
      "Stochastic Gradient Descent(1864): loss=139.02233454694144\n",
      "Stochastic Gradient Descent(1865): loss=1.2867923949007634\n",
      "Stochastic Gradient Descent(1866): loss=499.8864738511113\n",
      "Stochastic Gradient Descent(1867): loss=95.73616803220818\n",
      "Stochastic Gradient Descent(1868): loss=36.929682037930824\n",
      "Stochastic Gradient Descent(1869): loss=6.726162397150571\n",
      "Stochastic Gradient Descent(1870): loss=14.049830821684083\n",
      "Stochastic Gradient Descent(1871): loss=163.8370208639535\n",
      "Stochastic Gradient Descent(1872): loss=90.20854769814707\n",
      "Stochastic Gradient Descent(1873): loss=56.10360756590643\n",
      "Stochastic Gradient Descent(1874): loss=6.195693486821293\n",
      "Stochastic Gradient Descent(1875): loss=1.576835582272355\n",
      "Stochastic Gradient Descent(1876): loss=0.802384923375619\n",
      "Stochastic Gradient Descent(1877): loss=57.15531570703936\n",
      "Stochastic Gradient Descent(1878): loss=3.7714754372706283\n",
      "Stochastic Gradient Descent(1879): loss=45.06242444863061\n",
      "Stochastic Gradient Descent(1880): loss=20.77489658621116\n",
      "Stochastic Gradient Descent(1881): loss=288.26932774599896\n",
      "Stochastic Gradient Descent(1882): loss=260.1699286621088\n",
      "Stochastic Gradient Descent(1883): loss=11.69907573074327\n",
      "Stochastic Gradient Descent(1884): loss=342.0106993176291\n",
      "Stochastic Gradient Descent(1885): loss=23.271222190852964\n",
      "Stochastic Gradient Descent(1886): loss=52.127726631344736\n",
      "Stochastic Gradient Descent(1887): loss=1.5324932359094976\n",
      "Stochastic Gradient Descent(1888): loss=15.025054602719624\n",
      "Stochastic Gradient Descent(1889): loss=40.3497667517804\n",
      "Stochastic Gradient Descent(1890): loss=0.03494213774620116\n",
      "Stochastic Gradient Descent(1891): loss=21.87559256773378\n",
      "Stochastic Gradient Descent(1892): loss=0.2543847839504394\n",
      "Stochastic Gradient Descent(1893): loss=5.64325390911915\n",
      "Stochastic Gradient Descent(1894): loss=147.48915251487435\n",
      "Stochastic Gradient Descent(1895): loss=1.2990550536329264\n",
      "Stochastic Gradient Descent(1896): loss=1.435456914265897\n",
      "Stochastic Gradient Descent(1897): loss=43.83320886202517\n",
      "Stochastic Gradient Descent(1898): loss=586.5304870260491\n",
      "Stochastic Gradient Descent(1899): loss=2.417559727120683\n",
      "Stochastic Gradient Descent(1900): loss=13.04353946715038\n",
      "Stochastic Gradient Descent(1901): loss=38.98562762426095\n",
      "Stochastic Gradient Descent(1902): loss=118.02344781375369\n",
      "Stochastic Gradient Descent(1903): loss=20.335089051894922\n",
      "Stochastic Gradient Descent(1904): loss=836.0626501220088\n",
      "Stochastic Gradient Descent(1905): loss=93.10336939934386\n",
      "Stochastic Gradient Descent(1906): loss=28.073819613533974\n",
      "Stochastic Gradient Descent(1907): loss=154.2549156937668\n",
      "Stochastic Gradient Descent(1908): loss=21.629529107126036\n",
      "Stochastic Gradient Descent(1909): loss=26.06954148158135\n",
      "Stochastic Gradient Descent(1910): loss=29.786400085483262\n",
      "Stochastic Gradient Descent(1911): loss=97.99662013725107\n",
      "Stochastic Gradient Descent(1912): loss=24.593718262221667\n",
      "Stochastic Gradient Descent(1913): loss=41.242194486976935\n",
      "Stochastic Gradient Descent(1914): loss=432.2473594841208\n",
      "Stochastic Gradient Descent(1915): loss=37.31896079395956\n",
      "Stochastic Gradient Descent(1916): loss=9.90615452225397\n",
      "Stochastic Gradient Descent(1917): loss=58.34437053057264\n",
      "Stochastic Gradient Descent(1918): loss=32.05619057055053\n",
      "Stochastic Gradient Descent(1919): loss=12.858211144006765\n",
      "Stochastic Gradient Descent(1920): loss=256.4725259478123\n",
      "Stochastic Gradient Descent(1921): loss=3.617382896776618\n",
      "Stochastic Gradient Descent(1922): loss=425.81982189024853\n",
      "Stochastic Gradient Descent(1923): loss=75.47053143432007\n",
      "Stochastic Gradient Descent(1924): loss=68.15717522033384\n",
      "Stochastic Gradient Descent(1925): loss=45.67877930085236\n",
      "Stochastic Gradient Descent(1926): loss=6.6831601686009465\n",
      "Stochastic Gradient Descent(1927): loss=9.837647531697897\n",
      "Stochastic Gradient Descent(1928): loss=0.5391789088664414\n",
      "Stochastic Gradient Descent(1929): loss=183.09270570623954\n",
      "Stochastic Gradient Descent(1930): loss=1.9392077452450052\n",
      "Stochastic Gradient Descent(1931): loss=102.57391246489127\n",
      "Stochastic Gradient Descent(1932): loss=129.35235993127125\n",
      "Stochastic Gradient Descent(1933): loss=2600.631228677881\n",
      "Stochastic Gradient Descent(1934): loss=309.9182564089629\n",
      "Stochastic Gradient Descent(1935): loss=4036.1688716262356\n",
      "Stochastic Gradient Descent(1936): loss=843.8499088660358\n",
      "Stochastic Gradient Descent(1937): loss=1.8689680460618654\n",
      "Stochastic Gradient Descent(1938): loss=49.67160573878346\n",
      "Stochastic Gradient Descent(1939): loss=0.6704048890604886\n",
      "Stochastic Gradient Descent(1940): loss=96.96094710297275\n",
      "Stochastic Gradient Descent(1941): loss=77.27983312883885\n",
      "Stochastic Gradient Descent(1942): loss=4.876478261870098\n",
      "Stochastic Gradient Descent(1943): loss=119.51121846390483\n",
      "Stochastic Gradient Descent(1944): loss=209.94280466568898\n",
      "Stochastic Gradient Descent(1945): loss=58.84793917871983\n",
      "Stochastic Gradient Descent(1946): loss=75.99506398715725\n",
      "Stochastic Gradient Descent(1947): loss=31.387707723401807\n",
      "Stochastic Gradient Descent(1948): loss=142.77453816574365\n",
      "Stochastic Gradient Descent(1949): loss=762.8151021397006\n",
      "Stochastic Gradient Descent(1950): loss=16.489344467386278\n",
      "Stochastic Gradient Descent(1951): loss=52.813866667184286\n",
      "Stochastic Gradient Descent(1952): loss=63.85775892629845\n",
      "Stochastic Gradient Descent(1953): loss=2.3571850013968385\n",
      "Stochastic Gradient Descent(1954): loss=476.79877096078144\n",
      "Stochastic Gradient Descent(1955): loss=3.9698445048349646\n",
      "Stochastic Gradient Descent(1956): loss=115.7880489675267\n",
      "Stochastic Gradient Descent(1957): loss=1.2251295017710893\n",
      "Stochastic Gradient Descent(1958): loss=1.8320364396975573\n",
      "Stochastic Gradient Descent(1959): loss=96.01494679831407\n",
      "Stochastic Gradient Descent(1960): loss=8.012098461189003\n",
      "Stochastic Gradient Descent(1961): loss=0.6672955099122833\n",
      "Stochastic Gradient Descent(1962): loss=100.32877641589768\n",
      "Stochastic Gradient Descent(1963): loss=26.072977356657393\n",
      "Stochastic Gradient Descent(1964): loss=48.65000484993056\n",
      "Stochastic Gradient Descent(1965): loss=212.27942035592028\n",
      "Stochastic Gradient Descent(1966): loss=297.5917824471499\n",
      "Stochastic Gradient Descent(1967): loss=85.29997262360281\n",
      "Stochastic Gradient Descent(1968): loss=43.92527722728493\n",
      "Stochastic Gradient Descent(1969): loss=359.1925606403685\n",
      "Stochastic Gradient Descent(1970): loss=1.0128390292568634\n",
      "Stochastic Gradient Descent(1971): loss=84.94660802102375\n",
      "Stochastic Gradient Descent(1972): loss=7.403465558467974\n",
      "Stochastic Gradient Descent(1973): loss=67.69857843872974\n",
      "Stochastic Gradient Descent(1974): loss=1358.6291304270164\n",
      "Stochastic Gradient Descent(1975): loss=225.939165092742\n",
      "Stochastic Gradient Descent(1976): loss=39.25346199937986\n",
      "Stochastic Gradient Descent(1977): loss=121.57648099003764\n",
      "Stochastic Gradient Descent(1978): loss=28.038662200178916\n",
      "Stochastic Gradient Descent(1979): loss=2.557494568630077\n",
      "Stochastic Gradient Descent(1980): loss=41.85356773603872\n",
      "Stochastic Gradient Descent(1981): loss=60.06771426876754\n",
      "Stochastic Gradient Descent(1982): loss=13.203914657245836\n",
      "Stochastic Gradient Descent(1983): loss=346.9919732137974\n",
      "Stochastic Gradient Descent(1984): loss=123.87041520486466\n",
      "Stochastic Gradient Descent(1985): loss=310.92583209685847\n",
      "Stochastic Gradient Descent(1986): loss=8.062401521366677\n",
      "Stochastic Gradient Descent(1987): loss=82.05403472682663\n",
      "Stochastic Gradient Descent(1988): loss=7.121348461786392\n",
      "Stochastic Gradient Descent(1989): loss=67.6339704963045\n",
      "Stochastic Gradient Descent(1990): loss=0.674334550170769\n",
      "Stochastic Gradient Descent(1991): loss=87.59917808963114\n",
      "Stochastic Gradient Descent(1992): loss=3.1316733111424284\n",
      "Stochastic Gradient Descent(1993): loss=0.04160895918002667\n",
      "Stochastic Gradient Descent(1994): loss=2.995439897753753\n",
      "Stochastic Gradient Descent(1995): loss=10.152231384618341\n",
      "Stochastic Gradient Descent(1996): loss=617.2042237221891\n",
      "Stochastic Gradient Descent(1997): loss=4.997197703310011\n",
      "Stochastic Gradient Descent(1998): loss=163.79279833530725\n",
      "Stochastic Gradient Descent(1999): loss=281.0593952728759\n",
      "Stochastic Gradient Descent(2000): loss=0.00022333147966966557\n",
      "Stochastic Gradient Descent(2001): loss=68.90099171400526\n",
      "Stochastic Gradient Descent(2002): loss=5.604918428820625\n",
      "Stochastic Gradient Descent(2003): loss=0.00032015768202283177\n",
      "Stochastic Gradient Descent(2004): loss=37.97067878925361\n",
      "Stochastic Gradient Descent(2005): loss=110.23786129577061\n",
      "Stochastic Gradient Descent(2006): loss=5.476677878775337\n",
      "Stochastic Gradient Descent(2007): loss=41.822619064398616\n",
      "Stochastic Gradient Descent(2008): loss=302.8878077143301\n",
      "Stochastic Gradient Descent(2009): loss=73.8019900939699\n",
      "Stochastic Gradient Descent(2010): loss=39.8718614570462\n",
      "Stochastic Gradient Descent(2011): loss=30.07934248214547\n",
      "Stochastic Gradient Descent(2012): loss=2.230252720885609\n",
      "Stochastic Gradient Descent(2013): loss=23.654748345548587\n",
      "Stochastic Gradient Descent(2014): loss=32.88400623419964\n",
      "Stochastic Gradient Descent(2015): loss=8.080890660621927\n",
      "Stochastic Gradient Descent(2016): loss=29.378019241826088\n",
      "Stochastic Gradient Descent(2017): loss=1.3945386854549415\n",
      "Stochastic Gradient Descent(2018): loss=21.29233391176486\n",
      "Stochastic Gradient Descent(2019): loss=0.5571945407776386\n",
      "Stochastic Gradient Descent(2020): loss=166.7596229145448\n",
      "Stochastic Gradient Descent(2021): loss=0.057228698634068535\n",
      "Stochastic Gradient Descent(2022): loss=461.7017432700495\n",
      "Stochastic Gradient Descent(2023): loss=1.116748854791291\n",
      "Stochastic Gradient Descent(2024): loss=162.3348707013752\n",
      "Stochastic Gradient Descent(2025): loss=24.012626252180603\n",
      "Stochastic Gradient Descent(2026): loss=68.94643895619129\n",
      "Stochastic Gradient Descent(2027): loss=102.67391753578742\n",
      "Stochastic Gradient Descent(2028): loss=58.43366835463168\n",
      "Stochastic Gradient Descent(2029): loss=35.752915796917065\n",
      "Stochastic Gradient Descent(2030): loss=21.135481114247906\n",
      "Stochastic Gradient Descent(2031): loss=9.896845337358938\n",
      "Stochastic Gradient Descent(2032): loss=44.41085071957204\n",
      "Stochastic Gradient Descent(2033): loss=11.160479197892155\n",
      "Stochastic Gradient Descent(2034): loss=13.739643625682332\n",
      "Stochastic Gradient Descent(2035): loss=0.1803695145314891\n",
      "Stochastic Gradient Descent(2036): loss=19.889107088236976\n",
      "Stochastic Gradient Descent(2037): loss=3.484167785946619\n",
      "Stochastic Gradient Descent(2038): loss=2.3455952903290105\n",
      "Stochastic Gradient Descent(2039): loss=8.516469076233872\n",
      "Stochastic Gradient Descent(2040): loss=114.0030866424783\n",
      "Stochastic Gradient Descent(2041): loss=34.28175379974621\n",
      "Stochastic Gradient Descent(2042): loss=7.320322843573037\n",
      "Stochastic Gradient Descent(2043): loss=150.04647681823798\n",
      "Stochastic Gradient Descent(2044): loss=124.2675997928511\n",
      "Stochastic Gradient Descent(2045): loss=54.35853483106646\n",
      "Stochastic Gradient Descent(2046): loss=9.047862249736301\n",
      "Stochastic Gradient Descent(2047): loss=2.431769963674563\n",
      "Stochastic Gradient Descent(2048): loss=80.54561438895065\n",
      "Stochastic Gradient Descent(2049): loss=64.81382808842837\n",
      "Stochastic Gradient Descent(2050): loss=39.949768108242\n",
      "Stochastic Gradient Descent(2051): loss=0.9120626914653879\n",
      "Stochastic Gradient Descent(2052): loss=3.3687975598152433\n",
      "Stochastic Gradient Descent(2053): loss=82.94850259878007\n",
      "Stochastic Gradient Descent(2054): loss=50.04979991018336\n",
      "Stochastic Gradient Descent(2055): loss=10.768862380476266\n",
      "Stochastic Gradient Descent(2056): loss=4.205097739420838\n",
      "Stochastic Gradient Descent(2057): loss=6.813536584288056\n",
      "Stochastic Gradient Descent(2058): loss=112.42877811012367\n",
      "Stochastic Gradient Descent(2059): loss=162.50109060420561\n",
      "Stochastic Gradient Descent(2060): loss=45.60889788955872\n",
      "Stochastic Gradient Descent(2061): loss=12.567109595436932\n",
      "Stochastic Gradient Descent(2062): loss=2.4347579236737933\n",
      "Stochastic Gradient Descent(2063): loss=7.3500268492325445\n",
      "Stochastic Gradient Descent(2064): loss=5.974463773002017\n",
      "Stochastic Gradient Descent(2065): loss=1.5786866122313945\n",
      "Stochastic Gradient Descent(2066): loss=44.28614520943636\n",
      "Stochastic Gradient Descent(2067): loss=35.851362205050954\n",
      "Stochastic Gradient Descent(2068): loss=15.700588375044541\n",
      "Stochastic Gradient Descent(2069): loss=37.593512731159045\n",
      "Stochastic Gradient Descent(2070): loss=6.796567902391411\n",
      "Stochastic Gradient Descent(2071): loss=0.0044916986184482845\n",
      "Stochastic Gradient Descent(2072): loss=95.31541167001487\n",
      "Stochastic Gradient Descent(2073): loss=0.0010682641369043901\n",
      "Stochastic Gradient Descent(2074): loss=149.87165857050664\n",
      "Stochastic Gradient Descent(2075): loss=30.548373756833925\n",
      "Stochastic Gradient Descent(2076): loss=0.08319608262541908\n",
      "Stochastic Gradient Descent(2077): loss=32.55654582774614\n",
      "Stochastic Gradient Descent(2078): loss=17.100754163495075\n",
      "Stochastic Gradient Descent(2079): loss=30.641791156543032\n",
      "Stochastic Gradient Descent(2080): loss=32.01008572070748\n",
      "Stochastic Gradient Descent(2081): loss=969.499074775925\n",
      "Stochastic Gradient Descent(2082): loss=13.359996303205646\n",
      "Stochastic Gradient Descent(2083): loss=1572.5913540750494\n",
      "Stochastic Gradient Descent(2084): loss=160.93738601504356\n",
      "Stochastic Gradient Descent(2085): loss=16.721998826037815\n",
      "Stochastic Gradient Descent(2086): loss=11.223384995668491\n",
      "Stochastic Gradient Descent(2087): loss=26.445014312612763\n",
      "Stochastic Gradient Descent(2088): loss=64.75946645383036\n",
      "Stochastic Gradient Descent(2089): loss=6.490399693625283\n",
      "Stochastic Gradient Descent(2090): loss=114.12555385105586\n",
      "Stochastic Gradient Descent(2091): loss=17.755828738214035\n",
      "Stochastic Gradient Descent(2092): loss=0.2974350999292504\n",
      "Stochastic Gradient Descent(2093): loss=11.763030977791942\n",
      "Stochastic Gradient Descent(2094): loss=15.265723198322826\n",
      "Stochastic Gradient Descent(2095): loss=45.694393406837\n",
      "Stochastic Gradient Descent(2096): loss=11.333114641402917\n",
      "Stochastic Gradient Descent(2097): loss=188.17649263766495\n",
      "Stochastic Gradient Descent(2098): loss=2.422043296101445\n",
      "Stochastic Gradient Descent(2099): loss=104.59119478328722\n",
      "Stochastic Gradient Descent(2100): loss=49.56709937004937\n",
      "Stochastic Gradient Descent(2101): loss=18.296973980409803\n",
      "Stochastic Gradient Descent(2102): loss=18.783998647434533\n",
      "Stochastic Gradient Descent(2103): loss=232.1577592686493\n",
      "Stochastic Gradient Descent(2104): loss=4.8104155386107985\n",
      "Stochastic Gradient Descent(2105): loss=33.10460486714687\n",
      "Stochastic Gradient Descent(2106): loss=103.4615390920737\n",
      "Stochastic Gradient Descent(2107): loss=0.1439570655243664\n",
      "Stochastic Gradient Descent(2108): loss=40.404088547457185\n",
      "Stochastic Gradient Descent(2109): loss=13.585736051402371\n",
      "Stochastic Gradient Descent(2110): loss=45.61713188880222\n",
      "Stochastic Gradient Descent(2111): loss=12.862353437499438\n",
      "Stochastic Gradient Descent(2112): loss=37.82469301226149\n",
      "Stochastic Gradient Descent(2113): loss=210.69717072373174\n",
      "Stochastic Gradient Descent(2114): loss=182.78282330678024\n",
      "Stochastic Gradient Descent(2115): loss=36.88877571563102\n",
      "Stochastic Gradient Descent(2116): loss=23.71803689966508\n",
      "Stochastic Gradient Descent(2117): loss=3.466479969500462\n",
      "Stochastic Gradient Descent(2118): loss=17.32640915518151\n",
      "Stochastic Gradient Descent(2119): loss=9.898666268161232\n",
      "Stochastic Gradient Descent(2120): loss=110.75406465165837\n",
      "Stochastic Gradient Descent(2121): loss=12.073213621146518\n",
      "Stochastic Gradient Descent(2122): loss=20.86996529841514\n",
      "Stochastic Gradient Descent(2123): loss=30.916830906169455\n",
      "Stochastic Gradient Descent(2124): loss=1.366951519720414\n",
      "Stochastic Gradient Descent(2125): loss=2.6499570833525135\n",
      "Stochastic Gradient Descent(2126): loss=7.454992362985534\n",
      "Stochastic Gradient Descent(2127): loss=6.607717831567342\n",
      "Stochastic Gradient Descent(2128): loss=78.46058902544642\n",
      "Stochastic Gradient Descent(2129): loss=0.6044521609687601\n",
      "Stochastic Gradient Descent(2130): loss=7.550913501667195\n",
      "Stochastic Gradient Descent(2131): loss=1.1541201994845733\n",
      "Stochastic Gradient Descent(2132): loss=70.49919002995564\n",
      "Stochastic Gradient Descent(2133): loss=50.81864296884545\n",
      "Stochastic Gradient Descent(2134): loss=1.3281290942385864\n",
      "Stochastic Gradient Descent(2135): loss=76.55258864163054\n",
      "Stochastic Gradient Descent(2136): loss=0.5256567484277731\n",
      "Stochastic Gradient Descent(2137): loss=75.86710208581728\n",
      "Stochastic Gradient Descent(2138): loss=143.58480799715574\n",
      "Stochastic Gradient Descent(2139): loss=87.2786889578861\n",
      "Stochastic Gradient Descent(2140): loss=38.00326488606487\n",
      "Stochastic Gradient Descent(2141): loss=6.403508888807249\n",
      "Stochastic Gradient Descent(2142): loss=11.271188364956366\n",
      "Stochastic Gradient Descent(2143): loss=2.201502961751688\n",
      "Stochastic Gradient Descent(2144): loss=63.25008955057862\n",
      "Stochastic Gradient Descent(2145): loss=76.36357835457429\n",
      "Stochastic Gradient Descent(2146): loss=93.30617599952201\n",
      "Stochastic Gradient Descent(2147): loss=0.6321038730306067\n",
      "Stochastic Gradient Descent(2148): loss=6.371227096873226\n",
      "Stochastic Gradient Descent(2149): loss=12.638935230012443\n",
      "Stochastic Gradient Descent(2150): loss=15.263649520025504\n",
      "Stochastic Gradient Descent(2151): loss=109.16204195436916\n",
      "Stochastic Gradient Descent(2152): loss=88.90677946650747\n",
      "Stochastic Gradient Descent(2153): loss=104.30116192075086\n",
      "Stochastic Gradient Descent(2154): loss=2.3481243559519167\n",
      "Stochastic Gradient Descent(2155): loss=2.278198589518386\n",
      "Stochastic Gradient Descent(2156): loss=3.607515293184903\n",
      "Stochastic Gradient Descent(2157): loss=175.23756833785924\n",
      "Stochastic Gradient Descent(2158): loss=193.87117939304295\n",
      "Stochastic Gradient Descent(2159): loss=65.93467447640215\n",
      "Stochastic Gradient Descent(2160): loss=3.843679513561331\n",
      "Stochastic Gradient Descent(2161): loss=95.44782920492185\n",
      "Stochastic Gradient Descent(2162): loss=90.93999444475659\n",
      "Stochastic Gradient Descent(2163): loss=9.847608699462846\n",
      "Stochastic Gradient Descent(2164): loss=203.922933873198\n",
      "Stochastic Gradient Descent(2165): loss=14.073860117094705\n",
      "Stochastic Gradient Descent(2166): loss=9.603209959198061\n",
      "Stochastic Gradient Descent(2167): loss=0.22745974262679772\n",
      "Stochastic Gradient Descent(2168): loss=61.589504782545454\n",
      "Stochastic Gradient Descent(2169): loss=4.347902683339486\n",
      "Stochastic Gradient Descent(2170): loss=24.60542794107249\n",
      "Stochastic Gradient Descent(2171): loss=29.328811167918026\n",
      "Stochastic Gradient Descent(2172): loss=35.92265637177752\n",
      "Stochastic Gradient Descent(2173): loss=0.31114665682935416\n",
      "Stochastic Gradient Descent(2174): loss=64.96249307260251\n",
      "Stochastic Gradient Descent(2175): loss=321.11941923272104\n",
      "Stochastic Gradient Descent(2176): loss=79.36659094527204\n",
      "Stochastic Gradient Descent(2177): loss=0.7763869622784546\n",
      "Stochastic Gradient Descent(2178): loss=61.906050167506955\n",
      "Stochastic Gradient Descent(2179): loss=33.935667307902506\n",
      "Stochastic Gradient Descent(2180): loss=41.94031149702421\n",
      "Stochastic Gradient Descent(2181): loss=83.50863749615276\n",
      "Stochastic Gradient Descent(2182): loss=25.423005338078482\n",
      "Stochastic Gradient Descent(2183): loss=15.841260278360027\n",
      "Stochastic Gradient Descent(2184): loss=8.722474423353008\n",
      "Stochastic Gradient Descent(2185): loss=29.866386703685727\n",
      "Stochastic Gradient Descent(2186): loss=25.28324755617053\n",
      "Stochastic Gradient Descent(2187): loss=1.2547575046625972\n",
      "Stochastic Gradient Descent(2188): loss=6.53280034587377\n",
      "Stochastic Gradient Descent(2189): loss=0.23049840928112872\n",
      "Stochastic Gradient Descent(2190): loss=37.47352920557851\n",
      "Stochastic Gradient Descent(2191): loss=54.42299117030654\n",
      "Stochastic Gradient Descent(2192): loss=20.128084546288694\n",
      "Stochastic Gradient Descent(2193): loss=0.05809475903142314\n",
      "Stochastic Gradient Descent(2194): loss=0.7161152758926915\n",
      "Stochastic Gradient Descent(2195): loss=4.3291777329635375\n",
      "Stochastic Gradient Descent(2196): loss=0.327855573216786\n",
      "Stochastic Gradient Descent(2197): loss=40.18132377187028\n",
      "Stochastic Gradient Descent(2198): loss=1.5595122115564295\n",
      "Stochastic Gradient Descent(2199): loss=16.5205828098433\n",
      "Stochastic Gradient Descent(2200): loss=0.4474882609951579\n",
      "Stochastic Gradient Descent(2201): loss=188.70360720024394\n",
      "Stochastic Gradient Descent(2202): loss=180.32044903249457\n",
      "Stochastic Gradient Descent(2203): loss=85.91273552559585\n",
      "Stochastic Gradient Descent(2204): loss=0.13443133901105517\n",
      "Stochastic Gradient Descent(2205): loss=141.96894173505416\n",
      "Stochastic Gradient Descent(2206): loss=9.921830591782571\n",
      "Stochastic Gradient Descent(2207): loss=345.55366574661286\n",
      "Stochastic Gradient Descent(2208): loss=7.318284383789687\n",
      "Stochastic Gradient Descent(2209): loss=16.73138268305644\n",
      "Stochastic Gradient Descent(2210): loss=26.008854687883808\n",
      "Stochastic Gradient Descent(2211): loss=2.063107900379337\n",
      "Stochastic Gradient Descent(2212): loss=13.703701984591264\n",
      "Stochastic Gradient Descent(2213): loss=81.12886439378666\n",
      "Stochastic Gradient Descent(2214): loss=339.52704493007326\n",
      "Stochastic Gradient Descent(2215): loss=3.8574800846877597\n",
      "Stochastic Gradient Descent(2216): loss=8.297074227721861\n",
      "Stochastic Gradient Descent(2217): loss=15.0237113674755\n",
      "Stochastic Gradient Descent(2218): loss=38.242385474756766\n",
      "Stochastic Gradient Descent(2219): loss=304.21208906242884\n",
      "Stochastic Gradient Descent(2220): loss=8.565973104070318\n",
      "Stochastic Gradient Descent(2221): loss=31.310037799018495\n",
      "Stochastic Gradient Descent(2222): loss=24.397371731936772\n",
      "Stochastic Gradient Descent(2223): loss=6.9987418048590815\n",
      "Stochastic Gradient Descent(2224): loss=16.3370256429122\n",
      "Stochastic Gradient Descent(2225): loss=29.8457922957932\n",
      "Stochastic Gradient Descent(2226): loss=75.69414625773942\n",
      "Stochastic Gradient Descent(2227): loss=33.53287924362133\n",
      "Stochastic Gradient Descent(2228): loss=109.21854392074002\n",
      "Stochastic Gradient Descent(2229): loss=108.00791260508888\n",
      "Stochastic Gradient Descent(2230): loss=12.22068038021513\n",
      "Stochastic Gradient Descent(2231): loss=30.531896953316277\n",
      "Stochastic Gradient Descent(2232): loss=9.513894513706228\n",
      "Stochastic Gradient Descent(2233): loss=188.97426900084886\n",
      "Stochastic Gradient Descent(2234): loss=29.403620507701916\n",
      "Stochastic Gradient Descent(2235): loss=129.05726184367933\n",
      "Stochastic Gradient Descent(2236): loss=3.827488264932958\n",
      "Stochastic Gradient Descent(2237): loss=8.734419999626763\n",
      "Stochastic Gradient Descent(2238): loss=22.3797735783019\n",
      "Stochastic Gradient Descent(2239): loss=9.447214175201056\n",
      "Stochastic Gradient Descent(2240): loss=107.66647944112475\n",
      "Stochastic Gradient Descent(2241): loss=2.5874553370834272\n",
      "Stochastic Gradient Descent(2242): loss=6.313680908398039\n",
      "Stochastic Gradient Descent(2243): loss=7.881551781957958\n",
      "Stochastic Gradient Descent(2244): loss=1.706156993082426\n",
      "Stochastic Gradient Descent(2245): loss=0.012556170980424356\n",
      "Stochastic Gradient Descent(2246): loss=0.9171248627053924\n",
      "Stochastic Gradient Descent(2247): loss=15.678432293369744\n",
      "Stochastic Gradient Descent(2248): loss=168.73083896448912\n",
      "Stochastic Gradient Descent(2249): loss=71.54517258001995\n",
      "Stochastic Gradient Descent(2250): loss=30.962883606951173\n",
      "Stochastic Gradient Descent(2251): loss=63.67081393380331\n",
      "Stochastic Gradient Descent(2252): loss=8.081889172310945\n",
      "Stochastic Gradient Descent(2253): loss=32.9639304796296\n",
      "Stochastic Gradient Descent(2254): loss=52.993207644548846\n",
      "Stochastic Gradient Descent(2255): loss=76.69298599983723\n",
      "Stochastic Gradient Descent(2256): loss=0.0001766900928354266\n",
      "Stochastic Gradient Descent(2257): loss=79.1591321652622\n",
      "Stochastic Gradient Descent(2258): loss=70.40983214309237\n",
      "Stochastic Gradient Descent(2259): loss=46.780823174248255\n",
      "Stochastic Gradient Descent(2260): loss=54.11191262342191\n",
      "Stochastic Gradient Descent(2261): loss=112.31555441252685\n",
      "Stochastic Gradient Descent(2262): loss=90.5311456437972\n",
      "Stochastic Gradient Descent(2263): loss=12.922751523491794\n",
      "Stochastic Gradient Descent(2264): loss=24.073559820591115\n",
      "Stochastic Gradient Descent(2265): loss=128.15120077889387\n",
      "Stochastic Gradient Descent(2266): loss=61.38867717337359\n",
      "Stochastic Gradient Descent(2267): loss=36.94095353066779\n",
      "Stochastic Gradient Descent(2268): loss=17.121399894345526\n",
      "Stochastic Gradient Descent(2269): loss=88.41931500623345\n",
      "Stochastic Gradient Descent(2270): loss=22.463181982648226\n",
      "Stochastic Gradient Descent(2271): loss=40.16435152644826\n",
      "Stochastic Gradient Descent(2272): loss=78.10775207426845\n",
      "Stochastic Gradient Descent(2273): loss=21.33955885586749\n",
      "Stochastic Gradient Descent(2274): loss=19.588508970675935\n",
      "Stochastic Gradient Descent(2275): loss=89.92931947041536\n",
      "Stochastic Gradient Descent(2276): loss=216.8703160954575\n",
      "Stochastic Gradient Descent(2277): loss=27.031610447963924\n",
      "Stochastic Gradient Descent(2278): loss=165.22990967533747\n",
      "Stochastic Gradient Descent(2279): loss=2.725958619545847\n",
      "Stochastic Gradient Descent(2280): loss=18.149954192256118\n",
      "Stochastic Gradient Descent(2281): loss=38.694728093508985\n",
      "Stochastic Gradient Descent(2282): loss=44.474676267470734\n",
      "Stochastic Gradient Descent(2283): loss=2.6626474746724873\n",
      "Stochastic Gradient Descent(2284): loss=0.6447777403352933\n",
      "Stochastic Gradient Descent(2285): loss=73.9765652567126\n",
      "Stochastic Gradient Descent(2286): loss=8.860661814846255\n",
      "Stochastic Gradient Descent(2287): loss=12.939918017647583\n",
      "Stochastic Gradient Descent(2288): loss=3.6415091176338477\n",
      "Stochastic Gradient Descent(2289): loss=565.5090378363002\n",
      "Stochastic Gradient Descent(2290): loss=12.525757473132703\n",
      "Stochastic Gradient Descent(2291): loss=34.61091869887661\n",
      "Stochastic Gradient Descent(2292): loss=110.30128661816141\n",
      "Stochastic Gradient Descent(2293): loss=39.287828206184365\n",
      "Stochastic Gradient Descent(2294): loss=31.048816047469774\n",
      "Stochastic Gradient Descent(2295): loss=8.496330197503617\n",
      "Stochastic Gradient Descent(2296): loss=58.54634224626049\n",
      "Stochastic Gradient Descent(2297): loss=0.8392574037087437\n",
      "Stochastic Gradient Descent(2298): loss=1.0795060176960785\n",
      "Stochastic Gradient Descent(2299): loss=7.150194891722529\n",
      "Stochastic Gradient Descent(2300): loss=168.73740867730461\n",
      "Stochastic Gradient Descent(2301): loss=19.891380928227758\n",
      "Stochastic Gradient Descent(2302): loss=7.892371463613984\n",
      "Stochastic Gradient Descent(2303): loss=0.05545429844086553\n",
      "Stochastic Gradient Descent(2304): loss=106.69810080915408\n",
      "Stochastic Gradient Descent(2305): loss=18.23289818730059\n",
      "Stochastic Gradient Descent(2306): loss=192.59637048870914\n",
      "Stochastic Gradient Descent(2307): loss=1.9524424273903995\n",
      "Stochastic Gradient Descent(2308): loss=0.0017173814324705898\n",
      "Stochastic Gradient Descent(2309): loss=390.9015587845731\n",
      "Stochastic Gradient Descent(2310): loss=56.04456601806613\n",
      "Stochastic Gradient Descent(2311): loss=204.28951538480774\n",
      "Stochastic Gradient Descent(2312): loss=160.5959795268906\n",
      "Stochastic Gradient Descent(2313): loss=135.79067923242732\n",
      "Stochastic Gradient Descent(2314): loss=35.637034461949675\n",
      "Stochastic Gradient Descent(2315): loss=11.581628538619166\n",
      "Stochastic Gradient Descent(2316): loss=70.49826516535943\n",
      "Stochastic Gradient Descent(2317): loss=0.4906080232478826\n",
      "Stochastic Gradient Descent(2318): loss=31.552188028497657\n",
      "Stochastic Gradient Descent(2319): loss=151.4483582833984\n",
      "Stochastic Gradient Descent(2320): loss=47.17497937058516\n",
      "Stochastic Gradient Descent(2321): loss=0.23644018386971796\n",
      "Stochastic Gradient Descent(2322): loss=7.733538470224587\n",
      "Stochastic Gradient Descent(2323): loss=4.72627346615747\n",
      "Stochastic Gradient Descent(2324): loss=92.03994456932035\n",
      "Stochastic Gradient Descent(2325): loss=65.48916709611302\n",
      "Stochastic Gradient Descent(2326): loss=79.81500722709224\n",
      "Stochastic Gradient Descent(2327): loss=14.029359137452003\n",
      "Stochastic Gradient Descent(2328): loss=2.497226170491282\n",
      "Stochastic Gradient Descent(2329): loss=12.864907974627883\n",
      "Stochastic Gradient Descent(2330): loss=42.50291991428687\n",
      "Stochastic Gradient Descent(2331): loss=14.784237484362574\n",
      "Stochastic Gradient Descent(2332): loss=74.69970218905749\n",
      "Stochastic Gradient Descent(2333): loss=21.562133356390888\n",
      "Stochastic Gradient Descent(2334): loss=0.8004924972524311\n",
      "Stochastic Gradient Descent(2335): loss=4.099792098064431\n",
      "Stochastic Gradient Descent(2336): loss=9.882475871645639\n",
      "Stochastic Gradient Descent(2337): loss=2.6252731969750864\n",
      "Stochastic Gradient Descent(2338): loss=65.41724187776924\n",
      "Stochastic Gradient Descent(2339): loss=0.09522108966448085\n",
      "Stochastic Gradient Descent(2340): loss=6.321603204329573\n",
      "Stochastic Gradient Descent(2341): loss=11.799175244090007\n",
      "Stochastic Gradient Descent(2342): loss=24.6623851112494\n",
      "Stochastic Gradient Descent(2343): loss=365.64877828335466\n",
      "Stochastic Gradient Descent(2344): loss=5.428233435758923\n",
      "Stochastic Gradient Descent(2345): loss=80.7497693991661\n",
      "Stochastic Gradient Descent(2346): loss=5.7173744002390565\n",
      "Stochastic Gradient Descent(2347): loss=0.9297289504498859\n",
      "Stochastic Gradient Descent(2348): loss=7.0787411833109815\n",
      "Stochastic Gradient Descent(2349): loss=70.14976379489546\n",
      "Stochastic Gradient Descent(2350): loss=1.0884278827846894\n",
      "Stochastic Gradient Descent(2351): loss=359.29848084353097\n",
      "Stochastic Gradient Descent(2352): loss=738.70855911979\n",
      "Stochastic Gradient Descent(2353): loss=0.8173468562140406\n",
      "Stochastic Gradient Descent(2354): loss=112.81344797407205\n",
      "Stochastic Gradient Descent(2355): loss=0.002876328557744583\n",
      "Stochastic Gradient Descent(2356): loss=3.9827085730225558\n",
      "Stochastic Gradient Descent(2357): loss=2.4265594102262336\n",
      "Stochastic Gradient Descent(2358): loss=82.04521996896693\n",
      "Stochastic Gradient Descent(2359): loss=40.58594702440352\n",
      "Stochastic Gradient Descent(2360): loss=4.566214809813311\n",
      "Stochastic Gradient Descent(2361): loss=53.76555449427385\n",
      "Stochastic Gradient Descent(2362): loss=5.9266962092604105\n",
      "Stochastic Gradient Descent(2363): loss=17.777792639065108\n",
      "Stochastic Gradient Descent(2364): loss=0.0005241748351241723\n",
      "Stochastic Gradient Descent(2365): loss=157.69530567103882\n",
      "Stochastic Gradient Descent(2366): loss=4.543067335864022\n",
      "Stochastic Gradient Descent(2367): loss=5.143704498882952\n",
      "Stochastic Gradient Descent(2368): loss=31.3135242693798\n",
      "Stochastic Gradient Descent(2369): loss=97.64654687797861\n",
      "Stochastic Gradient Descent(2370): loss=3.263782738894204\n",
      "Stochastic Gradient Descent(2371): loss=18.36467782772789\n",
      "Stochastic Gradient Descent(2372): loss=0.03173392769283753\n",
      "Stochastic Gradient Descent(2373): loss=86.82197600254506\n",
      "Stochastic Gradient Descent(2374): loss=28.544420223230865\n",
      "Stochastic Gradient Descent(2375): loss=28.568080987754414\n",
      "Stochastic Gradient Descent(2376): loss=0.0437192433544403\n",
      "Stochastic Gradient Descent(2377): loss=24.883915739958724\n",
      "Stochastic Gradient Descent(2378): loss=21.97121499971281\n",
      "Stochastic Gradient Descent(2379): loss=43.112796221797744\n",
      "Stochastic Gradient Descent(2380): loss=6.9719269762488985\n",
      "Stochastic Gradient Descent(2381): loss=32.32989480765117\n",
      "Stochastic Gradient Descent(2382): loss=1.5536824788882275\n",
      "Stochastic Gradient Descent(2383): loss=18.188732107872976\n",
      "Stochastic Gradient Descent(2384): loss=0.6909102987832573\n",
      "Stochastic Gradient Descent(2385): loss=0.9189073188255293\n",
      "Stochastic Gradient Descent(2386): loss=2.107430927388876\n",
      "Stochastic Gradient Descent(2387): loss=25.57218966114121\n",
      "Stochastic Gradient Descent(2388): loss=15.762111708426387\n",
      "Stochastic Gradient Descent(2389): loss=5.04520655059425\n",
      "Stochastic Gradient Descent(2390): loss=9.988940457911156\n",
      "Stochastic Gradient Descent(2391): loss=13.245676875065314\n",
      "Stochastic Gradient Descent(2392): loss=0.21294519966190154\n",
      "Stochastic Gradient Descent(2393): loss=1.2654598004728592\n",
      "Stochastic Gradient Descent(2394): loss=51.58671180444605\n",
      "Stochastic Gradient Descent(2395): loss=0.2706341080059003\n",
      "Stochastic Gradient Descent(2396): loss=0.8451574166611493\n",
      "Stochastic Gradient Descent(2397): loss=20.956886160313122\n",
      "Stochastic Gradient Descent(2398): loss=113.7135626108237\n",
      "Stochastic Gradient Descent(2399): loss=9.273085205381252\n",
      "Stochastic Gradient Descent(2400): loss=5.9551149261636835\n",
      "Stochastic Gradient Descent(2401): loss=6.313555779678783\n",
      "Stochastic Gradient Descent(2402): loss=25.89554583049986\n",
      "Stochastic Gradient Descent(2403): loss=16.004392604442884\n",
      "Stochastic Gradient Descent(2404): loss=0.20897441939512473\n",
      "Stochastic Gradient Descent(2405): loss=33.2373965446844\n",
      "Stochastic Gradient Descent(2406): loss=18.27998551854458\n",
      "Stochastic Gradient Descent(2407): loss=29.99233004261387\n",
      "Stochastic Gradient Descent(2408): loss=82.42404375299822\n",
      "Stochastic Gradient Descent(2409): loss=3.5303987485243336\n",
      "Stochastic Gradient Descent(2410): loss=236.14917785193813\n",
      "Stochastic Gradient Descent(2411): loss=17.362048363046252\n",
      "Stochastic Gradient Descent(2412): loss=288.34713285714605\n",
      "Stochastic Gradient Descent(2413): loss=19.695993818389354\n",
      "Stochastic Gradient Descent(2414): loss=52.530359373332615\n",
      "Stochastic Gradient Descent(2415): loss=366.5720540128863\n",
      "Stochastic Gradient Descent(2416): loss=146.00850520188686\n",
      "Stochastic Gradient Descent(2417): loss=5.587907364130066\n",
      "Stochastic Gradient Descent(2418): loss=4.15037070942142\n",
      "Stochastic Gradient Descent(2419): loss=749.2602592661668\n",
      "Stochastic Gradient Descent(2420): loss=91.79148333304515\n",
      "Stochastic Gradient Descent(2421): loss=418.90756379252474\n",
      "Stochastic Gradient Descent(2422): loss=38.17124923914784\n",
      "Stochastic Gradient Descent(2423): loss=19.919812121380208\n",
      "Stochastic Gradient Descent(2424): loss=14.070163588583599\n",
      "Stochastic Gradient Descent(2425): loss=280.2662084044676\n",
      "Stochastic Gradient Descent(2426): loss=25.334188598142138\n",
      "Stochastic Gradient Descent(2427): loss=44.12384291294944\n",
      "Stochastic Gradient Descent(2428): loss=589.9487511152839\n",
      "Stochastic Gradient Descent(2429): loss=142.52098848690517\n",
      "Stochastic Gradient Descent(2430): loss=90.87496398260588\n",
      "Stochastic Gradient Descent(2431): loss=13.70727086583985\n",
      "Stochastic Gradient Descent(2432): loss=488.8221360424484\n",
      "Stochastic Gradient Descent(2433): loss=135.1396394533947\n",
      "Stochastic Gradient Descent(2434): loss=70.09655366329969\n",
      "Stochastic Gradient Descent(2435): loss=3.5854209385364406\n",
      "Stochastic Gradient Descent(2436): loss=130.6897966806786\n",
      "Stochastic Gradient Descent(2437): loss=1.4381487117984515\n",
      "Stochastic Gradient Descent(2438): loss=11.755424588499372\n",
      "Stochastic Gradient Descent(2439): loss=0.09898737664670364\n",
      "Stochastic Gradient Descent(2440): loss=46.2533231854737\n",
      "Stochastic Gradient Descent(2441): loss=14.694127812445531\n",
      "Stochastic Gradient Descent(2442): loss=21.663148331407534\n",
      "Stochastic Gradient Descent(2443): loss=39.32460443821607\n",
      "Stochastic Gradient Descent(2444): loss=6.576991965416636\n",
      "Stochastic Gradient Descent(2445): loss=29.16841761235117\n",
      "Stochastic Gradient Descent(2446): loss=81.44026231537251\n",
      "Stochastic Gradient Descent(2447): loss=40.942480716663745\n",
      "Stochastic Gradient Descent(2448): loss=49.54241908549992\n",
      "Stochastic Gradient Descent(2449): loss=3.327218089560334\n",
      "Stochastic Gradient Descent(2450): loss=99.9776897044495\n",
      "Stochastic Gradient Descent(2451): loss=43.07650024309925\n",
      "Stochastic Gradient Descent(2452): loss=10.54024697520878\n",
      "Stochastic Gradient Descent(2453): loss=63.27073056206218\n",
      "Stochastic Gradient Descent(2454): loss=22.362423735892886\n",
      "Stochastic Gradient Descent(2455): loss=6.615564569725532\n",
      "Stochastic Gradient Descent(2456): loss=11.114509350219041\n",
      "Stochastic Gradient Descent(2457): loss=5.950364673871989\n",
      "Stochastic Gradient Descent(2458): loss=8.34927988641606\n",
      "Stochastic Gradient Descent(2459): loss=0.12905209774470094\n",
      "Stochastic Gradient Descent(2460): loss=48.86213778546102\n",
      "Stochastic Gradient Descent(2461): loss=21.24038625162508\n",
      "Stochastic Gradient Descent(2462): loss=1.0703089657802554\n",
      "Stochastic Gradient Descent(2463): loss=112.36649677684112\n",
      "Stochastic Gradient Descent(2464): loss=185.03464611972234\n",
      "Stochastic Gradient Descent(2465): loss=139.0488670521582\n",
      "Stochastic Gradient Descent(2466): loss=15.502572104175252\n",
      "Stochastic Gradient Descent(2467): loss=140.16858619821917\n",
      "Stochastic Gradient Descent(2468): loss=0.0289238318761007\n",
      "Stochastic Gradient Descent(2469): loss=39.836130220626444\n",
      "Stochastic Gradient Descent(2470): loss=72.32644272036909\n",
      "Stochastic Gradient Descent(2471): loss=32.02154310557375\n",
      "Stochastic Gradient Descent(2472): loss=107.70684294864452\n",
      "Stochastic Gradient Descent(2473): loss=44.760682632270246\n",
      "Stochastic Gradient Descent(2474): loss=69.58713646786241\n",
      "Stochastic Gradient Descent(2475): loss=237.06653920558153\n",
      "Stochastic Gradient Descent(2476): loss=52.61250822809876\n",
      "Stochastic Gradient Descent(2477): loss=253.24590645739903\n",
      "Stochastic Gradient Descent(2478): loss=1.347850177326581\n",
      "Stochastic Gradient Descent(2479): loss=11.266306014292415\n",
      "Stochastic Gradient Descent(2480): loss=400.22993832897316\n",
      "Stochastic Gradient Descent(2481): loss=0.0005193591692050757\n",
      "Stochastic Gradient Descent(2482): loss=0.6281723365659563\n",
      "Stochastic Gradient Descent(2483): loss=297.38406107517187\n",
      "Stochastic Gradient Descent(2484): loss=149.26897911258388\n",
      "Stochastic Gradient Descent(2485): loss=35.34805376011466\n",
      "Stochastic Gradient Descent(2486): loss=8.226573364079586\n",
      "Stochastic Gradient Descent(2487): loss=53.923122983823724\n",
      "Stochastic Gradient Descent(2488): loss=3.0701813795637976\n",
      "Stochastic Gradient Descent(2489): loss=5.655395793118885\n",
      "Stochastic Gradient Descent(2490): loss=0.09812359743115266\n",
      "Stochastic Gradient Descent(2491): loss=21.9036446408066\n",
      "Stochastic Gradient Descent(2492): loss=28.743580557371487\n",
      "Stochastic Gradient Descent(2493): loss=35.161853030258115\n",
      "Stochastic Gradient Descent(2494): loss=12.660853805223834\n",
      "Stochastic Gradient Descent(2495): loss=25.79499518105118\n",
      "Stochastic Gradient Descent(2496): loss=94.99675669554765\n",
      "Stochastic Gradient Descent(2497): loss=4.9391064900893555\n",
      "Stochastic Gradient Descent(2498): loss=18.39897714055418\n",
      "Stochastic Gradient Descent(2499): loss=1.3884253970770128\n",
      "Stochastic Gradient Descent(2500): loss=3.3152558828815395\n",
      "Stochastic Gradient Descent(2501): loss=56.31376418726158\n",
      "Stochastic Gradient Descent(2502): loss=23.462407671526368\n",
      "Stochastic Gradient Descent(2503): loss=24.798655860243883\n",
      "Stochastic Gradient Descent(2504): loss=27.843646653437478\n",
      "Stochastic Gradient Descent(2505): loss=13.032736770790315\n",
      "Stochastic Gradient Descent(2506): loss=40.16139227147415\n",
      "Stochastic Gradient Descent(2507): loss=4.539773648556179\n",
      "Stochastic Gradient Descent(2508): loss=34.399696933855374\n",
      "Stochastic Gradient Descent(2509): loss=31.58563367837925\n",
      "Stochastic Gradient Descent(2510): loss=19.485908498721546\n",
      "Stochastic Gradient Descent(2511): loss=55.65418564415748\n",
      "Stochastic Gradient Descent(2512): loss=0.008842732268783293\n",
      "Stochastic Gradient Descent(2513): loss=0.04877068648754509\n",
      "Stochastic Gradient Descent(2514): loss=29.416586342264655\n",
      "Stochastic Gradient Descent(2515): loss=143.94940836059527\n",
      "Stochastic Gradient Descent(2516): loss=10.970656693228591\n",
      "Stochastic Gradient Descent(2517): loss=27.837583898811275\n",
      "Stochastic Gradient Descent(2518): loss=42.736892838729645\n",
      "Stochastic Gradient Descent(2519): loss=3.7702228178533925\n",
      "Stochastic Gradient Descent(2520): loss=6.865720008034245\n",
      "Stochastic Gradient Descent(2521): loss=0.7238009662084505\n",
      "Stochastic Gradient Descent(2522): loss=1.8427203527978169\n",
      "Stochastic Gradient Descent(2523): loss=5.797993575492269\n",
      "Stochastic Gradient Descent(2524): loss=34.262295623418574\n",
      "Stochastic Gradient Descent(2525): loss=0.5752324064717051\n",
      "Stochastic Gradient Descent(2526): loss=5.909190249788284\n",
      "Stochastic Gradient Descent(2527): loss=2.4342244148308905\n",
      "Stochastic Gradient Descent(2528): loss=108.32482519276977\n",
      "Stochastic Gradient Descent(2529): loss=15.947871633984159\n",
      "Stochastic Gradient Descent(2530): loss=6.577322331542614\n",
      "Stochastic Gradient Descent(2531): loss=0.07620152066640518\n",
      "Stochastic Gradient Descent(2532): loss=277.7151238907658\n",
      "Stochastic Gradient Descent(2533): loss=218.54034288523232\n",
      "Stochastic Gradient Descent(2534): loss=37.02083892597923\n",
      "Stochastic Gradient Descent(2535): loss=7.799593683024766\n",
      "Stochastic Gradient Descent(2536): loss=34.03454097301373\n",
      "Stochastic Gradient Descent(2537): loss=8.94247914329412\n",
      "Stochastic Gradient Descent(2538): loss=6.148229021856401\n",
      "Stochastic Gradient Descent(2539): loss=45.66523054540284\n",
      "Stochastic Gradient Descent(2540): loss=29.08259330469268\n",
      "Stochastic Gradient Descent(2541): loss=16.058063499969126\n",
      "Stochastic Gradient Descent(2542): loss=22.6205404435695\n",
      "Stochastic Gradient Descent(2543): loss=11.81518269337756\n",
      "Stochastic Gradient Descent(2544): loss=58.5424472210413\n",
      "Stochastic Gradient Descent(2545): loss=19.03682277376901\n",
      "Stochastic Gradient Descent(2546): loss=0.5525239662435001\n",
      "Stochastic Gradient Descent(2547): loss=110.5585135927889\n",
      "Stochastic Gradient Descent(2548): loss=7.461204537877678\n",
      "Stochastic Gradient Descent(2549): loss=24.239765694974505\n",
      "Stochastic Gradient Descent(2550): loss=1.6228548040557313\n",
      "Stochastic Gradient Descent(2551): loss=16.58258950279625\n",
      "Stochastic Gradient Descent(2552): loss=0.24480973973651918\n",
      "Stochastic Gradient Descent(2553): loss=65.08510611649459\n",
      "Stochastic Gradient Descent(2554): loss=48.41911759103626\n",
      "Stochastic Gradient Descent(2555): loss=58.12583045746309\n",
      "Stochastic Gradient Descent(2556): loss=124.1903490442961\n",
      "Stochastic Gradient Descent(2557): loss=6.6817661143097045\n",
      "Stochastic Gradient Descent(2558): loss=23.908220686954795\n",
      "Stochastic Gradient Descent(2559): loss=0.2554657390703347\n",
      "Stochastic Gradient Descent(2560): loss=0.07213005100040246\n",
      "Stochastic Gradient Descent(2561): loss=11.744624281676247\n",
      "Stochastic Gradient Descent(2562): loss=74.63540539784452\n",
      "Stochastic Gradient Descent(2563): loss=10.877755854255891\n",
      "Stochastic Gradient Descent(2564): loss=66.66238090616864\n",
      "Stochastic Gradient Descent(2565): loss=35.48236567458276\n",
      "Stochastic Gradient Descent(2566): loss=33.61810143529514\n",
      "Stochastic Gradient Descent(2567): loss=32.99352583503972\n",
      "Stochastic Gradient Descent(2568): loss=0.682825763851407\n",
      "Stochastic Gradient Descent(2569): loss=92.04187183811328\n",
      "Stochastic Gradient Descent(2570): loss=10.906604935132517\n",
      "Stochastic Gradient Descent(2571): loss=23.218410308734533\n",
      "Stochastic Gradient Descent(2572): loss=0.1908757186853858\n",
      "Stochastic Gradient Descent(2573): loss=1.4407696124745388\n",
      "Stochastic Gradient Descent(2574): loss=0.5146287620461678\n",
      "Stochastic Gradient Descent(2575): loss=0.1359606942261906\n",
      "Stochastic Gradient Descent(2576): loss=288.86085147908176\n",
      "Stochastic Gradient Descent(2577): loss=157.0081919796222\n",
      "Stochastic Gradient Descent(2578): loss=42.48767522237534\n",
      "Stochastic Gradient Descent(2579): loss=0.0003075511004035496\n",
      "Stochastic Gradient Descent(2580): loss=0.07799839725133043\n",
      "Stochastic Gradient Descent(2581): loss=193.4624417541002\n",
      "Stochastic Gradient Descent(2582): loss=30.495284354003658\n",
      "Stochastic Gradient Descent(2583): loss=17.071516616535757\n",
      "Stochastic Gradient Descent(2584): loss=67.8161696313384\n",
      "Stochastic Gradient Descent(2585): loss=1.486645526179467\n",
      "Stochastic Gradient Descent(2586): loss=2.3469043321196077\n",
      "Stochastic Gradient Descent(2587): loss=0.37208435351106256\n",
      "Stochastic Gradient Descent(2588): loss=173.47740037118436\n",
      "Stochastic Gradient Descent(2589): loss=7.253499415973953\n",
      "Stochastic Gradient Descent(2590): loss=1.1592048639198425\n",
      "Stochastic Gradient Descent(2591): loss=84.02857380076293\n",
      "Stochastic Gradient Descent(2592): loss=26.908590348425196\n",
      "Stochastic Gradient Descent(2593): loss=7.026613443162888\n",
      "Stochastic Gradient Descent(2594): loss=70.08019195355523\n",
      "Stochastic Gradient Descent(2595): loss=4.376215437255881\n",
      "Stochastic Gradient Descent(2596): loss=0.4470198824033298\n",
      "Stochastic Gradient Descent(2597): loss=13.117890813108446\n",
      "Stochastic Gradient Descent(2598): loss=169.253468869336\n",
      "Stochastic Gradient Descent(2599): loss=47.33882892320231\n",
      "Stochastic Gradient Descent(2600): loss=1.240279943937449\n",
      "Stochastic Gradient Descent(2601): loss=2.3361761087400437\n",
      "Stochastic Gradient Descent(2602): loss=0.35650109530516155\n",
      "Stochastic Gradient Descent(2603): loss=58.917272400647136\n",
      "Stochastic Gradient Descent(2604): loss=6.519986817922536\n",
      "Stochastic Gradient Descent(2605): loss=7.748781805166558\n",
      "Stochastic Gradient Descent(2606): loss=2.017032821037806\n",
      "Stochastic Gradient Descent(2607): loss=0.8154750208347706\n",
      "Stochastic Gradient Descent(2608): loss=11.923762919695061\n",
      "Stochastic Gradient Descent(2609): loss=4.3223996866151\n",
      "Stochastic Gradient Descent(2610): loss=67.08495043129003\n",
      "Stochastic Gradient Descent(2611): loss=151.39528725374439\n",
      "Stochastic Gradient Descent(2612): loss=21.18229227656279\n",
      "Stochastic Gradient Descent(2613): loss=5.916797384549299\n",
      "Stochastic Gradient Descent(2614): loss=38.30652076072495\n",
      "Stochastic Gradient Descent(2615): loss=1.6459371439882777\n",
      "Stochastic Gradient Descent(2616): loss=3.318856964438785\n",
      "Stochastic Gradient Descent(2617): loss=0.006731073099676902\n",
      "Stochastic Gradient Descent(2618): loss=5.749347208221194\n",
      "Stochastic Gradient Descent(2619): loss=0.7743144771860677\n",
      "Stochastic Gradient Descent(2620): loss=51.59326610260673\n",
      "Stochastic Gradient Descent(2621): loss=12.219011272987478\n",
      "Stochastic Gradient Descent(2622): loss=30.196186318837558\n",
      "Stochastic Gradient Descent(2623): loss=9.437380983013181\n",
      "Stochastic Gradient Descent(2624): loss=75.6960711820951\n",
      "Stochastic Gradient Descent(2625): loss=13.046611345814737\n",
      "Stochastic Gradient Descent(2626): loss=6.678369759018767\n",
      "Stochastic Gradient Descent(2627): loss=18.38635764667161\n",
      "Stochastic Gradient Descent(2628): loss=5.272181961363068\n",
      "Stochastic Gradient Descent(2629): loss=15.126789632338276\n",
      "Stochastic Gradient Descent(2630): loss=0.6045853145063133\n",
      "Stochastic Gradient Descent(2631): loss=113.72476991006552\n",
      "Stochastic Gradient Descent(2632): loss=40.020377910405784\n",
      "Stochastic Gradient Descent(2633): loss=33.16894210263521\n",
      "Stochastic Gradient Descent(2634): loss=25.154602753180374\n",
      "Stochastic Gradient Descent(2635): loss=36.874164477682804\n",
      "Stochastic Gradient Descent(2636): loss=11.973096046983201\n",
      "Stochastic Gradient Descent(2637): loss=154.29568535345956\n",
      "Stochastic Gradient Descent(2638): loss=6.668406793343844\n",
      "Stochastic Gradient Descent(2639): loss=28.985349625462348\n",
      "Stochastic Gradient Descent(2640): loss=23.536011338232075\n",
      "Stochastic Gradient Descent(2641): loss=0.054492436087583175\n",
      "Stochastic Gradient Descent(2642): loss=58.93146152068198\n",
      "Stochastic Gradient Descent(2643): loss=7.641434249997283\n",
      "Stochastic Gradient Descent(2644): loss=56.929071448767424\n",
      "Stochastic Gradient Descent(2645): loss=0.24078288848460563\n",
      "Stochastic Gradient Descent(2646): loss=10.57580273216986\n",
      "Stochastic Gradient Descent(2647): loss=0.297399574805849\n",
      "Stochastic Gradient Descent(2648): loss=10.132958815658808\n",
      "Stochastic Gradient Descent(2649): loss=0.3094634674742284\n",
      "Stochastic Gradient Descent(2650): loss=40.21700151580944\n",
      "Stochastic Gradient Descent(2651): loss=60.74653225334183\n",
      "Stochastic Gradient Descent(2652): loss=217.45264318757992\n",
      "Stochastic Gradient Descent(2653): loss=0.7121423322949998\n",
      "Stochastic Gradient Descent(2654): loss=23.029363260044757\n",
      "Stochastic Gradient Descent(2655): loss=12.662563706384711\n",
      "Stochastic Gradient Descent(2656): loss=189.44179171323063\n",
      "Stochastic Gradient Descent(2657): loss=99.86848466998177\n",
      "Stochastic Gradient Descent(2658): loss=22.43060811236862\n",
      "Stochastic Gradient Descent(2659): loss=3.631337743062283\n",
      "Stochastic Gradient Descent(2660): loss=127.29091825436345\n",
      "Stochastic Gradient Descent(2661): loss=0.5381826511496893\n",
      "Stochastic Gradient Descent(2662): loss=7.956966003831966\n",
      "Stochastic Gradient Descent(2663): loss=0.8108875029791283\n",
      "Stochastic Gradient Descent(2664): loss=222.23062322198837\n",
      "Stochastic Gradient Descent(2665): loss=0.5548146260061124\n",
      "Stochastic Gradient Descent(2666): loss=0.3191469908626867\n",
      "Stochastic Gradient Descent(2667): loss=5.409095612773381\n",
      "Stochastic Gradient Descent(2668): loss=3.0099106906154693\n",
      "Stochastic Gradient Descent(2669): loss=92.11021296943947\n",
      "Stochastic Gradient Descent(2670): loss=6.122248777638087\n",
      "Stochastic Gradient Descent(2671): loss=0.8181776395606492\n",
      "Stochastic Gradient Descent(2672): loss=145.75277232384022\n",
      "Stochastic Gradient Descent(2673): loss=33.76554051605084\n",
      "Stochastic Gradient Descent(2674): loss=156.96420603897693\n",
      "Stochastic Gradient Descent(2675): loss=0.0975310573095341\n",
      "Stochastic Gradient Descent(2676): loss=1.125499115549053\n",
      "Stochastic Gradient Descent(2677): loss=42.1659805583575\n",
      "Stochastic Gradient Descent(2678): loss=0.3212545164669572\n",
      "Stochastic Gradient Descent(2679): loss=23.397028854958272\n",
      "Stochastic Gradient Descent(2680): loss=9.337717214365231\n",
      "Stochastic Gradient Descent(2681): loss=0.12439159468597179\n",
      "Stochastic Gradient Descent(2682): loss=10.120131428339318\n",
      "Stochastic Gradient Descent(2683): loss=8.24751600377868\n",
      "Stochastic Gradient Descent(2684): loss=26.784954249760233\n",
      "Stochastic Gradient Descent(2685): loss=60.960571772443885\n",
      "Stochastic Gradient Descent(2686): loss=13.13488827079365\n",
      "Stochastic Gradient Descent(2687): loss=3.7333294321302053\n",
      "Stochastic Gradient Descent(2688): loss=20.51731004629782\n",
      "Stochastic Gradient Descent(2689): loss=27.440381837512007\n",
      "Stochastic Gradient Descent(2690): loss=0.2204265233871517\n",
      "Stochastic Gradient Descent(2691): loss=4.193088801390592\n",
      "Stochastic Gradient Descent(2692): loss=29.32339108130392\n",
      "Stochastic Gradient Descent(2693): loss=14.559773841533897\n",
      "Stochastic Gradient Descent(2694): loss=0.9452449173567032\n",
      "Stochastic Gradient Descent(2695): loss=23.350929440858174\n",
      "Stochastic Gradient Descent(2696): loss=54.76536343475049\n",
      "Stochastic Gradient Descent(2697): loss=18.861519513539516\n",
      "Stochastic Gradient Descent(2698): loss=1.730388106602656\n",
      "Stochastic Gradient Descent(2699): loss=0.8137991830555497\n",
      "Stochastic Gradient Descent(2700): loss=96.54385305355255\n",
      "Stochastic Gradient Descent(2701): loss=4.157806487930244\n",
      "Stochastic Gradient Descent(2702): loss=0.0013466297027580044\n",
      "Stochastic Gradient Descent(2703): loss=1.429004196501816\n",
      "Stochastic Gradient Descent(2704): loss=12.155445772002444\n",
      "Stochastic Gradient Descent(2705): loss=0.42952666589475846\n",
      "Stochastic Gradient Descent(2706): loss=61.87212132114739\n",
      "Stochastic Gradient Descent(2707): loss=3.2660401153905223\n",
      "Stochastic Gradient Descent(2708): loss=2.7997224908655642\n",
      "Stochastic Gradient Descent(2709): loss=10.453268183009024\n",
      "Stochastic Gradient Descent(2710): loss=11.143614349513598\n",
      "Stochastic Gradient Descent(2711): loss=3.735087694586441\n",
      "Stochastic Gradient Descent(2712): loss=0.7596622203043008\n",
      "Stochastic Gradient Descent(2713): loss=26.1583104122068\n",
      "Stochastic Gradient Descent(2714): loss=0.4578355500964476\n",
      "Stochastic Gradient Descent(2715): loss=0.04352420763037447\n",
      "Stochastic Gradient Descent(2716): loss=22.98459542840456\n",
      "Stochastic Gradient Descent(2717): loss=11.660948221781531\n",
      "Stochastic Gradient Descent(2718): loss=79.39995116925371\n",
      "Stochastic Gradient Descent(2719): loss=51.61804228120808\n",
      "Stochastic Gradient Descent(2720): loss=99.84243020639337\n",
      "Stochastic Gradient Descent(2721): loss=23.150828115761804\n",
      "Stochastic Gradient Descent(2722): loss=12.716763726679561\n",
      "Stochastic Gradient Descent(2723): loss=8.292855685644543\n",
      "Stochastic Gradient Descent(2724): loss=38.93877766343625\n",
      "Stochastic Gradient Descent(2725): loss=1.3568411989375135\n",
      "Stochastic Gradient Descent(2726): loss=1.4767067969733387\n",
      "Stochastic Gradient Descent(2727): loss=6.362180112352406\n",
      "Stochastic Gradient Descent(2728): loss=11.858512850580716\n",
      "Stochastic Gradient Descent(2729): loss=0.7185076392935285\n",
      "Stochastic Gradient Descent(2730): loss=43.05334289485104\n",
      "Stochastic Gradient Descent(2731): loss=8.301714258826086\n",
      "Stochastic Gradient Descent(2732): loss=109.10253471694892\n",
      "Stochastic Gradient Descent(2733): loss=251.60607030558108\n",
      "Stochastic Gradient Descent(2734): loss=75.44835594365297\n",
      "Stochastic Gradient Descent(2735): loss=13.538493075593081\n",
      "Stochastic Gradient Descent(2736): loss=86.27310696002812\n",
      "Stochastic Gradient Descent(2737): loss=81.23698169211242\n",
      "Stochastic Gradient Descent(2738): loss=13.219376464909969\n",
      "Stochastic Gradient Descent(2739): loss=29.823295714224212\n",
      "Stochastic Gradient Descent(2740): loss=61.52525923491996\n",
      "Stochastic Gradient Descent(2741): loss=26.575990916194215\n",
      "Stochastic Gradient Descent(2742): loss=5.390382701975037\n",
      "Stochastic Gradient Descent(2743): loss=0.26380884915932995\n",
      "Stochastic Gradient Descent(2744): loss=16.02461761660526\n",
      "Stochastic Gradient Descent(2745): loss=6.078944589735168\n",
      "Stochastic Gradient Descent(2746): loss=22.2024049257062\n",
      "Stochastic Gradient Descent(2747): loss=2.877056459309733\n",
      "Stochastic Gradient Descent(2748): loss=72.64871356278174\n",
      "Stochastic Gradient Descent(2749): loss=0.6732351929337337\n",
      "Stochastic Gradient Descent(2750): loss=6.4621906428995075\n",
      "Stochastic Gradient Descent(2751): loss=81.06870245308785\n",
      "Stochastic Gradient Descent(2752): loss=14.451240102739431\n",
      "Stochastic Gradient Descent(2753): loss=15.826030596837853\n",
      "Stochastic Gradient Descent(2754): loss=17.978512846506383\n",
      "Stochastic Gradient Descent(2755): loss=147.60317868888978\n",
      "Stochastic Gradient Descent(2756): loss=221.24933403704566\n",
      "Stochastic Gradient Descent(2757): loss=126.87572204822963\n",
      "Stochastic Gradient Descent(2758): loss=12.213786245093104\n",
      "Stochastic Gradient Descent(2759): loss=44.31589507629622\n",
      "Stochastic Gradient Descent(2760): loss=0.5429750901978221\n",
      "Stochastic Gradient Descent(2761): loss=1.1868389479503836\n",
      "Stochastic Gradient Descent(2762): loss=22.070082093882657\n",
      "Stochastic Gradient Descent(2763): loss=1.968568987442482\n",
      "Stochastic Gradient Descent(2764): loss=14.1100338978864\n",
      "Stochastic Gradient Descent(2765): loss=0.3964477527764636\n",
      "Stochastic Gradient Descent(2766): loss=0.49819559066360586\n",
      "Stochastic Gradient Descent(2767): loss=52.635704986714806\n",
      "Stochastic Gradient Descent(2768): loss=4.071232641562653\n",
      "Stochastic Gradient Descent(2769): loss=33.74674829864494\n",
      "Stochastic Gradient Descent(2770): loss=25.922227311572726\n",
      "Stochastic Gradient Descent(2771): loss=156.16285060208716\n",
      "Stochastic Gradient Descent(2772): loss=36.010456329091454\n",
      "Stochastic Gradient Descent(2773): loss=30.4206474104815\n",
      "Stochastic Gradient Descent(2774): loss=5.569229850344064\n",
      "Stochastic Gradient Descent(2775): loss=0.0009355361331111438\n",
      "Stochastic Gradient Descent(2776): loss=35.341134374934065\n",
      "Stochastic Gradient Descent(2777): loss=37.08678156971174\n",
      "Stochastic Gradient Descent(2778): loss=1.928820288617975\n",
      "Stochastic Gradient Descent(2779): loss=4.45484263910619\n",
      "Stochastic Gradient Descent(2780): loss=0.26321413192315624\n",
      "Stochastic Gradient Descent(2781): loss=3.1178844413974476\n",
      "Stochastic Gradient Descent(2782): loss=0.48229438282318887\n",
      "Stochastic Gradient Descent(2783): loss=69.29240933335403\n",
      "Stochastic Gradient Descent(2784): loss=28.544315874421002\n",
      "Stochastic Gradient Descent(2785): loss=16.382615889644125\n",
      "Stochastic Gradient Descent(2786): loss=8.498129884519109\n",
      "Stochastic Gradient Descent(2787): loss=0.9668729275745634\n",
      "Stochastic Gradient Descent(2788): loss=0.12231753070852801\n",
      "Stochastic Gradient Descent(2789): loss=72.73577207047757\n",
      "Stochastic Gradient Descent(2790): loss=56.59014527205324\n",
      "Stochastic Gradient Descent(2791): loss=70.81885806543688\n",
      "Stochastic Gradient Descent(2792): loss=1.04873457489635\n",
      "Stochastic Gradient Descent(2793): loss=0.19581884453576015\n",
      "Stochastic Gradient Descent(2794): loss=37.794314775515254\n",
      "Stochastic Gradient Descent(2795): loss=37.33221515196901\n",
      "Stochastic Gradient Descent(2796): loss=3.322054068464157\n",
      "Stochastic Gradient Descent(2797): loss=2.876567156632066\n",
      "Stochastic Gradient Descent(2798): loss=44.41241033434487\n",
      "Stochastic Gradient Descent(2799): loss=4.066770061170939\n",
      "Stochastic Gradient Descent(2800): loss=1.259500689945276\n",
      "Stochastic Gradient Descent(2801): loss=0.32808095667983683\n",
      "Stochastic Gradient Descent(2802): loss=61.9572975639064\n",
      "Stochastic Gradient Descent(2803): loss=5.661543719204783\n",
      "Stochastic Gradient Descent(2804): loss=3.268098349430373\n",
      "Stochastic Gradient Descent(2805): loss=14.631767815455229\n",
      "Stochastic Gradient Descent(2806): loss=2.2496245439486433\n",
      "Stochastic Gradient Descent(2807): loss=4.288263340942561\n",
      "Stochastic Gradient Descent(2808): loss=6.3575025685371775\n",
      "Stochastic Gradient Descent(2809): loss=0.38957142285264235\n",
      "Stochastic Gradient Descent(2810): loss=55.723775766092544\n",
      "Stochastic Gradient Descent(2811): loss=7.34333790361558\n",
      "Stochastic Gradient Descent(2812): loss=113.50115256774635\n",
      "Stochastic Gradient Descent(2813): loss=33.34226954767991\n",
      "Stochastic Gradient Descent(2814): loss=31.10412774820662\n",
      "Stochastic Gradient Descent(2815): loss=1.4744682289352773\n",
      "Stochastic Gradient Descent(2816): loss=0.46739667250657896\n",
      "Stochastic Gradient Descent(2817): loss=15.400505380256163\n",
      "Stochastic Gradient Descent(2818): loss=0.03453786162103797\n",
      "Stochastic Gradient Descent(2819): loss=19.37628982999503\n",
      "Stochastic Gradient Descent(2820): loss=7.072625799323674\n",
      "Stochastic Gradient Descent(2821): loss=2.67108483113831\n",
      "Stochastic Gradient Descent(2822): loss=26.47308053639707\n",
      "Stochastic Gradient Descent(2823): loss=1.9875150529396248\n",
      "Stochastic Gradient Descent(2824): loss=4.995043948654932\n",
      "Stochastic Gradient Descent(2825): loss=61.83025315430337\n",
      "Stochastic Gradient Descent(2826): loss=43.42289304749923\n",
      "Stochastic Gradient Descent(2827): loss=0.2574219346490866\n",
      "Stochastic Gradient Descent(2828): loss=17.9288794000397\n",
      "Stochastic Gradient Descent(2829): loss=101.44426443864162\n",
      "Stochastic Gradient Descent(2830): loss=25.060291947620044\n",
      "Stochastic Gradient Descent(2831): loss=50.79927236302527\n",
      "Stochastic Gradient Descent(2832): loss=15.787781265605597\n",
      "Stochastic Gradient Descent(2833): loss=7.8367831193945605\n",
      "Stochastic Gradient Descent(2834): loss=0.028912465634476046\n",
      "Stochastic Gradient Descent(2835): loss=0.7872329443063563\n",
      "Stochastic Gradient Descent(2836): loss=2.2225386168972308\n",
      "Stochastic Gradient Descent(2837): loss=8.669558463848855\n",
      "Stochastic Gradient Descent(2838): loss=5.124760636974227\n",
      "Stochastic Gradient Descent(2839): loss=17.688858786404978\n",
      "Stochastic Gradient Descent(2840): loss=43.45922644196918\n",
      "Stochastic Gradient Descent(2841): loss=613.9597677513392\n",
      "Stochastic Gradient Descent(2842): loss=30.304993413187916\n",
      "Stochastic Gradient Descent(2843): loss=8.284525801867602\n",
      "Stochastic Gradient Descent(2844): loss=3.8700000662021807\n",
      "Stochastic Gradient Descent(2845): loss=0.48753048339068633\n",
      "Stochastic Gradient Descent(2846): loss=303.99526440161117\n",
      "Stochastic Gradient Descent(2847): loss=828.7787873540371\n",
      "Stochastic Gradient Descent(2848): loss=0.012304612234907187\n",
      "Stochastic Gradient Descent(2849): loss=6.952441926923305\n",
      "Stochastic Gradient Descent(2850): loss=0.06416592450262813\n",
      "Stochastic Gradient Descent(2851): loss=32.946333903121044\n",
      "Stochastic Gradient Descent(2852): loss=5.356472705501953\n",
      "Stochastic Gradient Descent(2853): loss=46.39992101254073\n",
      "Stochastic Gradient Descent(2854): loss=107.60426868621113\n",
      "Stochastic Gradient Descent(2855): loss=4.743985991499789\n",
      "Stochastic Gradient Descent(2856): loss=37.34745784255881\n",
      "Stochastic Gradient Descent(2857): loss=12.31934939853728\n",
      "Stochastic Gradient Descent(2858): loss=18.18185780339466\n",
      "Stochastic Gradient Descent(2859): loss=4.777618653027632\n",
      "Stochastic Gradient Descent(2860): loss=4.064383729369197\n",
      "Stochastic Gradient Descent(2861): loss=19.086387191361617\n",
      "Stochastic Gradient Descent(2862): loss=4.513039390630877\n",
      "Stochastic Gradient Descent(2863): loss=26.95744356601853\n",
      "Stochastic Gradient Descent(2864): loss=3.8406844919429353\n",
      "Stochastic Gradient Descent(2865): loss=23.805835808959255\n",
      "Stochastic Gradient Descent(2866): loss=39.97238155534156\n",
      "Stochastic Gradient Descent(2867): loss=9.831935087604666\n",
      "Stochastic Gradient Descent(2868): loss=17.74339703582896\n",
      "Stochastic Gradient Descent(2869): loss=4.595716852327287\n",
      "Stochastic Gradient Descent(2870): loss=5.5921075949530765\n",
      "Stochastic Gradient Descent(2871): loss=1.2221077534253468\n",
      "Stochastic Gradient Descent(2872): loss=32.905655351762434\n",
      "Stochastic Gradient Descent(2873): loss=1.2447676522250446\n",
      "Stochastic Gradient Descent(2874): loss=5.952254734403554\n",
      "Stochastic Gradient Descent(2875): loss=22.818574155793588\n",
      "Stochastic Gradient Descent(2876): loss=0.03811568843138201\n",
      "Stochastic Gradient Descent(2877): loss=1.4631251102328136\n",
      "Stochastic Gradient Descent(2878): loss=39.50241635897461\n",
      "Stochastic Gradient Descent(2879): loss=4.117332912822306\n",
      "Stochastic Gradient Descent(2880): loss=48.26402851860677\n",
      "Stochastic Gradient Descent(2881): loss=0.11061690730136872\n",
      "Stochastic Gradient Descent(2882): loss=315.0090138198273\n",
      "Stochastic Gradient Descent(2883): loss=1.6324449525640028\n",
      "Stochastic Gradient Descent(2884): loss=184.4978449606216\n",
      "Stochastic Gradient Descent(2885): loss=4.05019738291554\n",
      "Stochastic Gradient Descent(2886): loss=63.96792685628787\n",
      "Stochastic Gradient Descent(2887): loss=81.54684427827873\n",
      "Stochastic Gradient Descent(2888): loss=0.019652916497052562\n",
      "Stochastic Gradient Descent(2889): loss=2.6125734557037794\n",
      "Stochastic Gradient Descent(2890): loss=1.3624305106999888\n",
      "Stochastic Gradient Descent(2891): loss=196.6621846711384\n",
      "Stochastic Gradient Descent(2892): loss=71.51178955704127\n",
      "Stochastic Gradient Descent(2893): loss=13.22280780641359\n",
      "Stochastic Gradient Descent(2894): loss=79.00755417565978\n",
      "Stochastic Gradient Descent(2895): loss=46.03233745896302\n",
      "Stochastic Gradient Descent(2896): loss=13.390401763128077\n",
      "Stochastic Gradient Descent(2897): loss=13.038078648975956\n",
      "Stochastic Gradient Descent(2898): loss=10.80431772826989\n",
      "Stochastic Gradient Descent(2899): loss=0.1013394153422528\n",
      "Stochastic Gradient Descent(2900): loss=99.59916242013409\n",
      "Stochastic Gradient Descent(2901): loss=1.8484389592629316\n",
      "Stochastic Gradient Descent(2902): loss=40.72040539547789\n",
      "Stochastic Gradient Descent(2903): loss=8.125165242372793\n",
      "Stochastic Gradient Descent(2904): loss=8.15350037301809\n",
      "Stochastic Gradient Descent(2905): loss=11.255182656859478\n",
      "Stochastic Gradient Descent(2906): loss=0.04382551310418215\n",
      "Stochastic Gradient Descent(2907): loss=20.631330986958826\n",
      "Stochastic Gradient Descent(2908): loss=8.670848084795146\n",
      "Stochastic Gradient Descent(2909): loss=7.697676811605498\n",
      "Stochastic Gradient Descent(2910): loss=7.460387202175711\n",
      "Stochastic Gradient Descent(2911): loss=1.7168057021230803\n",
      "Stochastic Gradient Descent(2912): loss=11.365455152322086\n",
      "Stochastic Gradient Descent(2913): loss=28.21652548798219\n",
      "Stochastic Gradient Descent(2914): loss=77.58106551142795\n",
      "Stochastic Gradient Descent(2915): loss=13.365382666322958\n",
      "Stochastic Gradient Descent(2916): loss=44.37340086759853\n",
      "Stochastic Gradient Descent(2917): loss=2.8257248472659655\n",
      "Stochastic Gradient Descent(2918): loss=50.045282320478854\n",
      "Stochastic Gradient Descent(2919): loss=135.39433478636968\n",
      "Stochastic Gradient Descent(2920): loss=25.247222029949985\n",
      "Stochastic Gradient Descent(2921): loss=9.714570790354518\n",
      "Stochastic Gradient Descent(2922): loss=7.3865804827244315\n",
      "Stochastic Gradient Descent(2923): loss=16.77733759352031\n",
      "Stochastic Gradient Descent(2924): loss=40.72497013040951\n",
      "Stochastic Gradient Descent(2925): loss=70.7204252247169\n",
      "Stochastic Gradient Descent(2926): loss=4.849449467847929\n",
      "Stochastic Gradient Descent(2927): loss=5.707634802960226\n",
      "Stochastic Gradient Descent(2928): loss=32.16535780882523\n",
      "Stochastic Gradient Descent(2929): loss=15.520289512764126\n",
      "Stochastic Gradient Descent(2930): loss=37.11214480391414\n",
      "Stochastic Gradient Descent(2931): loss=251.10844405273937\n",
      "Stochastic Gradient Descent(2932): loss=30.826489330521934\n",
      "Stochastic Gradient Descent(2933): loss=367.15115123932037\n",
      "Stochastic Gradient Descent(2934): loss=19.500217583841955\n",
      "Stochastic Gradient Descent(2935): loss=29.46701153732459\n",
      "Stochastic Gradient Descent(2936): loss=419.35227237061713\n",
      "Stochastic Gradient Descent(2937): loss=0.032727503949932436\n",
      "Stochastic Gradient Descent(2938): loss=0.004929996725990432\n",
      "Stochastic Gradient Descent(2939): loss=22.791944916952204\n",
      "Stochastic Gradient Descent(2940): loss=137.0488429601993\n",
      "Stochastic Gradient Descent(2941): loss=58.9598097535992\n",
      "Stochastic Gradient Descent(2942): loss=1.7134702579467578\n",
      "Stochastic Gradient Descent(2943): loss=53.452190827448945\n",
      "Stochastic Gradient Descent(2944): loss=57.07692706673536\n",
      "Stochastic Gradient Descent(2945): loss=161.2769567225712\n",
      "Stochastic Gradient Descent(2946): loss=26.799192179702878\n",
      "Stochastic Gradient Descent(2947): loss=4.5068334714374965\n",
      "Stochastic Gradient Descent(2948): loss=1.9572995736481518\n",
      "Stochastic Gradient Descent(2949): loss=17.152381567369734\n",
      "Stochastic Gradient Descent(2950): loss=53.031005483265055\n",
      "Stochastic Gradient Descent(2951): loss=35.455187435490124\n",
      "Stochastic Gradient Descent(2952): loss=20.15090297027515\n",
      "Stochastic Gradient Descent(2953): loss=0.059001753339725294\n",
      "Stochastic Gradient Descent(2954): loss=21.146255382094175\n",
      "Stochastic Gradient Descent(2955): loss=4.10567552666638\n",
      "Stochastic Gradient Descent(2956): loss=8.761325414679652\n",
      "Stochastic Gradient Descent(2957): loss=12.510756621692641\n",
      "Stochastic Gradient Descent(2958): loss=40.63678723056393\n",
      "Stochastic Gradient Descent(2959): loss=3.034097655433653\n",
      "Stochastic Gradient Descent(2960): loss=35.84805122738938\n",
      "Stochastic Gradient Descent(2961): loss=82.53160430627898\n",
      "Stochastic Gradient Descent(2962): loss=1.1248400822472453\n",
      "Stochastic Gradient Descent(2963): loss=0.0765331564823302\n",
      "Stochastic Gradient Descent(2964): loss=5.721114713486656\n",
      "Stochastic Gradient Descent(2965): loss=14.780791116855431\n",
      "Stochastic Gradient Descent(2966): loss=157.95327261300602\n",
      "Stochastic Gradient Descent(2967): loss=5.156047571053346\n",
      "Stochastic Gradient Descent(2968): loss=0.9579439326264104\n",
      "Stochastic Gradient Descent(2969): loss=0.004708438251744692\n",
      "Stochastic Gradient Descent(2970): loss=1.2647436690845364\n",
      "Stochastic Gradient Descent(2971): loss=11.352731091219772\n",
      "Stochastic Gradient Descent(2972): loss=2.852702599573203\n",
      "Stochastic Gradient Descent(2973): loss=9.601499098664082\n",
      "Stochastic Gradient Descent(2974): loss=1.3077171404757175\n",
      "Stochastic Gradient Descent(2975): loss=28.387004850874916\n",
      "Stochastic Gradient Descent(2976): loss=3.337864386439343\n",
      "Stochastic Gradient Descent(2977): loss=4.372032974089962e-06\n",
      "Stochastic Gradient Descent(2978): loss=2.068472684856049\n",
      "Stochastic Gradient Descent(2979): loss=0.20701865322611762\n",
      "Stochastic Gradient Descent(2980): loss=1.8675894379746873\n",
      "Stochastic Gradient Descent(2981): loss=9.269784594427904\n",
      "Stochastic Gradient Descent(2982): loss=0.1471405755037748\n",
      "Stochastic Gradient Descent(2983): loss=6.414301330557723\n",
      "Stochastic Gradient Descent(2984): loss=0.05818585563519283\n",
      "Stochastic Gradient Descent(2985): loss=0.1908981337063092\n",
      "Stochastic Gradient Descent(2986): loss=88.07516321714085\n",
      "Stochastic Gradient Descent(2987): loss=0.7508509367439334\n",
      "Stochastic Gradient Descent(2988): loss=22.658010312184068\n",
      "Stochastic Gradient Descent(2989): loss=179.35900583979736\n",
      "Stochastic Gradient Descent(2990): loss=0.9823853146312821\n",
      "Stochastic Gradient Descent(2991): loss=28.752302263214066\n",
      "Stochastic Gradient Descent(2992): loss=223.55333514092072\n",
      "Stochastic Gradient Descent(2993): loss=87.27578315561286\n",
      "Stochastic Gradient Descent(2994): loss=73.25437699680661\n",
      "Stochastic Gradient Descent(2995): loss=0.027748501856101865\n",
      "Stochastic Gradient Descent(2996): loss=0.16039134953548248\n",
      "Stochastic Gradient Descent(2997): loss=38.01982455167881\n",
      "Stochastic Gradient Descent(2998): loss=0.073485997859349\n",
      "Stochastic Gradient Descent(2999): loss=16.99637129905394\n",
      "Stochastic Gradient Descent(3000): loss=27.43600540679626\n",
      "Stochastic Gradient Descent(3001): loss=11.719865017823025\n",
      "Stochastic Gradient Descent(3002): loss=52.07611984478032\n",
      "Stochastic Gradient Descent(3003): loss=0.06576376359302967\n",
      "Stochastic Gradient Descent(3004): loss=0.4785423638727799\n",
      "Stochastic Gradient Descent(3005): loss=5.371339129004113\n",
      "Stochastic Gradient Descent(3006): loss=1.576749704082881\n",
      "Stochastic Gradient Descent(3007): loss=0.1207321630380024\n",
      "Stochastic Gradient Descent(3008): loss=17.413336982551215\n",
      "Stochastic Gradient Descent(3009): loss=17.574676370435313\n",
      "Stochastic Gradient Descent(3010): loss=2.3671964608686316\n",
      "Stochastic Gradient Descent(3011): loss=5.739468547869314\n",
      "Stochastic Gradient Descent(3012): loss=0.7838402684119606\n",
      "Stochastic Gradient Descent(3013): loss=69.33566423109458\n",
      "Stochastic Gradient Descent(3014): loss=36.70531228226435\n",
      "Stochastic Gradient Descent(3015): loss=26.624662165715268\n",
      "Stochastic Gradient Descent(3016): loss=5.703564342855843\n",
      "Stochastic Gradient Descent(3017): loss=16.842927004906123\n",
      "Stochastic Gradient Descent(3018): loss=0.016824933505318235\n",
      "Stochastic Gradient Descent(3019): loss=34.6409434488754\n",
      "Stochastic Gradient Descent(3020): loss=5.612412228377109\n",
      "Stochastic Gradient Descent(3021): loss=8.844835987150958\n",
      "Stochastic Gradient Descent(3022): loss=167.9412815129199\n",
      "Stochastic Gradient Descent(3023): loss=11.652084238836238\n",
      "Stochastic Gradient Descent(3024): loss=29.90608379399918\n",
      "Stochastic Gradient Descent(3025): loss=0.3799271244773477\n",
      "Stochastic Gradient Descent(3026): loss=14.061406908430138\n",
      "Stochastic Gradient Descent(3027): loss=2.9440043368792814\n",
      "Stochastic Gradient Descent(3028): loss=11.083643108979386\n",
      "Stochastic Gradient Descent(3029): loss=47.780699200934386\n",
      "Stochastic Gradient Descent(3030): loss=80.38947065143468\n",
      "Stochastic Gradient Descent(3031): loss=0.025497376107235514\n",
      "Stochastic Gradient Descent(3032): loss=0.29929056501534274\n",
      "Stochastic Gradient Descent(3033): loss=3.088379085873038\n",
      "Stochastic Gradient Descent(3034): loss=6.599246453835138\n",
      "Stochastic Gradient Descent(3035): loss=11.334606041407515\n",
      "Stochastic Gradient Descent(3036): loss=0.039821763573015125\n",
      "Stochastic Gradient Descent(3037): loss=12.018977843620338\n",
      "Stochastic Gradient Descent(3038): loss=9.93830071433531\n",
      "Stochastic Gradient Descent(3039): loss=0.01273715954737415\n",
      "Stochastic Gradient Descent(3040): loss=454.38046936733866\n",
      "Stochastic Gradient Descent(3041): loss=83.69339777191381\n",
      "Stochastic Gradient Descent(3042): loss=4.308007350531325\n",
      "Stochastic Gradient Descent(3043): loss=354.2546412954187\n",
      "Stochastic Gradient Descent(3044): loss=62.77214781964013\n",
      "Stochastic Gradient Descent(3045): loss=124.18763696847246\n",
      "Stochastic Gradient Descent(3046): loss=11.203166514074715\n",
      "Stochastic Gradient Descent(3047): loss=0.3141934074946662\n",
      "Stochastic Gradient Descent(3048): loss=43.35683773354177\n",
      "Stochastic Gradient Descent(3049): loss=31.114325387414464\n",
      "Stochastic Gradient Descent(3050): loss=18.199068429233698\n",
      "Stochastic Gradient Descent(3051): loss=8.812469508954907\n",
      "Stochastic Gradient Descent(3052): loss=32.93036476195395\n",
      "Stochastic Gradient Descent(3053): loss=82.17958431127282\n",
      "Stochastic Gradient Descent(3054): loss=2.423125543975434\n",
      "Stochastic Gradient Descent(3055): loss=5.493933544362746\n",
      "Stochastic Gradient Descent(3056): loss=3.822608724038566\n",
      "Stochastic Gradient Descent(3057): loss=0.732940272750115\n",
      "Stochastic Gradient Descent(3058): loss=53.82358538683112\n",
      "Stochastic Gradient Descent(3059): loss=0.6405078902902351\n",
      "Stochastic Gradient Descent(3060): loss=24.054707202937628\n",
      "Stochastic Gradient Descent(3061): loss=17.547849260717687\n",
      "Stochastic Gradient Descent(3062): loss=62.2484419797197\n",
      "Stochastic Gradient Descent(3063): loss=132.625479450538\n",
      "Stochastic Gradient Descent(3064): loss=3.7961897924688714\n",
      "Stochastic Gradient Descent(3065): loss=2.461230255855545\n",
      "Stochastic Gradient Descent(3066): loss=13.097562930879977\n",
      "Stochastic Gradient Descent(3067): loss=7.191967975686194\n",
      "Stochastic Gradient Descent(3068): loss=0.0678718508446627\n",
      "Stochastic Gradient Descent(3069): loss=48.3157168786568\n",
      "Stochastic Gradient Descent(3070): loss=6.634491967956712\n",
      "Stochastic Gradient Descent(3071): loss=12.67508086878382\n",
      "Stochastic Gradient Descent(3072): loss=1.9618358842098147\n",
      "Stochastic Gradient Descent(3073): loss=4.546598507455965\n",
      "Stochastic Gradient Descent(3074): loss=1.1122469707817533\n",
      "Stochastic Gradient Descent(3075): loss=7.623671174956505\n",
      "Stochastic Gradient Descent(3076): loss=0.3262343324685962\n",
      "Stochastic Gradient Descent(3077): loss=33.747365811891285\n",
      "Stochastic Gradient Descent(3078): loss=422.7668599128524\n",
      "Stochastic Gradient Descent(3079): loss=216.07389069447524\n",
      "Stochastic Gradient Descent(3080): loss=6.072641881922538\n",
      "Stochastic Gradient Descent(3081): loss=37.82858190642489\n",
      "Stochastic Gradient Descent(3082): loss=55.224736199509245\n",
      "Stochastic Gradient Descent(3083): loss=34.01022292228309\n",
      "Stochastic Gradient Descent(3084): loss=16.267948001617334\n",
      "Stochastic Gradient Descent(3085): loss=19.059287044948018\n",
      "Stochastic Gradient Descent(3086): loss=9.918276603508886\n",
      "Stochastic Gradient Descent(3087): loss=30.82289820702726\n",
      "Stochastic Gradient Descent(3088): loss=75.69281338030694\n",
      "Stochastic Gradient Descent(3089): loss=15.944355021083497\n",
      "Stochastic Gradient Descent(3090): loss=0.8160251408381511\n",
      "Stochastic Gradient Descent(3091): loss=1.1214422218464377\n",
      "Stochastic Gradient Descent(3092): loss=0.769595459125104\n",
      "Stochastic Gradient Descent(3093): loss=5.525896747728785\n",
      "Stochastic Gradient Descent(3094): loss=16.42751966940417\n",
      "Stochastic Gradient Descent(3095): loss=13.245812909730242\n",
      "Stochastic Gradient Descent(3096): loss=38.08441060630826\n",
      "Stochastic Gradient Descent(3097): loss=7.741605595415875\n",
      "Stochastic Gradient Descent(3098): loss=15.718038192075348\n",
      "Stochastic Gradient Descent(3099): loss=13.852475163209787\n",
      "Stochastic Gradient Descent(3100): loss=1.0373382370230166\n",
      "Stochastic Gradient Descent(3101): loss=14.537119995170146\n",
      "Stochastic Gradient Descent(3102): loss=2.6426726438587793\n",
      "Stochastic Gradient Descent(3103): loss=26.172240529400966\n",
      "Stochastic Gradient Descent(3104): loss=54.25280847531509\n",
      "Stochastic Gradient Descent(3105): loss=0.4035451471088516\n",
      "Stochastic Gradient Descent(3106): loss=0.7212678451012073\n",
      "Stochastic Gradient Descent(3107): loss=1.929893902483945\n",
      "Stochastic Gradient Descent(3108): loss=170.36189982286885\n",
      "Stochastic Gradient Descent(3109): loss=85.59579945779967\n",
      "Stochastic Gradient Descent(3110): loss=0.04223819750937628\n",
      "Stochastic Gradient Descent(3111): loss=48.929311812697144\n",
      "Stochastic Gradient Descent(3112): loss=17.701791117418026\n",
      "Stochastic Gradient Descent(3113): loss=1.7513600573489345\n",
      "Stochastic Gradient Descent(3114): loss=0.1271607068779114\n",
      "Stochastic Gradient Descent(3115): loss=189.18182879313267\n",
      "Stochastic Gradient Descent(3116): loss=0.04595027816932271\n",
      "Stochastic Gradient Descent(3117): loss=24.083089612219723\n",
      "Stochastic Gradient Descent(3118): loss=29.06300442483551\n",
      "Stochastic Gradient Descent(3119): loss=36.49495686099233\n",
      "Stochastic Gradient Descent(3120): loss=0.0371028129602281\n",
      "Stochastic Gradient Descent(3121): loss=64.02003341505075\n",
      "Stochastic Gradient Descent(3122): loss=10.06017995117922\n",
      "Stochastic Gradient Descent(3123): loss=1.4754887979698401\n",
      "Stochastic Gradient Descent(3124): loss=0.18221026742179866\n",
      "Stochastic Gradient Descent(3125): loss=22.084463860365627\n",
      "Stochastic Gradient Descent(3126): loss=20.445498291520675\n",
      "Stochastic Gradient Descent(3127): loss=70.16863744496548\n",
      "Stochastic Gradient Descent(3128): loss=17.854055076630374\n",
      "Stochastic Gradient Descent(3129): loss=12.26731228254068\n",
      "Stochastic Gradient Descent(3130): loss=1.9446880094503416\n",
      "Stochastic Gradient Descent(3131): loss=59.35511245452758\n",
      "Stochastic Gradient Descent(3132): loss=1.6527414855727984\n",
      "Stochastic Gradient Descent(3133): loss=8.138685809143224\n",
      "Stochastic Gradient Descent(3134): loss=5.177817656297561\n",
      "Stochastic Gradient Descent(3135): loss=0.24716220573725595\n",
      "Stochastic Gradient Descent(3136): loss=11.006594723437132\n",
      "Stochastic Gradient Descent(3137): loss=0.27975518274887023\n",
      "Stochastic Gradient Descent(3138): loss=8.59527801173765\n",
      "Stochastic Gradient Descent(3139): loss=5.888525617801661\n",
      "Stochastic Gradient Descent(3140): loss=28.01173817978462\n",
      "Stochastic Gradient Descent(3141): loss=0.24325309227132202\n",
      "Stochastic Gradient Descent(3142): loss=3.9043297397377064\n",
      "Stochastic Gradient Descent(3143): loss=19.68519444301993\n",
      "Stochastic Gradient Descent(3144): loss=69.91787590128317\n",
      "Stochastic Gradient Descent(3145): loss=21.909058558157405\n",
      "Stochastic Gradient Descent(3146): loss=21.58983474133151\n",
      "Stochastic Gradient Descent(3147): loss=19.397929085297687\n",
      "Stochastic Gradient Descent(3148): loss=1.1948385704033937\n",
      "Stochastic Gradient Descent(3149): loss=38.89444285959401\n",
      "Stochastic Gradient Descent(3150): loss=24.639030604320215\n",
      "Stochastic Gradient Descent(3151): loss=63.39780369208883\n",
      "Stochastic Gradient Descent(3152): loss=1.8426924850893986\n",
      "Stochastic Gradient Descent(3153): loss=10.471312295746353\n",
      "Stochastic Gradient Descent(3154): loss=9.480693669557002\n",
      "Stochastic Gradient Descent(3155): loss=0.525223536082971\n",
      "Stochastic Gradient Descent(3156): loss=13.35569754684757\n",
      "Stochastic Gradient Descent(3157): loss=4.691326812591546\n",
      "Stochastic Gradient Descent(3158): loss=33.36826841339188\n",
      "Stochastic Gradient Descent(3159): loss=144.7096311190151\n",
      "Stochastic Gradient Descent(3160): loss=20.50252506249142\n",
      "Stochastic Gradient Descent(3161): loss=42.48536160665722\n",
      "Stochastic Gradient Descent(3162): loss=7.932007140506328\n",
      "Stochastic Gradient Descent(3163): loss=57.749883425858485\n",
      "Stochastic Gradient Descent(3164): loss=13.720744025807603\n",
      "Stochastic Gradient Descent(3165): loss=3.572883850878211\n",
      "Stochastic Gradient Descent(3166): loss=54.11665304230511\n",
      "Stochastic Gradient Descent(3167): loss=69.97810959265952\n",
      "Stochastic Gradient Descent(3168): loss=19.926635721946557\n",
      "Stochastic Gradient Descent(3169): loss=0.9604537803023327\n",
      "Stochastic Gradient Descent(3170): loss=0.11856752269822758\n",
      "Stochastic Gradient Descent(3171): loss=32.31175465499704\n",
      "Stochastic Gradient Descent(3172): loss=31.80567179849408\n",
      "Stochastic Gradient Descent(3173): loss=7.217180703973342\n",
      "Stochastic Gradient Descent(3174): loss=5.39160327867854\n",
      "Stochastic Gradient Descent(3175): loss=181.67428950175866\n",
      "Stochastic Gradient Descent(3176): loss=37.02366675750033\n",
      "Stochastic Gradient Descent(3177): loss=3.3357073618070254\n",
      "Stochastic Gradient Descent(3178): loss=58.43841388525532\n",
      "Stochastic Gradient Descent(3179): loss=2.1290930480680794\n",
      "Stochastic Gradient Descent(3180): loss=27.257243874780254\n",
      "Stochastic Gradient Descent(3181): loss=7.939935575908722\n",
      "Stochastic Gradient Descent(3182): loss=45.18787545571362\n",
      "Stochastic Gradient Descent(3183): loss=60.55240665168527\n",
      "Stochastic Gradient Descent(3184): loss=69.85545688252184\n",
      "Stochastic Gradient Descent(3185): loss=4.100467660626564\n",
      "Stochastic Gradient Descent(3186): loss=76.05010318605125\n",
      "Stochastic Gradient Descent(3187): loss=1.5812930883695615\n",
      "Stochastic Gradient Descent(3188): loss=9.324655152452415\n",
      "Stochastic Gradient Descent(3189): loss=1.7828843786002058\n",
      "Stochastic Gradient Descent(3190): loss=33.516321687861684\n",
      "Stochastic Gradient Descent(3191): loss=0.683930052059468\n",
      "Stochastic Gradient Descent(3192): loss=124.97204950274308\n",
      "Stochastic Gradient Descent(3193): loss=36.30592293235329\n",
      "Stochastic Gradient Descent(3194): loss=181.9772383406579\n",
      "Stochastic Gradient Descent(3195): loss=10.496549081716886\n",
      "Stochastic Gradient Descent(3196): loss=35.223284918137026\n",
      "Stochastic Gradient Descent(3197): loss=146.83575510398126\n",
      "Stochastic Gradient Descent(3198): loss=37.937008423685064\n",
      "Stochastic Gradient Descent(3199): loss=15.022803762455734\n",
      "Stochastic Gradient Descent(3200): loss=73.08151743610084\n",
      "Stochastic Gradient Descent(3201): loss=0.18239688033983992\n",
      "Stochastic Gradient Descent(3202): loss=0.42110439283825873\n",
      "Stochastic Gradient Descent(3203): loss=12.928074062010058\n",
      "Stochastic Gradient Descent(3204): loss=2.0550846493178296\n",
      "Stochastic Gradient Descent(3205): loss=128.06329213726147\n",
      "Stochastic Gradient Descent(3206): loss=141.67777348772177\n",
      "Stochastic Gradient Descent(3207): loss=0.293294787178989\n",
      "Stochastic Gradient Descent(3208): loss=2.6134179754388063\n",
      "Stochastic Gradient Descent(3209): loss=0.002163930486276051\n",
      "Stochastic Gradient Descent(3210): loss=4.889695253516211\n",
      "Stochastic Gradient Descent(3211): loss=17.290704121922573\n",
      "Stochastic Gradient Descent(3212): loss=13.477091202971788\n",
      "Stochastic Gradient Descent(3213): loss=1.6911781852587642\n",
      "Stochastic Gradient Descent(3214): loss=74.19946803891081\n",
      "Stochastic Gradient Descent(3215): loss=0.0035512947845885057\n",
      "Stochastic Gradient Descent(3216): loss=10.679885421308322\n",
      "Stochastic Gradient Descent(3217): loss=0.5000596925580328\n",
      "Stochastic Gradient Descent(3218): loss=32.26219451304966\n",
      "Stochastic Gradient Descent(3219): loss=0.7952144109823364\n",
      "Stochastic Gradient Descent(3220): loss=0.18812181775829592\n",
      "Stochastic Gradient Descent(3221): loss=1.501287433994629\n",
      "Stochastic Gradient Descent(3222): loss=31.682135268322025\n",
      "Stochastic Gradient Descent(3223): loss=28.79583294071848\n",
      "Stochastic Gradient Descent(3224): loss=13.025441631739927\n",
      "Stochastic Gradient Descent(3225): loss=45.597088702540326\n",
      "Stochastic Gradient Descent(3226): loss=106.81794197361116\n",
      "Stochastic Gradient Descent(3227): loss=0.21672322893419868\n",
      "Stochastic Gradient Descent(3228): loss=47.33071684730147\n",
      "Stochastic Gradient Descent(3229): loss=0.0021049830465837967\n",
      "Stochastic Gradient Descent(3230): loss=22.436424053662165\n",
      "Stochastic Gradient Descent(3231): loss=0.10567920336544255\n",
      "Stochastic Gradient Descent(3232): loss=14.899248255181346\n",
      "Stochastic Gradient Descent(3233): loss=24.31814519337647\n",
      "Stochastic Gradient Descent(3234): loss=4.768193157799805\n",
      "Stochastic Gradient Descent(3235): loss=5.679095368322621\n",
      "Stochastic Gradient Descent(3236): loss=12.723084010044483\n",
      "Stochastic Gradient Descent(3237): loss=3.806128313457963\n",
      "Stochastic Gradient Descent(3238): loss=39.10073565161027\n",
      "Stochastic Gradient Descent(3239): loss=6.9010094719797825\n",
      "Stochastic Gradient Descent(3240): loss=15.22516950203438\n",
      "Stochastic Gradient Descent(3241): loss=17.28585385232215\n",
      "Stochastic Gradient Descent(3242): loss=37.639622140447656\n",
      "Stochastic Gradient Descent(3243): loss=212.979817242695\n",
      "Stochastic Gradient Descent(3244): loss=28.258014251301354\n",
      "Stochastic Gradient Descent(3245): loss=205.64453642077405\n",
      "Stochastic Gradient Descent(3246): loss=2.0343337599353255\n",
      "Stochastic Gradient Descent(3247): loss=39.680024212136516\n",
      "Stochastic Gradient Descent(3248): loss=0.9392444890182643\n",
      "Stochastic Gradient Descent(3249): loss=6.713934427741478\n",
      "Stochastic Gradient Descent(3250): loss=123.86996419643403\n",
      "Stochastic Gradient Descent(3251): loss=0.8994961325815036\n",
      "Stochastic Gradient Descent(3252): loss=4.020573181966807\n",
      "Stochastic Gradient Descent(3253): loss=36.851897845304855\n",
      "Stochastic Gradient Descent(3254): loss=48.5937283967351\n",
      "Stochastic Gradient Descent(3255): loss=7.000992961973952\n",
      "Stochastic Gradient Descent(3256): loss=75.78288479177478\n",
      "Stochastic Gradient Descent(3257): loss=22.6663924320233\n",
      "Stochastic Gradient Descent(3258): loss=11.894985103195841\n",
      "Stochastic Gradient Descent(3259): loss=13.467328361963089\n",
      "Stochastic Gradient Descent(3260): loss=0.7089935769277687\n",
      "Stochastic Gradient Descent(3261): loss=1.99614152705982\n",
      "Stochastic Gradient Descent(3262): loss=12.028258203191847\n",
      "Stochastic Gradient Descent(3263): loss=47.16102083895028\n",
      "Stochastic Gradient Descent(3264): loss=38.09000392664613\n",
      "Stochastic Gradient Descent(3265): loss=43.830235508026036\n",
      "Stochastic Gradient Descent(3266): loss=1.2403503757581535\n",
      "Stochastic Gradient Descent(3267): loss=135.68150552964457\n",
      "Stochastic Gradient Descent(3268): loss=19.080137019093648\n",
      "Stochastic Gradient Descent(3269): loss=9.127604436039876\n",
      "Stochastic Gradient Descent(3270): loss=7.231121917379414\n",
      "Stochastic Gradient Descent(3271): loss=26.19683927091876\n",
      "Stochastic Gradient Descent(3272): loss=1.6737330915679853\n",
      "Stochastic Gradient Descent(3273): loss=35.89820754272305\n",
      "Stochastic Gradient Descent(3274): loss=0.3451349164318853\n",
      "Stochastic Gradient Descent(3275): loss=25.278004887116435\n",
      "Stochastic Gradient Descent(3276): loss=8.612964344156238\n",
      "Stochastic Gradient Descent(3277): loss=6.111603409816076\n",
      "Stochastic Gradient Descent(3278): loss=34.321116154602066\n",
      "Stochastic Gradient Descent(3279): loss=0.020857018956177007\n",
      "Stochastic Gradient Descent(3280): loss=134.03790752503977\n",
      "Stochastic Gradient Descent(3281): loss=0.12254959402602411\n",
      "Stochastic Gradient Descent(3282): loss=45.02627767421108\n",
      "Stochastic Gradient Descent(3283): loss=0.25926104843235037\n",
      "Stochastic Gradient Descent(3284): loss=88.86020002819343\n",
      "Stochastic Gradient Descent(3285): loss=8.869534187728176\n",
      "Stochastic Gradient Descent(3286): loss=9.339367755953702\n",
      "Stochastic Gradient Descent(3287): loss=43.077578923052464\n",
      "Stochastic Gradient Descent(3288): loss=0.8743784558287736\n",
      "Stochastic Gradient Descent(3289): loss=2.38156210630508\n",
      "Stochastic Gradient Descent(3290): loss=19.61422673933876\n",
      "Stochastic Gradient Descent(3291): loss=18.794475559899553\n",
      "Stochastic Gradient Descent(3292): loss=26.814642293070403\n",
      "Stochastic Gradient Descent(3293): loss=35.002419992226116\n",
      "Stochastic Gradient Descent(3294): loss=8.172031506838252\n",
      "Stochastic Gradient Descent(3295): loss=20.84815616890881\n",
      "Stochastic Gradient Descent(3296): loss=3.670097657198694\n",
      "Stochastic Gradient Descent(3297): loss=35.538839442052364\n",
      "Stochastic Gradient Descent(3298): loss=0.24438277115730636\n",
      "Stochastic Gradient Descent(3299): loss=0.08673537936900864\n",
      "Stochastic Gradient Descent(3300): loss=7.573960373617168\n",
      "Stochastic Gradient Descent(3301): loss=12.032046496422582\n",
      "Stochastic Gradient Descent(3302): loss=0.2892733007893791\n",
      "Stochastic Gradient Descent(3303): loss=0.051489711278487796\n",
      "Stochastic Gradient Descent(3304): loss=3.5961966209791334\n",
      "Stochastic Gradient Descent(3305): loss=0.03270960601113689\n",
      "Stochastic Gradient Descent(3306): loss=2.091589583739391\n",
      "Stochastic Gradient Descent(3307): loss=0.019226522019109085\n",
      "Stochastic Gradient Descent(3308): loss=0.23646489720923092\n",
      "Stochastic Gradient Descent(3309): loss=1.4777863261408437\n",
      "Stochastic Gradient Descent(3310): loss=1.052654306912949\n",
      "Stochastic Gradient Descent(3311): loss=0.8911894583790451\n",
      "Stochastic Gradient Descent(3312): loss=3.084463551473974\n",
      "Stochastic Gradient Descent(3313): loss=36.49028227706249\n",
      "Stochastic Gradient Descent(3314): loss=7.826664094367323\n",
      "Stochastic Gradient Descent(3315): loss=40.51138026110509\n",
      "Stochastic Gradient Descent(3316): loss=162.03192337572978\n",
      "Stochastic Gradient Descent(3317): loss=0.10945771677336996\n",
      "Stochastic Gradient Descent(3318): loss=22.252182938860628\n",
      "Stochastic Gradient Descent(3319): loss=3.6067394419614995\n",
      "Stochastic Gradient Descent(3320): loss=6.81491466976993\n",
      "Stochastic Gradient Descent(3321): loss=106.71449483979282\n",
      "Stochastic Gradient Descent(3322): loss=5.483935224829598\n",
      "Stochastic Gradient Descent(3323): loss=14.008026845243469\n",
      "Stochastic Gradient Descent(3324): loss=3.437877365373235\n",
      "Stochastic Gradient Descent(3325): loss=7.6157687409421975\n",
      "Stochastic Gradient Descent(3326): loss=0.05292165706662214\n",
      "Stochastic Gradient Descent(3327): loss=20.372600642421197\n",
      "Stochastic Gradient Descent(3328): loss=1.7139131567839374\n",
      "Stochastic Gradient Descent(3329): loss=0.29722180381315494\n",
      "Stochastic Gradient Descent(3330): loss=19.95123457998284\n",
      "Stochastic Gradient Descent(3331): loss=0.015397885966963428\n",
      "Stochastic Gradient Descent(3332): loss=6.4275951211308735\n",
      "Stochastic Gradient Descent(3333): loss=0.9003531123949668\n",
      "Stochastic Gradient Descent(3334): loss=3.722958747217947\n",
      "Stochastic Gradient Descent(3335): loss=66.42780616646988\n",
      "Stochastic Gradient Descent(3336): loss=64.29932931102748\n",
      "Stochastic Gradient Descent(3337): loss=12.492938219931661\n",
      "Stochastic Gradient Descent(3338): loss=5.640030338512589\n",
      "Stochastic Gradient Descent(3339): loss=16.663486287304526\n",
      "Stochastic Gradient Descent(3340): loss=0.7007562465979693\n",
      "Stochastic Gradient Descent(3341): loss=0.18655447423203697\n",
      "Stochastic Gradient Descent(3342): loss=12.409272317442934\n",
      "Stochastic Gradient Descent(3343): loss=46.94496626072068\n",
      "Stochastic Gradient Descent(3344): loss=0.543173338641948\n",
      "Stochastic Gradient Descent(3345): loss=8.049880635652647\n",
      "Stochastic Gradient Descent(3346): loss=13.186800496001993\n",
      "Stochastic Gradient Descent(3347): loss=1.1370023343427758\n",
      "Stochastic Gradient Descent(3348): loss=9.38696862297156\n",
      "Stochastic Gradient Descent(3349): loss=2.4247035487019413\n",
      "Stochastic Gradient Descent(3350): loss=2.9704223873153697\n",
      "Stochastic Gradient Descent(3351): loss=16.69756287341093\n",
      "Stochastic Gradient Descent(3352): loss=9.054401442346043\n",
      "Stochastic Gradient Descent(3353): loss=5.371360442740708\n",
      "Stochastic Gradient Descent(3354): loss=0.2548140303373365\n",
      "Stochastic Gradient Descent(3355): loss=106.72655064831385\n",
      "Stochastic Gradient Descent(3356): loss=57.19028007491965\n",
      "Stochastic Gradient Descent(3357): loss=876.7370602895793\n",
      "Stochastic Gradient Descent(3358): loss=28.94029299452404\n",
      "Stochastic Gradient Descent(3359): loss=621.0752753276303\n",
      "Stochastic Gradient Descent(3360): loss=0.00011144279799651301\n",
      "Stochastic Gradient Descent(3361): loss=85.65914150908031\n",
      "Stochastic Gradient Descent(3362): loss=17.36481480307585\n",
      "Stochastic Gradient Descent(3363): loss=1.6766952679443021\n",
      "Stochastic Gradient Descent(3364): loss=35.98245379956643\n",
      "Stochastic Gradient Descent(3365): loss=0.03248645170038359\n",
      "Stochastic Gradient Descent(3366): loss=0.8138076241000575\n",
      "Stochastic Gradient Descent(3367): loss=9.52920833784104\n",
      "Stochastic Gradient Descent(3368): loss=1.1895537397913625\n",
      "Stochastic Gradient Descent(3369): loss=2.3218982912770425\n",
      "Stochastic Gradient Descent(3370): loss=10.17404948801361\n",
      "Stochastic Gradient Descent(3371): loss=37.109210501268265\n",
      "Stochastic Gradient Descent(3372): loss=4.551462826025149\n",
      "Stochastic Gradient Descent(3373): loss=25.791323797541388\n",
      "Stochastic Gradient Descent(3374): loss=7.065658707559212\n",
      "Stochastic Gradient Descent(3375): loss=4.227555407558819\n",
      "Stochastic Gradient Descent(3376): loss=12.249019007941985\n",
      "Stochastic Gradient Descent(3377): loss=12.80709976759126\n",
      "Stochastic Gradient Descent(3378): loss=7.761836843985064\n",
      "Stochastic Gradient Descent(3379): loss=2.655877559807414\n",
      "Stochastic Gradient Descent(3380): loss=12.178811849202583\n",
      "Stochastic Gradient Descent(3381): loss=4.84330785390302\n",
      "Stochastic Gradient Descent(3382): loss=1.7873554448770743\n",
      "Stochastic Gradient Descent(3383): loss=35.91375503950242\n",
      "Stochastic Gradient Descent(3384): loss=4.10201109428739\n",
      "Stochastic Gradient Descent(3385): loss=166.39104874918115\n",
      "Stochastic Gradient Descent(3386): loss=0.2895051493027689\n",
      "Stochastic Gradient Descent(3387): loss=4.947725627971152\n",
      "Stochastic Gradient Descent(3388): loss=8.537236372773757\n",
      "Stochastic Gradient Descent(3389): loss=0.0770818285103924\n",
      "Stochastic Gradient Descent(3390): loss=0.08483598722749934\n",
      "Stochastic Gradient Descent(3391): loss=41.19203236278868\n",
      "Stochastic Gradient Descent(3392): loss=70.49020084632113\n",
      "Stochastic Gradient Descent(3393): loss=0.002751862593129365\n",
      "Stochastic Gradient Descent(3394): loss=20.68257335956621\n",
      "Stochastic Gradient Descent(3395): loss=83.96957647253703\n",
      "Stochastic Gradient Descent(3396): loss=12.559946842944763\n",
      "Stochastic Gradient Descent(3397): loss=0.023721080446330846\n",
      "Stochastic Gradient Descent(3398): loss=10.433003615069602\n",
      "Stochastic Gradient Descent(3399): loss=6.519471640096763\n",
      "Stochastic Gradient Descent(3400): loss=0.11076019688440013\n",
      "Stochastic Gradient Descent(3401): loss=5.571151831478935\n",
      "Stochastic Gradient Descent(3402): loss=14.938667601654465\n",
      "Stochastic Gradient Descent(3403): loss=12.494827777231231\n",
      "Stochastic Gradient Descent(3404): loss=0.2634951247037902\n",
      "Stochastic Gradient Descent(3405): loss=0.0339599479754916\n",
      "Stochastic Gradient Descent(3406): loss=53.75982548197021\n",
      "Stochastic Gradient Descent(3407): loss=13.20413955851475\n",
      "Stochastic Gradient Descent(3408): loss=8.383256793584547\n",
      "Stochastic Gradient Descent(3409): loss=0.058554565885395385\n",
      "Stochastic Gradient Descent(3410): loss=12.699853963435498\n",
      "Stochastic Gradient Descent(3411): loss=93.15762846835065\n",
      "Stochastic Gradient Descent(3412): loss=13.436057446431603\n",
      "Stochastic Gradient Descent(3413): loss=29.29421217799394\n",
      "Stochastic Gradient Descent(3414): loss=15.436537537019543\n",
      "Stochastic Gradient Descent(3415): loss=15.567564604208023\n",
      "Stochastic Gradient Descent(3416): loss=117.74964579566709\n",
      "Stochastic Gradient Descent(3417): loss=0.002924081947920228\n",
      "Stochastic Gradient Descent(3418): loss=7.684563573770241\n",
      "Stochastic Gradient Descent(3419): loss=0.19556648830998075\n",
      "Stochastic Gradient Descent(3420): loss=0.25402373161842007\n",
      "Stochastic Gradient Descent(3421): loss=7.106913901466468\n",
      "Stochastic Gradient Descent(3422): loss=3.3012073858086994\n",
      "Stochastic Gradient Descent(3423): loss=26.322019211555027\n",
      "Stochastic Gradient Descent(3424): loss=6.074642229820395\n",
      "Stochastic Gradient Descent(3425): loss=3.0453939371460197\n",
      "Stochastic Gradient Descent(3426): loss=1.3659566954193474\n",
      "Stochastic Gradient Descent(3427): loss=24.582531829760644\n",
      "Stochastic Gradient Descent(3428): loss=4.532610722280325\n",
      "Stochastic Gradient Descent(3429): loss=10.520014895524975\n",
      "Stochastic Gradient Descent(3430): loss=44.54946640779746\n",
      "Stochastic Gradient Descent(3431): loss=11.895769436092968\n",
      "Stochastic Gradient Descent(3432): loss=9.46812346030985\n",
      "Stochastic Gradient Descent(3433): loss=0.45909565514370104\n",
      "Stochastic Gradient Descent(3434): loss=2.308537042826564\n",
      "Stochastic Gradient Descent(3435): loss=0.5828167641402308\n",
      "Stochastic Gradient Descent(3436): loss=0.2514078318532498\n",
      "Stochastic Gradient Descent(3437): loss=29.182923085530515\n",
      "Stochastic Gradient Descent(3438): loss=8.543323109434175\n",
      "Stochastic Gradient Descent(3439): loss=0.3257729071611673\n",
      "Stochastic Gradient Descent(3440): loss=7.7591586946394875\n",
      "Stochastic Gradient Descent(3441): loss=26.072325992066336\n",
      "Stochastic Gradient Descent(3442): loss=39.55726260937267\n",
      "Stochastic Gradient Descent(3443): loss=1.0071898884854635\n",
      "Stochastic Gradient Descent(3444): loss=12.54598236837609\n",
      "Stochastic Gradient Descent(3445): loss=53.50509983451637\n",
      "Stochastic Gradient Descent(3446): loss=12.450374196356004\n",
      "Stochastic Gradient Descent(3447): loss=11.90202186266946\n",
      "Stochastic Gradient Descent(3448): loss=15.186644713681916\n",
      "Stochastic Gradient Descent(3449): loss=8.739409556764125\n",
      "Stochastic Gradient Descent(3450): loss=11.516570434815293\n",
      "Stochastic Gradient Descent(3451): loss=2.317670751329359\n",
      "Stochastic Gradient Descent(3452): loss=43.723672215421246\n",
      "Stochastic Gradient Descent(3453): loss=0.9629834299950154\n",
      "Stochastic Gradient Descent(3454): loss=5.19911545456841\n",
      "Stochastic Gradient Descent(3455): loss=26.710803376329334\n",
      "Stochastic Gradient Descent(3456): loss=10.26320672720992\n",
      "Stochastic Gradient Descent(3457): loss=7.78129346338755\n",
      "Stochastic Gradient Descent(3458): loss=5.7607408820302215\n",
      "Stochastic Gradient Descent(3459): loss=19.0669256133115\n",
      "Stochastic Gradient Descent(3460): loss=48.83154151915622\n",
      "Stochastic Gradient Descent(3461): loss=2.805433115611554\n",
      "Stochastic Gradient Descent(3462): loss=8.318074559222028\n",
      "Stochastic Gradient Descent(3463): loss=24.67694226305719\n",
      "Stochastic Gradient Descent(3464): loss=3.3387583913142973\n",
      "Stochastic Gradient Descent(3465): loss=1.814073806347148\n",
      "Stochastic Gradient Descent(3466): loss=50.665104405006645\n",
      "Stochastic Gradient Descent(3467): loss=89.22016647802108\n",
      "Stochastic Gradient Descent(3468): loss=32.67815176378778\n",
      "Stochastic Gradient Descent(3469): loss=0.5564479340786797\n",
      "Stochastic Gradient Descent(3470): loss=0.031289460899735756\n",
      "Stochastic Gradient Descent(3471): loss=25.981524056827546\n",
      "Stochastic Gradient Descent(3472): loss=0.6839942157896913\n",
      "Stochastic Gradient Descent(3473): loss=0.7003687019194166\n",
      "Stochastic Gradient Descent(3474): loss=16.993142469122766\n",
      "Stochastic Gradient Descent(3475): loss=1.5530578447371213\n",
      "Stochastic Gradient Descent(3476): loss=1.051072338082202\n",
      "Stochastic Gradient Descent(3477): loss=2.7724647425234217\n",
      "Stochastic Gradient Descent(3478): loss=1.4893002841058198\n",
      "Stochastic Gradient Descent(3479): loss=2.3640379681811217\n",
      "Stochastic Gradient Descent(3480): loss=0.009508238505530515\n",
      "Stochastic Gradient Descent(3481): loss=0.20423231545703008\n",
      "Stochastic Gradient Descent(3482): loss=47.30254991603058\n",
      "Stochastic Gradient Descent(3483): loss=0.917772873529417\n",
      "Stochastic Gradient Descent(3484): loss=84.3255749815328\n",
      "Stochastic Gradient Descent(3485): loss=52.366981054564135\n",
      "Stochastic Gradient Descent(3486): loss=2.365527573707043\n",
      "Stochastic Gradient Descent(3487): loss=7.710556542377677\n",
      "Stochastic Gradient Descent(3488): loss=3.534316337345636\n",
      "Stochastic Gradient Descent(3489): loss=54.238538203945254\n",
      "Stochastic Gradient Descent(3490): loss=5.444336922759538\n",
      "Stochastic Gradient Descent(3491): loss=28.373968270405275\n",
      "Stochastic Gradient Descent(3492): loss=14.546811999212885\n",
      "Stochastic Gradient Descent(3493): loss=4.882362027212627\n",
      "Stochastic Gradient Descent(3494): loss=0.8757115803096528\n",
      "Stochastic Gradient Descent(3495): loss=9.649299555923331\n",
      "Stochastic Gradient Descent(3496): loss=15.858706466811608\n",
      "Stochastic Gradient Descent(3497): loss=0.010682950894747809\n",
      "Stochastic Gradient Descent(3498): loss=21.086123575111696\n",
      "Stochastic Gradient Descent(3499): loss=2.9376546094377542\n",
      "Stochastic Gradient Descent(3500): loss=14.931788495713375\n",
      "Stochastic Gradient Descent(3501): loss=0.4602364552958929\n",
      "Stochastic Gradient Descent(3502): loss=0.6156054687551171\n",
      "Stochastic Gradient Descent(3503): loss=78.79295735319101\n",
      "Stochastic Gradient Descent(3504): loss=44.779554368496356\n",
      "Stochastic Gradient Descent(3505): loss=52.81992428311372\n",
      "Stochastic Gradient Descent(3506): loss=1.4145515412742042\n",
      "Stochastic Gradient Descent(3507): loss=3.428005528478\n",
      "Stochastic Gradient Descent(3508): loss=0.13103217113744559\n",
      "Stochastic Gradient Descent(3509): loss=3.824027257994612\n",
      "Stochastic Gradient Descent(3510): loss=16.486678704357878\n",
      "Stochastic Gradient Descent(3511): loss=1.566740028583171\n",
      "Stochastic Gradient Descent(3512): loss=4.547952895541457\n",
      "Stochastic Gradient Descent(3513): loss=2.9147697325676654\n",
      "Stochastic Gradient Descent(3514): loss=4.485610043479926\n",
      "Stochastic Gradient Descent(3515): loss=17.445150448675847\n",
      "Stochastic Gradient Descent(3516): loss=4.396263679758497\n",
      "Stochastic Gradient Descent(3517): loss=0.3430002010180641\n",
      "Stochastic Gradient Descent(3518): loss=0.008256218659047333\n",
      "Stochastic Gradient Descent(3519): loss=14.372026980330537\n",
      "Stochastic Gradient Descent(3520): loss=0.09322344840992133\n",
      "Stochastic Gradient Descent(3521): loss=5.407134420558902\n",
      "Stochastic Gradient Descent(3522): loss=21.542060349470137\n",
      "Stochastic Gradient Descent(3523): loss=14.301634694736471\n",
      "Stochastic Gradient Descent(3524): loss=5.5058874204073724\n",
      "Stochastic Gradient Descent(3525): loss=2.480065413899529\n",
      "Stochastic Gradient Descent(3526): loss=13.663280487079298\n",
      "Stochastic Gradient Descent(3527): loss=26.615413826884005\n",
      "Stochastic Gradient Descent(3528): loss=27.338794519582827\n",
      "Stochastic Gradient Descent(3529): loss=5.499814419311763\n",
      "Stochastic Gradient Descent(3530): loss=94.83757098810776\n",
      "Stochastic Gradient Descent(3531): loss=0.2777534975436569\n",
      "Stochastic Gradient Descent(3532): loss=108.81753986647895\n",
      "Stochastic Gradient Descent(3533): loss=15.350775148650039\n",
      "Stochastic Gradient Descent(3534): loss=0.018477421303006007\n",
      "Stochastic Gradient Descent(3535): loss=2.5903955980082753\n",
      "Stochastic Gradient Descent(3536): loss=27.533987839745105\n",
      "Stochastic Gradient Descent(3537): loss=0.5229929598075145\n",
      "Stochastic Gradient Descent(3538): loss=18.314751421517272\n",
      "Stochastic Gradient Descent(3539): loss=22.01475044881023\n",
      "Stochastic Gradient Descent(3540): loss=162.79237301349434\n",
      "Stochastic Gradient Descent(3541): loss=19.616334635655356\n",
      "Stochastic Gradient Descent(3542): loss=4.087387873156458\n",
      "Stochastic Gradient Descent(3543): loss=1.5441463076066946\n",
      "Stochastic Gradient Descent(3544): loss=22.816718128268466\n",
      "Stochastic Gradient Descent(3545): loss=0.01880649674302819\n",
      "Stochastic Gradient Descent(3546): loss=88.36096822953662\n",
      "Stochastic Gradient Descent(3547): loss=3.2385574230562697\n",
      "Stochastic Gradient Descent(3548): loss=9.667861687725457\n",
      "Stochastic Gradient Descent(3549): loss=205.75697368334386\n",
      "Stochastic Gradient Descent(3550): loss=125.97392946011513\n",
      "Stochastic Gradient Descent(3551): loss=6.613035705792111\n",
      "Stochastic Gradient Descent(3552): loss=1.3071572836861294\n",
      "Stochastic Gradient Descent(3553): loss=144.46018242372872\n",
      "Stochastic Gradient Descent(3554): loss=2.5892845159202436\n",
      "Stochastic Gradient Descent(3555): loss=13.44307873501696\n",
      "Stochastic Gradient Descent(3556): loss=30.273674732403226\n",
      "Stochastic Gradient Descent(3557): loss=49.69181842107411\n",
      "Stochastic Gradient Descent(3558): loss=9.626027730583155\n",
      "Stochastic Gradient Descent(3559): loss=0.1422789107221529\n",
      "Stochastic Gradient Descent(3560): loss=12.218960749507021\n",
      "Stochastic Gradient Descent(3561): loss=3.048283516355109\n",
      "Stochastic Gradient Descent(3562): loss=11.518122160872554\n",
      "Stochastic Gradient Descent(3563): loss=27.082879971216396\n",
      "Stochastic Gradient Descent(3564): loss=4.328263102836162\n",
      "Stochastic Gradient Descent(3565): loss=74.8785051079308\n",
      "Stochastic Gradient Descent(3566): loss=0.9397911087207634\n",
      "Stochastic Gradient Descent(3567): loss=32.770367793194595\n",
      "Stochastic Gradient Descent(3568): loss=10.338121474641067\n",
      "Stochastic Gradient Descent(3569): loss=2.989270738050878\n",
      "Stochastic Gradient Descent(3570): loss=2.8803748959246565\n",
      "Stochastic Gradient Descent(3571): loss=12.328801189751479\n",
      "Stochastic Gradient Descent(3572): loss=1.5696269169778476\n",
      "Stochastic Gradient Descent(3573): loss=0.9450109062872517\n",
      "Stochastic Gradient Descent(3574): loss=47.93725381461852\n",
      "Stochastic Gradient Descent(3575): loss=30.686952946015573\n",
      "Stochastic Gradient Descent(3576): loss=12.087863650347295\n",
      "Stochastic Gradient Descent(3577): loss=3.025219868814956\n",
      "Stochastic Gradient Descent(3578): loss=0.00018383189644735571\n",
      "Stochastic Gradient Descent(3579): loss=0.35314269335072934\n",
      "Stochastic Gradient Descent(3580): loss=0.01857984734345364\n",
      "Stochastic Gradient Descent(3581): loss=0.1943218188340349\n",
      "Stochastic Gradient Descent(3582): loss=125.66897838537247\n",
      "Stochastic Gradient Descent(3583): loss=3.109173709156539\n",
      "Stochastic Gradient Descent(3584): loss=2.3327062349960843\n",
      "Stochastic Gradient Descent(3585): loss=35.13035779765218\n",
      "Stochastic Gradient Descent(3586): loss=35.95079164940346\n",
      "Stochastic Gradient Descent(3587): loss=2.768888955818981\n",
      "Stochastic Gradient Descent(3588): loss=5.358698360961201\n",
      "Stochastic Gradient Descent(3589): loss=2.908397365255338\n",
      "Stochastic Gradient Descent(3590): loss=0.0018009060843317303\n",
      "Stochastic Gradient Descent(3591): loss=76.01815421721231\n",
      "Stochastic Gradient Descent(3592): loss=7.683553166620799\n",
      "Stochastic Gradient Descent(3593): loss=36.344782888855185\n",
      "Stochastic Gradient Descent(3594): loss=1.6430695279599683\n",
      "Stochastic Gradient Descent(3595): loss=18.622272781620588\n",
      "Stochastic Gradient Descent(3596): loss=15.95583077965197\n",
      "Stochastic Gradient Descent(3597): loss=58.996390895771455\n",
      "Stochastic Gradient Descent(3598): loss=3.4877135986072\n",
      "Stochastic Gradient Descent(3599): loss=3.1172979239691263\n",
      "Stochastic Gradient Descent(3600): loss=24.975687208879933\n",
      "Stochastic Gradient Descent(3601): loss=0.31340952507842496\n",
      "Stochastic Gradient Descent(3602): loss=3.438986991936509\n",
      "Stochastic Gradient Descent(3603): loss=40.10911840034611\n",
      "Stochastic Gradient Descent(3604): loss=0.40453498245839686\n",
      "Stochastic Gradient Descent(3605): loss=1.3612719293961408\n",
      "Stochastic Gradient Descent(3606): loss=1.9081132494276016\n",
      "Stochastic Gradient Descent(3607): loss=19.44213745485133\n",
      "Stochastic Gradient Descent(3608): loss=1.9314188549653024\n",
      "Stochastic Gradient Descent(3609): loss=0.5491419966360285\n",
      "Stochastic Gradient Descent(3610): loss=60.428709725784614\n",
      "Stochastic Gradient Descent(3611): loss=34.65742655647461\n",
      "Stochastic Gradient Descent(3612): loss=14.037291528572645\n",
      "Stochastic Gradient Descent(3613): loss=7.032599299255864\n",
      "Stochastic Gradient Descent(3614): loss=30.97484753059368\n",
      "Stochastic Gradient Descent(3615): loss=11.843765662271807\n",
      "Stochastic Gradient Descent(3616): loss=3.1643134445079935\n",
      "Stochastic Gradient Descent(3617): loss=1.986135344260201\n",
      "Stochastic Gradient Descent(3618): loss=0.09761298404491388\n",
      "Stochastic Gradient Descent(3619): loss=12.374838877508756\n",
      "Stochastic Gradient Descent(3620): loss=0.46749148708456734\n",
      "Stochastic Gradient Descent(3621): loss=1.3855302627674662\n",
      "Stochastic Gradient Descent(3622): loss=13.736703853738629\n",
      "Stochastic Gradient Descent(3623): loss=0.2396186689040857\n",
      "Stochastic Gradient Descent(3624): loss=2.9388833567659876\n",
      "Stochastic Gradient Descent(3625): loss=79.45134761920863\n",
      "Stochastic Gradient Descent(3626): loss=83.87975459973677\n",
      "Stochastic Gradient Descent(3627): loss=0.580926409780517\n",
      "Stochastic Gradient Descent(3628): loss=265.438203281272\n",
      "Stochastic Gradient Descent(3629): loss=24.266287744129272\n",
      "Stochastic Gradient Descent(3630): loss=0.7760183690713024\n",
      "Stochastic Gradient Descent(3631): loss=3.7166262281038422\n",
      "Stochastic Gradient Descent(3632): loss=36.05280863651633\n",
      "Stochastic Gradient Descent(3633): loss=8.531145835678972\n",
      "Stochastic Gradient Descent(3634): loss=32.04456300551921\n",
      "Stochastic Gradient Descent(3635): loss=10.871491021713698\n",
      "Stochastic Gradient Descent(3636): loss=52.40804964300457\n",
      "Stochastic Gradient Descent(3637): loss=36.356734093010964\n",
      "Stochastic Gradient Descent(3638): loss=39.90003756759712\n",
      "Stochastic Gradient Descent(3639): loss=12.568705757838499\n",
      "Stochastic Gradient Descent(3640): loss=212.96327832007015\n",
      "Stochastic Gradient Descent(3641): loss=66.67460299350525\n",
      "Stochastic Gradient Descent(3642): loss=1.3210558619944341\n",
      "Stochastic Gradient Descent(3643): loss=4.253988681935083\n",
      "Stochastic Gradient Descent(3644): loss=2.517649220736202\n",
      "Stochastic Gradient Descent(3645): loss=13.031203764709835\n",
      "Stochastic Gradient Descent(3646): loss=10.25508313737559\n",
      "Stochastic Gradient Descent(3647): loss=21.074247793673845\n",
      "Stochastic Gradient Descent(3648): loss=0.6310225776348898\n",
      "Stochastic Gradient Descent(3649): loss=18.684887334415087\n",
      "Stochastic Gradient Descent(3650): loss=2.463898241153956\n",
      "Stochastic Gradient Descent(3651): loss=4.906413975322134\n",
      "Stochastic Gradient Descent(3652): loss=23.807169230857372\n",
      "Stochastic Gradient Descent(3653): loss=1.6228589485150589\n",
      "Stochastic Gradient Descent(3654): loss=5.267867568410994\n",
      "Stochastic Gradient Descent(3655): loss=7.756673892993361\n",
      "Stochastic Gradient Descent(3656): loss=11.299624541287239\n",
      "Stochastic Gradient Descent(3657): loss=3.348086531601385\n",
      "Stochastic Gradient Descent(3658): loss=9.283696109213052\n",
      "Stochastic Gradient Descent(3659): loss=10.069791921181173\n",
      "Stochastic Gradient Descent(3660): loss=6.776194408816118\n",
      "Stochastic Gradient Descent(3661): loss=17.48511676035032\n",
      "Stochastic Gradient Descent(3662): loss=46.683129626716955\n",
      "Stochastic Gradient Descent(3663): loss=10.695627373721313\n",
      "Stochastic Gradient Descent(3664): loss=41.56795378519523\n",
      "Stochastic Gradient Descent(3665): loss=54.460092255682426\n",
      "Stochastic Gradient Descent(3666): loss=97.70817160395252\n",
      "Stochastic Gradient Descent(3667): loss=2.8872578740566386\n",
      "Stochastic Gradient Descent(3668): loss=186.6709394811529\n",
      "Stochastic Gradient Descent(3669): loss=0.08748728798300245\n",
      "Stochastic Gradient Descent(3670): loss=31.540322018827762\n",
      "Stochastic Gradient Descent(3671): loss=0.269193504089292\n",
      "Stochastic Gradient Descent(3672): loss=23.400070528704553\n",
      "Stochastic Gradient Descent(3673): loss=1.0029557912498563\n",
      "Stochastic Gradient Descent(3674): loss=106.81117180493243\n",
      "Stochastic Gradient Descent(3675): loss=0.29627321505444854\n",
      "Stochastic Gradient Descent(3676): loss=0.11704166355736932\n",
      "Stochastic Gradient Descent(3677): loss=10.272231792157035\n",
      "Stochastic Gradient Descent(3678): loss=16.11716956367093\n",
      "Stochastic Gradient Descent(3679): loss=20.07042849389877\n",
      "Stochastic Gradient Descent(3680): loss=2.247412314868126\n",
      "Stochastic Gradient Descent(3681): loss=0.4144136833324824\n",
      "Stochastic Gradient Descent(3682): loss=42.22906951874956\n",
      "Stochastic Gradient Descent(3683): loss=0.22214554618830756\n",
      "Stochastic Gradient Descent(3684): loss=67.43536805577835\n",
      "Stochastic Gradient Descent(3685): loss=27.09196808087678\n",
      "Stochastic Gradient Descent(3686): loss=12.578269289422586\n",
      "Stochastic Gradient Descent(3687): loss=0.27202540156979815\n",
      "Stochastic Gradient Descent(3688): loss=0.8320951562037974\n",
      "Stochastic Gradient Descent(3689): loss=0.4293753287207573\n",
      "Stochastic Gradient Descent(3690): loss=5.82922428794333\n",
      "Stochastic Gradient Descent(3691): loss=0.07209138305912834\n",
      "Stochastic Gradient Descent(3692): loss=1.1490788356610064\n",
      "Stochastic Gradient Descent(3693): loss=9.172488380943655\n",
      "Stochastic Gradient Descent(3694): loss=1.290506488573492\n",
      "Stochastic Gradient Descent(3695): loss=139.0856581070469\n",
      "Stochastic Gradient Descent(3696): loss=2.695973764864831\n",
      "Stochastic Gradient Descent(3697): loss=20.226123083859008\n",
      "Stochastic Gradient Descent(3698): loss=1.4438351412207975\n",
      "Stochastic Gradient Descent(3699): loss=4.9099726425018195\n",
      "Stochastic Gradient Descent(3700): loss=0.13848517982157452\n",
      "Stochastic Gradient Descent(3701): loss=39.19348649464022\n",
      "Stochastic Gradient Descent(3702): loss=4.441402643046659\n",
      "Stochastic Gradient Descent(3703): loss=82.28917286514837\n",
      "Stochastic Gradient Descent(3704): loss=10.799946861785056\n",
      "Stochastic Gradient Descent(3705): loss=8.506651479380897\n",
      "Stochastic Gradient Descent(3706): loss=14.91601761768391\n",
      "Stochastic Gradient Descent(3707): loss=14.015982787037597\n",
      "Stochastic Gradient Descent(3708): loss=7.517503872035647\n",
      "Stochastic Gradient Descent(3709): loss=6.553424227220628\n",
      "Stochastic Gradient Descent(3710): loss=3.162438358573266\n",
      "Stochastic Gradient Descent(3711): loss=1.1307124498275647\n",
      "Stochastic Gradient Descent(3712): loss=3.43262222524805\n",
      "Stochastic Gradient Descent(3713): loss=1.230975885928575\n",
      "Stochastic Gradient Descent(3714): loss=4.112570335673788e-05\n",
      "Stochastic Gradient Descent(3715): loss=6.669748248971968\n",
      "Stochastic Gradient Descent(3716): loss=13.01916510651959\n",
      "Stochastic Gradient Descent(3717): loss=1.257793275347097e-05\n",
      "Stochastic Gradient Descent(3718): loss=38.17846130659111\n",
      "Stochastic Gradient Descent(3719): loss=0.004559435777625456\n",
      "Stochastic Gradient Descent(3720): loss=5.652042821700941\n",
      "Stochastic Gradient Descent(3721): loss=0.3204596916332481\n",
      "Stochastic Gradient Descent(3722): loss=12.030979769382693\n",
      "Stochastic Gradient Descent(3723): loss=4.550874431972864\n",
      "Stochastic Gradient Descent(3724): loss=0.05917098763579389\n",
      "Stochastic Gradient Descent(3725): loss=30.549988604546922\n",
      "Stochastic Gradient Descent(3726): loss=53.69783226510894\n",
      "Stochastic Gradient Descent(3727): loss=32.70361320591353\n",
      "Stochastic Gradient Descent(3728): loss=6.050078109974641\n",
      "Stochastic Gradient Descent(3729): loss=0.3747990794614992\n",
      "Stochastic Gradient Descent(3730): loss=1.4467706730650403\n",
      "Stochastic Gradient Descent(3731): loss=22.34513867218207\n",
      "Stochastic Gradient Descent(3732): loss=7.757535283045715\n",
      "Stochastic Gradient Descent(3733): loss=2.986957091946921\n",
      "Stochastic Gradient Descent(3734): loss=46.684065535799355\n",
      "Stochastic Gradient Descent(3735): loss=86.93987810492281\n",
      "Stochastic Gradient Descent(3736): loss=12.147422930392498\n",
      "Stochastic Gradient Descent(3737): loss=1.8597519797897824\n",
      "Stochastic Gradient Descent(3738): loss=1.8689339720370355\n",
      "Stochastic Gradient Descent(3739): loss=99.1082483369426\n",
      "Stochastic Gradient Descent(3740): loss=66.29976792455336\n",
      "Stochastic Gradient Descent(3741): loss=17.310281604680686\n",
      "Stochastic Gradient Descent(3742): loss=4.050896359495488\n",
      "Stochastic Gradient Descent(3743): loss=0.13382781230845323\n",
      "Stochastic Gradient Descent(3744): loss=3.8466455145586633\n",
      "Stochastic Gradient Descent(3745): loss=0.06725156154917099\n",
      "Stochastic Gradient Descent(3746): loss=1.0016085038490534\n",
      "Stochastic Gradient Descent(3747): loss=20.985883208758924\n",
      "Stochastic Gradient Descent(3748): loss=30.106813247946643\n",
      "Stochastic Gradient Descent(3749): loss=30.810427708040176\n",
      "Stochastic Gradient Descent(3750): loss=1.2099469813453212\n",
      "Stochastic Gradient Descent(3751): loss=24.555571589166743\n",
      "Stochastic Gradient Descent(3752): loss=0.23535878680951455\n",
      "Stochastic Gradient Descent(3753): loss=7.483390876840018\n",
      "Stochastic Gradient Descent(3754): loss=4.233248416702654\n",
      "Stochastic Gradient Descent(3755): loss=0.009311398481185553\n",
      "Stochastic Gradient Descent(3756): loss=2.50789071690277\n",
      "Stochastic Gradient Descent(3757): loss=0.010132501527545412\n",
      "Stochastic Gradient Descent(3758): loss=0.37052614652331156\n",
      "Stochastic Gradient Descent(3759): loss=1.1151159492872014\n",
      "Stochastic Gradient Descent(3760): loss=6.64690226333544\n",
      "Stochastic Gradient Descent(3761): loss=7.924875312052224\n",
      "Stochastic Gradient Descent(3762): loss=3.305250803742576\n",
      "Stochastic Gradient Descent(3763): loss=10.882355690876171\n",
      "Stochastic Gradient Descent(3764): loss=12.352752403753025\n",
      "Stochastic Gradient Descent(3765): loss=0.1787456076888747\n",
      "Stochastic Gradient Descent(3766): loss=0.8181641383521926\n",
      "Stochastic Gradient Descent(3767): loss=1.8665716795754708\n",
      "Stochastic Gradient Descent(3768): loss=4.39548710903533\n",
      "Stochastic Gradient Descent(3769): loss=44.445151618846026\n",
      "Stochastic Gradient Descent(3770): loss=64.67583888450213\n",
      "Stochastic Gradient Descent(3771): loss=46.205622718663335\n",
      "Stochastic Gradient Descent(3772): loss=0.13048174281820765\n",
      "Stochastic Gradient Descent(3773): loss=2.421886267565609\n",
      "Stochastic Gradient Descent(3774): loss=4.090868477714481\n",
      "Stochastic Gradient Descent(3775): loss=8.96205368018576\n",
      "Stochastic Gradient Descent(3776): loss=14.448651758115686\n",
      "Stochastic Gradient Descent(3777): loss=27.70848842201185\n",
      "Stochastic Gradient Descent(3778): loss=0.3078989539595849\n",
      "Stochastic Gradient Descent(3779): loss=0.5594014582139454\n",
      "Stochastic Gradient Descent(3780): loss=0.2117774853620528\n",
      "Stochastic Gradient Descent(3781): loss=1.1557638544333093\n",
      "Stochastic Gradient Descent(3782): loss=4.077641874472152\n",
      "Stochastic Gradient Descent(3783): loss=1.5384837567603298\n",
      "Stochastic Gradient Descent(3784): loss=3.015983023920147\n",
      "Stochastic Gradient Descent(3785): loss=10.134899135972722\n",
      "Stochastic Gradient Descent(3786): loss=6.426393028743072\n",
      "Stochastic Gradient Descent(3787): loss=7.255320601553215\n",
      "Stochastic Gradient Descent(3788): loss=8.628210704883278\n",
      "Stochastic Gradient Descent(3789): loss=2.9652781834573494\n",
      "Stochastic Gradient Descent(3790): loss=0.10348284146323153\n",
      "Stochastic Gradient Descent(3791): loss=1.8502615236343578\n",
      "Stochastic Gradient Descent(3792): loss=20.966887491875305\n",
      "Stochastic Gradient Descent(3793): loss=0.00010439264221918751\n",
      "Stochastic Gradient Descent(3794): loss=6.328973838500567\n",
      "Stochastic Gradient Descent(3795): loss=4.952428912278037\n",
      "Stochastic Gradient Descent(3796): loss=6.20356604125489\n",
      "Stochastic Gradient Descent(3797): loss=0.08614350634440149\n",
      "Stochastic Gradient Descent(3798): loss=0.3353479979428825\n",
      "Stochastic Gradient Descent(3799): loss=7.6209960028919435\n",
      "Stochastic Gradient Descent(3800): loss=3.0731001113596417\n",
      "Stochastic Gradient Descent(3801): loss=7.903854370834205\n",
      "Stochastic Gradient Descent(3802): loss=2.5220717132022896\n",
      "Stochastic Gradient Descent(3803): loss=17.29136604318801\n",
      "Stochastic Gradient Descent(3804): loss=13.216551234558457\n",
      "Stochastic Gradient Descent(3805): loss=11.136989978987184\n",
      "Stochastic Gradient Descent(3806): loss=6.969307464307993\n",
      "Stochastic Gradient Descent(3807): loss=14.967517325308336\n",
      "Stochastic Gradient Descent(3808): loss=0.6802145319988605\n",
      "Stochastic Gradient Descent(3809): loss=24.606051393180365\n",
      "Stochastic Gradient Descent(3810): loss=23.0025489438231\n",
      "Stochastic Gradient Descent(3811): loss=3.9641746407016103\n",
      "Stochastic Gradient Descent(3812): loss=1.0515145772219776\n",
      "Stochastic Gradient Descent(3813): loss=10.996426220001998\n",
      "Stochastic Gradient Descent(3814): loss=0.38988425235765045\n",
      "Stochastic Gradient Descent(3815): loss=0.06218200907422195\n",
      "Stochastic Gradient Descent(3816): loss=9.456712854479278\n",
      "Stochastic Gradient Descent(3817): loss=12.270367708282022\n",
      "Stochastic Gradient Descent(3818): loss=0.40645590393954284\n",
      "Stochastic Gradient Descent(3819): loss=1.3611196564746435\n",
      "Stochastic Gradient Descent(3820): loss=0.22870689736871969\n",
      "Stochastic Gradient Descent(3821): loss=4.164226335303001\n",
      "Stochastic Gradient Descent(3822): loss=0.5979540757612967\n",
      "Stochastic Gradient Descent(3823): loss=26.215216924639492\n",
      "Stochastic Gradient Descent(3824): loss=3.997625349832018\n",
      "Stochastic Gradient Descent(3825): loss=12.831082916820742\n",
      "Stochastic Gradient Descent(3826): loss=4.695162125951673\n",
      "Stochastic Gradient Descent(3827): loss=14.13003034003585\n",
      "Stochastic Gradient Descent(3828): loss=20.318616089306754\n",
      "Stochastic Gradient Descent(3829): loss=17.238563941052135\n",
      "Stochastic Gradient Descent(3830): loss=65.00791769377389\n",
      "Stochastic Gradient Descent(3831): loss=0.056049597904285244\n",
      "Stochastic Gradient Descent(3832): loss=7.593036084129839\n",
      "Stochastic Gradient Descent(3833): loss=25.54679006024295\n",
      "Stochastic Gradient Descent(3834): loss=25.53238987788343\n",
      "Stochastic Gradient Descent(3835): loss=26.422246254041706\n",
      "Stochastic Gradient Descent(3836): loss=2.9473679121892133\n",
      "Stochastic Gradient Descent(3837): loss=51.2705650594384\n",
      "Stochastic Gradient Descent(3838): loss=0.473027775842876\n",
      "Stochastic Gradient Descent(3839): loss=5.122298881101784\n",
      "Stochastic Gradient Descent(3840): loss=29.840225666573133\n",
      "Stochastic Gradient Descent(3841): loss=10.37953758261307\n",
      "Stochastic Gradient Descent(3842): loss=24.534669451825025\n",
      "Stochastic Gradient Descent(3843): loss=27.186703701690604\n",
      "Stochastic Gradient Descent(3844): loss=12.810284285168164\n",
      "Stochastic Gradient Descent(3845): loss=9.59281606190553\n",
      "Stochastic Gradient Descent(3846): loss=0.6285930399971297\n",
      "Stochastic Gradient Descent(3847): loss=8.477690484054445\n",
      "Stochastic Gradient Descent(3848): loss=0.3859235280931977\n",
      "Stochastic Gradient Descent(3849): loss=21.83766302740132\n",
      "Stochastic Gradient Descent(3850): loss=3.991591732354486\n",
      "Stochastic Gradient Descent(3851): loss=4.751722583053747\n",
      "Stochastic Gradient Descent(3852): loss=15.874132255370027\n",
      "Stochastic Gradient Descent(3853): loss=2.403545952079324\n",
      "Stochastic Gradient Descent(3854): loss=4.205584934720931\n",
      "Stochastic Gradient Descent(3855): loss=48.3716760033181\n",
      "Stochastic Gradient Descent(3856): loss=12.25840663878032\n",
      "Stochastic Gradient Descent(3857): loss=7.262375976257086\n",
      "Stochastic Gradient Descent(3858): loss=9.204769401770426\n",
      "Stochastic Gradient Descent(3859): loss=3.0477136992217235\n",
      "Stochastic Gradient Descent(3860): loss=0.030201268302819027\n",
      "Stochastic Gradient Descent(3861): loss=10.81247673723232\n",
      "Stochastic Gradient Descent(3862): loss=1.1556749993891755\n",
      "Stochastic Gradient Descent(3863): loss=6.6821164250584175\n",
      "Stochastic Gradient Descent(3864): loss=20.036065251297988\n",
      "Stochastic Gradient Descent(3865): loss=3.685897429688547\n",
      "Stochastic Gradient Descent(3866): loss=4.113874517890541\n",
      "Stochastic Gradient Descent(3867): loss=17.130679236767428\n",
      "Stochastic Gradient Descent(3868): loss=0.007026400287971597\n",
      "Stochastic Gradient Descent(3869): loss=0.009254062682738987\n",
      "Stochastic Gradient Descent(3870): loss=0.4220033884600205\n",
      "Stochastic Gradient Descent(3871): loss=25.555426177637866\n",
      "Stochastic Gradient Descent(3872): loss=4.836815124554997\n",
      "Stochastic Gradient Descent(3873): loss=0.005058665448560451\n",
      "Stochastic Gradient Descent(3874): loss=3.5359261358182343\n",
      "Stochastic Gradient Descent(3875): loss=3.42587365387356\n",
      "Stochastic Gradient Descent(3876): loss=5.219911991966196\n",
      "Stochastic Gradient Descent(3877): loss=3.512759463483975\n",
      "Stochastic Gradient Descent(3878): loss=20.84911936263632\n",
      "Stochastic Gradient Descent(3879): loss=8.126215078403048\n",
      "Stochastic Gradient Descent(3880): loss=3.8343460075275466\n",
      "Stochastic Gradient Descent(3881): loss=8.530063278469195\n",
      "Stochastic Gradient Descent(3882): loss=35.31807980056117\n",
      "Stochastic Gradient Descent(3883): loss=0.3089750751397972\n",
      "Stochastic Gradient Descent(3884): loss=10.522100072863324\n",
      "Stochastic Gradient Descent(3885): loss=1.149935263473168\n",
      "Stochastic Gradient Descent(3886): loss=27.921221228796334\n",
      "Stochastic Gradient Descent(3887): loss=2.1735702508125216\n",
      "Stochastic Gradient Descent(3888): loss=107.96310265729895\n",
      "Stochastic Gradient Descent(3889): loss=4.730384308619523\n",
      "Stochastic Gradient Descent(3890): loss=38.71196219418817\n",
      "Stochastic Gradient Descent(3891): loss=5.265446442220799\n",
      "Stochastic Gradient Descent(3892): loss=7.26881008491023\n",
      "Stochastic Gradient Descent(3893): loss=143.38719324899864\n",
      "Stochastic Gradient Descent(3894): loss=6.967893616296554\n",
      "Stochastic Gradient Descent(3895): loss=1.4350241174097174\n",
      "Stochastic Gradient Descent(3896): loss=7.1904571517943445\n",
      "Stochastic Gradient Descent(3897): loss=0.16689346435461577\n",
      "Stochastic Gradient Descent(3898): loss=0.4453151617579143\n",
      "Stochastic Gradient Descent(3899): loss=1.7918887296497665\n",
      "Stochastic Gradient Descent(3900): loss=0.015825605610563065\n",
      "Stochastic Gradient Descent(3901): loss=7.466539219374044\n",
      "Stochastic Gradient Descent(3902): loss=20.58269932879181\n",
      "Stochastic Gradient Descent(3903): loss=105.6841232527542\n",
      "Stochastic Gradient Descent(3904): loss=38.234990725062836\n",
      "Stochastic Gradient Descent(3905): loss=0.3207117713688031\n",
      "Stochastic Gradient Descent(3906): loss=11.000818525298614\n",
      "Stochastic Gradient Descent(3907): loss=0.0015794451029810918\n",
      "Stochastic Gradient Descent(3908): loss=53.23075491783025\n",
      "Stochastic Gradient Descent(3909): loss=4.342654820417016\n",
      "Stochastic Gradient Descent(3910): loss=5.137330840400126\n",
      "Stochastic Gradient Descent(3911): loss=17.28688448766994\n",
      "Stochastic Gradient Descent(3912): loss=0.33714308264864684\n",
      "Stochastic Gradient Descent(3913): loss=0.3835141564011924\n",
      "Stochastic Gradient Descent(3914): loss=4.012761224208564\n",
      "Stochastic Gradient Descent(3915): loss=82.77669028160321\n",
      "Stochastic Gradient Descent(3916): loss=1.2672529805920865\n",
      "Stochastic Gradient Descent(3917): loss=6.036409222916\n",
      "Stochastic Gradient Descent(3918): loss=8.542559291127516\n",
      "Stochastic Gradient Descent(3919): loss=38.950273069081526\n",
      "Stochastic Gradient Descent(3920): loss=1.1127828860794806\n",
      "Stochastic Gradient Descent(3921): loss=10.122739127164712\n",
      "Stochastic Gradient Descent(3922): loss=14.597686933725122\n",
      "Stochastic Gradient Descent(3923): loss=23.643181182812565\n",
      "Stochastic Gradient Descent(3924): loss=0.6188039335203663\n",
      "Stochastic Gradient Descent(3925): loss=5.8100830583500525\n",
      "Stochastic Gradient Descent(3926): loss=4.164173847537856\n",
      "Stochastic Gradient Descent(3927): loss=7.274322423087356\n",
      "Stochastic Gradient Descent(3928): loss=0.603296194557905\n",
      "Stochastic Gradient Descent(3929): loss=3.260860421955592\n",
      "Stochastic Gradient Descent(3930): loss=37.03363573667465\n",
      "Stochastic Gradient Descent(3931): loss=2.2610232733033757\n",
      "Stochastic Gradient Descent(3932): loss=5.278977116853018\n",
      "Stochastic Gradient Descent(3933): loss=1.131855244052668\n",
      "Stochastic Gradient Descent(3934): loss=45.75567644040478\n",
      "Stochastic Gradient Descent(3935): loss=0.03256521243848793\n",
      "Stochastic Gradient Descent(3936): loss=23.875952766769963\n",
      "Stochastic Gradient Descent(3937): loss=2.3680528004259696\n",
      "Stochastic Gradient Descent(3938): loss=2.8073655268258073\n",
      "Stochastic Gradient Descent(3939): loss=0.9076799398675363\n",
      "Stochastic Gradient Descent(3940): loss=1.8762279415903997\n",
      "Stochastic Gradient Descent(3941): loss=43.08863241609046\n",
      "Stochastic Gradient Descent(3942): loss=7.032330429908453\n",
      "Stochastic Gradient Descent(3943): loss=6.278834159495851\n",
      "Stochastic Gradient Descent(3944): loss=2.4557847593798217\n",
      "Stochastic Gradient Descent(3945): loss=3.907457145800305\n",
      "Stochastic Gradient Descent(3946): loss=36.527217438712555\n",
      "Stochastic Gradient Descent(3947): loss=0.8805674892621181\n",
      "Stochastic Gradient Descent(3948): loss=4.3212558758614605\n",
      "Stochastic Gradient Descent(3949): loss=7.003673274053801\n",
      "Stochastic Gradient Descent(3950): loss=10.422542633026131\n",
      "Stochastic Gradient Descent(3951): loss=0.3416508941802151\n",
      "Stochastic Gradient Descent(3952): loss=9.113214132446725\n",
      "Stochastic Gradient Descent(3953): loss=0.5964461317678212\n",
      "Stochastic Gradient Descent(3954): loss=0.9534436643574091\n",
      "Stochastic Gradient Descent(3955): loss=0.022636152585539006\n",
      "Stochastic Gradient Descent(3956): loss=1.7145516990089982\n",
      "Stochastic Gradient Descent(3957): loss=0.24621704134153366\n",
      "Stochastic Gradient Descent(3958): loss=29.427762036025833\n",
      "Stochastic Gradient Descent(3959): loss=2.2538163727305305\n",
      "Stochastic Gradient Descent(3960): loss=13.774538679046692\n",
      "Stochastic Gradient Descent(3961): loss=4.608049040419273\n",
      "Stochastic Gradient Descent(3962): loss=0.6503303752557351\n",
      "Stochastic Gradient Descent(3963): loss=0.01738715069655456\n",
      "Stochastic Gradient Descent(3964): loss=7.805202231269557\n",
      "Stochastic Gradient Descent(3965): loss=4.830014213682926\n",
      "Stochastic Gradient Descent(3966): loss=10.309893144953437\n",
      "Stochastic Gradient Descent(3967): loss=4.984947549543171\n",
      "Stochastic Gradient Descent(3968): loss=19.45254936846187\n",
      "Stochastic Gradient Descent(3969): loss=0.18405980650965892\n",
      "Stochastic Gradient Descent(3970): loss=3.6883697876556214\n",
      "Stochastic Gradient Descent(3971): loss=2.063979366129377\n",
      "Stochastic Gradient Descent(3972): loss=43.81405499464304\n",
      "Stochastic Gradient Descent(3973): loss=38.439022041598584\n",
      "Stochastic Gradient Descent(3974): loss=5.401020038014484\n",
      "Stochastic Gradient Descent(3975): loss=0.05877389076831105\n",
      "Stochastic Gradient Descent(3976): loss=3.59770418954324\n",
      "Stochastic Gradient Descent(3977): loss=17.66448033614373\n",
      "Stochastic Gradient Descent(3978): loss=17.720083194072775\n",
      "Stochastic Gradient Descent(3979): loss=2.847248086118186\n",
      "Stochastic Gradient Descent(3980): loss=2.9770400177841303\n",
      "Stochastic Gradient Descent(3981): loss=2.0659850571105736\n",
      "Stochastic Gradient Descent(3982): loss=1.1335097902204814\n",
      "Stochastic Gradient Descent(3983): loss=0.8298135105164259\n",
      "Stochastic Gradient Descent(3984): loss=1053.7802746896898\n",
      "Stochastic Gradient Descent(3985): loss=488.2390651746957\n",
      "Stochastic Gradient Descent(3986): loss=2056.5481363913614\n",
      "Stochastic Gradient Descent(3987): loss=476.71686079838304\n",
      "Stochastic Gradient Descent(3988): loss=272.44808355146006\n",
      "Stochastic Gradient Descent(3989): loss=51.25906966463776\n",
      "Stochastic Gradient Descent(3990): loss=826.209941236416\n",
      "Stochastic Gradient Descent(3991): loss=1.4897795455695688\n",
      "Stochastic Gradient Descent(3992): loss=25.492440522307128\n",
      "Stochastic Gradient Descent(3993): loss=9.499787284971944\n",
      "Stochastic Gradient Descent(3994): loss=27.122057638508124\n",
      "Stochastic Gradient Descent(3995): loss=235.60241655173704\n",
      "Stochastic Gradient Descent(3996): loss=516.0360954155678\n",
      "Stochastic Gradient Descent(3997): loss=842.3618776882\n",
      "Stochastic Gradient Descent(3998): loss=5.267821368181826\n",
      "Stochastic Gradient Descent(3999): loss=2.3490276259248697\n",
      "Stochastic Gradient Descent(4000): loss=0.5054157676933212\n",
      "Stochastic Gradient Descent(4001): loss=0.7025530778197563\n",
      "Stochastic Gradient Descent(4002): loss=16.335551751918594\n",
      "Stochastic Gradient Descent(4003): loss=28.140865885828042\n",
      "Stochastic Gradient Descent(4004): loss=1.7121030450864745\n",
      "Stochastic Gradient Descent(4005): loss=0.036737788259058476\n",
      "Stochastic Gradient Descent(4006): loss=7.523253969263776\n",
      "Stochastic Gradient Descent(4007): loss=81.29657848988725\n",
      "Stochastic Gradient Descent(4008): loss=83.20591914864875\n",
      "Stochastic Gradient Descent(4009): loss=5.802070836079286\n",
      "Stochastic Gradient Descent(4010): loss=112.4764818073401\n",
      "Stochastic Gradient Descent(4011): loss=66.95763187432027\n",
      "Stochastic Gradient Descent(4012): loss=29.805928501798675\n",
      "Stochastic Gradient Descent(4013): loss=228.78884003756122\n",
      "Stochastic Gradient Descent(4014): loss=7.69813193611052\n",
      "Stochastic Gradient Descent(4015): loss=19.567264831099145\n",
      "Stochastic Gradient Descent(4016): loss=5.0824536909676405\n",
      "Stochastic Gradient Descent(4017): loss=44.38785786145634\n",
      "Stochastic Gradient Descent(4018): loss=55.938541894477886\n",
      "Stochastic Gradient Descent(4019): loss=2.3170559009593816\n",
      "Stochastic Gradient Descent(4020): loss=1.387012084174664\n",
      "Stochastic Gradient Descent(4021): loss=124.10795580062745\n",
      "Stochastic Gradient Descent(4022): loss=164.46566029780521\n",
      "Stochastic Gradient Descent(4023): loss=2.99534548577626\n",
      "Stochastic Gradient Descent(4024): loss=27.498965900471287\n",
      "Stochastic Gradient Descent(4025): loss=8.311445428930186\n",
      "Stochastic Gradient Descent(4026): loss=2.1300409058214935\n",
      "Stochastic Gradient Descent(4027): loss=17.83817868889476\n",
      "Stochastic Gradient Descent(4028): loss=5.507524178550752\n",
      "Stochastic Gradient Descent(4029): loss=0.0047235335006606755\n",
      "Stochastic Gradient Descent(4030): loss=12.332610816836585\n",
      "Stochastic Gradient Descent(4031): loss=17.964476472526293\n",
      "Stochastic Gradient Descent(4032): loss=31.576216113667964\n",
      "Stochastic Gradient Descent(4033): loss=17.981202750471017\n",
      "Stochastic Gradient Descent(4034): loss=0.17784789668863965\n",
      "Stochastic Gradient Descent(4035): loss=5.668050508457907\n",
      "Stochastic Gradient Descent(4036): loss=7.844044694113966\n",
      "Stochastic Gradient Descent(4037): loss=2.8253287889433665\n",
      "Stochastic Gradient Descent(4038): loss=49.835736854615995\n",
      "Stochastic Gradient Descent(4039): loss=39.57055243352147\n",
      "Stochastic Gradient Descent(4040): loss=58.63694590599982\n",
      "Stochastic Gradient Descent(4041): loss=7.038629342149478\n",
      "Stochastic Gradient Descent(4042): loss=2.1110666848058686\n",
      "Stochastic Gradient Descent(4043): loss=19.61944631081537\n",
      "Stochastic Gradient Descent(4044): loss=17.477662941883633\n",
      "Stochastic Gradient Descent(4045): loss=2.4307904785372454\n",
      "Stochastic Gradient Descent(4046): loss=1.569808014342584\n",
      "Stochastic Gradient Descent(4047): loss=0.012275110135253857\n",
      "Stochastic Gradient Descent(4048): loss=2.7489413747622953\n",
      "Stochastic Gradient Descent(4049): loss=15.652035098047275\n",
      "Stochastic Gradient Descent(4050): loss=44.47330860431917\n",
      "Stochastic Gradient Descent(4051): loss=3.750960398970642\n",
      "Stochastic Gradient Descent(4052): loss=38.446430379519306\n",
      "Stochastic Gradient Descent(4053): loss=0.003630691387732651\n",
      "Stochastic Gradient Descent(4054): loss=8.523044021477697\n",
      "Stochastic Gradient Descent(4055): loss=9.611930079152774\n",
      "Stochastic Gradient Descent(4056): loss=52.27183325940828\n",
      "Stochastic Gradient Descent(4057): loss=176.42116596887607\n",
      "Stochastic Gradient Descent(4058): loss=0.4669950644224817\n",
      "Stochastic Gradient Descent(4059): loss=0.08586938921392648\n",
      "Stochastic Gradient Descent(4060): loss=0.04760478811270582\n",
      "Stochastic Gradient Descent(4061): loss=0.20371143588968985\n",
      "Stochastic Gradient Descent(4062): loss=56.30831977011885\n",
      "Stochastic Gradient Descent(4063): loss=5.422659756710376\n",
      "Stochastic Gradient Descent(4064): loss=30.601025160349383\n",
      "Stochastic Gradient Descent(4065): loss=2.4413959986518767\n",
      "Stochastic Gradient Descent(4066): loss=11.676869592303314\n",
      "Stochastic Gradient Descent(4067): loss=0.5688929271157975\n",
      "Stochastic Gradient Descent(4068): loss=0.6508534958352561\n",
      "Stochastic Gradient Descent(4069): loss=0.4464454655950207\n",
      "Stochastic Gradient Descent(4070): loss=7.219181633512927\n",
      "Stochastic Gradient Descent(4071): loss=9.185114088257137\n",
      "Stochastic Gradient Descent(4072): loss=9.338339855756473\n",
      "Stochastic Gradient Descent(4073): loss=0.3739097206114094\n",
      "Stochastic Gradient Descent(4074): loss=14.711912283129614\n",
      "Stochastic Gradient Descent(4075): loss=38.64209448181894\n",
      "Stochastic Gradient Descent(4076): loss=2.321006244865371\n",
      "Stochastic Gradient Descent(4077): loss=15.24407338768077\n",
      "Stochastic Gradient Descent(4078): loss=1.3071681562360495\n",
      "Stochastic Gradient Descent(4079): loss=15.688992918120077\n",
      "Stochastic Gradient Descent(4080): loss=14.135219652429205\n",
      "Stochastic Gradient Descent(4081): loss=90.11091106791638\n",
      "Stochastic Gradient Descent(4082): loss=90.10827043732363\n",
      "Stochastic Gradient Descent(4083): loss=51.53105049405812\n",
      "Stochastic Gradient Descent(4084): loss=10.57000844634537\n",
      "Stochastic Gradient Descent(4085): loss=49.92760513070979\n",
      "Stochastic Gradient Descent(4086): loss=26.962569436313274\n",
      "Stochastic Gradient Descent(4087): loss=1.5127652587990434\n",
      "Stochastic Gradient Descent(4088): loss=1.5509181612879297\n",
      "Stochastic Gradient Descent(4089): loss=4.669788172720018\n",
      "Stochastic Gradient Descent(4090): loss=26.558126170094276\n",
      "Stochastic Gradient Descent(4091): loss=25.909842252953613\n",
      "Stochastic Gradient Descent(4092): loss=10.43630522029827\n",
      "Stochastic Gradient Descent(4093): loss=0.6631352164515751\n",
      "Stochastic Gradient Descent(4094): loss=0.6673238392961212\n",
      "Stochastic Gradient Descent(4095): loss=0.007286834021462059\n",
      "Stochastic Gradient Descent(4096): loss=0.32722001808672857\n",
      "Stochastic Gradient Descent(4097): loss=8.103832497583506\n",
      "Stochastic Gradient Descent(4098): loss=4.7667627254331775\n",
      "Stochastic Gradient Descent(4099): loss=4.408244292618394\n",
      "Stochastic Gradient Descent(4100): loss=63.26174290541765\n",
      "Stochastic Gradient Descent(4101): loss=6.880350824358705\n",
      "Stochastic Gradient Descent(4102): loss=6.335560806663529\n",
      "Stochastic Gradient Descent(4103): loss=40.475529302736014\n",
      "Stochastic Gradient Descent(4104): loss=61.555444644770176\n",
      "Stochastic Gradient Descent(4105): loss=9.925744373974933\n",
      "Stochastic Gradient Descent(4106): loss=5.728889311132379\n",
      "Stochastic Gradient Descent(4107): loss=207.93310869298355\n",
      "Stochastic Gradient Descent(4108): loss=30.840683766941634\n",
      "Stochastic Gradient Descent(4109): loss=16.8279131370582\n",
      "Stochastic Gradient Descent(4110): loss=23.645509407201228\n",
      "Stochastic Gradient Descent(4111): loss=20.549579186861532\n",
      "Stochastic Gradient Descent(4112): loss=17.620865038835245\n",
      "Stochastic Gradient Descent(4113): loss=16.69070273083488\n",
      "Stochastic Gradient Descent(4114): loss=0.10381990420178439\n",
      "Stochastic Gradient Descent(4115): loss=0.347947374511741\n",
      "Stochastic Gradient Descent(4116): loss=25.474001798262936\n",
      "Stochastic Gradient Descent(4117): loss=6.968300069481496\n",
      "Stochastic Gradient Descent(4118): loss=14.313805928227627\n",
      "Stochastic Gradient Descent(4119): loss=3.492924122626603\n",
      "Stochastic Gradient Descent(4120): loss=52.87555756404765\n",
      "Stochastic Gradient Descent(4121): loss=33.437162990542944\n",
      "Stochastic Gradient Descent(4122): loss=8.40879280009578\n",
      "Stochastic Gradient Descent(4123): loss=2.0895552230937753\n",
      "Stochastic Gradient Descent(4124): loss=0.18565636816610456\n",
      "Stochastic Gradient Descent(4125): loss=19.367978229218004\n",
      "Stochastic Gradient Descent(4126): loss=22.11262919144032\n",
      "Stochastic Gradient Descent(4127): loss=16.27457159434517\n",
      "Stochastic Gradient Descent(4128): loss=0.5691187202793999\n",
      "Stochastic Gradient Descent(4129): loss=3.0763875727738954\n",
      "Stochastic Gradient Descent(4130): loss=1.533616829012559\n",
      "Stochastic Gradient Descent(4131): loss=0.19331518097912784\n",
      "Stochastic Gradient Descent(4132): loss=88.97290496741242\n",
      "Stochastic Gradient Descent(4133): loss=9.582994706263346\n",
      "Stochastic Gradient Descent(4134): loss=1.1351200781437734\n",
      "Stochastic Gradient Descent(4135): loss=0.01965470182713263\n",
      "Stochastic Gradient Descent(4136): loss=9.270950300094226\n",
      "Stochastic Gradient Descent(4137): loss=13.562636346169374\n",
      "Stochastic Gradient Descent(4138): loss=1.573098354218575\n",
      "Stochastic Gradient Descent(4139): loss=4.624161803007736\n",
      "Stochastic Gradient Descent(4140): loss=4.917134655603412\n",
      "Stochastic Gradient Descent(4141): loss=21.111938490543896\n",
      "Stochastic Gradient Descent(4142): loss=9.190332943112834\n",
      "Stochastic Gradient Descent(4143): loss=69.09384578331378\n",
      "Stochastic Gradient Descent(4144): loss=7.090746359571642\n",
      "Stochastic Gradient Descent(4145): loss=0.36826499777701954\n",
      "Stochastic Gradient Descent(4146): loss=29.775232679359902\n",
      "Stochastic Gradient Descent(4147): loss=3.5860723512648685\n",
      "Stochastic Gradient Descent(4148): loss=11.627092172146419\n",
      "Stochastic Gradient Descent(4149): loss=9.347949354186378\n",
      "Stochastic Gradient Descent(4150): loss=4.973590139197435\n",
      "Stochastic Gradient Descent(4151): loss=0.15326910512175995\n",
      "Stochastic Gradient Descent(4152): loss=22.403160585852962\n",
      "Stochastic Gradient Descent(4153): loss=2.7021869037389434\n",
      "Stochastic Gradient Descent(4154): loss=8.782141068372193\n",
      "Stochastic Gradient Descent(4155): loss=5.978446769780799\n",
      "Stochastic Gradient Descent(4156): loss=0.013041351083093728\n",
      "Stochastic Gradient Descent(4157): loss=14.270166547176684\n",
      "Stochastic Gradient Descent(4158): loss=21.331962525332397\n",
      "Stochastic Gradient Descent(4159): loss=1.9138821189745299\n",
      "Stochastic Gradient Descent(4160): loss=20.084231048918994\n",
      "Stochastic Gradient Descent(4161): loss=1.9966951310142118\n",
      "Stochastic Gradient Descent(4162): loss=30.686140345962425\n",
      "Stochastic Gradient Descent(4163): loss=4.2542827481505485\n",
      "Stochastic Gradient Descent(4164): loss=1.3190462592770325\n",
      "Stochastic Gradient Descent(4165): loss=17.260231137624995\n",
      "Stochastic Gradient Descent(4166): loss=3.0884834598349244\n",
      "Stochastic Gradient Descent(4167): loss=0.8109738583231536\n",
      "Stochastic Gradient Descent(4168): loss=0.7379766262217718\n",
      "Stochastic Gradient Descent(4169): loss=0.3936067386649994\n",
      "Stochastic Gradient Descent(4170): loss=8.100369847042646\n",
      "Stochastic Gradient Descent(4171): loss=3.8030524618605703\n",
      "Stochastic Gradient Descent(4172): loss=0.5019335955696768\n",
      "Stochastic Gradient Descent(4173): loss=10.21218531737427\n",
      "Stochastic Gradient Descent(4174): loss=1.5057671403933606\n",
      "Stochastic Gradient Descent(4175): loss=1.76047082556133\n",
      "Stochastic Gradient Descent(4176): loss=0.022832806690910496\n",
      "Stochastic Gradient Descent(4177): loss=23.188679505986933\n",
      "Stochastic Gradient Descent(4178): loss=0.8027690727036111\n",
      "Stochastic Gradient Descent(4179): loss=43.546998155005\n",
      "Stochastic Gradient Descent(4180): loss=2.3031310654422144\n",
      "Stochastic Gradient Descent(4181): loss=18.694941118377635\n",
      "Stochastic Gradient Descent(4182): loss=11.7616894308282\n",
      "Stochastic Gradient Descent(4183): loss=0.020726778908087325\n",
      "Stochastic Gradient Descent(4184): loss=2.200003088653254\n",
      "Stochastic Gradient Descent(4185): loss=10.346103089559982\n",
      "Stochastic Gradient Descent(4186): loss=3.128820638704304\n",
      "Stochastic Gradient Descent(4187): loss=23.276883955857404\n",
      "Stochastic Gradient Descent(4188): loss=12.879420523119286\n",
      "Stochastic Gradient Descent(4189): loss=22.94382540418767\n",
      "Stochastic Gradient Descent(4190): loss=1.7600817512294464\n",
      "Stochastic Gradient Descent(4191): loss=74.22895793720463\n",
      "Stochastic Gradient Descent(4192): loss=1.4189460637870919\n",
      "Stochastic Gradient Descent(4193): loss=1.4761394755884505\n",
      "Stochastic Gradient Descent(4194): loss=1.9791207032025242\n",
      "Stochastic Gradient Descent(4195): loss=1.942092792386735\n",
      "Stochastic Gradient Descent(4196): loss=204.1114624811811\n",
      "Stochastic Gradient Descent(4197): loss=163.96175250206932\n",
      "Stochastic Gradient Descent(4198): loss=50.91867830059885\n",
      "Stochastic Gradient Descent(4199): loss=7.253908027102247\n",
      "Stochastic Gradient Descent(4200): loss=2.2863181847430343\n",
      "Stochastic Gradient Descent(4201): loss=63.95745392367875\n",
      "Stochastic Gradient Descent(4202): loss=19.558785736753926\n",
      "Stochastic Gradient Descent(4203): loss=0.18035106680369453\n",
      "Stochastic Gradient Descent(4204): loss=1.1766866962413105\n",
      "Stochastic Gradient Descent(4205): loss=9.16611145496965\n",
      "Stochastic Gradient Descent(4206): loss=21.25936510553761\n",
      "Stochastic Gradient Descent(4207): loss=8.761951700145358\n",
      "Stochastic Gradient Descent(4208): loss=16.360703505728928\n",
      "Stochastic Gradient Descent(4209): loss=172.64100998125255\n",
      "Stochastic Gradient Descent(4210): loss=7.352258063732558\n",
      "Stochastic Gradient Descent(4211): loss=0.2927716401331365\n",
      "Stochastic Gradient Descent(4212): loss=18.07060075989986\n",
      "Stochastic Gradient Descent(4213): loss=21.844464720092848\n",
      "Stochastic Gradient Descent(4214): loss=1.4037223829805854\n",
      "Stochastic Gradient Descent(4215): loss=1.1188950885038842\n",
      "Stochastic Gradient Descent(4216): loss=5.381275301958701\n",
      "Stochastic Gradient Descent(4217): loss=3.33030831178054\n",
      "Stochastic Gradient Descent(4218): loss=2.874020250002481\n",
      "Stochastic Gradient Descent(4219): loss=2.256575132208356\n",
      "Stochastic Gradient Descent(4220): loss=5.184104844676163\n",
      "Stochastic Gradient Descent(4221): loss=21.69097679440938\n",
      "Stochastic Gradient Descent(4222): loss=3.6492873075832137\n",
      "Stochastic Gradient Descent(4223): loss=12.726006518935852\n",
      "Stochastic Gradient Descent(4224): loss=0.6411477085815411\n",
      "Stochastic Gradient Descent(4225): loss=15.665436481179585\n",
      "Stochastic Gradient Descent(4226): loss=0.01219892850516091\n",
      "Stochastic Gradient Descent(4227): loss=44.66061618056878\n",
      "Stochastic Gradient Descent(4228): loss=12.740843088130722\n",
      "Stochastic Gradient Descent(4229): loss=7.191334353624181\n",
      "Stochastic Gradient Descent(4230): loss=3.1664045327668937\n",
      "Stochastic Gradient Descent(4231): loss=13.059142776273799\n",
      "Stochastic Gradient Descent(4232): loss=2.386732341178139\n",
      "Stochastic Gradient Descent(4233): loss=3.9359754812647347\n",
      "Stochastic Gradient Descent(4234): loss=16.451885843068286\n",
      "Stochastic Gradient Descent(4235): loss=0.09847216967363336\n",
      "Stochastic Gradient Descent(4236): loss=15.035815850786497\n",
      "Stochastic Gradient Descent(4237): loss=6.972378395604874\n",
      "Stochastic Gradient Descent(4238): loss=1.0036961431478848\n",
      "Stochastic Gradient Descent(4239): loss=1.5334478498567414\n",
      "Stochastic Gradient Descent(4240): loss=0.0042244485981725284\n",
      "Stochastic Gradient Descent(4241): loss=3.798126211775536\n",
      "Stochastic Gradient Descent(4242): loss=7.1765842443347685\n",
      "Stochastic Gradient Descent(4243): loss=4.909369915494358\n",
      "Stochastic Gradient Descent(4244): loss=0.3523080072444633\n",
      "Stochastic Gradient Descent(4245): loss=0.00010291461704975026\n",
      "Stochastic Gradient Descent(4246): loss=33.78901860563367\n",
      "Stochastic Gradient Descent(4247): loss=0.34706887370420053\n",
      "Stochastic Gradient Descent(4248): loss=12.638848973938957\n",
      "Stochastic Gradient Descent(4249): loss=2.262504487408996\n",
      "Stochastic Gradient Descent(4250): loss=6.8685324938315215\n",
      "Stochastic Gradient Descent(4251): loss=2.7877627673640446\n",
      "Stochastic Gradient Descent(4252): loss=14.603758854316126\n",
      "Stochastic Gradient Descent(4253): loss=0.23719202690877492\n",
      "Stochastic Gradient Descent(4254): loss=4.842768369796267\n",
      "Stochastic Gradient Descent(4255): loss=0.1795064438955396\n",
      "Stochastic Gradient Descent(4256): loss=53.58067277635705\n",
      "Stochastic Gradient Descent(4257): loss=41.18514965587534\n",
      "Stochastic Gradient Descent(4258): loss=0.5241785096637428\n",
      "Stochastic Gradient Descent(4259): loss=5.511703110647445\n",
      "Stochastic Gradient Descent(4260): loss=5.444314573586726\n",
      "Stochastic Gradient Descent(4261): loss=9.611794187198628\n",
      "Stochastic Gradient Descent(4262): loss=28.804812028195983\n",
      "Stochastic Gradient Descent(4263): loss=33.91293201182584\n",
      "Stochastic Gradient Descent(4264): loss=19.02513674864676\n",
      "Stochastic Gradient Descent(4265): loss=10.972130596040701\n",
      "Stochastic Gradient Descent(4266): loss=68.1603819322217\n",
      "Stochastic Gradient Descent(4267): loss=22.626820715928037\n",
      "Stochastic Gradient Descent(4268): loss=16.30061154255838\n",
      "Stochastic Gradient Descent(4269): loss=0.36811412293777\n",
      "Stochastic Gradient Descent(4270): loss=8.527873245971158\n",
      "Stochastic Gradient Descent(4271): loss=102.8135551280795\n",
      "Stochastic Gradient Descent(4272): loss=9.25014557551745\n",
      "Stochastic Gradient Descent(4273): loss=0.7442615027201165\n",
      "Stochastic Gradient Descent(4274): loss=1.0378489008061282\n",
      "Stochastic Gradient Descent(4275): loss=15.005369418536565\n",
      "Stochastic Gradient Descent(4276): loss=0.47239477146450964\n",
      "Stochastic Gradient Descent(4277): loss=0.44872673800939916\n",
      "Stochastic Gradient Descent(4278): loss=9.239833772558358\n",
      "Stochastic Gradient Descent(4279): loss=0.09475300133855541\n",
      "Stochastic Gradient Descent(4280): loss=1.2049361123101041\n",
      "Stochastic Gradient Descent(4281): loss=0.0013102904634818304\n",
      "Stochastic Gradient Descent(4282): loss=4.799653181919877\n",
      "Stochastic Gradient Descent(4283): loss=1.1751118309391344\n",
      "Stochastic Gradient Descent(4284): loss=1.720372868889865\n",
      "Stochastic Gradient Descent(4285): loss=0.0932654259685388\n",
      "Stochastic Gradient Descent(4286): loss=0.25212690404239063\n",
      "Stochastic Gradient Descent(4287): loss=0.5177069919770394\n",
      "Stochastic Gradient Descent(4288): loss=3.8588173374785533\n",
      "Stochastic Gradient Descent(4289): loss=12.510837724294376\n",
      "Stochastic Gradient Descent(4290): loss=19.157996492932604\n",
      "Stochastic Gradient Descent(4291): loss=3.5792836283689136\n",
      "Stochastic Gradient Descent(4292): loss=3.749104525485006\n",
      "Stochastic Gradient Descent(4293): loss=3.0881464393533333\n",
      "Stochastic Gradient Descent(4294): loss=0.8339258618236577\n",
      "Stochastic Gradient Descent(4295): loss=11.047138525747496\n",
      "Stochastic Gradient Descent(4296): loss=0.042297594254595394\n",
      "Stochastic Gradient Descent(4297): loss=51.01676335822897\n",
      "Stochastic Gradient Descent(4298): loss=2.5756074473609805\n",
      "Stochastic Gradient Descent(4299): loss=0.879518760230896\n",
      "Stochastic Gradient Descent(4300): loss=1.331044509609233\n",
      "Stochastic Gradient Descent(4301): loss=4.429123041547537\n",
      "Stochastic Gradient Descent(4302): loss=0.20232499204828097\n",
      "Stochastic Gradient Descent(4303): loss=1.8341996985050146\n",
      "Stochastic Gradient Descent(4304): loss=6.120406545051082\n",
      "Stochastic Gradient Descent(4305): loss=42.62550829491712\n",
      "Stochastic Gradient Descent(4306): loss=72.06649108781588\n",
      "Stochastic Gradient Descent(4307): loss=0.3259610576815321\n",
      "Stochastic Gradient Descent(4308): loss=4.370396026662258\n",
      "Stochastic Gradient Descent(4309): loss=7.229126221504928\n",
      "Stochastic Gradient Descent(4310): loss=9.744725580572208\n",
      "Stochastic Gradient Descent(4311): loss=25.79664872653119\n",
      "Stochastic Gradient Descent(4312): loss=5.90467321923245\n",
      "Stochastic Gradient Descent(4313): loss=0.49834551597869964\n",
      "Stochastic Gradient Descent(4314): loss=1.4168045984940631\n",
      "Stochastic Gradient Descent(4315): loss=3.6783557236385374\n",
      "Stochastic Gradient Descent(4316): loss=38.81337600358014\n",
      "Stochastic Gradient Descent(4317): loss=11.72235411004463\n",
      "Stochastic Gradient Descent(4318): loss=6.771311555414667\n",
      "Stochastic Gradient Descent(4319): loss=0.9035397082892935\n",
      "Stochastic Gradient Descent(4320): loss=2.4522159612008063\n",
      "Stochastic Gradient Descent(4321): loss=2.919388661565519\n",
      "Stochastic Gradient Descent(4322): loss=6.177380603729855\n",
      "Stochastic Gradient Descent(4323): loss=22.87526655151214\n",
      "Stochastic Gradient Descent(4324): loss=1.9029807561185215\n",
      "Stochastic Gradient Descent(4325): loss=0.013828090299905029\n",
      "Stochastic Gradient Descent(4326): loss=6.810373570055478\n",
      "Stochastic Gradient Descent(4327): loss=3.9617627131575888\n",
      "Stochastic Gradient Descent(4328): loss=1.4482719584529253\n",
      "Stochastic Gradient Descent(4329): loss=227.3058886005854\n",
      "Stochastic Gradient Descent(4330): loss=1.8480342664163925\n",
      "Stochastic Gradient Descent(4331): loss=213.75724044816596\n",
      "Stochastic Gradient Descent(4332): loss=58.67533656599238\n",
      "Stochastic Gradient Descent(4333): loss=19.704551314121233\n",
      "Stochastic Gradient Descent(4334): loss=23.476800760777145\n",
      "Stochastic Gradient Descent(4335): loss=7.692241572424837\n",
      "Stochastic Gradient Descent(4336): loss=40.16261543763785\n",
      "Stochastic Gradient Descent(4337): loss=39.45284986420773\n",
      "Stochastic Gradient Descent(4338): loss=0.9651167650806666\n",
      "Stochastic Gradient Descent(4339): loss=110.68004792744257\n",
      "Stochastic Gradient Descent(4340): loss=160.7365809314848\n",
      "Stochastic Gradient Descent(4341): loss=1.4236387695963444\n",
      "Stochastic Gradient Descent(4342): loss=2.6982471645504282\n",
      "Stochastic Gradient Descent(4343): loss=4.83494599757737\n",
      "Stochastic Gradient Descent(4344): loss=21.084185115217835\n",
      "Stochastic Gradient Descent(4345): loss=5.324874939782407\n",
      "Stochastic Gradient Descent(4346): loss=2.103135407218611\n",
      "Stochastic Gradient Descent(4347): loss=31.303240152889092\n",
      "Stochastic Gradient Descent(4348): loss=34.88401057669327\n",
      "Stochastic Gradient Descent(4349): loss=25.22300828542376\n",
      "Stochastic Gradient Descent(4350): loss=24.72980637831114\n",
      "Stochastic Gradient Descent(4351): loss=15.335790909276849\n",
      "Stochastic Gradient Descent(4352): loss=0.09095171752123245\n",
      "Stochastic Gradient Descent(4353): loss=2.212648238613539\n",
      "Stochastic Gradient Descent(4354): loss=1.0446501388198284\n",
      "Stochastic Gradient Descent(4355): loss=42.041223436145785\n",
      "Stochastic Gradient Descent(4356): loss=2.080432962818919\n",
      "Stochastic Gradient Descent(4357): loss=6.75257556421903\n",
      "Stochastic Gradient Descent(4358): loss=33.45709868393324\n",
      "Stochastic Gradient Descent(4359): loss=8.842177375264423\n",
      "Stochastic Gradient Descent(4360): loss=17.648217442041314\n",
      "Stochastic Gradient Descent(4361): loss=31.39511876589353\n",
      "Stochastic Gradient Descent(4362): loss=33.32983509764834\n",
      "Stochastic Gradient Descent(4363): loss=3.017101346244569\n",
      "Stochastic Gradient Descent(4364): loss=0.6843560548429793\n",
      "Stochastic Gradient Descent(4365): loss=2.9627507614421478\n",
      "Stochastic Gradient Descent(4366): loss=12.703841016347686\n",
      "Stochastic Gradient Descent(4367): loss=1.2457291858627997\n",
      "Stochastic Gradient Descent(4368): loss=3.9100607482652565\n",
      "Stochastic Gradient Descent(4369): loss=1.0349615993672736\n",
      "Stochastic Gradient Descent(4370): loss=0.46362939031303224\n",
      "Stochastic Gradient Descent(4371): loss=38.90206404042057\n",
      "Stochastic Gradient Descent(4372): loss=4.899896072227221\n",
      "Stochastic Gradient Descent(4373): loss=0.3397090564861875\n",
      "Stochastic Gradient Descent(4374): loss=4.249786313439901\n",
      "Stochastic Gradient Descent(4375): loss=0.6333317868567042\n",
      "Stochastic Gradient Descent(4376): loss=19.36906637787988\n",
      "Stochastic Gradient Descent(4377): loss=0.09528867590973811\n",
      "Stochastic Gradient Descent(4378): loss=3.0618732038146215\n",
      "Stochastic Gradient Descent(4379): loss=0.0006107925437263162\n",
      "Stochastic Gradient Descent(4380): loss=4.541977037253344\n",
      "Stochastic Gradient Descent(4381): loss=10.516789918033437\n",
      "Stochastic Gradient Descent(4382): loss=2.83481236977606\n",
      "Stochastic Gradient Descent(4383): loss=0.6197960825849713\n",
      "Stochastic Gradient Descent(4384): loss=1.4251826644571954\n",
      "Stochastic Gradient Descent(4385): loss=2.056022172841993\n",
      "Stochastic Gradient Descent(4386): loss=6.234018567455169\n",
      "Stochastic Gradient Descent(4387): loss=274.4043389007624\n",
      "Stochastic Gradient Descent(4388): loss=553.3690663556124\n",
      "Stochastic Gradient Descent(4389): loss=0.16967347806710847\n",
      "Stochastic Gradient Descent(4390): loss=305.6486840977547\n",
      "Stochastic Gradient Descent(4391): loss=24.326876094026993\n",
      "Stochastic Gradient Descent(4392): loss=25.71903438844738\n",
      "Stochastic Gradient Descent(4393): loss=7.342902276376195\n",
      "Stochastic Gradient Descent(4394): loss=9.815046046455183\n",
      "Stochastic Gradient Descent(4395): loss=13.715766876006\n",
      "Stochastic Gradient Descent(4396): loss=17.966807506801405\n",
      "Stochastic Gradient Descent(4397): loss=4.283368366811586\n",
      "Stochastic Gradient Descent(4398): loss=5.776262932885058\n",
      "Stochastic Gradient Descent(4399): loss=2.468623542689569\n",
      "Stochastic Gradient Descent(4400): loss=5.169875407951269\n",
      "Stochastic Gradient Descent(4401): loss=8.711848675642191\n",
      "Stochastic Gradient Descent(4402): loss=0.007489490962509389\n",
      "Stochastic Gradient Descent(4403): loss=15.208302098957322\n",
      "Stochastic Gradient Descent(4404): loss=19.164924798006215\n",
      "Stochastic Gradient Descent(4405): loss=3.330719163174469\n",
      "Stochastic Gradient Descent(4406): loss=23.98387973240644\n",
      "Stochastic Gradient Descent(4407): loss=1.0289009197900598\n",
      "Stochastic Gradient Descent(4408): loss=0.03203043131462628\n",
      "Stochastic Gradient Descent(4409): loss=0.012482329663588889\n",
      "Stochastic Gradient Descent(4410): loss=14.543951777923468\n",
      "Stochastic Gradient Descent(4411): loss=18.631189757087057\n",
      "Stochastic Gradient Descent(4412): loss=26.59539653444876\n",
      "Stochastic Gradient Descent(4413): loss=5.940090167242865\n",
      "Stochastic Gradient Descent(4414): loss=2.707019541590202\n",
      "Stochastic Gradient Descent(4415): loss=1.133054780599415\n",
      "Stochastic Gradient Descent(4416): loss=6.11394533206811\n",
      "Stochastic Gradient Descent(4417): loss=0.004329742134196319\n",
      "Stochastic Gradient Descent(4418): loss=4.674635859929991\n",
      "Stochastic Gradient Descent(4419): loss=13.948685009716362\n",
      "Stochastic Gradient Descent(4420): loss=0.06889463111787889\n",
      "Stochastic Gradient Descent(4421): loss=0.9096816604444198\n",
      "Stochastic Gradient Descent(4422): loss=20.86811767199411\n",
      "Stochastic Gradient Descent(4423): loss=68.16413388388922\n",
      "Stochastic Gradient Descent(4424): loss=2.6839565056106935\n",
      "Stochastic Gradient Descent(4425): loss=16.991934577308786\n",
      "Stochastic Gradient Descent(4426): loss=30.591459994175747\n",
      "Stochastic Gradient Descent(4427): loss=2.3005822280029853\n",
      "Stochastic Gradient Descent(4428): loss=0.07139233052774104\n",
      "Stochastic Gradient Descent(4429): loss=0.26800764364462276\n",
      "Stochastic Gradient Descent(4430): loss=13.44840686010946\n",
      "Stochastic Gradient Descent(4431): loss=40.71106389516982\n",
      "Stochastic Gradient Descent(4432): loss=5.0059140233136175\n",
      "Stochastic Gradient Descent(4433): loss=9.687709533330933\n",
      "Stochastic Gradient Descent(4434): loss=6.6109126864580405\n",
      "Stochastic Gradient Descent(4435): loss=1.1786649393020532\n",
      "Stochastic Gradient Descent(4436): loss=8.170053518466664\n",
      "Stochastic Gradient Descent(4437): loss=12.452778057719247\n",
      "Stochastic Gradient Descent(4438): loss=9.953498578779934\n",
      "Stochastic Gradient Descent(4439): loss=4.3059923364955015\n",
      "Stochastic Gradient Descent(4440): loss=2.4430228100386007\n",
      "Stochastic Gradient Descent(4441): loss=0.4170072177470759\n",
      "Stochastic Gradient Descent(4442): loss=1.400619007510962\n",
      "Stochastic Gradient Descent(4443): loss=54.18209008813954\n",
      "Stochastic Gradient Descent(4444): loss=9.641703333438434\n",
      "Stochastic Gradient Descent(4445): loss=2.90037444686112\n",
      "Stochastic Gradient Descent(4446): loss=26.98459329062871\n",
      "Stochastic Gradient Descent(4447): loss=15.81284986347609\n",
      "Stochastic Gradient Descent(4448): loss=3.0573512500865743\n",
      "Stochastic Gradient Descent(4449): loss=1.2240253606555855\n",
      "Stochastic Gradient Descent(4450): loss=14.40205964463824\n",
      "Stochastic Gradient Descent(4451): loss=11.028351736706743\n",
      "Stochastic Gradient Descent(4452): loss=22.35771648169896\n",
      "Stochastic Gradient Descent(4453): loss=8.721416471252223\n",
      "Stochastic Gradient Descent(4454): loss=5.377337527415835\n",
      "Stochastic Gradient Descent(4455): loss=6.150361031313265\n",
      "Stochastic Gradient Descent(4456): loss=0.3875938882781084\n",
      "Stochastic Gradient Descent(4457): loss=19.84260991031282\n",
      "Stochastic Gradient Descent(4458): loss=3.2951503609420203\n",
      "Stochastic Gradient Descent(4459): loss=0.7779543472670168\n",
      "Stochastic Gradient Descent(4460): loss=8.875421318408067\n",
      "Stochastic Gradient Descent(4461): loss=5.737340191638261\n",
      "Stochastic Gradient Descent(4462): loss=22.35385909314964\n",
      "Stochastic Gradient Descent(4463): loss=5.518051514494065\n",
      "Stochastic Gradient Descent(4464): loss=19.290927175347083\n",
      "Stochastic Gradient Descent(4465): loss=0.010002461865927695\n",
      "Stochastic Gradient Descent(4466): loss=0.0036032929634879944\n",
      "Stochastic Gradient Descent(4467): loss=11.271361936331948\n",
      "Stochastic Gradient Descent(4468): loss=0.43076460021112345\n",
      "Stochastic Gradient Descent(4469): loss=0.9640142119186124\n",
      "Stochastic Gradient Descent(4470): loss=2.388866177763661\n",
      "Stochastic Gradient Descent(4471): loss=6.628498713071637\n",
      "Stochastic Gradient Descent(4472): loss=1.0331787966124648\n",
      "Stochastic Gradient Descent(4473): loss=2.2366729186971206\n",
      "Stochastic Gradient Descent(4474): loss=0.1175848269573283\n",
      "Stochastic Gradient Descent(4475): loss=34.57957715359706\n",
      "Stochastic Gradient Descent(4476): loss=2.120910838517937\n",
      "Stochastic Gradient Descent(4477): loss=7.394777634861132\n",
      "Stochastic Gradient Descent(4478): loss=0.415983076806773\n",
      "Stochastic Gradient Descent(4479): loss=1.1986128593860879\n",
      "Stochastic Gradient Descent(4480): loss=0.21664755271090191\n",
      "Stochastic Gradient Descent(4481): loss=8.439642358638489\n",
      "Stochastic Gradient Descent(4482): loss=1.7871323069787624\n",
      "Stochastic Gradient Descent(4483): loss=7.86236302843042\n",
      "Stochastic Gradient Descent(4484): loss=17.315265021210983\n",
      "Stochastic Gradient Descent(4485): loss=0.3158933420274626\n",
      "Stochastic Gradient Descent(4486): loss=0.39411925389277086\n",
      "Stochastic Gradient Descent(4487): loss=36.66588084201169\n",
      "Stochastic Gradient Descent(4488): loss=2.4178033671238914\n",
      "Stochastic Gradient Descent(4489): loss=23.045321160020364\n",
      "Stochastic Gradient Descent(4490): loss=57.9748701989313\n",
      "Stochastic Gradient Descent(4491): loss=9.742455683415304\n",
      "Stochastic Gradient Descent(4492): loss=32.241427320236504\n",
      "Stochastic Gradient Descent(4493): loss=0.09889069928756589\n",
      "Stochastic Gradient Descent(4494): loss=1.2516615212205109\n",
      "Stochastic Gradient Descent(4495): loss=11.169145716061175\n",
      "Stochastic Gradient Descent(4496): loss=1.8418938073608493\n",
      "Stochastic Gradient Descent(4497): loss=3.6987956046581267\n",
      "Stochastic Gradient Descent(4498): loss=1.8410895928181124\n",
      "Stochastic Gradient Descent(4499): loss=3.281483555590929\n",
      "Stochastic Gradient Descent(4500): loss=9.89037731310537\n",
      "Stochastic Gradient Descent(4501): loss=0.05047899850988972\n",
      "Stochastic Gradient Descent(4502): loss=0.36066193999082136\n",
      "Stochastic Gradient Descent(4503): loss=0.1934308819850402\n",
      "Stochastic Gradient Descent(4504): loss=56.63292333181692\n",
      "Stochastic Gradient Descent(4505): loss=8.380525780470988\n",
      "Stochastic Gradient Descent(4506): loss=1.6881533111325655\n",
      "Stochastic Gradient Descent(4507): loss=0.026677762049277087\n",
      "Stochastic Gradient Descent(4508): loss=4.818457314438134\n",
      "Stochastic Gradient Descent(4509): loss=5.678215426256661\n",
      "Stochastic Gradient Descent(4510): loss=0.3060339059313321\n",
      "Stochastic Gradient Descent(4511): loss=9.733521525567586\n",
      "Stochastic Gradient Descent(4512): loss=1.8988854235648227\n",
      "Stochastic Gradient Descent(4513): loss=1.478547634507301\n",
      "Stochastic Gradient Descent(4514): loss=19.44075569462486\n",
      "Stochastic Gradient Descent(4515): loss=25.002630713006116\n",
      "Stochastic Gradient Descent(4516): loss=1.5411766551189263\n",
      "Stochastic Gradient Descent(4517): loss=11.160093248752974\n",
      "Stochastic Gradient Descent(4518): loss=0.009421416097917731\n",
      "Stochastic Gradient Descent(4519): loss=18.50456709129148\n",
      "Stochastic Gradient Descent(4520): loss=15.495098299430873\n",
      "Stochastic Gradient Descent(4521): loss=9.93747595671757\n",
      "Stochastic Gradient Descent(4522): loss=0.19859370954049255\n",
      "Stochastic Gradient Descent(4523): loss=0.003996394470614953\n",
      "Stochastic Gradient Descent(4524): loss=9.51955886302307\n",
      "Stochastic Gradient Descent(4525): loss=6.179195090618008\n",
      "Stochastic Gradient Descent(4526): loss=3.4707076830796075\n",
      "Stochastic Gradient Descent(4527): loss=8.81482495384025\n",
      "Stochastic Gradient Descent(4528): loss=1.2913075771604838\n",
      "Stochastic Gradient Descent(4529): loss=6.100142002861603\n",
      "Stochastic Gradient Descent(4530): loss=18.245913019412864\n",
      "Stochastic Gradient Descent(4531): loss=11.79028081690755\n",
      "Stochastic Gradient Descent(4532): loss=36.150862910384255\n",
      "Stochastic Gradient Descent(4533): loss=0.7092763317533701\n",
      "Stochastic Gradient Descent(4534): loss=7.586683975546689\n",
      "Stochastic Gradient Descent(4535): loss=23.112954136385646\n",
      "Stochastic Gradient Descent(4536): loss=6.577603054719747\n",
      "Stochastic Gradient Descent(4537): loss=5.907432576306377\n",
      "Stochastic Gradient Descent(4538): loss=5.0332804208398585\n",
      "Stochastic Gradient Descent(4539): loss=0.9570926614746447\n",
      "Stochastic Gradient Descent(4540): loss=41.53230783267223\n",
      "Stochastic Gradient Descent(4541): loss=0.09036256072835065\n",
      "Stochastic Gradient Descent(4542): loss=11.868913435156694\n",
      "Stochastic Gradient Descent(4543): loss=0.027039948880523187\n",
      "Stochastic Gradient Descent(4544): loss=3.3503365859258016\n",
      "Stochastic Gradient Descent(4545): loss=15.15136174883043\n",
      "Stochastic Gradient Descent(4546): loss=25.834512027756062\n",
      "Stochastic Gradient Descent(4547): loss=21.21378753703626\n",
      "Stochastic Gradient Descent(4548): loss=3.2625753932117325\n",
      "Stochastic Gradient Descent(4549): loss=0.7569059021725755\n",
      "Stochastic Gradient Descent(4550): loss=10.30886132661929\n",
      "Stochastic Gradient Descent(4551): loss=8.847401680552172\n",
      "Stochastic Gradient Descent(4552): loss=7.643209296629582\n",
      "Stochastic Gradient Descent(4553): loss=3.641057380905696\n",
      "Stochastic Gradient Descent(4554): loss=0.9011772471268796\n",
      "Stochastic Gradient Descent(4555): loss=0.3420505512723084\n",
      "Stochastic Gradient Descent(4556): loss=19.248788048250304\n",
      "Stochastic Gradient Descent(4557): loss=0.49140010097604586\n",
      "Stochastic Gradient Descent(4558): loss=2.404764977351882\n",
      "Stochastic Gradient Descent(4559): loss=3.650504930862011\n",
      "Stochastic Gradient Descent(4560): loss=16.17893239570184\n",
      "Stochastic Gradient Descent(4561): loss=26.860983281796635\n",
      "Stochastic Gradient Descent(4562): loss=6.6443452687200315\n",
      "Stochastic Gradient Descent(4563): loss=1.5945708002005612\n",
      "Stochastic Gradient Descent(4564): loss=9.53190673300634\n",
      "Stochastic Gradient Descent(4565): loss=1.3692861475858071\n",
      "Stochastic Gradient Descent(4566): loss=18.362370539106138\n",
      "Stochastic Gradient Descent(4567): loss=0.5738849269365275\n",
      "Stochastic Gradient Descent(4568): loss=34.758252920349186\n",
      "Stochastic Gradient Descent(4569): loss=8.985610373958368\n",
      "Stochastic Gradient Descent(4570): loss=15.225422788535063\n",
      "Stochastic Gradient Descent(4571): loss=21.954719438426906\n",
      "Stochastic Gradient Descent(4572): loss=15.324855153278941\n",
      "Stochastic Gradient Descent(4573): loss=23.605291843053813\n",
      "Stochastic Gradient Descent(4574): loss=0.07609905056674345\n",
      "Stochastic Gradient Descent(4575): loss=3.6792071326415483\n",
      "Stochastic Gradient Descent(4576): loss=66.83827437419315\n",
      "Stochastic Gradient Descent(4577): loss=0.0019917794838880385\n",
      "Stochastic Gradient Descent(4578): loss=0.051325781364770784\n",
      "Stochastic Gradient Descent(4579): loss=0.32252232280248005\n",
      "Stochastic Gradient Descent(4580): loss=35.27586617268488\n",
      "Stochastic Gradient Descent(4581): loss=2.1841928837292266\n",
      "Stochastic Gradient Descent(4582): loss=3.1970263123988505\n",
      "Stochastic Gradient Descent(4583): loss=1.3939410207097287\n",
      "Stochastic Gradient Descent(4584): loss=2.3321116916847004\n",
      "Stochastic Gradient Descent(4585): loss=0.5466614013247962\n",
      "Stochastic Gradient Descent(4586): loss=0.3485230062123771\n",
      "Stochastic Gradient Descent(4587): loss=0.7558591036863651\n",
      "Stochastic Gradient Descent(4588): loss=9.270823174960858\n",
      "Stochastic Gradient Descent(4589): loss=3.4138057410294382\n",
      "Stochastic Gradient Descent(4590): loss=4.212798788057733\n",
      "Stochastic Gradient Descent(4591): loss=54.60720660013247\n",
      "Stochastic Gradient Descent(4592): loss=22.609303910417168\n",
      "Stochastic Gradient Descent(4593): loss=11.975583774569865\n",
      "Stochastic Gradient Descent(4594): loss=0.017068972656024858\n",
      "Stochastic Gradient Descent(4595): loss=1.5225940300899776\n",
      "Stochastic Gradient Descent(4596): loss=1.5913609783184341\n",
      "Stochastic Gradient Descent(4597): loss=149.131877318939\n",
      "Stochastic Gradient Descent(4598): loss=7.474407760247948\n",
      "Stochastic Gradient Descent(4599): loss=22.84571447934301\n",
      "Stochastic Gradient Descent(4600): loss=9.497077223743055\n",
      "Stochastic Gradient Descent(4601): loss=4.061483129835166\n",
      "Stochastic Gradient Descent(4602): loss=0.3928779001504933\n",
      "Stochastic Gradient Descent(4603): loss=33.77656022762459\n",
      "Stochastic Gradient Descent(4604): loss=0.004135958032726808\n",
      "Stochastic Gradient Descent(4605): loss=0.03789443515133943\n",
      "Stochastic Gradient Descent(4606): loss=0.05710730515665835\n",
      "Stochastic Gradient Descent(4607): loss=30.39455620563064\n",
      "Stochastic Gradient Descent(4608): loss=16.964777754201048\n",
      "Stochastic Gradient Descent(4609): loss=10.209127021814457\n",
      "Stochastic Gradient Descent(4610): loss=4.129450807262524\n",
      "Stochastic Gradient Descent(4611): loss=10.70173483020485\n",
      "Stochastic Gradient Descent(4612): loss=1.7467064897068705\n",
      "Stochastic Gradient Descent(4613): loss=2.824343394958842\n",
      "Stochastic Gradient Descent(4614): loss=4.444712076869136\n",
      "Stochastic Gradient Descent(4615): loss=10.258908384858561\n",
      "Stochastic Gradient Descent(4616): loss=0.18335836155038504\n",
      "Stochastic Gradient Descent(4617): loss=2.421072220812782\n",
      "Stochastic Gradient Descent(4618): loss=0.17080585544044072\n",
      "Stochastic Gradient Descent(4619): loss=25.781257014622575\n",
      "Stochastic Gradient Descent(4620): loss=2.3233833132051056\n",
      "Stochastic Gradient Descent(4621): loss=8.810001948873506\n",
      "Stochastic Gradient Descent(4622): loss=55.76858925043526\n",
      "Stochastic Gradient Descent(4623): loss=10.872912883860698\n",
      "Stochastic Gradient Descent(4624): loss=5.409188807515574\n",
      "Stochastic Gradient Descent(4625): loss=31.02716101855286\n",
      "Stochastic Gradient Descent(4626): loss=0.526026025366025\n",
      "Stochastic Gradient Descent(4627): loss=0.22156961779513307\n",
      "Stochastic Gradient Descent(4628): loss=9.227504189961902\n",
      "Stochastic Gradient Descent(4629): loss=20.52335159002342\n",
      "Stochastic Gradient Descent(4630): loss=0.3653992428677091\n",
      "Stochastic Gradient Descent(4631): loss=0.017525113254697803\n",
      "Stochastic Gradient Descent(4632): loss=21.394237241024307\n",
      "Stochastic Gradient Descent(4633): loss=4.687414567465593\n",
      "Stochastic Gradient Descent(4634): loss=6.3575188538598635\n",
      "Stochastic Gradient Descent(4635): loss=0.6043917539554704\n",
      "Stochastic Gradient Descent(4636): loss=1.5667386630288955\n",
      "Stochastic Gradient Descent(4637): loss=3.9698490421310937\n",
      "Stochastic Gradient Descent(4638): loss=6.8460782951595425\n",
      "Stochastic Gradient Descent(4639): loss=19.230563258861043\n",
      "Stochastic Gradient Descent(4640): loss=7.5114472236666066\n",
      "Stochastic Gradient Descent(4641): loss=5.935752279502139\n",
      "Stochastic Gradient Descent(4642): loss=0.4676070899813766\n",
      "Stochastic Gradient Descent(4643): loss=0.1291433805582997\n",
      "Stochastic Gradient Descent(4644): loss=3.1295603953149955\n",
      "Stochastic Gradient Descent(4645): loss=0.008430976275332938\n",
      "Stochastic Gradient Descent(4646): loss=5.870938262211448\n",
      "Stochastic Gradient Descent(4647): loss=0.25583884915081767\n",
      "Stochastic Gradient Descent(4648): loss=4.42200354981672\n",
      "Stochastic Gradient Descent(4649): loss=24.911629451979913\n",
      "Stochastic Gradient Descent(4650): loss=0.15139102154023454\n",
      "Stochastic Gradient Descent(4651): loss=5.424107398539639\n",
      "Stochastic Gradient Descent(4652): loss=0.03903982960925641\n",
      "Stochastic Gradient Descent(4653): loss=3.965088344006856\n",
      "Stochastic Gradient Descent(4654): loss=3.2315356992194726\n",
      "Stochastic Gradient Descent(4655): loss=0.0023936762613259136\n",
      "Stochastic Gradient Descent(4656): loss=21.223421672230035\n",
      "Stochastic Gradient Descent(4657): loss=31.572483494043293\n",
      "Stochastic Gradient Descent(4658): loss=88.08591922859094\n",
      "Stochastic Gradient Descent(4659): loss=12.777520847540538\n",
      "Stochastic Gradient Descent(4660): loss=4.922830068726369\n",
      "Stochastic Gradient Descent(4661): loss=13.098111103747803\n",
      "Stochastic Gradient Descent(4662): loss=0.03866206411461349\n",
      "Stochastic Gradient Descent(4663): loss=10.823401575096558\n",
      "Stochastic Gradient Descent(4664): loss=6.378485870300312\n",
      "Stochastic Gradient Descent(4665): loss=6.721090859878783\n",
      "Stochastic Gradient Descent(4666): loss=0.5001028631358918\n",
      "Stochastic Gradient Descent(4667): loss=39.27993602490637\n",
      "Stochastic Gradient Descent(4668): loss=25.314723982562327\n",
      "Stochastic Gradient Descent(4669): loss=0.8896916428166886\n",
      "Stochastic Gradient Descent(4670): loss=0.01261463072981155\n",
      "Stochastic Gradient Descent(4671): loss=20.90675053782015\n",
      "Stochastic Gradient Descent(4672): loss=1.4212381715045177\n",
      "Stochastic Gradient Descent(4673): loss=6.587315788429711\n",
      "Stochastic Gradient Descent(4674): loss=7.340585447972134\n",
      "Stochastic Gradient Descent(4675): loss=0.6270956644896802\n",
      "Stochastic Gradient Descent(4676): loss=14.003169231975003\n",
      "Stochastic Gradient Descent(4677): loss=15.675936296988215\n",
      "Stochastic Gradient Descent(4678): loss=11.18122610228421\n",
      "Stochastic Gradient Descent(4679): loss=0.5965522321991926\n",
      "Stochastic Gradient Descent(4680): loss=0.07578281649612931\n",
      "Stochastic Gradient Descent(4681): loss=17.154896361638283\n",
      "Stochastic Gradient Descent(4682): loss=2.7913303441209303\n",
      "Stochastic Gradient Descent(4683): loss=6.812360741695906\n",
      "Stochastic Gradient Descent(4684): loss=0.7047759788284722\n",
      "Stochastic Gradient Descent(4685): loss=1.7717184697009372\n",
      "Stochastic Gradient Descent(4686): loss=0.919984402238356\n",
      "Stochastic Gradient Descent(4687): loss=11.685945531505254\n",
      "Stochastic Gradient Descent(4688): loss=2.098469096970711\n",
      "Stochastic Gradient Descent(4689): loss=0.06390696689513005\n",
      "Stochastic Gradient Descent(4690): loss=2.4780353163427415\n",
      "Stochastic Gradient Descent(4691): loss=9.513915368636155\n",
      "Stochastic Gradient Descent(4692): loss=9.940815062368578\n",
      "Stochastic Gradient Descent(4693): loss=1.9077571311141293\n",
      "Stochastic Gradient Descent(4694): loss=7.4909747894792345\n",
      "Stochastic Gradient Descent(4695): loss=6.594245079081328\n",
      "Stochastic Gradient Descent(4696): loss=1.125760995223585\n",
      "Stochastic Gradient Descent(4697): loss=4.115920173177532\n",
      "Stochastic Gradient Descent(4698): loss=7.989601845930082\n",
      "Stochastic Gradient Descent(4699): loss=12.248469010756574\n",
      "Stochastic Gradient Descent(4700): loss=20.266897733139082\n",
      "Stochastic Gradient Descent(4701): loss=0.7403634585191669\n",
      "Stochastic Gradient Descent(4702): loss=4.698657493291897\n",
      "Stochastic Gradient Descent(4703): loss=0.2774868910431627\n",
      "Stochastic Gradient Descent(4704): loss=7.34361666673805\n",
      "Stochastic Gradient Descent(4705): loss=1.0531214592515517e-05\n",
      "Stochastic Gradient Descent(4706): loss=23.18418060806481\n",
      "Stochastic Gradient Descent(4707): loss=0.6823437766588937\n",
      "Stochastic Gradient Descent(4708): loss=0.3905869760970432\n",
      "Stochastic Gradient Descent(4709): loss=2.4789545100485197\n",
      "Stochastic Gradient Descent(4710): loss=0.8525926298864803\n",
      "Stochastic Gradient Descent(4711): loss=5.115417651082061\n",
      "Stochastic Gradient Descent(4712): loss=11.250061092850341\n",
      "Stochastic Gradient Descent(4713): loss=0.9233044125577131\n",
      "Stochastic Gradient Descent(4714): loss=20.346501007412964\n",
      "Stochastic Gradient Descent(4715): loss=0.13716492382620504\n",
      "Stochastic Gradient Descent(4716): loss=19.896601146236556\n",
      "Stochastic Gradient Descent(4717): loss=41.49865581684882\n",
      "Stochastic Gradient Descent(4718): loss=1.2635208231670847\n",
      "Stochastic Gradient Descent(4719): loss=14.642610448873679\n",
      "Stochastic Gradient Descent(4720): loss=4.81009003359912\n",
      "Stochastic Gradient Descent(4721): loss=0.12121316176204376\n",
      "Stochastic Gradient Descent(4722): loss=14.681402676497497\n",
      "Stochastic Gradient Descent(4723): loss=13.774249261550999\n",
      "Stochastic Gradient Descent(4724): loss=6.830624504817308\n",
      "Stochastic Gradient Descent(4725): loss=0.6251249251854092\n",
      "Stochastic Gradient Descent(4726): loss=32.4165453211092\n",
      "Stochastic Gradient Descent(4727): loss=0.0511772062524225\n",
      "Stochastic Gradient Descent(4728): loss=18.773403643041277\n",
      "Stochastic Gradient Descent(4729): loss=6.249900422670065\n",
      "Stochastic Gradient Descent(4730): loss=0.2677196206582401\n",
      "Stochastic Gradient Descent(4731): loss=3.2139250254542673\n",
      "Stochastic Gradient Descent(4732): loss=74.99083676758997\n",
      "Stochastic Gradient Descent(4733): loss=0.4978864167239393\n",
      "Stochastic Gradient Descent(4734): loss=0.17935280335070233\n",
      "Stochastic Gradient Descent(4735): loss=15.485958046452968\n",
      "Stochastic Gradient Descent(4736): loss=19.412595005453777\n",
      "Stochastic Gradient Descent(4737): loss=1.173758299803013\n",
      "Stochastic Gradient Descent(4738): loss=6.377245392323052\n",
      "Stochastic Gradient Descent(4739): loss=16.41203824683033\n",
      "Stochastic Gradient Descent(4740): loss=8.78979034195527\n",
      "Stochastic Gradient Descent(4741): loss=18.52803081036485\n",
      "Stochastic Gradient Descent(4742): loss=3.115724695006952\n",
      "Stochastic Gradient Descent(4743): loss=0.0008963215453479296\n",
      "Stochastic Gradient Descent(4744): loss=0.13808709814013254\n",
      "Stochastic Gradient Descent(4745): loss=0.23050203037511016\n",
      "Stochastic Gradient Descent(4746): loss=3.6776335607123123\n",
      "Stochastic Gradient Descent(4747): loss=36.603444747848414\n",
      "Stochastic Gradient Descent(4748): loss=18.318837318555897\n",
      "Stochastic Gradient Descent(4749): loss=0.12303376252710635\n",
      "Stochastic Gradient Descent(4750): loss=0.01710827334183997\n",
      "Stochastic Gradient Descent(4751): loss=26.93953616589559\n",
      "Stochastic Gradient Descent(4752): loss=0.010079762535445947\n",
      "Stochastic Gradient Descent(4753): loss=1.2590955384854539\n",
      "Stochastic Gradient Descent(4754): loss=2.784189679778529\n",
      "Stochastic Gradient Descent(4755): loss=5.429247999618703\n",
      "Stochastic Gradient Descent(4756): loss=2.2034641376605553\n",
      "Stochastic Gradient Descent(4757): loss=5.11980282116767\n",
      "Stochastic Gradient Descent(4758): loss=52.17038039338825\n",
      "Stochastic Gradient Descent(4759): loss=1.3417898731444535\n",
      "Stochastic Gradient Descent(4760): loss=4.475229460315321\n",
      "Stochastic Gradient Descent(4761): loss=16.245847762322708\n",
      "Stochastic Gradient Descent(4762): loss=1.1227889304658945\n",
      "Stochastic Gradient Descent(4763): loss=6.679043894613512\n",
      "Stochastic Gradient Descent(4764): loss=3.7286391627712265\n",
      "Stochastic Gradient Descent(4765): loss=11.416353211691506\n",
      "Stochastic Gradient Descent(4766): loss=0.5182900851333824\n",
      "Stochastic Gradient Descent(4767): loss=0.02897106750751194\n",
      "Stochastic Gradient Descent(4768): loss=0.017687782532521824\n",
      "Stochastic Gradient Descent(4769): loss=16.673715925915424\n",
      "Stochastic Gradient Descent(4770): loss=2.875332595714336\n",
      "Stochastic Gradient Descent(4771): loss=0.5973014014555322\n",
      "Stochastic Gradient Descent(4772): loss=11.64843866265478\n",
      "Stochastic Gradient Descent(4773): loss=4.252685491468875\n",
      "Stochastic Gradient Descent(4774): loss=0.00018772958747999695\n",
      "Stochastic Gradient Descent(4775): loss=6.927858060161224\n",
      "Stochastic Gradient Descent(4776): loss=89.6596885004546\n",
      "Stochastic Gradient Descent(4777): loss=5.579032334986529\n",
      "Stochastic Gradient Descent(4778): loss=0.8875361037274878\n",
      "Stochastic Gradient Descent(4779): loss=0.7026289192799302\n",
      "Stochastic Gradient Descent(4780): loss=14.342100679815657\n",
      "Stochastic Gradient Descent(4781): loss=34.94646950442627\n",
      "Stochastic Gradient Descent(4782): loss=9.093298577916464\n",
      "Stochastic Gradient Descent(4783): loss=0.31253229017027584\n",
      "Stochastic Gradient Descent(4784): loss=0.22978750696525943\n",
      "Stochastic Gradient Descent(4785): loss=20.1493889531497\n",
      "Stochastic Gradient Descent(4786): loss=0.5073982895014251\n",
      "Stochastic Gradient Descent(4787): loss=4.953814195652135\n",
      "Stochastic Gradient Descent(4788): loss=13.598380156351043\n",
      "Stochastic Gradient Descent(4789): loss=1.0956379311210993\n",
      "Stochastic Gradient Descent(4790): loss=0.020312078053611436\n",
      "Stochastic Gradient Descent(4791): loss=4.143454441496029\n",
      "Stochastic Gradient Descent(4792): loss=1.0463479968161198\n",
      "Stochastic Gradient Descent(4793): loss=1.9565945621193843\n",
      "Stochastic Gradient Descent(4794): loss=1.474172642763784\n",
      "Stochastic Gradient Descent(4795): loss=0.2251102138558038\n",
      "Stochastic Gradient Descent(4796): loss=0.4656782404523817\n",
      "Stochastic Gradient Descent(4797): loss=1.7475842722357837\n",
      "Stochastic Gradient Descent(4798): loss=25.06202165345973\n",
      "Stochastic Gradient Descent(4799): loss=0.6389722674335411\n",
      "Stochastic Gradient Descent(4800): loss=16.527580197445563\n",
      "Stochastic Gradient Descent(4801): loss=11.076622438601808\n",
      "Stochastic Gradient Descent(4802): loss=54.87448630703086\n",
      "Stochastic Gradient Descent(4803): loss=2.8134366824635357\n",
      "Stochastic Gradient Descent(4804): loss=1.078447866839272\n",
      "Stochastic Gradient Descent(4805): loss=0.010187145591294708\n",
      "Stochastic Gradient Descent(4806): loss=7.898020425272249\n",
      "Stochastic Gradient Descent(4807): loss=2.060901866622171\n",
      "Stochastic Gradient Descent(4808): loss=7.144100451413522\n",
      "Stochastic Gradient Descent(4809): loss=4.811768465728609\n",
      "Stochastic Gradient Descent(4810): loss=10.226223380072886\n",
      "Stochastic Gradient Descent(4811): loss=1.0482208835317666\n",
      "Stochastic Gradient Descent(4812): loss=6.047584487133064\n",
      "Stochastic Gradient Descent(4813): loss=0.6085264283379044\n",
      "Stochastic Gradient Descent(4814): loss=0.5299824771928736\n",
      "Stochastic Gradient Descent(4815): loss=16.969458409905393\n",
      "Stochastic Gradient Descent(4816): loss=10.144606674801441\n",
      "Stochastic Gradient Descent(4817): loss=0.017444601715825354\n",
      "Stochastic Gradient Descent(4818): loss=5.234940903906253\n",
      "Stochastic Gradient Descent(4819): loss=0.7923666072617587\n",
      "Stochastic Gradient Descent(4820): loss=18.581460501279004\n",
      "Stochastic Gradient Descent(4821): loss=5.292320304171979\n",
      "Stochastic Gradient Descent(4822): loss=23.945045535320705\n",
      "Stochastic Gradient Descent(4823): loss=21.350159273863817\n",
      "Stochastic Gradient Descent(4824): loss=1.2697844989062745\n",
      "Stochastic Gradient Descent(4825): loss=9.588968628914564\n",
      "Stochastic Gradient Descent(4826): loss=3.3643354413962823\n",
      "Stochastic Gradient Descent(4827): loss=4.819134167857152\n",
      "Stochastic Gradient Descent(4828): loss=3.3318218927000403\n",
      "Stochastic Gradient Descent(4829): loss=0.09008822599920834\n",
      "Stochastic Gradient Descent(4830): loss=8.177572782009491\n",
      "Stochastic Gradient Descent(4831): loss=1.9184725038213084\n",
      "Stochastic Gradient Descent(4832): loss=58.270532782048406\n",
      "Stochastic Gradient Descent(4833): loss=0.8212498452501451\n",
      "Stochastic Gradient Descent(4834): loss=50.436529645626145\n",
      "Stochastic Gradient Descent(4835): loss=11.579979071635472\n",
      "Stochastic Gradient Descent(4836): loss=7.337341740432477\n",
      "Stochastic Gradient Descent(4837): loss=7.632596297362082\n",
      "Stochastic Gradient Descent(4838): loss=0.5668111653395246\n",
      "Stochastic Gradient Descent(4839): loss=1.8716391049239811\n",
      "Stochastic Gradient Descent(4840): loss=18.74713799464916\n",
      "Stochastic Gradient Descent(4841): loss=0.055806519355597974\n",
      "Stochastic Gradient Descent(4842): loss=2.7975185291235363\n",
      "Stochastic Gradient Descent(4843): loss=10.412227702981909\n",
      "Stochastic Gradient Descent(4844): loss=9.929563224987959\n",
      "Stochastic Gradient Descent(4845): loss=0.020870952533720913\n",
      "Stochastic Gradient Descent(4846): loss=5.153289293615077\n",
      "Stochastic Gradient Descent(4847): loss=1.0094818662332938\n",
      "Stochastic Gradient Descent(4848): loss=21.00529211810834\n",
      "Stochastic Gradient Descent(4849): loss=0.9215519743014444\n",
      "Stochastic Gradient Descent(4850): loss=0.0030438385083521665\n",
      "Stochastic Gradient Descent(4851): loss=8.945982873643716\n",
      "Stochastic Gradient Descent(4852): loss=1.987168884715799\n",
      "Stochastic Gradient Descent(4853): loss=0.04013106588591979\n",
      "Stochastic Gradient Descent(4854): loss=0.9098022397347904\n",
      "Stochastic Gradient Descent(4855): loss=36.1515848930552\n",
      "Stochastic Gradient Descent(4856): loss=0.07831298907853967\n",
      "Stochastic Gradient Descent(4857): loss=0.044926933229875315\n",
      "Stochastic Gradient Descent(4858): loss=0.00842339546558716\n",
      "Stochastic Gradient Descent(4859): loss=0.7064620827840327\n",
      "Stochastic Gradient Descent(4860): loss=15.686400557275855\n",
      "Stochastic Gradient Descent(4861): loss=0.557364822613106\n",
      "Stochastic Gradient Descent(4862): loss=2.2556069390756557\n",
      "Stochastic Gradient Descent(4863): loss=0.45633638162196005\n",
      "Stochastic Gradient Descent(4864): loss=0.5376484584657485\n",
      "Stochastic Gradient Descent(4865): loss=48.27341483719195\n",
      "Stochastic Gradient Descent(4866): loss=2.9805703579682374\n",
      "Stochastic Gradient Descent(4867): loss=26.032708161264505\n",
      "Stochastic Gradient Descent(4868): loss=18.30703010980248\n",
      "Stochastic Gradient Descent(4869): loss=48.13945055741832\n",
      "Stochastic Gradient Descent(4870): loss=0.035233040882650196\n",
      "Stochastic Gradient Descent(4871): loss=0.12257848610797627\n",
      "Stochastic Gradient Descent(4872): loss=1.6278477137686553\n",
      "Stochastic Gradient Descent(4873): loss=1.1694815409024035\n",
      "Stochastic Gradient Descent(4874): loss=5.184137381068343\n",
      "Stochastic Gradient Descent(4875): loss=3.833228250183369\n",
      "Stochastic Gradient Descent(4876): loss=3.368343872972479\n",
      "Stochastic Gradient Descent(4877): loss=0.318515928204245\n",
      "Stochastic Gradient Descent(4878): loss=4.9758636415131825\n",
      "Stochastic Gradient Descent(4879): loss=11.276089138365734\n",
      "Stochastic Gradient Descent(4880): loss=2.366175404035136\n",
      "Stochastic Gradient Descent(4881): loss=0.011116298045352324\n",
      "Stochastic Gradient Descent(4882): loss=6.406872048645155\n",
      "Stochastic Gradient Descent(4883): loss=0.09861941621990014\n",
      "Stochastic Gradient Descent(4884): loss=4.789636429224871\n",
      "Stochastic Gradient Descent(4885): loss=3.3799218645825153\n",
      "Stochastic Gradient Descent(4886): loss=0.6274793099958763\n",
      "Stochastic Gradient Descent(4887): loss=5.535728924594372\n",
      "Stochastic Gradient Descent(4888): loss=0.3156303745524097\n",
      "Stochastic Gradient Descent(4889): loss=12.894467661936856\n",
      "Stochastic Gradient Descent(4890): loss=0.5293761058569261\n",
      "Stochastic Gradient Descent(4891): loss=0.007240454458628648\n",
      "Stochastic Gradient Descent(4892): loss=59.225307899109694\n",
      "Stochastic Gradient Descent(4893): loss=0.016830424726656835\n",
      "Stochastic Gradient Descent(4894): loss=4.86305230786919\n",
      "Stochastic Gradient Descent(4895): loss=21.72668016476318\n",
      "Stochastic Gradient Descent(4896): loss=0.5944660126989165\n",
      "Stochastic Gradient Descent(4897): loss=30.6444331727505\n",
      "Stochastic Gradient Descent(4898): loss=1.811393681308778\n",
      "Stochastic Gradient Descent(4899): loss=29.242117750341116\n",
      "Stochastic Gradient Descent(4900): loss=7.982679362952679\n",
      "Stochastic Gradient Descent(4901): loss=0.013447939653850943\n",
      "Stochastic Gradient Descent(4902): loss=2.2426404491749587\n",
      "Stochastic Gradient Descent(4903): loss=2.571617680924687\n",
      "Stochastic Gradient Descent(4904): loss=2.500860792213421\n",
      "Stochastic Gradient Descent(4905): loss=0.17179042716668053\n",
      "Stochastic Gradient Descent(4906): loss=8.43972726261551\n",
      "Stochastic Gradient Descent(4907): loss=0.14472911930694007\n",
      "Stochastic Gradient Descent(4908): loss=11.485166236273042\n",
      "Stochastic Gradient Descent(4909): loss=0.35693726358572586\n",
      "Stochastic Gradient Descent(4910): loss=2.0335340019272197\n",
      "Stochastic Gradient Descent(4911): loss=2.9396066512472756\n",
      "Stochastic Gradient Descent(4912): loss=5.249543947690162\n",
      "Stochastic Gradient Descent(4913): loss=2.230336066031118\n",
      "Stochastic Gradient Descent(4914): loss=9.53183703290502\n",
      "Stochastic Gradient Descent(4915): loss=2.050139336914813\n",
      "Stochastic Gradient Descent(4916): loss=9.532844957654271\n",
      "Stochastic Gradient Descent(4917): loss=0.15153333806602318\n",
      "Stochastic Gradient Descent(4918): loss=0.19649027679836964\n",
      "Stochastic Gradient Descent(4919): loss=303.70395456552717\n",
      "Stochastic Gradient Descent(4920): loss=546.3104708020608\n",
      "Stochastic Gradient Descent(4921): loss=1.5231586314596073\n",
      "Stochastic Gradient Descent(4922): loss=11.064284185660947\n",
      "Stochastic Gradient Descent(4923): loss=4.544369327777961\n",
      "Stochastic Gradient Descent(4924): loss=30.05662308170908\n",
      "Stochastic Gradient Descent(4925): loss=53.085889567304044\n",
      "Stochastic Gradient Descent(4926): loss=1.1134205268558606\n",
      "Stochastic Gradient Descent(4927): loss=0.12336081821009003\n",
      "Stochastic Gradient Descent(4928): loss=0.17119505274859884\n",
      "Stochastic Gradient Descent(4929): loss=2.8548058308193385\n",
      "Stochastic Gradient Descent(4930): loss=9.50105385578464\n",
      "Stochastic Gradient Descent(4931): loss=1.6625934812781213\n",
      "Stochastic Gradient Descent(4932): loss=11.942394943970571\n",
      "Stochastic Gradient Descent(4933): loss=7.810522577677024\n",
      "Stochastic Gradient Descent(4934): loss=5.858801864232748\n",
      "Stochastic Gradient Descent(4935): loss=1.3019529183357237\n",
      "Stochastic Gradient Descent(4936): loss=0.5924470468723955\n",
      "Stochastic Gradient Descent(4937): loss=2.2823663370194396\n",
      "Stochastic Gradient Descent(4938): loss=0.22715495527452206\n",
      "Stochastic Gradient Descent(4939): loss=0.15681308092940444\n",
      "Stochastic Gradient Descent(4940): loss=3.756519572587116\n",
      "Stochastic Gradient Descent(4941): loss=24.74011565442093\n",
      "Stochastic Gradient Descent(4942): loss=34.16481893502799\n",
      "Stochastic Gradient Descent(4943): loss=0.017556482429070885\n",
      "Stochastic Gradient Descent(4944): loss=0.554987396851661\n",
      "Stochastic Gradient Descent(4945): loss=9.624568540294167\n",
      "Stochastic Gradient Descent(4946): loss=0.13476479354930285\n",
      "Stochastic Gradient Descent(4947): loss=3.9669953143027774\n",
      "Stochastic Gradient Descent(4948): loss=0.11421218813074768\n",
      "Stochastic Gradient Descent(4949): loss=10.64307367866608\n",
      "Stochastic Gradient Descent(4950): loss=11.87145999562486\n",
      "Stochastic Gradient Descent(4951): loss=54.654032546128654\n",
      "Stochastic Gradient Descent(4952): loss=0.18204574190267267\n",
      "Stochastic Gradient Descent(4953): loss=0.02986431783141898\n",
      "Stochastic Gradient Descent(4954): loss=0.07160917088250231\n",
      "Stochastic Gradient Descent(4955): loss=12.642181135640115\n",
      "Stochastic Gradient Descent(4956): loss=0.01563416751807067\n",
      "Stochastic Gradient Descent(4957): loss=41.99606158232782\n",
      "Stochastic Gradient Descent(4958): loss=4.077999296166249\n",
      "Stochastic Gradient Descent(4959): loss=3.2270175463098\n",
      "Stochastic Gradient Descent(4960): loss=2.644552814874669\n",
      "Stochastic Gradient Descent(4961): loss=0.6580675082650369\n",
      "Stochastic Gradient Descent(4962): loss=5.285434057126407\n",
      "Stochastic Gradient Descent(4963): loss=7.159830664340911\n",
      "Stochastic Gradient Descent(4964): loss=1.177995149007038\n",
      "Stochastic Gradient Descent(4965): loss=2.611570941759515\n",
      "Stochastic Gradient Descent(4966): loss=33.44082975746657\n",
      "Stochastic Gradient Descent(4967): loss=7.203289564839189\n",
      "Stochastic Gradient Descent(4968): loss=10.1019645770839\n",
      "Stochastic Gradient Descent(4969): loss=1.5950661633943721\n",
      "Stochastic Gradient Descent(4970): loss=12.115286681674164\n",
      "Stochastic Gradient Descent(4971): loss=2.0147372910860355\n",
      "Stochastic Gradient Descent(4972): loss=0.4646667304711581\n",
      "Stochastic Gradient Descent(4973): loss=12.722916324877229\n",
      "Stochastic Gradient Descent(4974): loss=0.009594171167017307\n",
      "Stochastic Gradient Descent(4975): loss=9.45832224528661\n",
      "Stochastic Gradient Descent(4976): loss=3.4298167737557477\n",
      "Stochastic Gradient Descent(4977): loss=4.928308750546714\n",
      "Stochastic Gradient Descent(4978): loss=0.018191989426841578\n",
      "Stochastic Gradient Descent(4979): loss=50.65681903084018\n",
      "Stochastic Gradient Descent(4980): loss=2.6270019643637736\n",
      "Stochastic Gradient Descent(4981): loss=8.48702116267672\n",
      "Stochastic Gradient Descent(4982): loss=24.32807017210731\n",
      "Stochastic Gradient Descent(4983): loss=0.6551525254236304\n",
      "Stochastic Gradient Descent(4984): loss=12.627312592415874\n",
      "Stochastic Gradient Descent(4985): loss=0.330306457686674\n",
      "Stochastic Gradient Descent(4986): loss=2.6943853820897825\n",
      "Stochastic Gradient Descent(4987): loss=0.9166049146909007\n",
      "Stochastic Gradient Descent(4988): loss=0.20050546899952046\n",
      "Stochastic Gradient Descent(4989): loss=0.0027276065515837184\n",
      "Stochastic Gradient Descent(4990): loss=5.9612169349771715\n",
      "Stochastic Gradient Descent(4991): loss=1.868175169602372\n",
      "Stochastic Gradient Descent(4992): loss=0.20162127299541002\n",
      "Stochastic Gradient Descent(4993): loss=3.828805774757728\n",
      "Stochastic Gradient Descent(4994): loss=0.8383074030686696\n",
      "Stochastic Gradient Descent(4995): loss=27.09210826226106\n",
      "Stochastic Gradient Descent(4996): loss=2.309859628855899\n",
      "Stochastic Gradient Descent(4997): loss=1.9981017172376025\n",
      "Stochastic Gradient Descent(4998): loss=0.04881392978933675\n",
      "Stochastic Gradient Descent(4999): loss=13.03827691014765\n",
      "Stochastic Gradient Descent(5000): loss=1.1198725736364248\n",
      "Stochastic Gradient Descent(5001): loss=4.665249745747546\n",
      "Stochastic Gradient Descent(5002): loss=15.966408955778176\n",
      "Stochastic Gradient Descent(5003): loss=6.571423358591553\n",
      "Stochastic Gradient Descent(5004): loss=8.903788534712952\n",
      "Stochastic Gradient Descent(5005): loss=0.7882300035063268\n",
      "Stochastic Gradient Descent(5006): loss=12.944149053898098\n",
      "Stochastic Gradient Descent(5007): loss=24.596028846510748\n",
      "Stochastic Gradient Descent(5008): loss=3.4568065315489163\n",
      "Stochastic Gradient Descent(5009): loss=1.5269741584271135\n",
      "Stochastic Gradient Descent(5010): loss=0.0008923896542824847\n",
      "Stochastic Gradient Descent(5011): loss=9.037150588902131\n",
      "Stochastic Gradient Descent(5012): loss=11.003458062753175\n",
      "Stochastic Gradient Descent(5013): loss=2.2195110346887725\n",
      "Stochastic Gradient Descent(5014): loss=0.1148849906124611\n",
      "Stochastic Gradient Descent(5015): loss=0.640462861495272\n",
      "Stochastic Gradient Descent(5016): loss=1.4552303203801602\n",
      "Stochastic Gradient Descent(5017): loss=3.804988721018899\n",
      "Stochastic Gradient Descent(5018): loss=29.610241376863364\n",
      "Stochastic Gradient Descent(5019): loss=9.977164868219049\n",
      "Stochastic Gradient Descent(5020): loss=2.0435934022013926\n",
      "Stochastic Gradient Descent(5021): loss=47.42385752269438\n",
      "Stochastic Gradient Descent(5022): loss=78.70179081731487\n",
      "Stochastic Gradient Descent(5023): loss=6.627700061628239\n",
      "Stochastic Gradient Descent(5024): loss=243.56458230594384\n",
      "Stochastic Gradient Descent(5025): loss=53.14275502293297\n",
      "Stochastic Gradient Descent(5026): loss=33.91131346919379\n",
      "Stochastic Gradient Descent(5027): loss=20.163033978012596\n",
      "Stochastic Gradient Descent(5028): loss=4.603415840515255\n",
      "Stochastic Gradient Descent(5029): loss=0.7006648562637773\n",
      "Stochastic Gradient Descent(5030): loss=90.38744351912528\n",
      "Stochastic Gradient Descent(5031): loss=0.08974673180399073\n",
      "Stochastic Gradient Descent(5032): loss=1.6652163741094061\n",
      "Stochastic Gradient Descent(5033): loss=7.353732822032484\n",
      "Stochastic Gradient Descent(5034): loss=1.9242911822182776\n",
      "Stochastic Gradient Descent(5035): loss=0.09594763958595018\n",
      "Stochastic Gradient Descent(5036): loss=41.81197440848271\n",
      "Stochastic Gradient Descent(5037): loss=3.0064857695574827\n",
      "Stochastic Gradient Descent(5038): loss=8.004069179568754\n",
      "Stochastic Gradient Descent(5039): loss=0.010174191805565787\n",
      "Stochastic Gradient Descent(5040): loss=9.238739486967232\n",
      "Stochastic Gradient Descent(5041): loss=0.19230293565682566\n",
      "Stochastic Gradient Descent(5042): loss=1.6460056954012101\n",
      "Stochastic Gradient Descent(5043): loss=8.419442744665316\n",
      "Stochastic Gradient Descent(5044): loss=0.29824048850190293\n",
      "Stochastic Gradient Descent(5045): loss=2.9836260485139734\n",
      "Stochastic Gradient Descent(5046): loss=0.12457243467848629\n",
      "Stochastic Gradient Descent(5047): loss=0.32066702482117887\n",
      "Stochastic Gradient Descent(5048): loss=3.6060440003362495\n",
      "Stochastic Gradient Descent(5049): loss=1.7496393162537343\n",
      "Stochastic Gradient Descent(5050): loss=5.29226265725636\n",
      "Stochastic Gradient Descent(5051): loss=22.869424363586194\n",
      "Stochastic Gradient Descent(5052): loss=11.796248732863942\n",
      "Stochastic Gradient Descent(5053): loss=3.0229897933217944\n",
      "Stochastic Gradient Descent(5054): loss=10.994257635776801\n",
      "Stochastic Gradient Descent(5055): loss=17.256202728691548\n",
      "Stochastic Gradient Descent(5056): loss=9.570457461953954\n",
      "Stochastic Gradient Descent(5057): loss=6.841412493373435\n",
      "Stochastic Gradient Descent(5058): loss=62.41866783982267\n",
      "Stochastic Gradient Descent(5059): loss=89.87998691629777\n",
      "Stochastic Gradient Descent(5060): loss=9.139286460384689\n",
      "Stochastic Gradient Descent(5061): loss=8.388385820998757\n",
      "Stochastic Gradient Descent(5062): loss=0.9879617180599\n",
      "Stochastic Gradient Descent(5063): loss=3.1615288358640354\n",
      "Stochastic Gradient Descent(5064): loss=26.947368863808524\n",
      "Stochastic Gradient Descent(5065): loss=3.4732816025294238\n",
      "Stochastic Gradient Descent(5066): loss=29.124163627260533\n",
      "Stochastic Gradient Descent(5067): loss=10.340247930290762\n",
      "Stochastic Gradient Descent(5068): loss=0.023480001982958926\n",
      "Stochastic Gradient Descent(5069): loss=24.397810391805997\n",
      "Stochastic Gradient Descent(5070): loss=11.87346183743006\n",
      "Stochastic Gradient Descent(5071): loss=9.905283771772433\n",
      "Stochastic Gradient Descent(5072): loss=1.0508327602985206\n",
      "Stochastic Gradient Descent(5073): loss=1.6829323355889625\n",
      "Stochastic Gradient Descent(5074): loss=0.6634018978593412\n",
      "Stochastic Gradient Descent(5075): loss=16.476731832499897\n",
      "Stochastic Gradient Descent(5076): loss=14.197673503617262\n",
      "Stochastic Gradient Descent(5077): loss=0.11512584572600804\n",
      "Stochastic Gradient Descent(5078): loss=4.973342622195475\n",
      "Stochastic Gradient Descent(5079): loss=105.78057341442404\n",
      "Stochastic Gradient Descent(5080): loss=7.358412345969299\n",
      "Stochastic Gradient Descent(5081): loss=10.338912486374195\n",
      "Stochastic Gradient Descent(5082): loss=2.9698998946113195\n",
      "Stochastic Gradient Descent(5083): loss=8.258752388996967\n",
      "Stochastic Gradient Descent(5084): loss=27.71117047666601\n",
      "Stochastic Gradient Descent(5085): loss=1.8029629160435188\n",
      "Stochastic Gradient Descent(5086): loss=24.151620692246876\n",
      "Stochastic Gradient Descent(5087): loss=10.057494152781013\n",
      "Stochastic Gradient Descent(5088): loss=36.25379690017779\n",
      "Stochastic Gradient Descent(5089): loss=0.22147048840187916\n",
      "Stochastic Gradient Descent(5090): loss=14.454987976884565\n",
      "Stochastic Gradient Descent(5091): loss=21.346772825539077\n",
      "Stochastic Gradient Descent(5092): loss=1.8473724324964305\n",
      "Stochastic Gradient Descent(5093): loss=1.4600577834124755\n",
      "Stochastic Gradient Descent(5094): loss=1.3699694731500045\n",
      "Stochastic Gradient Descent(5095): loss=1.5415441697116536\n",
      "Stochastic Gradient Descent(5096): loss=6.378806602112794\n",
      "Stochastic Gradient Descent(5097): loss=0.2949539384043673\n",
      "Stochastic Gradient Descent(5098): loss=40.76647077126559\n",
      "Stochastic Gradient Descent(5099): loss=0.028058534772155568\n",
      "Stochastic Gradient Descent(5100): loss=2.3125209219985763\n",
      "Stochastic Gradient Descent(5101): loss=12.026942295101167\n",
      "Stochastic Gradient Descent(5102): loss=1.6685033668492506\n",
      "Stochastic Gradient Descent(5103): loss=10.06367240066158\n",
      "Stochastic Gradient Descent(5104): loss=0.9674149908693775\n",
      "Stochastic Gradient Descent(5105): loss=25.5418085868046\n",
      "Stochastic Gradient Descent(5106): loss=0.12353785623405297\n",
      "Stochastic Gradient Descent(5107): loss=7.737244479967748\n",
      "Stochastic Gradient Descent(5108): loss=19.17706818677143\n",
      "Stochastic Gradient Descent(5109): loss=6.174796048354842\n",
      "Stochastic Gradient Descent(5110): loss=0.14514826910425416\n",
      "Stochastic Gradient Descent(5111): loss=9.346492971824619\n",
      "Stochastic Gradient Descent(5112): loss=0.7026728194704452\n",
      "Stochastic Gradient Descent(5113): loss=13.004330070443503\n",
      "Stochastic Gradient Descent(5114): loss=2.007023482491381\n",
      "Stochastic Gradient Descent(5115): loss=29.193708094150963\n",
      "Stochastic Gradient Descent(5116): loss=0.0948767902150217\n",
      "Stochastic Gradient Descent(5117): loss=2.573045813876314\n",
      "Stochastic Gradient Descent(5118): loss=3.245163783297995\n",
      "Stochastic Gradient Descent(5119): loss=19.561367090689394\n",
      "Stochastic Gradient Descent(5120): loss=6.650209927589854\n",
      "Stochastic Gradient Descent(5121): loss=1.2656549226214675\n",
      "Stochastic Gradient Descent(5122): loss=11.73736930116645\n",
      "Stochastic Gradient Descent(5123): loss=20.722972841630526\n",
      "Stochastic Gradient Descent(5124): loss=0.05693032037379911\n",
      "Stochastic Gradient Descent(5125): loss=2.127618949258657\n",
      "Stochastic Gradient Descent(5126): loss=0.09426693549522368\n",
      "Stochastic Gradient Descent(5127): loss=3.271562266317462\n",
      "Stochastic Gradient Descent(5128): loss=4.2875636297765904\n",
      "Stochastic Gradient Descent(5129): loss=10.421678449422787\n",
      "Stochastic Gradient Descent(5130): loss=0.9299898156358972\n",
      "Stochastic Gradient Descent(5131): loss=0.0012491050956362944\n",
      "Stochastic Gradient Descent(5132): loss=14.114381259893252\n",
      "Stochastic Gradient Descent(5133): loss=0.7545845949306604\n",
      "Stochastic Gradient Descent(5134): loss=14.719536928978819\n",
      "Stochastic Gradient Descent(5135): loss=0.6828826617557893\n",
      "Stochastic Gradient Descent(5136): loss=3.9895097207921237\n",
      "Stochastic Gradient Descent(5137): loss=23.421375940536617\n",
      "Stochastic Gradient Descent(5138): loss=15.759311304218416\n",
      "Stochastic Gradient Descent(5139): loss=44.416580797417225\n",
      "Stochastic Gradient Descent(5140): loss=16.553606245108273\n",
      "Stochastic Gradient Descent(5141): loss=0.07493109503888658\n",
      "Stochastic Gradient Descent(5142): loss=1.9823166168353998\n",
      "Stochastic Gradient Descent(5143): loss=24.36006069830602\n",
      "Stochastic Gradient Descent(5144): loss=4.9969407204294\n",
      "Stochastic Gradient Descent(5145): loss=3.462413368271194\n",
      "Stochastic Gradient Descent(5146): loss=23.964517015392065\n",
      "Stochastic Gradient Descent(5147): loss=28.102189434399303\n",
      "Stochastic Gradient Descent(5148): loss=3.63790251859261\n",
      "Stochastic Gradient Descent(5149): loss=0.23802540974600855\n",
      "Stochastic Gradient Descent(5150): loss=9.394129879231125\n",
      "Stochastic Gradient Descent(5151): loss=19.07821547687105\n",
      "Stochastic Gradient Descent(5152): loss=8.992338906073483\n",
      "Stochastic Gradient Descent(5153): loss=0.22592314985754627\n",
      "Stochastic Gradient Descent(5154): loss=9.192633755093656\n",
      "Stochastic Gradient Descent(5155): loss=0.019129113835565172\n",
      "Stochastic Gradient Descent(5156): loss=1.5845456671719416\n",
      "Stochastic Gradient Descent(5157): loss=12.663178390555725\n",
      "Stochastic Gradient Descent(5158): loss=2.690543630905155\n",
      "Stochastic Gradient Descent(5159): loss=0.9073439282007867\n",
      "Stochastic Gradient Descent(5160): loss=0.03704449128980216\n",
      "Stochastic Gradient Descent(5161): loss=0.44567298740620015\n",
      "Stochastic Gradient Descent(5162): loss=1.1621931860364532\n",
      "Stochastic Gradient Descent(5163): loss=2.585322732000002\n",
      "Stochastic Gradient Descent(5164): loss=0.7262267570577967\n",
      "Stochastic Gradient Descent(5165): loss=4.373213717711161\n",
      "Stochastic Gradient Descent(5166): loss=0.6276285694062688\n",
      "Stochastic Gradient Descent(5167): loss=1.860461984714892\n",
      "Stochastic Gradient Descent(5168): loss=0.05301522063456076\n",
      "Stochastic Gradient Descent(5169): loss=2.7028334086138828\n",
      "Stochastic Gradient Descent(5170): loss=0.06130704878286686\n",
      "Stochastic Gradient Descent(5171): loss=0.013297299236096224\n",
      "Stochastic Gradient Descent(5172): loss=8.090580104801148\n",
      "Stochastic Gradient Descent(5173): loss=11.395458289433007\n",
      "Stochastic Gradient Descent(5174): loss=4.021152394895665\n",
      "Stochastic Gradient Descent(5175): loss=2.8637386343440263\n",
      "Stochastic Gradient Descent(5176): loss=2.169910629838493\n",
      "Stochastic Gradient Descent(5177): loss=11.312531824817272\n",
      "Stochastic Gradient Descent(5178): loss=0.07389061115901925\n",
      "Stochastic Gradient Descent(5179): loss=0.023049022704257627\n",
      "Stochastic Gradient Descent(5180): loss=15.866871521779865\n",
      "Stochastic Gradient Descent(5181): loss=2.959259521473761\n",
      "Stochastic Gradient Descent(5182): loss=1.2397993372233496\n",
      "Stochastic Gradient Descent(5183): loss=0.6321186985943973\n",
      "Stochastic Gradient Descent(5184): loss=1.6070259558847337\n",
      "Stochastic Gradient Descent(5185): loss=0.15736512490933965\n",
      "Stochastic Gradient Descent(5186): loss=42.27574749620951\n",
      "Stochastic Gradient Descent(5187): loss=6.393997915608115\n",
      "Stochastic Gradient Descent(5188): loss=0.5137557283181985\n",
      "Stochastic Gradient Descent(5189): loss=0.9427871930200172\n",
      "Stochastic Gradient Descent(5190): loss=14.229582229475152\n",
      "Stochastic Gradient Descent(5191): loss=3.500381679635034\n",
      "Stochastic Gradient Descent(5192): loss=14.84424940112939\n",
      "Stochastic Gradient Descent(5193): loss=4.330801586055197\n",
      "Stochastic Gradient Descent(5194): loss=6.838859664856729\n",
      "Stochastic Gradient Descent(5195): loss=15.197804496004451\n",
      "Stochastic Gradient Descent(5196): loss=1.0419382464503812\n",
      "Stochastic Gradient Descent(5197): loss=24.94794110557356\n",
      "Stochastic Gradient Descent(5198): loss=18.003380841381293\n",
      "Stochastic Gradient Descent(5199): loss=5.437241091905907\n",
      "Stochastic Gradient Descent(5200): loss=0.37297968337769916\n",
      "Stochastic Gradient Descent(5201): loss=16.96783419974592\n",
      "Stochastic Gradient Descent(5202): loss=0.07599892484291144\n",
      "Stochastic Gradient Descent(5203): loss=4.692775734512938\n",
      "Stochastic Gradient Descent(5204): loss=5.348001942582403\n",
      "Stochastic Gradient Descent(5205): loss=0.09269458939285649\n",
      "Stochastic Gradient Descent(5206): loss=2.2003957084380046\n",
      "Stochastic Gradient Descent(5207): loss=3.0540988060297347\n",
      "Stochastic Gradient Descent(5208): loss=3.627363652253683\n",
      "Stochastic Gradient Descent(5209): loss=63.979512080672514\n",
      "Stochastic Gradient Descent(5210): loss=0.19486961136545713\n",
      "Stochastic Gradient Descent(5211): loss=0.5344801835596538\n",
      "Stochastic Gradient Descent(5212): loss=21.81694746398863\n",
      "Stochastic Gradient Descent(5213): loss=0.2492477594968192\n",
      "Stochastic Gradient Descent(5214): loss=14.74185848258314\n",
      "Stochastic Gradient Descent(5215): loss=0.9296387687168415\n",
      "Stochastic Gradient Descent(5216): loss=27.60314142675835\n",
      "Stochastic Gradient Descent(5217): loss=11.317250787499368\n",
      "Stochastic Gradient Descent(5218): loss=38.26833846201024\n",
      "Stochastic Gradient Descent(5219): loss=3.9200643595555977\n",
      "Stochastic Gradient Descent(5220): loss=0.0019433867098387256\n",
      "Stochastic Gradient Descent(5221): loss=23.213441844491175\n",
      "Stochastic Gradient Descent(5222): loss=0.0003599439111673341\n",
      "Stochastic Gradient Descent(5223): loss=20.534472588486867\n",
      "Stochastic Gradient Descent(5224): loss=1.6130661718587644\n",
      "Stochastic Gradient Descent(5225): loss=0.6978729239252188\n",
      "Stochastic Gradient Descent(5226): loss=2.3405455065612837\n",
      "Stochastic Gradient Descent(5227): loss=39.58471577543367\n",
      "Stochastic Gradient Descent(5228): loss=1.5787656767059661\n",
      "Stochastic Gradient Descent(5229): loss=5.852468180388578\n",
      "Stochastic Gradient Descent(5230): loss=33.44604054141462\n",
      "Stochastic Gradient Descent(5231): loss=22.15740265425379\n",
      "Stochastic Gradient Descent(5232): loss=0.6331277151375658\n",
      "Stochastic Gradient Descent(5233): loss=10.945635969695566\n",
      "Stochastic Gradient Descent(5234): loss=1.452356601208758\n",
      "Stochastic Gradient Descent(5235): loss=15.41795751432407\n",
      "Stochastic Gradient Descent(5236): loss=2.2161113007788305\n",
      "Stochastic Gradient Descent(5237): loss=20.03385405065805\n",
      "Stochastic Gradient Descent(5238): loss=1.1786874200002242\n",
      "Stochastic Gradient Descent(5239): loss=2.705776437203123\n",
      "Stochastic Gradient Descent(5240): loss=7.801635688086106\n",
      "Stochastic Gradient Descent(5241): loss=11.786352395263084\n",
      "Stochastic Gradient Descent(5242): loss=6.934598742705894\n",
      "Stochastic Gradient Descent(5243): loss=25.629720636654337\n",
      "Stochastic Gradient Descent(5244): loss=26.53058100497959\n",
      "Stochastic Gradient Descent(5245): loss=0.31320851930866495\n",
      "Stochastic Gradient Descent(5246): loss=12.629249511701508\n",
      "Stochastic Gradient Descent(5247): loss=0.4252278262673207\n",
      "Stochastic Gradient Descent(5248): loss=3.4804675917927925\n",
      "Stochastic Gradient Descent(5249): loss=3.8286359262870198\n",
      "Stochastic Gradient Descent(5250): loss=5.656152568732795\n",
      "Stochastic Gradient Descent(5251): loss=0.018833316875636504\n",
      "Stochastic Gradient Descent(5252): loss=6.638563878564838\n",
      "Stochastic Gradient Descent(5253): loss=0.47262430596825655\n",
      "Stochastic Gradient Descent(5254): loss=0.9960394245785691\n",
      "Stochastic Gradient Descent(5255): loss=1.6513205003991904\n",
      "Stochastic Gradient Descent(5256): loss=0.9389531368409725\n",
      "Stochastic Gradient Descent(5257): loss=0.37191535505460027\n",
      "Stochastic Gradient Descent(5258): loss=2.0212474902623305\n",
      "Stochastic Gradient Descent(5259): loss=15.576520528042863\n",
      "Stochastic Gradient Descent(5260): loss=1.4553197650697993\n",
      "Stochastic Gradient Descent(5261): loss=28.91254019656021\n",
      "Stochastic Gradient Descent(5262): loss=6.90664757914153\n",
      "Stochastic Gradient Descent(5263): loss=0.008619967109955613\n",
      "Stochastic Gradient Descent(5264): loss=3.8739229284881684\n",
      "Stochastic Gradient Descent(5265): loss=11.992599010969316\n",
      "Stochastic Gradient Descent(5266): loss=15.532873739630949\n",
      "Stochastic Gradient Descent(5267): loss=1.2854756597359003\n",
      "Stochastic Gradient Descent(5268): loss=22.70192492435655\n",
      "Stochastic Gradient Descent(5269): loss=0.16708049012628484\n",
      "Stochastic Gradient Descent(5270): loss=5.615772048235107\n",
      "Stochastic Gradient Descent(5271): loss=3.2153462973494897\n",
      "Stochastic Gradient Descent(5272): loss=0.22228051830430556\n",
      "Stochastic Gradient Descent(5273): loss=0.7847780089660326\n",
      "Stochastic Gradient Descent(5274): loss=0.13045920939245068\n",
      "Stochastic Gradient Descent(5275): loss=10.636858572982053\n",
      "Stochastic Gradient Descent(5276): loss=3.0443743938824617\n",
      "Stochastic Gradient Descent(5277): loss=15.365103911894318\n",
      "Stochastic Gradient Descent(5278): loss=2.763443896806444\n",
      "Stochastic Gradient Descent(5279): loss=1.7907613003680785\n",
      "Stochastic Gradient Descent(5280): loss=34.316763747370736\n",
      "Stochastic Gradient Descent(5281): loss=8.89180878548744\n",
      "Stochastic Gradient Descent(5282): loss=1.2313250754897644\n",
      "Stochastic Gradient Descent(5283): loss=16.22080198727312\n",
      "Stochastic Gradient Descent(5284): loss=0.00032139855336068933\n",
      "Stochastic Gradient Descent(5285): loss=0.41268503167052323\n",
      "Stochastic Gradient Descent(5286): loss=0.01898198288558707\n",
      "Stochastic Gradient Descent(5287): loss=18.207983982296767\n",
      "Stochastic Gradient Descent(5288): loss=7.605973387822349\n",
      "Stochastic Gradient Descent(5289): loss=19.570719844616963\n",
      "Stochastic Gradient Descent(5290): loss=0.7220451684240621\n",
      "Stochastic Gradient Descent(5291): loss=22.30798529540934\n",
      "Stochastic Gradient Descent(5292): loss=0.12992194267027207\n",
      "Stochastic Gradient Descent(5293): loss=4.0326210237360955\n",
      "Stochastic Gradient Descent(5294): loss=16.430731019072002\n",
      "Stochastic Gradient Descent(5295): loss=12.156041056371002\n",
      "Stochastic Gradient Descent(5296): loss=10.693741060820168\n",
      "Stochastic Gradient Descent(5297): loss=0.7915187361614462\n",
      "Stochastic Gradient Descent(5298): loss=8.84446928992847\n",
      "Stochastic Gradient Descent(5299): loss=2.998710247732153\n",
      "Stochastic Gradient Descent(5300): loss=6.314862171302116\n",
      "Stochastic Gradient Descent(5301): loss=4.600736206164684\n",
      "Stochastic Gradient Descent(5302): loss=0.03610529862465906\n",
      "Stochastic Gradient Descent(5303): loss=11.558386769538487\n",
      "Stochastic Gradient Descent(5304): loss=2.881584049844225\n",
      "Stochastic Gradient Descent(5305): loss=16.914849613499154\n",
      "Stochastic Gradient Descent(5306): loss=13.472342351349527\n",
      "Stochastic Gradient Descent(5307): loss=0.07015481151250934\n",
      "Stochastic Gradient Descent(5308): loss=0.9509903872356404\n",
      "Stochastic Gradient Descent(5309): loss=7.1435325535568\n",
      "Stochastic Gradient Descent(5310): loss=3.290839347433793\n",
      "Stochastic Gradient Descent(5311): loss=0.18333831538155204\n",
      "Stochastic Gradient Descent(5312): loss=0.47642408203655184\n",
      "Stochastic Gradient Descent(5313): loss=16.818787488098643\n",
      "Stochastic Gradient Descent(5314): loss=7.2725143860447465\n",
      "Stochastic Gradient Descent(5315): loss=6.678092545685755\n",
      "Stochastic Gradient Descent(5316): loss=5.0436275787231795\n",
      "Stochastic Gradient Descent(5317): loss=17.22208124290947\n",
      "Stochastic Gradient Descent(5318): loss=7.995460580110092\n",
      "Stochastic Gradient Descent(5319): loss=0.18011048499377902\n",
      "Stochastic Gradient Descent(5320): loss=26.575411180541533\n",
      "Stochastic Gradient Descent(5321): loss=16.101303298827535\n",
      "Stochastic Gradient Descent(5322): loss=6.04329251822881\n",
      "Stochastic Gradient Descent(5323): loss=4.482556639211854\n",
      "Stochastic Gradient Descent(5324): loss=5.533247982198154\n",
      "Stochastic Gradient Descent(5325): loss=1.5694755861740883\n",
      "Stochastic Gradient Descent(5326): loss=3.4573241370059047\n",
      "Stochastic Gradient Descent(5327): loss=12.458745631527584\n",
      "Stochastic Gradient Descent(5328): loss=0.6729568148375097\n",
      "Stochastic Gradient Descent(5329): loss=0.08616195100711263\n",
      "Stochastic Gradient Descent(5330): loss=0.21395588855593445\n",
      "Stochastic Gradient Descent(5331): loss=1.1715113503452672\n",
      "Stochastic Gradient Descent(5332): loss=3.0334455248019814\n",
      "Stochastic Gradient Descent(5333): loss=4.35077100627156\n",
      "Stochastic Gradient Descent(5334): loss=13.110861679007725\n",
      "Stochastic Gradient Descent(5335): loss=0.5176772070646636\n",
      "Stochastic Gradient Descent(5336): loss=4.511606726180762\n",
      "Stochastic Gradient Descent(5337): loss=29.96403579047643\n",
      "Stochastic Gradient Descent(5338): loss=4.612720932411277\n",
      "Stochastic Gradient Descent(5339): loss=6.874204718307017\n",
      "Stochastic Gradient Descent(5340): loss=15.164585734030755\n",
      "Stochastic Gradient Descent(5341): loss=6.544121285978115\n",
      "Stochastic Gradient Descent(5342): loss=0.04408277896365625\n",
      "Stochastic Gradient Descent(5343): loss=0.9160782838737881\n",
      "Stochastic Gradient Descent(5344): loss=5.804836072746533\n",
      "Stochastic Gradient Descent(5345): loss=3.3223690951134937\n",
      "Stochastic Gradient Descent(5346): loss=0.05135008884547854\n",
      "Stochastic Gradient Descent(5347): loss=19.820012795330488\n",
      "Stochastic Gradient Descent(5348): loss=0.3181408988234381\n",
      "Stochastic Gradient Descent(5349): loss=18.064068027648904\n",
      "Stochastic Gradient Descent(5350): loss=1.7464641938813856\n",
      "Stochastic Gradient Descent(5351): loss=0.44217115362119863\n",
      "Stochastic Gradient Descent(5352): loss=4.05124195354852\n",
      "Stochastic Gradient Descent(5353): loss=1.075848281904007\n",
      "Stochastic Gradient Descent(5354): loss=0.766679951462204\n",
      "Stochastic Gradient Descent(5355): loss=4.077514744395068\n",
      "Stochastic Gradient Descent(5356): loss=9.795780466676577\n",
      "Stochastic Gradient Descent(5357): loss=0.0029038103594056382\n",
      "Stochastic Gradient Descent(5358): loss=0.2243311750506891\n",
      "Stochastic Gradient Descent(5359): loss=2.9522317221240426\n",
      "Stochastic Gradient Descent(5360): loss=4.158896827766273\n",
      "Stochastic Gradient Descent(5361): loss=0.19734987823062924\n",
      "Stochastic Gradient Descent(5362): loss=11.452414364199397\n",
      "Stochastic Gradient Descent(5363): loss=5.608358365561486\n",
      "Stochastic Gradient Descent(5364): loss=0.08850623111147414\n",
      "Stochastic Gradient Descent(5365): loss=14.356086103657175\n",
      "Stochastic Gradient Descent(5366): loss=0.9874360159638312\n",
      "Stochastic Gradient Descent(5367): loss=0.34930384034598344\n",
      "Stochastic Gradient Descent(5368): loss=5.938871509527635\n",
      "Stochastic Gradient Descent(5369): loss=87.82506233387276\n",
      "Stochastic Gradient Descent(5370): loss=2.3479508914296905\n",
      "Stochastic Gradient Descent(5371): loss=0.8009435062084804\n",
      "Stochastic Gradient Descent(5372): loss=32.85415918004411\n",
      "Stochastic Gradient Descent(5373): loss=62.426144594487106\n",
      "Stochastic Gradient Descent(5374): loss=4.399081960105369\n",
      "Stochastic Gradient Descent(5375): loss=9.8666029201332\n",
      "Stochastic Gradient Descent(5376): loss=10.4818327148505\n",
      "Stochastic Gradient Descent(5377): loss=1.8903853552094751\n",
      "Stochastic Gradient Descent(5378): loss=0.2713635052318563\n",
      "Stochastic Gradient Descent(5379): loss=0.03130936399353126\n",
      "Stochastic Gradient Descent(5380): loss=4.424486790413671\n",
      "Stochastic Gradient Descent(5381): loss=1.089494269671531\n",
      "Stochastic Gradient Descent(5382): loss=0.00024011489041348682\n",
      "Stochastic Gradient Descent(5383): loss=1.1151237321927225\n",
      "Stochastic Gradient Descent(5384): loss=8.707657343285332\n",
      "Stochastic Gradient Descent(5385): loss=0.8587211220861293\n",
      "Stochastic Gradient Descent(5386): loss=1.1282613201047484\n",
      "Stochastic Gradient Descent(5387): loss=38.320075572483454\n",
      "Stochastic Gradient Descent(5388): loss=2.2840497688759904\n",
      "Stochastic Gradient Descent(5389): loss=2.7811634528204947e-06\n",
      "Stochastic Gradient Descent(5390): loss=12.262296760639618\n",
      "Stochastic Gradient Descent(5391): loss=22.81610170779131\n",
      "Stochastic Gradient Descent(5392): loss=0.047347111151821566\n",
      "Stochastic Gradient Descent(5393): loss=0.2687161903566407\n",
      "Stochastic Gradient Descent(5394): loss=2.9716809891143194\n",
      "Stochastic Gradient Descent(5395): loss=0.12636611083693775\n",
      "Stochastic Gradient Descent(5396): loss=3.942063264150151\n",
      "Stochastic Gradient Descent(5397): loss=0.26528473867757224\n",
      "Stochastic Gradient Descent(5398): loss=1.5357948037929405\n",
      "Stochastic Gradient Descent(5399): loss=0.4184319195917039\n",
      "Stochastic Gradient Descent(5400): loss=0.15710523673722152\n",
      "Stochastic Gradient Descent(5401): loss=15.962597913353614\n",
      "Stochastic Gradient Descent(5402): loss=0.008589970297075866\n",
      "Stochastic Gradient Descent(5403): loss=15.477578276097839\n",
      "Stochastic Gradient Descent(5404): loss=21.275720195649217\n",
      "Stochastic Gradient Descent(5405): loss=24.120666532462824\n",
      "Stochastic Gradient Descent(5406): loss=22.819136396791308\n",
      "Stochastic Gradient Descent(5407): loss=6.673707716167191\n",
      "Stochastic Gradient Descent(5408): loss=7.0271637879241045\n",
      "Stochastic Gradient Descent(5409): loss=0.13682045229853482\n",
      "Stochastic Gradient Descent(5410): loss=4.090897093241098\n",
      "Stochastic Gradient Descent(5411): loss=1.3798550962923866\n",
      "Stochastic Gradient Descent(5412): loss=1.7597842220745525\n",
      "Stochastic Gradient Descent(5413): loss=12.281427370646764\n",
      "Stochastic Gradient Descent(5414): loss=9.519817283559986\n",
      "Stochastic Gradient Descent(5415): loss=4.3170599215304915\n",
      "Stochastic Gradient Descent(5416): loss=4.061092301937082\n",
      "Stochastic Gradient Descent(5417): loss=10.116806821086055\n",
      "Stochastic Gradient Descent(5418): loss=0.4588682412658738\n",
      "Stochastic Gradient Descent(5419): loss=2.011558814922338\n",
      "Stochastic Gradient Descent(5420): loss=12.735526993361137\n",
      "Stochastic Gradient Descent(5421): loss=14.20589674131564\n",
      "Stochastic Gradient Descent(5422): loss=3.9100127166611918\n",
      "Stochastic Gradient Descent(5423): loss=41.603446599967384\n",
      "Stochastic Gradient Descent(5424): loss=2.2183523939225545\n",
      "Stochastic Gradient Descent(5425): loss=3.6029303988689403\n",
      "Stochastic Gradient Descent(5426): loss=2.351819188056634\n",
      "Stochastic Gradient Descent(5427): loss=0.1340593386211904\n",
      "Stochastic Gradient Descent(5428): loss=6.186394780487035\n",
      "Stochastic Gradient Descent(5429): loss=6.942419734872084\n",
      "Stochastic Gradient Descent(5430): loss=7.144302687725728\n",
      "Stochastic Gradient Descent(5431): loss=0.004446712715609853\n",
      "Stochastic Gradient Descent(5432): loss=1.1995402612551986\n",
      "Stochastic Gradient Descent(5433): loss=51.483290947715254\n",
      "Stochastic Gradient Descent(5434): loss=0.027568766202742245\n",
      "Stochastic Gradient Descent(5435): loss=0.9611973980203306\n",
      "Stochastic Gradient Descent(5436): loss=4.967750502495931\n",
      "Stochastic Gradient Descent(5437): loss=0.8584051566675251\n",
      "Stochastic Gradient Descent(5438): loss=4.375252090006117\n",
      "Stochastic Gradient Descent(5439): loss=7.268828266087417\n",
      "Stochastic Gradient Descent(5440): loss=7.381708683848008\n",
      "Stochastic Gradient Descent(5441): loss=4.303983679975329\n",
      "Stochastic Gradient Descent(5442): loss=0.515466906834501\n",
      "Stochastic Gradient Descent(5443): loss=0.07131361275294677\n",
      "Stochastic Gradient Descent(5444): loss=11.947607882839373\n",
      "Stochastic Gradient Descent(5445): loss=0.04697707412589354\n",
      "Stochastic Gradient Descent(5446): loss=0.3147455369866192\n",
      "Stochastic Gradient Descent(5447): loss=1.66227323184616\n",
      "Stochastic Gradient Descent(5448): loss=10.641820418323402\n",
      "Stochastic Gradient Descent(5449): loss=0.11109528387100354\n",
      "Stochastic Gradient Descent(5450): loss=3.1381341359480204\n",
      "Stochastic Gradient Descent(5451): loss=8.347412296236175\n",
      "Stochastic Gradient Descent(5452): loss=1.1512589718259318\n",
      "Stochastic Gradient Descent(5453): loss=20.037248655152663\n",
      "Stochastic Gradient Descent(5454): loss=0.24903544881374354\n",
      "Stochastic Gradient Descent(5455): loss=0.9111001306175495\n",
      "Stochastic Gradient Descent(5456): loss=7.654956780623708\n",
      "Stochastic Gradient Descent(5457): loss=6.961097208775945\n",
      "Stochastic Gradient Descent(5458): loss=0.36612759599972744\n",
      "Stochastic Gradient Descent(5459): loss=1.3138289084757824\n",
      "Stochastic Gradient Descent(5460): loss=2.9566120162698892\n",
      "Stochastic Gradient Descent(5461): loss=23.00069749118108\n",
      "Stochastic Gradient Descent(5462): loss=0.49297979020778426\n",
      "Stochastic Gradient Descent(5463): loss=1.9382032990740314\n",
      "Stochastic Gradient Descent(5464): loss=0.46720956097064675\n",
      "Stochastic Gradient Descent(5465): loss=16.281138415398864\n",
      "Stochastic Gradient Descent(5466): loss=0.12476831177504448\n",
      "Stochastic Gradient Descent(5467): loss=0.8086054003554346\n",
      "Stochastic Gradient Descent(5468): loss=2.543957361017567\n",
      "Stochastic Gradient Descent(5469): loss=1.897017002723657\n",
      "Stochastic Gradient Descent(5470): loss=15.98573238061224\n",
      "Stochastic Gradient Descent(5471): loss=0.8357262039812027\n",
      "Stochastic Gradient Descent(5472): loss=4.012009650763083\n",
      "Stochastic Gradient Descent(5473): loss=2.0439557529979586\n",
      "Stochastic Gradient Descent(5474): loss=3.0833810435550024\n",
      "Stochastic Gradient Descent(5475): loss=9.878391077221899\n",
      "Stochastic Gradient Descent(5476): loss=8.455627151079511\n",
      "Stochastic Gradient Descent(5477): loss=9.986614140810254\n",
      "Stochastic Gradient Descent(5478): loss=0.3276811508709042\n",
      "Stochastic Gradient Descent(5479): loss=15.248005489638654\n",
      "Stochastic Gradient Descent(5480): loss=0.6619103031958735\n",
      "Stochastic Gradient Descent(5481): loss=0.5769911516814931\n",
      "Stochastic Gradient Descent(5482): loss=0.9341444518930382\n",
      "Stochastic Gradient Descent(5483): loss=6.24738726873192\n",
      "Stochastic Gradient Descent(5484): loss=22.100580061705926\n",
      "Stochastic Gradient Descent(5485): loss=5.752342644861401\n",
      "Stochastic Gradient Descent(5486): loss=51.037791745702464\n",
      "Stochastic Gradient Descent(5487): loss=4.786509583855295\n",
      "Stochastic Gradient Descent(5488): loss=9.252224600001435\n",
      "Stochastic Gradient Descent(5489): loss=27.934596834353034\n",
      "Stochastic Gradient Descent(5490): loss=3.613287331030666\n",
      "Stochastic Gradient Descent(5491): loss=2.08302988502236\n",
      "Stochastic Gradient Descent(5492): loss=0.039689870457106126\n",
      "Stochastic Gradient Descent(5493): loss=1.03144898926247\n",
      "Stochastic Gradient Descent(5494): loss=0.3045876212755167\n",
      "Stochastic Gradient Descent(5495): loss=13.306680825919525\n",
      "Stochastic Gradient Descent(5496): loss=0.2752809901473816\n",
      "Stochastic Gradient Descent(5497): loss=1.319416127793822\n",
      "Stochastic Gradient Descent(5498): loss=0.7035911728942978\n",
      "Stochastic Gradient Descent(5499): loss=11.142371037985061\n",
      "Stochastic Gradient Descent(5500): loss=1.868835105912798\n",
      "Stochastic Gradient Descent(5501): loss=3.79855732912861\n",
      "Stochastic Gradient Descent(5502): loss=27.512111022593803\n",
      "Stochastic Gradient Descent(5503): loss=13.313964600535353\n",
      "Stochastic Gradient Descent(5504): loss=8.522381397835892\n",
      "Stochastic Gradient Descent(5505): loss=0.8947811020604374\n",
      "Stochastic Gradient Descent(5506): loss=0.20910378861309367\n",
      "Stochastic Gradient Descent(5507): loss=1.086145086904458\n",
      "Stochastic Gradient Descent(5508): loss=4.846794867159003\n",
      "Stochastic Gradient Descent(5509): loss=0.6323278426173715\n",
      "Stochastic Gradient Descent(5510): loss=0.004035927831041342\n",
      "Stochastic Gradient Descent(5511): loss=9.874178739095244\n",
      "Stochastic Gradient Descent(5512): loss=18.475869596952382\n",
      "Stochastic Gradient Descent(5513): loss=8.77055224462453\n",
      "Stochastic Gradient Descent(5514): loss=5.428601363638539\n",
      "Stochastic Gradient Descent(5515): loss=0.005170368258378711\n",
      "Stochastic Gradient Descent(5516): loss=0.33749378658776563\n",
      "Stochastic Gradient Descent(5517): loss=13.454254835990652\n",
      "Stochastic Gradient Descent(5518): loss=74.49528017400077\n",
      "Stochastic Gradient Descent(5519): loss=4.04395879456641\n",
      "Stochastic Gradient Descent(5520): loss=6.864713065738396\n",
      "Stochastic Gradient Descent(5521): loss=18.440105283644947\n",
      "Stochastic Gradient Descent(5522): loss=0.7667070662463821\n",
      "Stochastic Gradient Descent(5523): loss=5.996963882895023\n",
      "Stochastic Gradient Descent(5524): loss=41.18856615199358\n",
      "Stochastic Gradient Descent(5525): loss=2.3034691053658425\n",
      "Stochastic Gradient Descent(5526): loss=1.1071165032688128\n",
      "Stochastic Gradient Descent(5527): loss=50.764553803926546\n",
      "Stochastic Gradient Descent(5528): loss=1.0067163928289056\n",
      "Stochastic Gradient Descent(5529): loss=18.181561847385364\n",
      "Stochastic Gradient Descent(5530): loss=0.010660579584082978\n",
      "Stochastic Gradient Descent(5531): loss=1.33815430582968\n",
      "Stochastic Gradient Descent(5532): loss=0.23745588769891235\n",
      "Stochastic Gradient Descent(5533): loss=0.010453873298341914\n",
      "Stochastic Gradient Descent(5534): loss=2.172271541049071\n",
      "Stochastic Gradient Descent(5535): loss=2.5966733789469205\n",
      "Stochastic Gradient Descent(5536): loss=15.190167966227332\n",
      "Stochastic Gradient Descent(5537): loss=0.055799244440821426\n",
      "Stochastic Gradient Descent(5538): loss=0.8509892613987573\n",
      "Stochastic Gradient Descent(5539): loss=7.166357592712579\n",
      "Stochastic Gradient Descent(5540): loss=2.5576747730109184\n",
      "Stochastic Gradient Descent(5541): loss=20.145541472067965\n",
      "Stochastic Gradient Descent(5542): loss=0.04809198889856378\n",
      "Stochastic Gradient Descent(5543): loss=18.87459511451989\n",
      "Stochastic Gradient Descent(5544): loss=6.461933807573169\n",
      "Stochastic Gradient Descent(5545): loss=2.2675829295884906\n",
      "Stochastic Gradient Descent(5546): loss=5.336259538355944\n",
      "Stochastic Gradient Descent(5547): loss=6.132123642257746\n",
      "Stochastic Gradient Descent(5548): loss=6.744994356578542\n",
      "Stochastic Gradient Descent(5549): loss=4.84214260947691\n",
      "Stochastic Gradient Descent(5550): loss=0.0004940901607244534\n",
      "Stochastic Gradient Descent(5551): loss=8.486568678922222\n",
      "Stochastic Gradient Descent(5552): loss=0.5542817799146438\n",
      "Stochastic Gradient Descent(5553): loss=1.0849286008962096\n",
      "Stochastic Gradient Descent(5554): loss=0.7639495309423047\n",
      "Stochastic Gradient Descent(5555): loss=0.23040090283266132\n",
      "Stochastic Gradient Descent(5556): loss=0.6329207593941822\n",
      "Stochastic Gradient Descent(5557): loss=5.723166204095735\n",
      "Stochastic Gradient Descent(5558): loss=7.273652399434864\n",
      "Stochastic Gradient Descent(5559): loss=22.04902199225768\n",
      "Stochastic Gradient Descent(5560): loss=0.37593097088771243\n",
      "Stochastic Gradient Descent(5561): loss=36.16985015921459\n",
      "Stochastic Gradient Descent(5562): loss=8.59875156603402\n",
      "Stochastic Gradient Descent(5563): loss=10.244664905046761\n",
      "Stochastic Gradient Descent(5564): loss=103.14503508380137\n",
      "Stochastic Gradient Descent(5565): loss=2.657024637494469\n",
      "Stochastic Gradient Descent(5566): loss=2.1409483337549045\n",
      "Stochastic Gradient Descent(5567): loss=0.041213174373640526\n",
      "Stochastic Gradient Descent(5568): loss=2.5849086591808703\n",
      "Stochastic Gradient Descent(5569): loss=3.0978269360683792\n",
      "Stochastic Gradient Descent(5570): loss=31.571681901973676\n",
      "Stochastic Gradient Descent(5571): loss=1.0912030642633994\n",
      "Stochastic Gradient Descent(5572): loss=1.0525106055287596\n",
      "Stochastic Gradient Descent(5573): loss=18.41063472184838\n",
      "Stochastic Gradient Descent(5574): loss=0.41245334786410864\n",
      "Stochastic Gradient Descent(5575): loss=2.674929761395376\n",
      "Stochastic Gradient Descent(5576): loss=1.9446978702881488\n",
      "Stochastic Gradient Descent(5577): loss=4.364423823630238\n",
      "Stochastic Gradient Descent(5578): loss=7.143614007797938\n",
      "Stochastic Gradient Descent(5579): loss=2.4407255154901524\n",
      "Stochastic Gradient Descent(5580): loss=1.3634090752746142\n",
      "Stochastic Gradient Descent(5581): loss=0.23324869177505747\n",
      "Stochastic Gradient Descent(5582): loss=0.02732405872286921\n",
      "Stochastic Gradient Descent(5583): loss=4.542512234509242\n",
      "Stochastic Gradient Descent(5584): loss=30.5590023301955\n",
      "Stochastic Gradient Descent(5585): loss=1.3666738440683772\n",
      "Stochastic Gradient Descent(5586): loss=1.144221950282462\n",
      "Stochastic Gradient Descent(5587): loss=22.388074121431355\n",
      "Stochastic Gradient Descent(5588): loss=1.5347603412798638\n",
      "Stochastic Gradient Descent(5589): loss=0.038854251862188154\n",
      "Stochastic Gradient Descent(5590): loss=0.004508340184871714\n",
      "Stochastic Gradient Descent(5591): loss=2.9289781915033943\n",
      "Stochastic Gradient Descent(5592): loss=7.074380236170905\n",
      "Stochastic Gradient Descent(5593): loss=0.42670102482656336\n",
      "Stochastic Gradient Descent(5594): loss=0.0869816427857054\n",
      "Stochastic Gradient Descent(5595): loss=0.5456758999423257\n",
      "Stochastic Gradient Descent(5596): loss=6.555673050071881\n",
      "Stochastic Gradient Descent(5597): loss=3.4284387811474857\n",
      "Stochastic Gradient Descent(5598): loss=8.683870083360226\n",
      "Stochastic Gradient Descent(5599): loss=1.5227953526187166\n",
      "Stochastic Gradient Descent(5600): loss=5.790404197216075\n",
      "Stochastic Gradient Descent(5601): loss=5.704538636866242\n",
      "Stochastic Gradient Descent(5602): loss=3.767414173013226\n",
      "Stochastic Gradient Descent(5603): loss=6.823117907162733\n",
      "Stochastic Gradient Descent(5604): loss=9.952130485912477\n",
      "Stochastic Gradient Descent(5605): loss=20.11149547871567\n",
      "Stochastic Gradient Descent(5606): loss=57.993259237489525\n",
      "Stochastic Gradient Descent(5607): loss=21.70384383142872\n",
      "Stochastic Gradient Descent(5608): loss=3.9804809569918764\n",
      "Stochastic Gradient Descent(5609): loss=9.381424672444199\n",
      "Stochastic Gradient Descent(5610): loss=1.3569270851454653\n",
      "Stochastic Gradient Descent(5611): loss=0.9501768303946512\n",
      "Stochastic Gradient Descent(5612): loss=2.887436570899628\n",
      "Stochastic Gradient Descent(5613): loss=11.24729232924164\n",
      "Stochastic Gradient Descent(5614): loss=0.003135288490346931\n",
      "Stochastic Gradient Descent(5615): loss=3.924380705193066\n",
      "Stochastic Gradient Descent(5616): loss=2.029296176900874\n",
      "Stochastic Gradient Descent(5617): loss=83.31854141604627\n",
      "Stochastic Gradient Descent(5618): loss=1.648412253897954\n",
      "Stochastic Gradient Descent(5619): loss=54.84658443620679\n",
      "Stochastic Gradient Descent(5620): loss=24.773132662778075\n",
      "Stochastic Gradient Descent(5621): loss=4.388152613747906\n",
      "Stochastic Gradient Descent(5622): loss=11.421698448573299\n",
      "Stochastic Gradient Descent(5623): loss=13.970462290017483\n",
      "Stochastic Gradient Descent(5624): loss=2.4829385021383277\n",
      "Stochastic Gradient Descent(5625): loss=3.5458966169990323e-08\n",
      "Stochastic Gradient Descent(5626): loss=12.344515750876223\n",
      "Stochastic Gradient Descent(5627): loss=1.540365069917917\n",
      "Stochastic Gradient Descent(5628): loss=0.004715868276372022\n",
      "Stochastic Gradient Descent(5629): loss=4.655591187420117\n",
      "Stochastic Gradient Descent(5630): loss=4.407355595177008\n",
      "Stochastic Gradient Descent(5631): loss=0.15818755333712728\n",
      "Stochastic Gradient Descent(5632): loss=6.5551312753992965\n",
      "Stochastic Gradient Descent(5633): loss=6.913661796621965\n",
      "Stochastic Gradient Descent(5634): loss=0.35053204254977643\n",
      "Stochastic Gradient Descent(5635): loss=3.437181203459947\n",
      "Stochastic Gradient Descent(5636): loss=21.063984566261702\n",
      "Stochastic Gradient Descent(5637): loss=31.232370632034584\n",
      "Stochastic Gradient Descent(5638): loss=25.477871294920064\n",
      "Stochastic Gradient Descent(5639): loss=11.005050763230816\n",
      "Stochastic Gradient Descent(5640): loss=9.19894373732066\n",
      "Stochastic Gradient Descent(5641): loss=1.6589004392952114\n",
      "Stochastic Gradient Descent(5642): loss=5.1322098983714115\n",
      "Stochastic Gradient Descent(5643): loss=0.04888817432280713\n",
      "Stochastic Gradient Descent(5644): loss=6.48895243186661\n",
      "Stochastic Gradient Descent(5645): loss=3.89781723420425\n",
      "Stochastic Gradient Descent(5646): loss=0.9282496499386339\n",
      "Stochastic Gradient Descent(5647): loss=0.3419340978143309\n",
      "Stochastic Gradient Descent(5648): loss=4.1743822311752874e-05\n",
      "Stochastic Gradient Descent(5649): loss=0.996135069158921\n",
      "Stochastic Gradient Descent(5650): loss=13.554488573057078\n",
      "Stochastic Gradient Descent(5651): loss=10.322977439327142\n",
      "Stochastic Gradient Descent(5652): loss=5.941000109960581\n",
      "Stochastic Gradient Descent(5653): loss=1.6039297549522786\n",
      "Stochastic Gradient Descent(5654): loss=0.192432412654497\n",
      "Stochastic Gradient Descent(5655): loss=0.056447076720956876\n",
      "Stochastic Gradient Descent(5656): loss=13.588889503079987\n",
      "Stochastic Gradient Descent(5657): loss=0.0926134982936416\n",
      "Stochastic Gradient Descent(5658): loss=3.0030525915179225\n",
      "Stochastic Gradient Descent(5659): loss=2.5379904700802545\n",
      "Stochastic Gradient Descent(5660): loss=1.3525310247260944\n",
      "Stochastic Gradient Descent(5661): loss=25.219852463904257\n",
      "Stochastic Gradient Descent(5662): loss=0.03227221922849732\n",
      "Stochastic Gradient Descent(5663): loss=0.18464029632674\n",
      "Stochastic Gradient Descent(5664): loss=1.0789193936009551\n",
      "Stochastic Gradient Descent(5665): loss=6.569356970806736\n",
      "Stochastic Gradient Descent(5666): loss=1.1368743713883487\n",
      "Stochastic Gradient Descent(5667): loss=0.13810430316670194\n",
      "Stochastic Gradient Descent(5668): loss=0.08268379967274594\n",
      "Stochastic Gradient Descent(5669): loss=10.50247818375059\n",
      "Stochastic Gradient Descent(5670): loss=6.6358641663869555\n",
      "Stochastic Gradient Descent(5671): loss=5.331353303654358\n",
      "Stochastic Gradient Descent(5672): loss=0.5548687009832731\n",
      "Stochastic Gradient Descent(5673): loss=2.551170330933666\n",
      "Stochastic Gradient Descent(5674): loss=20.807774597427773\n",
      "Stochastic Gradient Descent(5675): loss=0.8963822750929905\n",
      "Stochastic Gradient Descent(5676): loss=6.4122419615126365\n",
      "Stochastic Gradient Descent(5677): loss=5.32937257177089\n",
      "Stochastic Gradient Descent(5678): loss=16.097508387715454\n",
      "Stochastic Gradient Descent(5679): loss=8.080107194850601\n",
      "Stochastic Gradient Descent(5680): loss=0.12261971252951902\n",
      "Stochastic Gradient Descent(5681): loss=4.570214946593999\n",
      "Stochastic Gradient Descent(5682): loss=0.4433170687251099\n",
      "Stochastic Gradient Descent(5683): loss=0.08375299522854819\n",
      "Stochastic Gradient Descent(5684): loss=0.20029735983010627\n",
      "Stochastic Gradient Descent(5685): loss=1.7461960016388167\n",
      "Stochastic Gradient Descent(5686): loss=16.569429288873707\n",
      "Stochastic Gradient Descent(5687): loss=2.673452514361119\n",
      "Stochastic Gradient Descent(5688): loss=0.3353705700598528\n",
      "Stochastic Gradient Descent(5689): loss=12.167603521613893\n",
      "Stochastic Gradient Descent(5690): loss=9.631875025533263\n",
      "Stochastic Gradient Descent(5691): loss=0.003987318197966001\n",
      "Stochastic Gradient Descent(5692): loss=2.87292206775361\n",
      "Stochastic Gradient Descent(5693): loss=10.541947346034764\n",
      "Stochastic Gradient Descent(5694): loss=14.991892643554705\n",
      "Stochastic Gradient Descent(5695): loss=5.420273415063296\n",
      "Stochastic Gradient Descent(5696): loss=15.19208155229908\n",
      "Stochastic Gradient Descent(5697): loss=1.2262875678195932\n",
      "Stochastic Gradient Descent(5698): loss=4.413236319841246\n",
      "Stochastic Gradient Descent(5699): loss=15.592617517388856\n",
      "Stochastic Gradient Descent(5700): loss=3.2103485310049673\n",
      "Stochastic Gradient Descent(5701): loss=2.407048427183388\n",
      "Stochastic Gradient Descent(5702): loss=3.9520297584872868\n",
      "Stochastic Gradient Descent(5703): loss=2.5967667572890676\n",
      "Stochastic Gradient Descent(5704): loss=1.6941940290387862\n",
      "Stochastic Gradient Descent(5705): loss=1.118928326159339\n",
      "Stochastic Gradient Descent(5706): loss=17.09867041727273\n",
      "Stochastic Gradient Descent(5707): loss=0.16263649348779888\n",
      "Stochastic Gradient Descent(5708): loss=0.23317735285030733\n",
      "Stochastic Gradient Descent(5709): loss=3.1855357835127776\n",
      "Stochastic Gradient Descent(5710): loss=0.3111951088514268\n",
      "Stochastic Gradient Descent(5711): loss=4.418647068674293\n",
      "Stochastic Gradient Descent(5712): loss=0.8660126502571351\n",
      "Stochastic Gradient Descent(5713): loss=0.4437668455768369\n",
      "Stochastic Gradient Descent(5714): loss=1.0347456642682435\n",
      "Stochastic Gradient Descent(5715): loss=7.9473075432461915\n",
      "Stochastic Gradient Descent(5716): loss=0.20896981777366674\n",
      "Stochastic Gradient Descent(5717): loss=4.127099544358778\n",
      "Stochastic Gradient Descent(5718): loss=0.6715338004391824\n",
      "Stochastic Gradient Descent(5719): loss=2.237926837693346\n",
      "Stochastic Gradient Descent(5720): loss=1.0013361029493744\n",
      "Stochastic Gradient Descent(5721): loss=17.611977828670614\n",
      "Stochastic Gradient Descent(5722): loss=1.4409873677897311\n",
      "Stochastic Gradient Descent(5723): loss=0.667826327956279\n",
      "Stochastic Gradient Descent(5724): loss=6.345005848214714\n",
      "Stochastic Gradient Descent(5725): loss=3.2948621902514628\n",
      "Stochastic Gradient Descent(5726): loss=1.132335983117532\n",
      "Stochastic Gradient Descent(5727): loss=5.533090631620497\n",
      "Stochastic Gradient Descent(5728): loss=1.3792136119237817\n",
      "Stochastic Gradient Descent(5729): loss=33.75025227076306\n",
      "Stochastic Gradient Descent(5730): loss=41.075662974659195\n",
      "Stochastic Gradient Descent(5731): loss=12.259905446295818\n",
      "Stochastic Gradient Descent(5732): loss=1.6324034193430215\n",
      "Stochastic Gradient Descent(5733): loss=3.690043572573363\n",
      "Stochastic Gradient Descent(5734): loss=34.44402652380571\n",
      "Stochastic Gradient Descent(5735): loss=1.9230538538309312\n",
      "Stochastic Gradient Descent(5736): loss=0.003941042051233456\n",
      "Stochastic Gradient Descent(5737): loss=0.42554476286530024\n",
      "Stochastic Gradient Descent(5738): loss=122.81076200441298\n",
      "Stochastic Gradient Descent(5739): loss=34.23810029115642\n",
      "Stochastic Gradient Descent(5740): loss=100.49142406933144\n",
      "Stochastic Gradient Descent(5741): loss=28.38066906998168\n",
      "Stochastic Gradient Descent(5742): loss=0.22968958738158107\n",
      "Stochastic Gradient Descent(5743): loss=119.76371578458026\n",
      "Stochastic Gradient Descent(5744): loss=277.73334242803753\n",
      "Stochastic Gradient Descent(5745): loss=148.6086387174069\n",
      "Stochastic Gradient Descent(5746): loss=6.617002834025781\n",
      "Stochastic Gradient Descent(5747): loss=4.525727127796241\n",
      "Stochastic Gradient Descent(5748): loss=0.0021555747084019247\n",
      "Stochastic Gradient Descent(5749): loss=0.4629819714822846\n",
      "Stochastic Gradient Descent(5750): loss=2.3296720076896174\n",
      "Stochastic Gradient Descent(5751): loss=1.0346298596230203\n",
      "Stochastic Gradient Descent(5752): loss=11.270429902841148\n",
      "Stochastic Gradient Descent(5753): loss=25.672834875717758\n",
      "Stochastic Gradient Descent(5754): loss=8.068828422639179\n",
      "Stochastic Gradient Descent(5755): loss=0.7920666915623659\n",
      "Stochastic Gradient Descent(5756): loss=0.6861048548232388\n",
      "Stochastic Gradient Descent(5757): loss=0.10172563441841817\n",
      "Stochastic Gradient Descent(5758): loss=4.984930485544425\n",
      "Stochastic Gradient Descent(5759): loss=0.8689674363693264\n",
      "Stochastic Gradient Descent(5760): loss=3.3804969407998113\n",
      "Stochastic Gradient Descent(5761): loss=6.710258955813428\n",
      "Stochastic Gradient Descent(5762): loss=17.45704318863906\n",
      "Stochastic Gradient Descent(5763): loss=6.244636737314337\n",
      "Stochastic Gradient Descent(5764): loss=0.04093390206478426\n",
      "Stochastic Gradient Descent(5765): loss=17.035941901208517\n",
      "Stochastic Gradient Descent(5766): loss=25.985392287171\n",
      "Stochastic Gradient Descent(5767): loss=2.1752156462908956\n",
      "Stochastic Gradient Descent(5768): loss=5.925313471845886\n",
      "Stochastic Gradient Descent(5769): loss=13.8904740585503\n",
      "Stochastic Gradient Descent(5770): loss=7.683837023792502\n",
      "Stochastic Gradient Descent(5771): loss=9.044119571579397\n",
      "Stochastic Gradient Descent(5772): loss=28.857228745373305\n",
      "Stochastic Gradient Descent(5773): loss=2.5700488265069765\n",
      "Stochastic Gradient Descent(5774): loss=1.9001086982832545\n",
      "Stochastic Gradient Descent(5775): loss=12.238435737626244\n",
      "Stochastic Gradient Descent(5776): loss=14.304926232493248\n",
      "Stochastic Gradient Descent(5777): loss=11.103764364945008\n",
      "Stochastic Gradient Descent(5778): loss=4.8963177820787935\n",
      "Stochastic Gradient Descent(5779): loss=0.10047684972697979\n",
      "Stochastic Gradient Descent(5780): loss=2.944005881852051\n",
      "Stochastic Gradient Descent(5781): loss=6.8424896196574885\n",
      "Stochastic Gradient Descent(5782): loss=0.04741705358711881\n",
      "Stochastic Gradient Descent(5783): loss=1.7465952532950697\n",
      "Stochastic Gradient Descent(5784): loss=9.86518091004029\n",
      "Stochastic Gradient Descent(5785): loss=5.854303688887956\n",
      "Stochastic Gradient Descent(5786): loss=0.09791661982100948\n",
      "Stochastic Gradient Descent(5787): loss=6.533697580708085\n",
      "Stochastic Gradient Descent(5788): loss=0.17387258935035604\n",
      "Stochastic Gradient Descent(5789): loss=0.8496386193204403\n",
      "Stochastic Gradient Descent(5790): loss=0.1987118123787898\n",
      "Stochastic Gradient Descent(5791): loss=0.18576178585285338\n",
      "Stochastic Gradient Descent(5792): loss=30.660751315445484\n",
      "Stochastic Gradient Descent(5793): loss=6.59443441814618\n",
      "Stochastic Gradient Descent(5794): loss=4.578465136783459\n",
      "Stochastic Gradient Descent(5795): loss=0.919381593911339\n",
      "Stochastic Gradient Descent(5796): loss=18.480829213460787\n",
      "Stochastic Gradient Descent(5797): loss=0.0142964123193236\n",
      "Stochastic Gradient Descent(5798): loss=17.43356878100753\n",
      "Stochastic Gradient Descent(5799): loss=21.739177167342405\n",
      "Stochastic Gradient Descent(5800): loss=6.063334289947262\n",
      "Stochastic Gradient Descent(5801): loss=6.758522822949009\n",
      "Stochastic Gradient Descent(5802): loss=3.6551588553787844\n",
      "Stochastic Gradient Descent(5803): loss=0.5388884619533377\n",
      "Stochastic Gradient Descent(5804): loss=8.84447180330532\n",
      "Stochastic Gradient Descent(5805): loss=10.177882428282155\n",
      "Stochastic Gradient Descent(5806): loss=0.0002798449224365128\n",
      "Stochastic Gradient Descent(5807): loss=0.9334799738889379\n",
      "Stochastic Gradient Descent(5808): loss=5.528907323358933\n",
      "Stochastic Gradient Descent(5809): loss=12.743350184613915\n",
      "Stochastic Gradient Descent(5810): loss=31.636823439414275\n",
      "Stochastic Gradient Descent(5811): loss=2.5345220615540933\n",
      "Stochastic Gradient Descent(5812): loss=79.51528750822376\n",
      "Stochastic Gradient Descent(5813): loss=9.15112046557664\n",
      "Stochastic Gradient Descent(5814): loss=22.753606299527377\n",
      "Stochastic Gradient Descent(5815): loss=0.33446663595057946\n",
      "Stochastic Gradient Descent(5816): loss=0.12620006553759272\n",
      "Stochastic Gradient Descent(5817): loss=168.93634507499098\n",
      "Stochastic Gradient Descent(5818): loss=420.1790961174812\n",
      "Stochastic Gradient Descent(5819): loss=34.189765433604144\n",
      "Stochastic Gradient Descent(5820): loss=146.97584366586437\n",
      "Stochastic Gradient Descent(5821): loss=32.14105263535446\n",
      "Stochastic Gradient Descent(5822): loss=13.830648603772746\n",
      "Stochastic Gradient Descent(5823): loss=7.319616276836316\n",
      "Stochastic Gradient Descent(5824): loss=5.062764433244302\n",
      "Stochastic Gradient Descent(5825): loss=7.873168445999643\n",
      "Stochastic Gradient Descent(5826): loss=7.870902521091656\n",
      "Stochastic Gradient Descent(5827): loss=4.731685589861938\n",
      "Stochastic Gradient Descent(5828): loss=0.0014968076394728438\n",
      "Stochastic Gradient Descent(5829): loss=26.575751063767875\n",
      "Stochastic Gradient Descent(5830): loss=14.256495267802919\n",
      "Stochastic Gradient Descent(5831): loss=6.297346176598672\n",
      "Stochastic Gradient Descent(5832): loss=17.219229845694333\n",
      "Stochastic Gradient Descent(5833): loss=15.62394714430922\n",
      "Stochastic Gradient Descent(5834): loss=4.960343974908012\n",
      "Stochastic Gradient Descent(5835): loss=176.84372967085943\n",
      "Stochastic Gradient Descent(5836): loss=8.008442524465783\n",
      "Stochastic Gradient Descent(5837): loss=69.43599965158272\n",
      "Stochastic Gradient Descent(5838): loss=23.923574240444587\n",
      "Stochastic Gradient Descent(5839): loss=68.73724812588887\n",
      "Stochastic Gradient Descent(5840): loss=58.31700133217364\n",
      "Stochastic Gradient Descent(5841): loss=4.104281075867091\n",
      "Stochastic Gradient Descent(5842): loss=0.0009673920433481434\n",
      "Stochastic Gradient Descent(5843): loss=1.8912673427538016\n",
      "Stochastic Gradient Descent(5844): loss=20.031187699102205\n",
      "Stochastic Gradient Descent(5845): loss=0.7888643038753593\n",
      "Stochastic Gradient Descent(5846): loss=0.06833699663214692\n",
      "Stochastic Gradient Descent(5847): loss=0.9899660889678898\n",
      "Stochastic Gradient Descent(5848): loss=0.04304743761595833\n",
      "Stochastic Gradient Descent(5849): loss=5.645549966557734\n",
      "Stochastic Gradient Descent(5850): loss=1.7052807581201557\n",
      "Stochastic Gradient Descent(5851): loss=0.15987696169855733\n",
      "Stochastic Gradient Descent(5852): loss=5.609648995076222\n",
      "Stochastic Gradient Descent(5853): loss=13.146112021611046\n",
      "Stochastic Gradient Descent(5854): loss=4.14178475735794\n",
      "Stochastic Gradient Descent(5855): loss=2.0262017513314747\n",
      "Stochastic Gradient Descent(5856): loss=2.139075693323941\n",
      "Stochastic Gradient Descent(5857): loss=176.2062494099527\n",
      "Stochastic Gradient Descent(5858): loss=0.10549223750456502\n",
      "Stochastic Gradient Descent(5859): loss=174.21669090770035\n",
      "Stochastic Gradient Descent(5860): loss=2.779043429912325\n",
      "Stochastic Gradient Descent(5861): loss=195.09578716203515\n",
      "Stochastic Gradient Descent(5862): loss=22.7576664143709\n",
      "Stochastic Gradient Descent(5863): loss=0.7318594331052198\n",
      "Stochastic Gradient Descent(5864): loss=1.300611069322598\n",
      "Stochastic Gradient Descent(5865): loss=23.108122499482175\n",
      "Stochastic Gradient Descent(5866): loss=171.816791984256\n",
      "Stochastic Gradient Descent(5867): loss=2.2751316275804467\n",
      "Stochastic Gradient Descent(5868): loss=0.6335734479489886\n",
      "Stochastic Gradient Descent(5869): loss=30.83913026280309\n",
      "Stochastic Gradient Descent(5870): loss=0.08017105557054081\n",
      "Stochastic Gradient Descent(5871): loss=0.6408584701877473\n",
      "Stochastic Gradient Descent(5872): loss=0.13615525833977535\n",
      "Stochastic Gradient Descent(5873): loss=0.0006444390400487742\n",
      "Stochastic Gradient Descent(5874): loss=4.209959296469023\n",
      "Stochastic Gradient Descent(5875): loss=0.001600279747833999\n",
      "Stochastic Gradient Descent(5876): loss=42.49984100698949\n",
      "Stochastic Gradient Descent(5877): loss=2.5876573144957775\n",
      "Stochastic Gradient Descent(5878): loss=0.7627259604395044\n",
      "Stochastic Gradient Descent(5879): loss=5.187050692251815\n",
      "Stochastic Gradient Descent(5880): loss=0.2578812226655302\n",
      "Stochastic Gradient Descent(5881): loss=9.874659024153221\n",
      "Stochastic Gradient Descent(5882): loss=2.950629318312564\n",
      "Stochastic Gradient Descent(5883): loss=0.033026718002297416\n",
      "Stochastic Gradient Descent(5884): loss=30.77741082384108\n",
      "Stochastic Gradient Descent(5885): loss=31.579758380591233\n",
      "Stochastic Gradient Descent(5886): loss=27.597207234186484\n",
      "Stochastic Gradient Descent(5887): loss=0.5340362803746187\n",
      "Stochastic Gradient Descent(5888): loss=23.682145787055283\n",
      "Stochastic Gradient Descent(5889): loss=2.9319591931169304\n",
      "Stochastic Gradient Descent(5890): loss=0.4661547220344338\n",
      "Stochastic Gradient Descent(5891): loss=3.4228080847792985\n",
      "Stochastic Gradient Descent(5892): loss=18.096387494976597\n",
      "Stochastic Gradient Descent(5893): loss=2.617118820913526\n",
      "Stochastic Gradient Descent(5894): loss=0.008361493858396879\n",
      "Stochastic Gradient Descent(5895): loss=0.7364710491785292\n",
      "Stochastic Gradient Descent(5896): loss=15.746521676390401\n",
      "Stochastic Gradient Descent(5897): loss=5.631923151638208\n",
      "Stochastic Gradient Descent(5898): loss=0.7006129231886803\n",
      "Stochastic Gradient Descent(5899): loss=5.308933982738431\n",
      "Stochastic Gradient Descent(5900): loss=3.820456834452412\n",
      "Stochastic Gradient Descent(5901): loss=16.73290986339251\n",
      "Stochastic Gradient Descent(5902): loss=0.31056832792449346\n",
      "Stochastic Gradient Descent(5903): loss=7.357758891246953\n",
      "Stochastic Gradient Descent(5904): loss=13.330084805027948\n",
      "Stochastic Gradient Descent(5905): loss=3.881584540388671\n",
      "Stochastic Gradient Descent(5906): loss=0.837328438431173\n",
      "Stochastic Gradient Descent(5907): loss=0.04897752175121811\n",
      "Stochastic Gradient Descent(5908): loss=3.1295113872233813\n",
      "Stochastic Gradient Descent(5909): loss=0.24858038803761764\n",
      "Stochastic Gradient Descent(5910): loss=5.046969806057008\n",
      "Stochastic Gradient Descent(5911): loss=1.4579778476283551\n",
      "Stochastic Gradient Descent(5912): loss=28.35191892017811\n",
      "Stochastic Gradient Descent(5913): loss=19.58120097757106\n",
      "Stochastic Gradient Descent(5914): loss=2.985068587200382\n",
      "Stochastic Gradient Descent(5915): loss=42.892541407791896\n",
      "Stochastic Gradient Descent(5916): loss=0.5232108390573232\n",
      "Stochastic Gradient Descent(5917): loss=0.05615188364969765\n",
      "Stochastic Gradient Descent(5918): loss=1.0207550473346958\n",
      "Stochastic Gradient Descent(5919): loss=4.8477191593719615\n",
      "Stochastic Gradient Descent(5920): loss=0.3414516066983084\n",
      "Stochastic Gradient Descent(5921): loss=17.379644129134867\n",
      "Stochastic Gradient Descent(5922): loss=3.9071484622710315\n",
      "Stochastic Gradient Descent(5923): loss=13.72840619589091\n",
      "Stochastic Gradient Descent(5924): loss=6.59662509548959\n",
      "Stochastic Gradient Descent(5925): loss=2.19089123445881\n",
      "Stochastic Gradient Descent(5926): loss=1.7732998840556173\n",
      "Stochastic Gradient Descent(5927): loss=2.1879538803471528\n",
      "Stochastic Gradient Descent(5928): loss=12.075893349375603\n",
      "Stochastic Gradient Descent(5929): loss=15.20618200469993\n",
      "Stochastic Gradient Descent(5930): loss=1.2662457802157874\n",
      "Stochastic Gradient Descent(5931): loss=3.510245520696051\n",
      "Stochastic Gradient Descent(5932): loss=1.0515751218664633\n",
      "Stochastic Gradient Descent(5933): loss=0.0002485692153005661\n",
      "Stochastic Gradient Descent(5934): loss=1.263022671751796\n",
      "Stochastic Gradient Descent(5935): loss=0.4931485947848152\n",
      "Stochastic Gradient Descent(5936): loss=20.42886467614883\n",
      "Stochastic Gradient Descent(5937): loss=37.39767785353274\n",
      "Stochastic Gradient Descent(5938): loss=3.7095847740133134\n",
      "Stochastic Gradient Descent(5939): loss=2.9714851508564615\n",
      "Stochastic Gradient Descent(5940): loss=8.114807677522403\n",
      "Stochastic Gradient Descent(5941): loss=3.6505653554046242\n",
      "Stochastic Gradient Descent(5942): loss=0.4750557756627027\n",
      "Stochastic Gradient Descent(5943): loss=19.604377947473598\n",
      "Stochastic Gradient Descent(5944): loss=0.7558577533248\n",
      "Stochastic Gradient Descent(5945): loss=10.146056221033346\n",
      "Stochastic Gradient Descent(5946): loss=1.1539519472411846\n",
      "Stochastic Gradient Descent(5947): loss=17.170012373357878\n",
      "Stochastic Gradient Descent(5948): loss=0.15933072489703493\n",
      "Stochastic Gradient Descent(5949): loss=3.2390746613411667\n",
      "Stochastic Gradient Descent(5950): loss=5.344375765310581\n",
      "Stochastic Gradient Descent(5951): loss=0.05181688653893838\n",
      "Stochastic Gradient Descent(5952): loss=61.71495047907202\n",
      "Stochastic Gradient Descent(5953): loss=8.85531578214945\n",
      "Stochastic Gradient Descent(5954): loss=11.034994368998634\n",
      "Stochastic Gradient Descent(5955): loss=0.10257668020985508\n",
      "Stochastic Gradient Descent(5956): loss=48.00666989677819\n",
      "Stochastic Gradient Descent(5957): loss=9.670100861308423\n",
      "Stochastic Gradient Descent(5958): loss=0.70632931434602\n",
      "Stochastic Gradient Descent(5959): loss=0.006933482129307609\n",
      "Stochastic Gradient Descent(5960): loss=0.09291727998230677\n",
      "Stochastic Gradient Descent(5961): loss=13.090788966215515\n",
      "Stochastic Gradient Descent(5962): loss=0.9470129574654756\n",
      "Stochastic Gradient Descent(5963): loss=7.404550633254059\n",
      "Stochastic Gradient Descent(5964): loss=0.013675048188978592\n",
      "Stochastic Gradient Descent(5965): loss=34.92347362325759\n",
      "Stochastic Gradient Descent(5966): loss=22.776895502053435\n",
      "Stochastic Gradient Descent(5967): loss=11.723971897451898\n",
      "Stochastic Gradient Descent(5968): loss=1.6884762026510762\n",
      "Stochastic Gradient Descent(5969): loss=0.027845045778230142\n",
      "Stochastic Gradient Descent(5970): loss=0.6712730898554697\n",
      "Stochastic Gradient Descent(5971): loss=0.8568505528361674\n",
      "Stochastic Gradient Descent(5972): loss=1.1355822409063894\n",
      "Stochastic Gradient Descent(5973): loss=1.9004350487327473\n",
      "Stochastic Gradient Descent(5974): loss=4.251159241761864\n",
      "Stochastic Gradient Descent(5975): loss=8.509044590084596\n",
      "Stochastic Gradient Descent(5976): loss=0.759501126307153\n",
      "Stochastic Gradient Descent(5977): loss=14.886786816614796\n",
      "Stochastic Gradient Descent(5978): loss=0.10604530415126605\n",
      "Stochastic Gradient Descent(5979): loss=0.42169439359991095\n",
      "Stochastic Gradient Descent(5980): loss=1.3763007959548865\n",
      "Stochastic Gradient Descent(5981): loss=0.4047365537356003\n",
      "Stochastic Gradient Descent(5982): loss=12.348666740120835\n",
      "Stochastic Gradient Descent(5983): loss=24.85710186318865\n",
      "Stochastic Gradient Descent(5984): loss=6.627679239598081\n",
      "Stochastic Gradient Descent(5985): loss=0.1532935017806131\n",
      "Stochastic Gradient Descent(5986): loss=3.786121822582317\n",
      "Stochastic Gradient Descent(5987): loss=0.6949008701328788\n",
      "Stochastic Gradient Descent(5988): loss=38.22409298633281\n",
      "Stochastic Gradient Descent(5989): loss=4.817110965493425\n",
      "Stochastic Gradient Descent(5990): loss=2.4607672054096272\n",
      "Stochastic Gradient Descent(5991): loss=0.6313128270850138\n",
      "Stochastic Gradient Descent(5992): loss=15.40001317637627\n",
      "Stochastic Gradient Descent(5993): loss=11.11585617607065\n",
      "Stochastic Gradient Descent(5994): loss=1.4680422533348823\n",
      "Stochastic Gradient Descent(5995): loss=6.461686606902668\n",
      "Stochastic Gradient Descent(5996): loss=10.541576894807918\n",
      "Stochastic Gradient Descent(5997): loss=2.6750838507010535\n",
      "Stochastic Gradient Descent(5998): loss=0.13294553370137024\n",
      "Stochastic Gradient Descent(5999): loss=7.920603745848775\n",
      "Stochastic Gradient Descent(6000): loss=10.229799966444208\n",
      "Stochastic Gradient Descent(6001): loss=29.229262774620675\n",
      "Stochastic Gradient Descent(6002): loss=0.00034511405548174435\n",
      "Stochastic Gradient Descent(6003): loss=1.154123476720233\n",
      "Stochastic Gradient Descent(6004): loss=0.32599662561622833\n",
      "Stochastic Gradient Descent(6005): loss=0.405876950593358\n",
      "Stochastic Gradient Descent(6006): loss=1.8944531270934255\n",
      "Stochastic Gradient Descent(6007): loss=2.252608115225818\n",
      "Stochastic Gradient Descent(6008): loss=14.966468336237545\n",
      "Stochastic Gradient Descent(6009): loss=22.222469246232652\n",
      "Stochastic Gradient Descent(6010): loss=1.3664561826288213\n",
      "Stochastic Gradient Descent(6011): loss=1.813015237504261\n",
      "Stochastic Gradient Descent(6012): loss=32.29995180400383\n",
      "Stochastic Gradient Descent(6013): loss=0.006829112030173057\n",
      "Stochastic Gradient Descent(6014): loss=0.012617458164150934\n",
      "Stochastic Gradient Descent(6015): loss=12.428581140178682\n",
      "Stochastic Gradient Descent(6016): loss=1.8038818823070402\n",
      "Stochastic Gradient Descent(6017): loss=1.139836619802579\n",
      "Stochastic Gradient Descent(6018): loss=1.1303149359744626\n",
      "Stochastic Gradient Descent(6019): loss=0.037396773743547156\n",
      "Stochastic Gradient Descent(6020): loss=0.18052720990648552\n",
      "Stochastic Gradient Descent(6021): loss=0.7965687870658523\n",
      "Stochastic Gradient Descent(6022): loss=4.237631663628267\n",
      "Stochastic Gradient Descent(6023): loss=11.02744737125922\n",
      "Stochastic Gradient Descent(6024): loss=0.03402823734811609\n",
      "Stochastic Gradient Descent(6025): loss=0.47739452195258675\n",
      "Stochastic Gradient Descent(6026): loss=1.4897136911641629\n",
      "Stochastic Gradient Descent(6027): loss=10.41794375194723\n",
      "Stochastic Gradient Descent(6028): loss=1.7985580829320342\n",
      "Stochastic Gradient Descent(6029): loss=5.497855230734259\n",
      "Stochastic Gradient Descent(6030): loss=1.6667489633971813\n",
      "Stochastic Gradient Descent(6031): loss=4.844597183143481\n",
      "Stochastic Gradient Descent(6032): loss=16.546031849919487\n",
      "Stochastic Gradient Descent(6033): loss=8.508050122141011\n",
      "Stochastic Gradient Descent(6034): loss=2.319796468800011\n",
      "Stochastic Gradient Descent(6035): loss=2.093388738873308\n",
      "Stochastic Gradient Descent(6036): loss=1.449313959961058\n",
      "Stochastic Gradient Descent(6037): loss=0.6434318906003008\n",
      "Stochastic Gradient Descent(6038): loss=3.609172394804112\n",
      "Stochastic Gradient Descent(6039): loss=2.4370815189114508\n",
      "Stochastic Gradient Descent(6040): loss=1.3996504904257379\n",
      "Stochastic Gradient Descent(6041): loss=9.832476813257383\n",
      "Stochastic Gradient Descent(6042): loss=1.3525598205803844\n",
      "Stochastic Gradient Descent(6043): loss=28.043871992293703\n",
      "Stochastic Gradient Descent(6044): loss=10.608241653004\n",
      "Stochastic Gradient Descent(6045): loss=8.114766871087928\n",
      "Stochastic Gradient Descent(6046): loss=0.24981109399153106\n",
      "Stochastic Gradient Descent(6047): loss=1.0508362623443563\n",
      "Stochastic Gradient Descent(6048): loss=18.498598635411987\n",
      "Stochastic Gradient Descent(6049): loss=9.10439774404425\n",
      "Stochastic Gradient Descent(6050): loss=10.081959229541505\n",
      "Stochastic Gradient Descent(6051): loss=24.319456237235798\n",
      "Stochastic Gradient Descent(6052): loss=0.07429820573900624\n",
      "Stochastic Gradient Descent(6053): loss=10.521289388547821\n",
      "Stochastic Gradient Descent(6054): loss=0.43446975948860805\n",
      "Stochastic Gradient Descent(6055): loss=0.1752499356946997\n",
      "Stochastic Gradient Descent(6056): loss=3.022874428349635\n",
      "Stochastic Gradient Descent(6057): loss=2.053553246242975\n",
      "Stochastic Gradient Descent(6058): loss=23.475695593051466\n",
      "Stochastic Gradient Descent(6059): loss=12.718267950804789\n",
      "Stochastic Gradient Descent(6060): loss=1.9588426155240697\n",
      "Stochastic Gradient Descent(6061): loss=9.658850810353227\n",
      "Stochastic Gradient Descent(6062): loss=0.39922722782939446\n",
      "Stochastic Gradient Descent(6063): loss=3.3260261448406867\n",
      "Stochastic Gradient Descent(6064): loss=2.112600777541123\n",
      "Stochastic Gradient Descent(6065): loss=15.075003867439017\n",
      "Stochastic Gradient Descent(6066): loss=5.920720370529742\n",
      "Stochastic Gradient Descent(6067): loss=1.9846958213067072\n",
      "Stochastic Gradient Descent(6068): loss=0.3243544617478157\n",
      "Stochastic Gradient Descent(6069): loss=0.8286068152957617\n",
      "Stochastic Gradient Descent(6070): loss=13.751569405131676\n",
      "Stochastic Gradient Descent(6071): loss=2.062009692525103\n",
      "Stochastic Gradient Descent(6072): loss=11.957556109756128\n",
      "Stochastic Gradient Descent(6073): loss=0.9118724054349487\n",
      "Stochastic Gradient Descent(6074): loss=10.41543486860795\n",
      "Stochastic Gradient Descent(6075): loss=10.820102794137913\n",
      "Stochastic Gradient Descent(6076): loss=9.907285511674349\n",
      "Stochastic Gradient Descent(6077): loss=1.6238827752576426\n",
      "Stochastic Gradient Descent(6078): loss=3.2334530063131277\n",
      "Stochastic Gradient Descent(6079): loss=4.403339642257282\n",
      "Stochastic Gradient Descent(6080): loss=0.6403861935557691\n",
      "Stochastic Gradient Descent(6081): loss=0.32561333048684266\n",
      "Stochastic Gradient Descent(6082): loss=10.86255050810885\n",
      "Stochastic Gradient Descent(6083): loss=4.731136705221789\n",
      "Stochastic Gradient Descent(6084): loss=0.05991944700823172\n",
      "Stochastic Gradient Descent(6085): loss=0.08589953895060211\n",
      "Stochastic Gradient Descent(6086): loss=1.8333754591296032\n",
      "Stochastic Gradient Descent(6087): loss=5.347390213051113\n",
      "Stochastic Gradient Descent(6088): loss=15.623463521393399\n",
      "Stochastic Gradient Descent(6089): loss=1.3091388472090062\n",
      "Stochastic Gradient Descent(6090): loss=0.2792132949140187\n",
      "Stochastic Gradient Descent(6091): loss=1.9501822963404947\n",
      "Stochastic Gradient Descent(6092): loss=0.11918900695488623\n",
      "Stochastic Gradient Descent(6093): loss=2.5685020210434745e-06\n",
      "Stochastic Gradient Descent(6094): loss=4.604829998744717\n",
      "Stochastic Gradient Descent(6095): loss=0.1520943300782705\n",
      "Stochastic Gradient Descent(6096): loss=0.030166652493508805\n",
      "Stochastic Gradient Descent(6097): loss=0.5469305397605317\n",
      "Stochastic Gradient Descent(6098): loss=16.24002605049128\n",
      "Stochastic Gradient Descent(6099): loss=5.091193943976294\n",
      "Stochastic Gradient Descent(6100): loss=1.702871388803657\n",
      "Stochastic Gradient Descent(6101): loss=4.383737636999048\n",
      "Stochastic Gradient Descent(6102): loss=9.463798645357288\n",
      "Stochastic Gradient Descent(6103): loss=5.971218115163441\n",
      "Stochastic Gradient Descent(6104): loss=19.77446148498362\n",
      "Stochastic Gradient Descent(6105): loss=0.03494089729793076\n",
      "Stochastic Gradient Descent(6106): loss=20.349802496140267\n",
      "Stochastic Gradient Descent(6107): loss=1.3953072280538816\n",
      "Stochastic Gradient Descent(6108): loss=5.01645012565007\n",
      "Stochastic Gradient Descent(6109): loss=0.08319027362112398\n",
      "Stochastic Gradient Descent(6110): loss=3.204310090517516e-05\n",
      "Stochastic Gradient Descent(6111): loss=0.0037906005254650255\n",
      "Stochastic Gradient Descent(6112): loss=0.49323077629863066\n",
      "Stochastic Gradient Descent(6113): loss=0.741364704002878\n",
      "Stochastic Gradient Descent(6114): loss=5.974969721135535\n",
      "Stochastic Gradient Descent(6115): loss=5.6308282938966325\n",
      "Stochastic Gradient Descent(6116): loss=15.84141111017188\n",
      "Stochastic Gradient Descent(6117): loss=0.018482988817262458\n",
      "Stochastic Gradient Descent(6118): loss=7.399763210896962\n",
      "Stochastic Gradient Descent(6119): loss=12.450315079925081\n",
      "Stochastic Gradient Descent(6120): loss=0.40893612586914235\n",
      "Stochastic Gradient Descent(6121): loss=1.213200745375728\n",
      "Stochastic Gradient Descent(6122): loss=3.254949641536999\n",
      "Stochastic Gradient Descent(6123): loss=13.234337801261303\n",
      "Stochastic Gradient Descent(6124): loss=7.72309717124234\n",
      "Stochastic Gradient Descent(6125): loss=8.558177176277614\n",
      "Stochastic Gradient Descent(6126): loss=9.890364325896515\n",
      "Stochastic Gradient Descent(6127): loss=3.6986486018220934\n",
      "Stochastic Gradient Descent(6128): loss=1.643196855216321\n",
      "Stochastic Gradient Descent(6129): loss=9.445263710244644\n",
      "Stochastic Gradient Descent(6130): loss=1.5152193290247369\n",
      "Stochastic Gradient Descent(6131): loss=0.28110640027867195\n",
      "Stochastic Gradient Descent(6132): loss=0.8825969626465195\n",
      "Stochastic Gradient Descent(6133): loss=14.818868119275923\n",
      "Stochastic Gradient Descent(6134): loss=24.028917611499246\n",
      "Stochastic Gradient Descent(6135): loss=1.4786417252741648\n",
      "Stochastic Gradient Descent(6136): loss=5.788526717842081\n",
      "Stochastic Gradient Descent(6137): loss=1.841598910937964\n",
      "Stochastic Gradient Descent(6138): loss=18.15857980977711\n",
      "Stochastic Gradient Descent(6139): loss=1.0508516838088269\n",
      "Stochastic Gradient Descent(6140): loss=12.665167486708906\n",
      "Stochastic Gradient Descent(6141): loss=0.16517554410819024\n",
      "Stochastic Gradient Descent(6142): loss=0.033044993716290876\n",
      "Stochastic Gradient Descent(6143): loss=0.17667989047286653\n",
      "Stochastic Gradient Descent(6144): loss=0.08851696933234178\n",
      "Stochastic Gradient Descent(6145): loss=0.34168747974763286\n",
      "Stochastic Gradient Descent(6146): loss=11.655616188160124\n",
      "Stochastic Gradient Descent(6147): loss=3.4637309525688456\n",
      "Stochastic Gradient Descent(6148): loss=12.817396150127818\n",
      "Stochastic Gradient Descent(6149): loss=43.8865092500346\n",
      "Stochastic Gradient Descent(6150): loss=0.8004193432085677\n",
      "Stochastic Gradient Descent(6151): loss=0.015378133636168762\n",
      "Stochastic Gradient Descent(6152): loss=11.42442851307479\n",
      "Stochastic Gradient Descent(6153): loss=8.810770216494863\n",
      "Stochastic Gradient Descent(6154): loss=13.776541910837066\n",
      "Stochastic Gradient Descent(6155): loss=1.3928790821757493\n",
      "Stochastic Gradient Descent(6156): loss=0.21356750192799404\n",
      "Stochastic Gradient Descent(6157): loss=0.4725420188439202\n",
      "Stochastic Gradient Descent(6158): loss=2.573831425015751\n",
      "Stochastic Gradient Descent(6159): loss=0.6036812305123785\n",
      "Stochastic Gradient Descent(6160): loss=21.507695542069307\n",
      "Stochastic Gradient Descent(6161): loss=8.74357997104928\n",
      "Stochastic Gradient Descent(6162): loss=3.5554702843595942\n",
      "Stochastic Gradient Descent(6163): loss=2.206707794662615\n",
      "Stochastic Gradient Descent(6164): loss=18.65684111210605\n",
      "Stochastic Gradient Descent(6165): loss=3.5374972314581283\n",
      "Stochastic Gradient Descent(6166): loss=0.005112092918826394\n",
      "Stochastic Gradient Descent(6167): loss=0.551157006420847\n",
      "Stochastic Gradient Descent(6168): loss=2.915660239565024\n",
      "Stochastic Gradient Descent(6169): loss=0.1346673802197493\n",
      "Stochastic Gradient Descent(6170): loss=43.87977699555204\n",
      "Stochastic Gradient Descent(6171): loss=14.60161018516631\n",
      "Stochastic Gradient Descent(6172): loss=10.284542188387848\n",
      "Stochastic Gradient Descent(6173): loss=4.356185510468946\n",
      "Stochastic Gradient Descent(6174): loss=19.387161540268007\n",
      "Stochastic Gradient Descent(6175): loss=0.007063839606566037\n",
      "Stochastic Gradient Descent(6176): loss=0.6312538323483812\n",
      "Stochastic Gradient Descent(6177): loss=11.077054292066634\n",
      "Stochastic Gradient Descent(6178): loss=10.662853065944127\n",
      "Stochastic Gradient Descent(6179): loss=6.022662158018174\n",
      "Stochastic Gradient Descent(6180): loss=16.45738674048572\n",
      "Stochastic Gradient Descent(6181): loss=1.5292263105353792\n",
      "Stochastic Gradient Descent(6182): loss=0.5538422171402326\n",
      "Stochastic Gradient Descent(6183): loss=1.2668733961654257\n",
      "Stochastic Gradient Descent(6184): loss=3.11018516740581\n",
      "Stochastic Gradient Descent(6185): loss=0.040334203486781114\n",
      "Stochastic Gradient Descent(6186): loss=11.574274776920692\n",
      "Stochastic Gradient Descent(6187): loss=5.573898029033546\n",
      "Stochastic Gradient Descent(6188): loss=14.206804320201554\n",
      "Stochastic Gradient Descent(6189): loss=0.7014683787814989\n",
      "Stochastic Gradient Descent(6190): loss=0.2376874243319732\n",
      "Stochastic Gradient Descent(6191): loss=1.0857340356244867\n",
      "Stochastic Gradient Descent(6192): loss=1.4187097628103287\n",
      "Stochastic Gradient Descent(6193): loss=1.1749051305531115\n",
      "Stochastic Gradient Descent(6194): loss=3.028950182181937\n",
      "Stochastic Gradient Descent(6195): loss=9.4116870192864\n",
      "Stochastic Gradient Descent(6196): loss=1.585714202860248\n",
      "Stochastic Gradient Descent(6197): loss=14.179785084658295\n",
      "Stochastic Gradient Descent(6198): loss=18.70357262847751\n",
      "Stochastic Gradient Descent(6199): loss=0.2752452422842277\n",
      "Stochastic Gradient Descent(6200): loss=4.614354392847807\n",
      "Stochastic Gradient Descent(6201): loss=4.668489846652379\n",
      "Stochastic Gradient Descent(6202): loss=0.024687157377478793\n",
      "Stochastic Gradient Descent(6203): loss=0.00012474355623678753\n",
      "Stochastic Gradient Descent(6204): loss=4.871487698734679\n",
      "Stochastic Gradient Descent(6205): loss=25.673020982700805\n",
      "Stochastic Gradient Descent(6206): loss=9.2767247670526\n",
      "Stochastic Gradient Descent(6207): loss=1.238945122544376\n",
      "Stochastic Gradient Descent(6208): loss=6.688836428587998\n",
      "Stochastic Gradient Descent(6209): loss=1.9257164906114024\n",
      "Stochastic Gradient Descent(6210): loss=18.91150199299895\n",
      "Stochastic Gradient Descent(6211): loss=7.206755807907775\n",
      "Stochastic Gradient Descent(6212): loss=0.503546881943032\n",
      "Stochastic Gradient Descent(6213): loss=0.0290699340171368\n",
      "Stochastic Gradient Descent(6214): loss=1.657469865395132\n",
      "Stochastic Gradient Descent(6215): loss=21.32001247908286\n",
      "Stochastic Gradient Descent(6216): loss=7.272482644796412\n",
      "Stochastic Gradient Descent(6217): loss=0.1335482723478177\n",
      "Stochastic Gradient Descent(6218): loss=0.29271169695162336\n",
      "Stochastic Gradient Descent(6219): loss=0.5263997498664733\n",
      "Stochastic Gradient Descent(6220): loss=26.881507646665614\n",
      "Stochastic Gradient Descent(6221): loss=1.4428845466997684\n",
      "Stochastic Gradient Descent(6222): loss=2.0643420214845305\n",
      "Stochastic Gradient Descent(6223): loss=3.413475923049897\n",
      "Stochastic Gradient Descent(6224): loss=4.055855881088268\n",
      "Stochastic Gradient Descent(6225): loss=0.031209093959230303\n",
      "Stochastic Gradient Descent(6226): loss=0.2824013934469609\n",
      "Stochastic Gradient Descent(6227): loss=8.088162234133081\n",
      "Stochastic Gradient Descent(6228): loss=4.080117585621106\n",
      "Stochastic Gradient Descent(6229): loss=3.839583761460636\n",
      "Stochastic Gradient Descent(6230): loss=13.456989561977759\n",
      "Stochastic Gradient Descent(6231): loss=7.014477125794683\n",
      "Stochastic Gradient Descent(6232): loss=11.148839069587414\n",
      "Stochastic Gradient Descent(6233): loss=61.46470317119569\n",
      "Stochastic Gradient Descent(6234): loss=12.637981425441893\n",
      "Stochastic Gradient Descent(6235): loss=0.4459985720208337\n",
      "Stochastic Gradient Descent(6236): loss=2.8937785412134382\n",
      "Stochastic Gradient Descent(6237): loss=47.141084240606574\n",
      "Stochastic Gradient Descent(6238): loss=4.6507614544275695\n",
      "Stochastic Gradient Descent(6239): loss=3.4154939292040853\n",
      "Stochastic Gradient Descent(6240): loss=35.68538037344002\n",
      "Stochastic Gradient Descent(6241): loss=0.16080522063965771\n",
      "Stochastic Gradient Descent(6242): loss=3.595295005688171\n",
      "Stochastic Gradient Descent(6243): loss=4.76746043841108\n",
      "Stochastic Gradient Descent(6244): loss=1.1370211460851933\n",
      "Stochastic Gradient Descent(6245): loss=14.990252099689654\n",
      "Stochastic Gradient Descent(6246): loss=3.497849347950857\n",
      "Stochastic Gradient Descent(6247): loss=9.443602052267096\n",
      "Stochastic Gradient Descent(6248): loss=39.62946509130384\n",
      "Stochastic Gradient Descent(6249): loss=5.10320558808169\n",
      "Stochastic Gradient Descent(6250): loss=0.4812647477762501\n",
      "Stochastic Gradient Descent(6251): loss=3.954318787114004\n",
      "Stochastic Gradient Descent(6252): loss=1.7269166934386213\n",
      "Stochastic Gradient Descent(6253): loss=12.777548206136409\n",
      "Stochastic Gradient Descent(6254): loss=0.06731521747413642\n",
      "Stochastic Gradient Descent(6255): loss=26.585849618984646\n",
      "Stochastic Gradient Descent(6256): loss=0.8630770635050744\n",
      "Stochastic Gradient Descent(6257): loss=43.547538704673244\n",
      "Stochastic Gradient Descent(6258): loss=13.013453430676615\n",
      "Stochastic Gradient Descent(6259): loss=4.883147221928995\n",
      "Stochastic Gradient Descent(6260): loss=0.20815016360233232\n",
      "Stochastic Gradient Descent(6261): loss=4.73244173044782\n",
      "Stochastic Gradient Descent(6262): loss=13.93896386726038\n",
      "Stochastic Gradient Descent(6263): loss=0.5972091984212577\n",
      "Stochastic Gradient Descent(6264): loss=1.771534528061066\n",
      "Stochastic Gradient Descent(6265): loss=15.734550615212813\n",
      "Stochastic Gradient Descent(6266): loss=1.9238184075038436\n",
      "Stochastic Gradient Descent(6267): loss=5.1566827023679735\n",
      "Stochastic Gradient Descent(6268): loss=10.087498744387789\n",
      "Stochastic Gradient Descent(6269): loss=6.977849921059773\n",
      "Stochastic Gradient Descent(6270): loss=0.6229350927684703\n",
      "Stochastic Gradient Descent(6271): loss=3.429083942492171\n",
      "Stochastic Gradient Descent(6272): loss=0.17700211735092872\n",
      "Stochastic Gradient Descent(6273): loss=46.13181644934161\n",
      "Stochastic Gradient Descent(6274): loss=2.151917695794521\n",
      "Stochastic Gradient Descent(6275): loss=1.2664980876169272\n",
      "Stochastic Gradient Descent(6276): loss=1.0265067817147266\n",
      "Stochastic Gradient Descent(6277): loss=25.59379895933881\n",
      "Stochastic Gradient Descent(6278): loss=3.7089866625640115\n",
      "Stochastic Gradient Descent(6279): loss=20.357659307737404\n",
      "Stochastic Gradient Descent(6280): loss=15.264768427868777\n",
      "Stochastic Gradient Descent(6281): loss=17.1283585198871\n",
      "Stochastic Gradient Descent(6282): loss=7.292887654672246\n",
      "Stochastic Gradient Descent(6283): loss=1.151061665149537\n",
      "Stochastic Gradient Descent(6284): loss=2.0539041411211953\n",
      "Stochastic Gradient Descent(6285): loss=0.030430099841558287\n",
      "Stochastic Gradient Descent(6286): loss=2.6808594622523576\n",
      "Stochastic Gradient Descent(6287): loss=0.527545449798877\n",
      "Stochastic Gradient Descent(6288): loss=1.0493345272469465\n",
      "Stochastic Gradient Descent(6289): loss=0.02379225388656801\n",
      "Stochastic Gradient Descent(6290): loss=0.22136917502413886\n",
      "Stochastic Gradient Descent(6291): loss=26.89352725891946\n",
      "Stochastic Gradient Descent(6292): loss=20.84340196581589\n",
      "Stochastic Gradient Descent(6293): loss=16.4153736689918\n",
      "Stochastic Gradient Descent(6294): loss=0.09110428141575069\n",
      "Stochastic Gradient Descent(6295): loss=127.08107845231856\n",
      "Stochastic Gradient Descent(6296): loss=0.0017891420842061595\n",
      "Stochastic Gradient Descent(6297): loss=18.76448545968719\n",
      "Stochastic Gradient Descent(6298): loss=0.33345143578814135\n",
      "Stochastic Gradient Descent(6299): loss=7.030410455596806\n",
      "Stochastic Gradient Descent(6300): loss=3.0350731229780687\n",
      "Stochastic Gradient Descent(6301): loss=1.2806295521379665\n",
      "Stochastic Gradient Descent(6302): loss=16.60191818031344\n",
      "Stochastic Gradient Descent(6303): loss=0.024803124690342966\n",
      "Stochastic Gradient Descent(6304): loss=5.124803645210113\n",
      "Stochastic Gradient Descent(6305): loss=0.03669893503646824\n",
      "Stochastic Gradient Descent(6306): loss=0.7918980724521792\n",
      "Stochastic Gradient Descent(6307): loss=1.3237024424995716\n",
      "Stochastic Gradient Descent(6308): loss=2.200435320422256\n",
      "Stochastic Gradient Descent(6309): loss=3.063358972106022\n",
      "Stochastic Gradient Descent(6310): loss=0.6036766838267149\n",
      "Stochastic Gradient Descent(6311): loss=0.38248725376185144\n",
      "Stochastic Gradient Descent(6312): loss=1.7468786909249983\n",
      "Stochastic Gradient Descent(6313): loss=0.4969716304206938\n",
      "Stochastic Gradient Descent(6314): loss=0.19453470582687626\n",
      "Stochastic Gradient Descent(6315): loss=1.001608386276879\n",
      "Stochastic Gradient Descent(6316): loss=0.5844500873067758\n",
      "Stochastic Gradient Descent(6317): loss=0.13482069993301715\n",
      "Stochastic Gradient Descent(6318): loss=12.768370751802587\n",
      "Stochastic Gradient Descent(6319): loss=0.7292232267077987\n",
      "Stochastic Gradient Descent(6320): loss=0.001818876772256177\n",
      "Stochastic Gradient Descent(6321): loss=0.04826610531838282\n",
      "Stochastic Gradient Descent(6322): loss=0.3651929664769448\n",
      "Stochastic Gradient Descent(6323): loss=6.858052268874445\n",
      "Stochastic Gradient Descent(6324): loss=0.8048796934100862\n",
      "Stochastic Gradient Descent(6325): loss=0.37708382767985343\n",
      "Stochastic Gradient Descent(6326): loss=0.05522586101682454\n",
      "Stochastic Gradient Descent(6327): loss=0.6497791792307986\n",
      "Stochastic Gradient Descent(6328): loss=13.783211883949326\n",
      "Stochastic Gradient Descent(6329): loss=9.52607101581891\n",
      "Stochastic Gradient Descent(6330): loss=2.1974498988167785\n",
      "Stochastic Gradient Descent(6331): loss=25.595657527318117\n",
      "Stochastic Gradient Descent(6332): loss=4.81495000050039\n",
      "Stochastic Gradient Descent(6333): loss=0.0025674929224366984\n",
      "Stochastic Gradient Descent(6334): loss=1.587860871922738\n",
      "Stochastic Gradient Descent(6335): loss=0.14228600157090893\n",
      "Stochastic Gradient Descent(6336): loss=0.02881346821520153\n",
      "Stochastic Gradient Descent(6337): loss=0.013263355112021947\n",
      "Stochastic Gradient Descent(6338): loss=26.19146973136527\n",
      "Stochastic Gradient Descent(6339): loss=23.31210582223724\n",
      "Stochastic Gradient Descent(6340): loss=9.693035334487707\n",
      "Stochastic Gradient Descent(6341): loss=3.62942261066487\n",
      "Stochastic Gradient Descent(6342): loss=4.622792208895456\n",
      "Stochastic Gradient Descent(6343): loss=3.1522772534019703\n",
      "Stochastic Gradient Descent(6344): loss=1.4409319779725092\n",
      "Stochastic Gradient Descent(6345): loss=1.0615687121608903\n",
      "Stochastic Gradient Descent(6346): loss=13.323175970241545\n",
      "Stochastic Gradient Descent(6347): loss=0.41265109804802863\n",
      "Stochastic Gradient Descent(6348): loss=2.8604299864304488\n",
      "Stochastic Gradient Descent(6349): loss=1.1785922938307447\n",
      "Stochastic Gradient Descent(6350): loss=2.8736880268827445\n",
      "Stochastic Gradient Descent(6351): loss=0.6204252503214907\n",
      "Stochastic Gradient Descent(6352): loss=6.547514059027723\n",
      "Stochastic Gradient Descent(6353): loss=0.1314950172334952\n",
      "Stochastic Gradient Descent(6354): loss=8.207962363845432\n",
      "Stochastic Gradient Descent(6355): loss=4.772271231549581\n",
      "Stochastic Gradient Descent(6356): loss=21.965943664675212\n",
      "Stochastic Gradient Descent(6357): loss=1.5046716374198623\n",
      "Stochastic Gradient Descent(6358): loss=13.698661989472777\n",
      "Stochastic Gradient Descent(6359): loss=7.664795812421603\n",
      "Stochastic Gradient Descent(6360): loss=1.5576770489100298\n",
      "Stochastic Gradient Descent(6361): loss=18.09916980453788\n",
      "Stochastic Gradient Descent(6362): loss=8.62539872633794\n",
      "Stochastic Gradient Descent(6363): loss=0.4269157287767217\n",
      "Stochastic Gradient Descent(6364): loss=2.0641515255500154\n",
      "Stochastic Gradient Descent(6365): loss=0.4094352847582507\n",
      "Stochastic Gradient Descent(6366): loss=9.821143101995226\n",
      "Stochastic Gradient Descent(6367): loss=0.007127439386202139\n",
      "Stochastic Gradient Descent(6368): loss=0.005418231597172269\n",
      "Stochastic Gradient Descent(6369): loss=23.530065781615786\n",
      "Stochastic Gradient Descent(6370): loss=9.335917316309734\n",
      "Stochastic Gradient Descent(6371): loss=1.3579249129555715\n",
      "Stochastic Gradient Descent(6372): loss=0.746442172912288\n",
      "Stochastic Gradient Descent(6373): loss=2.1510199802819483\n",
      "Stochastic Gradient Descent(6374): loss=0.007671491059719315\n",
      "Stochastic Gradient Descent(6375): loss=1.6634047573830781\n",
      "Stochastic Gradient Descent(6376): loss=4.3585262940885325\n",
      "Stochastic Gradient Descent(6377): loss=1.4633673933656755\n",
      "Stochastic Gradient Descent(6378): loss=1.57389166222393\n",
      "Stochastic Gradient Descent(6379): loss=10.157696286954092\n",
      "Stochastic Gradient Descent(6380): loss=0.02079889840784665\n",
      "Stochastic Gradient Descent(6381): loss=0.8349106067229194\n",
      "Stochastic Gradient Descent(6382): loss=4.147673906472511\n",
      "Stochastic Gradient Descent(6383): loss=2.2119831148020612\n",
      "Stochastic Gradient Descent(6384): loss=0.696339736203777\n",
      "Stochastic Gradient Descent(6385): loss=9.948493874780086\n",
      "Stochastic Gradient Descent(6386): loss=8.981901530683674\n",
      "Stochastic Gradient Descent(6387): loss=17.325290673594626\n",
      "Stochastic Gradient Descent(6388): loss=6.293597070053586\n",
      "Stochastic Gradient Descent(6389): loss=0.41993505847752555\n",
      "Stochastic Gradient Descent(6390): loss=6.493603442827085\n",
      "Stochastic Gradient Descent(6391): loss=6.0565179301755006\n",
      "Stochastic Gradient Descent(6392): loss=0.2065242415966003\n",
      "Stochastic Gradient Descent(6393): loss=10.119159436856798\n",
      "Stochastic Gradient Descent(6394): loss=0.8660370802050691\n",
      "Stochastic Gradient Descent(6395): loss=13.083031188554571\n",
      "Stochastic Gradient Descent(6396): loss=0.7159033680301964\n",
      "Stochastic Gradient Descent(6397): loss=0.6492326989089366\n",
      "Stochastic Gradient Descent(6398): loss=2.834585607622737\n",
      "Stochastic Gradient Descent(6399): loss=0.8622095743593668\n",
      "Stochastic Gradient Descent(6400): loss=9.902297261916871\n",
      "Stochastic Gradient Descent(6401): loss=2.4154214301398187\n",
      "Stochastic Gradient Descent(6402): loss=19.810151659784204\n",
      "Stochastic Gradient Descent(6403): loss=0.546646231575612\n",
      "Stochastic Gradient Descent(6404): loss=16.634809515775963\n",
      "Stochastic Gradient Descent(6405): loss=0.9764608631995979\n",
      "Stochastic Gradient Descent(6406): loss=17.105159115663394\n",
      "Stochastic Gradient Descent(6407): loss=0.06447329740947612\n",
      "Stochastic Gradient Descent(6408): loss=0.05574933486116384\n",
      "Stochastic Gradient Descent(6409): loss=0.05805313685046102\n",
      "Stochastic Gradient Descent(6410): loss=3.629026808753842\n",
      "Stochastic Gradient Descent(6411): loss=19.498063917945093\n",
      "Stochastic Gradient Descent(6412): loss=3.2010664996231077\n",
      "Stochastic Gradient Descent(6413): loss=2.1663731328825957\n",
      "Stochastic Gradient Descent(6414): loss=2.127921077288429\n",
      "Stochastic Gradient Descent(6415): loss=10.621437934285655\n",
      "Stochastic Gradient Descent(6416): loss=9.409291566631639\n",
      "Stochastic Gradient Descent(6417): loss=5.897450543694456\n",
      "Stochastic Gradient Descent(6418): loss=9.748620170795807\n",
      "Stochastic Gradient Descent(6419): loss=0.6185756409941651\n",
      "Stochastic Gradient Descent(6420): loss=0.20102006147824023\n",
      "Stochastic Gradient Descent(6421): loss=14.812384029448708\n",
      "Stochastic Gradient Descent(6422): loss=0.033897488863724214\n",
      "Stochastic Gradient Descent(6423): loss=0.7098119399467618\n",
      "Stochastic Gradient Descent(6424): loss=44.57404125767651\n",
      "Stochastic Gradient Descent(6425): loss=30.11113611763805\n",
      "Stochastic Gradient Descent(6426): loss=3.5003559525992762\n",
      "Stochastic Gradient Descent(6427): loss=59.694251632124704\n",
      "Stochastic Gradient Descent(6428): loss=11.314012413211413\n",
      "Stochastic Gradient Descent(6429): loss=46.06814769252031\n",
      "Stochastic Gradient Descent(6430): loss=20.58581847850544\n",
      "Stochastic Gradient Descent(6431): loss=1.211818552003886\n",
      "Stochastic Gradient Descent(6432): loss=3.3506549718023932\n",
      "Stochastic Gradient Descent(6433): loss=1.0412665971784258\n",
      "Stochastic Gradient Descent(6434): loss=21.284868537708636\n",
      "Stochastic Gradient Descent(6435): loss=0.4041322520017264\n",
      "Stochastic Gradient Descent(6436): loss=0.07111107741851544\n",
      "Stochastic Gradient Descent(6437): loss=1.2850167386088094\n",
      "Stochastic Gradient Descent(6438): loss=0.03342458510770384\n",
      "Stochastic Gradient Descent(6439): loss=1.2076946742523784\n",
      "Stochastic Gradient Descent(6440): loss=1.0822690806955566\n",
      "Stochastic Gradient Descent(6441): loss=9.198361540516384\n",
      "Stochastic Gradient Descent(6442): loss=1.4079608112682231\n",
      "Stochastic Gradient Descent(6443): loss=0.644716464019701\n",
      "Stochastic Gradient Descent(6444): loss=0.39653685369117825\n",
      "Stochastic Gradient Descent(6445): loss=23.859596696689138\n",
      "Stochastic Gradient Descent(6446): loss=0.10980846280203302\n",
      "Stochastic Gradient Descent(6447): loss=6.195618679325422\n",
      "Stochastic Gradient Descent(6448): loss=6.063395358032431\n",
      "Stochastic Gradient Descent(6449): loss=50.175434542079124\n",
      "Stochastic Gradient Descent(6450): loss=0.012406028257030521\n",
      "Stochastic Gradient Descent(6451): loss=8.432431322991134\n",
      "Stochastic Gradient Descent(6452): loss=3.583062855392673\n",
      "Stochastic Gradient Descent(6453): loss=0.1277688605940568\n",
      "Stochastic Gradient Descent(6454): loss=0.00101602164378615\n",
      "Stochastic Gradient Descent(6455): loss=2.1926290544339317\n",
      "Stochastic Gradient Descent(6456): loss=1.7943363702641906\n",
      "Stochastic Gradient Descent(6457): loss=0.8510884689739555\n",
      "Stochastic Gradient Descent(6458): loss=5.626849663109363\n",
      "Stochastic Gradient Descent(6459): loss=11.418648541969835\n",
      "Stochastic Gradient Descent(6460): loss=0.058451257250390555\n",
      "Stochastic Gradient Descent(6461): loss=9.4657062890061\n",
      "Stochastic Gradient Descent(6462): loss=0.3235563071562548\n",
      "Stochastic Gradient Descent(6463): loss=0.3325765800917106\n",
      "Stochastic Gradient Descent(6464): loss=16.358047350792827\n",
      "Stochastic Gradient Descent(6465): loss=0.0022216990431476406\n",
      "Stochastic Gradient Descent(6466): loss=8.461094930488292\n",
      "Stochastic Gradient Descent(6467): loss=18.611003667343194\n",
      "Stochastic Gradient Descent(6468): loss=4.285979460814756\n",
      "Stochastic Gradient Descent(6469): loss=13.557104198737232\n",
      "Stochastic Gradient Descent(6470): loss=0.9709445381078234\n",
      "Stochastic Gradient Descent(6471): loss=1.590494667477639\n",
      "Stochastic Gradient Descent(6472): loss=3.405133635666372\n",
      "Stochastic Gradient Descent(6473): loss=1.2170504419535284\n",
      "Stochastic Gradient Descent(6474): loss=0.23675954700503168\n",
      "Stochastic Gradient Descent(6475): loss=47.04295133419804\n",
      "Stochastic Gradient Descent(6476): loss=16.577392123340196\n",
      "Stochastic Gradient Descent(6477): loss=25.6286768995232\n",
      "Stochastic Gradient Descent(6478): loss=1.7790480915734275\n",
      "Stochastic Gradient Descent(6479): loss=0.06609031543953069\n",
      "Stochastic Gradient Descent(6480): loss=13.721163708895812\n",
      "Stochastic Gradient Descent(6481): loss=0.6834497338022552\n",
      "Stochastic Gradient Descent(6482): loss=9.00466313511171\n",
      "Stochastic Gradient Descent(6483): loss=0.3307612177656519\n",
      "Stochastic Gradient Descent(6484): loss=1.079881340837259\n",
      "Stochastic Gradient Descent(6485): loss=4.048337910736392\n",
      "Stochastic Gradient Descent(6486): loss=0.3785657010749122\n",
      "Stochastic Gradient Descent(6487): loss=12.375961423167382\n",
      "Stochastic Gradient Descent(6488): loss=8.897005729015541\n",
      "Stochastic Gradient Descent(6489): loss=11.556290125659274\n",
      "Stochastic Gradient Descent(6490): loss=0.2237057820694882\n",
      "Stochastic Gradient Descent(6491): loss=0.7709485874994363\n",
      "Stochastic Gradient Descent(6492): loss=0.6621626213480668\n",
      "Stochastic Gradient Descent(6493): loss=1.325158975551608\n",
      "Stochastic Gradient Descent(6494): loss=4.844759650335757\n",
      "Stochastic Gradient Descent(6495): loss=12.886039365352303\n",
      "Stochastic Gradient Descent(6496): loss=7.088800715217245\n",
      "Stochastic Gradient Descent(6497): loss=0.8996782127369404\n",
      "Stochastic Gradient Descent(6498): loss=14.842726196535427\n",
      "Stochastic Gradient Descent(6499): loss=3.952029089969309\n",
      "Stochastic Gradient Descent(6500): loss=1.058036946355988\n",
      "Stochastic Gradient Descent(6501): loss=2.0170016953349115\n",
      "Stochastic Gradient Descent(6502): loss=0.8754347449508889\n",
      "Stochastic Gradient Descent(6503): loss=0.05275695874639597\n",
      "Stochastic Gradient Descent(6504): loss=0.12639123397811097\n",
      "Stochastic Gradient Descent(6505): loss=1.092822488668112\n",
      "Stochastic Gradient Descent(6506): loss=5.3245404045332485\n",
      "Stochastic Gradient Descent(6507): loss=0.009122804138939044\n",
      "Stochastic Gradient Descent(6508): loss=1.0606757304379066e-07\n",
      "Stochastic Gradient Descent(6509): loss=51.424315792133655\n",
      "Stochastic Gradient Descent(6510): loss=46.961003141004376\n",
      "Stochastic Gradient Descent(6511): loss=2.5593265456666208\n",
      "Stochastic Gradient Descent(6512): loss=0.0858083835338047\n",
      "Stochastic Gradient Descent(6513): loss=2.3138100090569833\n",
      "Stochastic Gradient Descent(6514): loss=1.4075437085315752\n",
      "Stochastic Gradient Descent(6515): loss=4.446525605481475\n",
      "Stochastic Gradient Descent(6516): loss=2.617218200013103\n",
      "Stochastic Gradient Descent(6517): loss=36.6508454737571\n",
      "Stochastic Gradient Descent(6518): loss=0.1616983151364741\n",
      "Stochastic Gradient Descent(6519): loss=6.181438665718666\n",
      "Stochastic Gradient Descent(6520): loss=45.70963169574155\n",
      "Stochastic Gradient Descent(6521): loss=1.5448087441959137\n",
      "Stochastic Gradient Descent(6522): loss=5.418508342346627\n",
      "Stochastic Gradient Descent(6523): loss=0.20516099055713927\n",
      "Stochastic Gradient Descent(6524): loss=5.121440874411695\n",
      "Stochastic Gradient Descent(6525): loss=1.5263130565530694\n",
      "Stochastic Gradient Descent(6526): loss=0.1809763107710658\n",
      "Stochastic Gradient Descent(6527): loss=6.435392619760866\n",
      "Stochastic Gradient Descent(6528): loss=0.38472337454781946\n",
      "Stochastic Gradient Descent(6529): loss=5.144670661324187\n",
      "Stochastic Gradient Descent(6530): loss=28.52473320054479\n",
      "Stochastic Gradient Descent(6531): loss=12.234970256607381\n",
      "Stochastic Gradient Descent(6532): loss=3.413360042687685\n",
      "Stochastic Gradient Descent(6533): loss=0.01183873719099722\n",
      "Stochastic Gradient Descent(6534): loss=0.5938545786542422\n",
      "Stochastic Gradient Descent(6535): loss=0.20037173111283793\n",
      "Stochastic Gradient Descent(6536): loss=21.963683389015245\n",
      "Stochastic Gradient Descent(6537): loss=38.147590158654324\n",
      "Stochastic Gradient Descent(6538): loss=0.604599765706398\n",
      "Stochastic Gradient Descent(6539): loss=5.422567844188198\n",
      "Stochastic Gradient Descent(6540): loss=6.348969470965844\n",
      "Stochastic Gradient Descent(6541): loss=1.3464706606408994\n",
      "Stochastic Gradient Descent(6542): loss=6.033176555231536\n",
      "Stochastic Gradient Descent(6543): loss=17.407965116990724\n",
      "Stochastic Gradient Descent(6544): loss=5.027340023749362\n",
      "Stochastic Gradient Descent(6545): loss=15.423309343396367\n",
      "Stochastic Gradient Descent(6546): loss=2.0705640623093955\n",
      "Stochastic Gradient Descent(6547): loss=1.0898671542361893\n",
      "Stochastic Gradient Descent(6548): loss=5.633211395586343\n",
      "Stochastic Gradient Descent(6549): loss=1.6763546812549897\n",
      "Stochastic Gradient Descent(6550): loss=0.4366855761018003\n",
      "Stochastic Gradient Descent(6551): loss=0.054801261532893074\n",
      "Stochastic Gradient Descent(6552): loss=1.8230666583582757\n",
      "Stochastic Gradient Descent(6553): loss=9.563700691522175\n",
      "Stochastic Gradient Descent(6554): loss=0.006979350405817293\n",
      "Stochastic Gradient Descent(6555): loss=0.9226945271336029\n",
      "Stochastic Gradient Descent(6556): loss=14.351027339528699\n",
      "Stochastic Gradient Descent(6557): loss=1.3002984074463364\n",
      "Stochastic Gradient Descent(6558): loss=18.61482084585714\n",
      "Stochastic Gradient Descent(6559): loss=18.078524396438365\n",
      "Stochastic Gradient Descent(6560): loss=0.1068673068227936\n",
      "Stochastic Gradient Descent(6561): loss=5.261145599668592\n",
      "Stochastic Gradient Descent(6562): loss=0.5211817106983526\n",
      "Stochastic Gradient Descent(6563): loss=0.5904108816479299\n",
      "Stochastic Gradient Descent(6564): loss=12.0092769735048\n",
      "Stochastic Gradient Descent(6565): loss=0.6971906424637767\n",
      "Stochastic Gradient Descent(6566): loss=21.83148338938312\n",
      "Stochastic Gradient Descent(6567): loss=5.510904677118289\n",
      "Stochastic Gradient Descent(6568): loss=2.5160890165014758\n",
      "Stochastic Gradient Descent(6569): loss=11.859125770445134\n",
      "Stochastic Gradient Descent(6570): loss=0.669243988400611\n",
      "Stochastic Gradient Descent(6571): loss=0.16872299973936847\n",
      "Stochastic Gradient Descent(6572): loss=8.062129743361634\n",
      "Stochastic Gradient Descent(6573): loss=0.11368937736235951\n",
      "Stochastic Gradient Descent(6574): loss=10.041199071958447\n",
      "Stochastic Gradient Descent(6575): loss=5.869279370276283\n",
      "Stochastic Gradient Descent(6576): loss=0.16159824530884612\n",
      "Stochastic Gradient Descent(6577): loss=22.30379668327098\n",
      "Stochastic Gradient Descent(6578): loss=37.031001051844854\n",
      "Stochastic Gradient Descent(6579): loss=0.7113036356842249\n",
      "Stochastic Gradient Descent(6580): loss=11.572406173148215\n",
      "Stochastic Gradient Descent(6581): loss=2.4997233421878406\n",
      "Stochastic Gradient Descent(6582): loss=8.688393271726525\n",
      "Stochastic Gradient Descent(6583): loss=0.012721971753162205\n",
      "Stochastic Gradient Descent(6584): loss=0.6454603131669528\n",
      "Stochastic Gradient Descent(6585): loss=1.596037291801195\n",
      "Stochastic Gradient Descent(6586): loss=12.940562767533422\n",
      "Stochastic Gradient Descent(6587): loss=1.099033924490251\n",
      "Stochastic Gradient Descent(6588): loss=14.091388705731077\n",
      "Stochastic Gradient Descent(6589): loss=13.847626198445797\n",
      "Stochastic Gradient Descent(6590): loss=2.857606982265063\n",
      "Stochastic Gradient Descent(6591): loss=0.6950340665929977\n",
      "Stochastic Gradient Descent(6592): loss=6.0390841979706025\n",
      "Stochastic Gradient Descent(6593): loss=25.603855225909644\n",
      "Stochastic Gradient Descent(6594): loss=7.72293832193248\n",
      "Stochastic Gradient Descent(6595): loss=0.717471854190889\n",
      "Stochastic Gradient Descent(6596): loss=17.985951951957258\n",
      "Stochastic Gradient Descent(6597): loss=10.362334193916812\n",
      "Stochastic Gradient Descent(6598): loss=2.9905731297911915\n",
      "Stochastic Gradient Descent(6599): loss=2.8507750567110604\n",
      "Stochastic Gradient Descent(6600): loss=0.05442833594947168\n",
      "Stochastic Gradient Descent(6601): loss=8.795883165015638\n",
      "Stochastic Gradient Descent(6602): loss=0.3736060881755122\n",
      "Stochastic Gradient Descent(6603): loss=2.0019538188144006\n",
      "Stochastic Gradient Descent(6604): loss=3.2511808719234248\n",
      "Stochastic Gradient Descent(6605): loss=2.2506744990355165\n",
      "Stochastic Gradient Descent(6606): loss=1.106802346062997\n",
      "Stochastic Gradient Descent(6607): loss=8.889904393182972\n",
      "Stochastic Gradient Descent(6608): loss=18.769084236596203\n",
      "Stochastic Gradient Descent(6609): loss=0.45633808405881116\n",
      "Stochastic Gradient Descent(6610): loss=0.05824302157556885\n",
      "Stochastic Gradient Descent(6611): loss=0.2313434357015695\n",
      "Stochastic Gradient Descent(6612): loss=1.6338293637986268\n",
      "Stochastic Gradient Descent(6613): loss=12.45210705782221\n",
      "Stochastic Gradient Descent(6614): loss=3.820209082746064\n",
      "Stochastic Gradient Descent(6615): loss=18.8643493801551\n",
      "Stochastic Gradient Descent(6616): loss=0.04437545430304834\n",
      "Stochastic Gradient Descent(6617): loss=1.333652778025019\n",
      "Stochastic Gradient Descent(6618): loss=5.9797745682285095\n",
      "Stochastic Gradient Descent(6619): loss=0.2673527389613781\n",
      "Stochastic Gradient Descent(6620): loss=2.599738100295451\n",
      "Stochastic Gradient Descent(6621): loss=0.9825460373564898\n",
      "Stochastic Gradient Descent(6622): loss=0.5855046669805994\n",
      "Stochastic Gradient Descent(6623): loss=0.047924349244208575\n",
      "Stochastic Gradient Descent(6624): loss=4.565530659577667\n",
      "Stochastic Gradient Descent(6625): loss=13.289795796579558\n",
      "Stochastic Gradient Descent(6626): loss=3.4782195044194446\n",
      "Stochastic Gradient Descent(6627): loss=0.06261750491823556\n",
      "Stochastic Gradient Descent(6628): loss=1.374024096729196\n",
      "Stochastic Gradient Descent(6629): loss=20.028703560717094\n",
      "Stochastic Gradient Descent(6630): loss=6.0974971431583125\n",
      "Stochastic Gradient Descent(6631): loss=0.051762015734056324\n",
      "Stochastic Gradient Descent(6632): loss=0.5629509684743423\n",
      "Stochastic Gradient Descent(6633): loss=0.48457951728569154\n",
      "Stochastic Gradient Descent(6634): loss=1.3209499201610342\n",
      "Stochastic Gradient Descent(6635): loss=13.414400859935306\n",
      "Stochastic Gradient Descent(6636): loss=9.84584933354241\n",
      "Stochastic Gradient Descent(6637): loss=3.4509437978979025\n",
      "Stochastic Gradient Descent(6638): loss=26.51417038288791\n",
      "Stochastic Gradient Descent(6639): loss=0.16343784654342133\n",
      "Stochastic Gradient Descent(6640): loss=0.4234349451654666\n",
      "Stochastic Gradient Descent(6641): loss=0.11161824000803769\n",
      "Stochastic Gradient Descent(6642): loss=1.5333292690192584\n",
      "Stochastic Gradient Descent(6643): loss=1.8903871280265112\n",
      "Stochastic Gradient Descent(6644): loss=26.6958405360436\n",
      "Stochastic Gradient Descent(6645): loss=7.386222435762254\n",
      "Stochastic Gradient Descent(6646): loss=0.8898426640892724\n",
      "Stochastic Gradient Descent(6647): loss=0.24845241556386669\n",
      "Stochastic Gradient Descent(6648): loss=0.2565359485053963\n",
      "Stochastic Gradient Descent(6649): loss=14.288411793808535\n",
      "Stochastic Gradient Descent(6650): loss=1.9626024512897997\n",
      "Stochastic Gradient Descent(6651): loss=3.260290233234515\n",
      "Stochastic Gradient Descent(6652): loss=0.4783843667968662\n",
      "Stochastic Gradient Descent(6653): loss=3.5391587036875483\n",
      "Stochastic Gradient Descent(6654): loss=10.268869917480556\n",
      "Stochastic Gradient Descent(6655): loss=3.0192483687513985\n",
      "Stochastic Gradient Descent(6656): loss=13.989188086261574\n",
      "Stochastic Gradient Descent(6657): loss=2.771230812821379\n",
      "Stochastic Gradient Descent(6658): loss=8.620570381306043\n",
      "Stochastic Gradient Descent(6659): loss=21.46303979683465\n",
      "Stochastic Gradient Descent(6660): loss=8.03887087498528\n",
      "Stochastic Gradient Descent(6661): loss=12.034203951368788\n",
      "Stochastic Gradient Descent(6662): loss=0.14074615165005017\n",
      "Stochastic Gradient Descent(6663): loss=0.18228253897583338\n",
      "Stochastic Gradient Descent(6664): loss=31.5912698466565\n",
      "Stochastic Gradient Descent(6665): loss=0.0002224022408366712\n",
      "Stochastic Gradient Descent(6666): loss=12.74813165684262\n",
      "Stochastic Gradient Descent(6667): loss=18.77099839999448\n",
      "Stochastic Gradient Descent(6668): loss=0.37889916531615664\n",
      "Stochastic Gradient Descent(6669): loss=8.7640152547939\n",
      "Stochastic Gradient Descent(6670): loss=0.02537742900407375\n",
      "Stochastic Gradient Descent(6671): loss=1.4428378867130442\n",
      "Stochastic Gradient Descent(6672): loss=26.03452647958289\n",
      "Stochastic Gradient Descent(6673): loss=0.50557943284529\n",
      "Stochastic Gradient Descent(6674): loss=5.8884881439238725\n",
      "Stochastic Gradient Descent(6675): loss=14.106037112665499\n",
      "Stochastic Gradient Descent(6676): loss=0.1772310618322339\n",
      "Stochastic Gradient Descent(6677): loss=22.442173878298043\n",
      "Stochastic Gradient Descent(6678): loss=0.19546185282510511\n",
      "Stochastic Gradient Descent(6679): loss=4.234188625305975\n",
      "Stochastic Gradient Descent(6680): loss=1.2612893568890493\n",
      "Stochastic Gradient Descent(6681): loss=11.585613471805893\n",
      "Stochastic Gradient Descent(6682): loss=1.8062120514566722\n",
      "Stochastic Gradient Descent(6683): loss=0.6382756360634322\n",
      "Stochastic Gradient Descent(6684): loss=1.4286518117069122\n",
      "Stochastic Gradient Descent(6685): loss=2.9290541337343283\n",
      "Stochastic Gradient Descent(6686): loss=0.06352683201873491\n",
      "Stochastic Gradient Descent(6687): loss=2.782833574217566\n",
      "Stochastic Gradient Descent(6688): loss=10.246554705955926\n",
      "Stochastic Gradient Descent(6689): loss=12.778136774338085\n",
      "Stochastic Gradient Descent(6690): loss=0.0041685189424884705\n",
      "Stochastic Gradient Descent(6691): loss=9.697630069395554\n",
      "Stochastic Gradient Descent(6692): loss=0.06355314189280205\n",
      "Stochastic Gradient Descent(6693): loss=1.5234875250479853\n",
      "Stochastic Gradient Descent(6694): loss=4.048399415790169\n",
      "Stochastic Gradient Descent(6695): loss=0.3479576604048143\n",
      "Stochastic Gradient Descent(6696): loss=1.8133129278102778\n",
      "Stochastic Gradient Descent(6697): loss=0.001499612229729315\n",
      "Stochastic Gradient Descent(6698): loss=19.842525742653063\n",
      "Stochastic Gradient Descent(6699): loss=0.6869997719999333\n",
      "Stochastic Gradient Descent(6700): loss=1.844727291280916\n",
      "Stochastic Gradient Descent(6701): loss=11.136572011471245\n",
      "Stochastic Gradient Descent(6702): loss=3.1300873093684833\n",
      "Stochastic Gradient Descent(6703): loss=11.893878507345617\n",
      "Stochastic Gradient Descent(6704): loss=5.288063869913032\n",
      "Stochastic Gradient Descent(6705): loss=2.0293049977626354\n",
      "Stochastic Gradient Descent(6706): loss=2.116711591317032\n",
      "Stochastic Gradient Descent(6707): loss=1.8817430933993753\n",
      "Stochastic Gradient Descent(6708): loss=24.392954919310494\n",
      "Stochastic Gradient Descent(6709): loss=1.5759039410927504\n",
      "Stochastic Gradient Descent(6710): loss=0.044661856651888404\n",
      "Stochastic Gradient Descent(6711): loss=4.397639350657921\n",
      "Stochastic Gradient Descent(6712): loss=2.820731921315073\n",
      "Stochastic Gradient Descent(6713): loss=10.3687763586726\n",
      "Stochastic Gradient Descent(6714): loss=0.16905346886916664\n",
      "Stochastic Gradient Descent(6715): loss=0.4529894668329538\n",
      "Stochastic Gradient Descent(6716): loss=0.7530486432725838\n",
      "Stochastic Gradient Descent(6717): loss=2.215489150132359\n",
      "Stochastic Gradient Descent(6718): loss=1.3289044538482386\n",
      "Stochastic Gradient Descent(6719): loss=14.42317644087718\n",
      "Stochastic Gradient Descent(6720): loss=0.05588661930977334\n",
      "Stochastic Gradient Descent(6721): loss=1.5430104018010964\n",
      "Stochastic Gradient Descent(6722): loss=9.273520876497884\n",
      "Stochastic Gradient Descent(6723): loss=2.220694594451546\n",
      "Stochastic Gradient Descent(6724): loss=0.09318364512791416\n",
      "Stochastic Gradient Descent(6725): loss=16.087738146626965\n",
      "Stochastic Gradient Descent(6726): loss=17.04974983697448\n",
      "Stochastic Gradient Descent(6727): loss=0.010846942529096052\n",
      "Stochastic Gradient Descent(6728): loss=11.368042524278794\n",
      "Stochastic Gradient Descent(6729): loss=0.6141779732410202\n",
      "Stochastic Gradient Descent(6730): loss=2.7394055645431736\n",
      "Stochastic Gradient Descent(6731): loss=29.787227897995752\n",
      "Stochastic Gradient Descent(6732): loss=0.07561338385509667\n",
      "Stochastic Gradient Descent(6733): loss=0.12006296616821444\n",
      "Stochastic Gradient Descent(6734): loss=3.637431889349708\n",
      "Stochastic Gradient Descent(6735): loss=16.068731944497312\n",
      "Stochastic Gradient Descent(6736): loss=3.7404858937153533\n",
      "Stochastic Gradient Descent(6737): loss=5.878061038810835\n",
      "Stochastic Gradient Descent(6738): loss=9.333829808361623\n",
      "Stochastic Gradient Descent(6739): loss=10.92338662880243\n",
      "Stochastic Gradient Descent(6740): loss=0.004618669744202041\n",
      "Stochastic Gradient Descent(6741): loss=3.057210884663469\n",
      "Stochastic Gradient Descent(6742): loss=5.47612067654461\n",
      "Stochastic Gradient Descent(6743): loss=2.592411717940515\n",
      "Stochastic Gradient Descent(6744): loss=0.5962648586149034\n",
      "Stochastic Gradient Descent(6745): loss=9.661661550351166\n",
      "Stochastic Gradient Descent(6746): loss=0.8715344255482323\n",
      "Stochastic Gradient Descent(6747): loss=3.5496717445930153\n",
      "Stochastic Gradient Descent(6748): loss=0.5230646971426179\n",
      "Stochastic Gradient Descent(6749): loss=2.039170441166289\n",
      "Stochastic Gradient Descent(6750): loss=0.4421537779999746\n",
      "Stochastic Gradient Descent(6751): loss=0.1037496373620134\n",
      "Stochastic Gradient Descent(6752): loss=3.8730167002012155\n",
      "Stochastic Gradient Descent(6753): loss=0.645763492355265\n",
      "Stochastic Gradient Descent(6754): loss=40.6340755342038\n",
      "Stochastic Gradient Descent(6755): loss=0.898469126475866\n",
      "Stochastic Gradient Descent(6756): loss=1.1921525170261382\n",
      "Stochastic Gradient Descent(6757): loss=0.8659071274752174\n",
      "Stochastic Gradient Descent(6758): loss=16.826891210206274\n",
      "Stochastic Gradient Descent(6759): loss=0.922625392010141\n",
      "Stochastic Gradient Descent(6760): loss=5.250250361539436\n",
      "Stochastic Gradient Descent(6761): loss=2.290372605121802\n",
      "Stochastic Gradient Descent(6762): loss=7.060262087103082\n",
      "Stochastic Gradient Descent(6763): loss=1.491665273991896\n",
      "Stochastic Gradient Descent(6764): loss=2.34653549873511\n",
      "Stochastic Gradient Descent(6765): loss=1.8042009234702279\n",
      "Stochastic Gradient Descent(6766): loss=4.101698725727553\n",
      "Stochastic Gradient Descent(6767): loss=1.453458641249958\n",
      "Stochastic Gradient Descent(6768): loss=4.649265437251022\n",
      "Stochastic Gradient Descent(6769): loss=30.645513824667827\n",
      "Stochastic Gradient Descent(6770): loss=5.024316544552541\n",
      "Stochastic Gradient Descent(6771): loss=0.005717395592156688\n",
      "Stochastic Gradient Descent(6772): loss=7.271497323940795\n",
      "Stochastic Gradient Descent(6773): loss=0.21539793403148194\n",
      "Stochastic Gradient Descent(6774): loss=0.7969576360554924\n",
      "Stochastic Gradient Descent(6775): loss=5.090820497121286\n",
      "Stochastic Gradient Descent(6776): loss=0.014406019650079172\n",
      "Stochastic Gradient Descent(6777): loss=0.5980457764972555\n",
      "Stochastic Gradient Descent(6778): loss=1.7133675041157284\n",
      "Stochastic Gradient Descent(6779): loss=0.14287932514162566\n",
      "Stochastic Gradient Descent(6780): loss=2.444291867355076\n",
      "Stochastic Gradient Descent(6781): loss=0.19068408998187766\n",
      "Stochastic Gradient Descent(6782): loss=0.04615080814401982\n",
      "Stochastic Gradient Descent(6783): loss=0.6395357616902836\n",
      "Stochastic Gradient Descent(6784): loss=2.569415832736631\n",
      "Stochastic Gradient Descent(6785): loss=0.20269734272953585\n",
      "Stochastic Gradient Descent(6786): loss=1.231698484392362\n",
      "Stochastic Gradient Descent(6787): loss=0.13830368655048722\n",
      "Stochastic Gradient Descent(6788): loss=0.7548449757038779\n",
      "Stochastic Gradient Descent(6789): loss=21.73338934485889\n",
      "Stochastic Gradient Descent(6790): loss=0.7407681758170928\n",
      "Stochastic Gradient Descent(6791): loss=2.128872632128582\n",
      "Stochastic Gradient Descent(6792): loss=1.6713801579718923\n",
      "Stochastic Gradient Descent(6793): loss=2.269881839116073\n",
      "Stochastic Gradient Descent(6794): loss=6.246251887670062\n",
      "Stochastic Gradient Descent(6795): loss=0.05261002655099029\n",
      "Stochastic Gradient Descent(6796): loss=5.223620408355783\n",
      "Stochastic Gradient Descent(6797): loss=3.4355741671042703\n",
      "Stochastic Gradient Descent(6798): loss=39.048282402170905\n",
      "Stochastic Gradient Descent(6799): loss=1.8189989189130793\n",
      "Stochastic Gradient Descent(6800): loss=0.9868785145115903\n",
      "Stochastic Gradient Descent(6801): loss=5.227056268313898\n",
      "Stochastic Gradient Descent(6802): loss=5.80794479048933\n",
      "Stochastic Gradient Descent(6803): loss=0.0811704507770486\n",
      "Stochastic Gradient Descent(6804): loss=3.5685174390172\n",
      "Stochastic Gradient Descent(6805): loss=2.8357438765087735\n",
      "Stochastic Gradient Descent(6806): loss=1.0146061936253237\n",
      "Stochastic Gradient Descent(6807): loss=8.510893372808747\n",
      "Stochastic Gradient Descent(6808): loss=1.9104619140300927\n",
      "Stochastic Gradient Descent(6809): loss=2.122166658925936\n",
      "Stochastic Gradient Descent(6810): loss=11.872657072186142\n",
      "Stochastic Gradient Descent(6811): loss=13.00661503763295\n",
      "Stochastic Gradient Descent(6812): loss=4.881139995058366\n",
      "Stochastic Gradient Descent(6813): loss=23.203959686322076\n",
      "Stochastic Gradient Descent(6814): loss=0.590022304108472\n",
      "Stochastic Gradient Descent(6815): loss=19.180401266841454\n",
      "Stochastic Gradient Descent(6816): loss=20.174211206002692\n",
      "Stochastic Gradient Descent(6817): loss=5.161807475154146\n",
      "Stochastic Gradient Descent(6818): loss=5.918978935869293\n",
      "Stochastic Gradient Descent(6819): loss=8.705298981460235\n",
      "Stochastic Gradient Descent(6820): loss=0.43540796055294667\n",
      "Stochastic Gradient Descent(6821): loss=5.063943780065017\n",
      "Stochastic Gradient Descent(6822): loss=11.055993758374825\n",
      "Stochastic Gradient Descent(6823): loss=13.182200147419621\n",
      "Stochastic Gradient Descent(6824): loss=4.0035345829626445\n",
      "Stochastic Gradient Descent(6825): loss=3.7950053848251293\n",
      "Stochastic Gradient Descent(6826): loss=5.864829180816473e-06\n",
      "Stochastic Gradient Descent(6827): loss=3.875412322463603\n",
      "Stochastic Gradient Descent(6828): loss=9.265510107925246\n",
      "Stochastic Gradient Descent(6829): loss=0.49196011317382404\n",
      "Stochastic Gradient Descent(6830): loss=20.500524820676354\n",
      "Stochastic Gradient Descent(6831): loss=0.0013185931083943168\n",
      "Stochastic Gradient Descent(6832): loss=7.615380084720905\n",
      "Stochastic Gradient Descent(6833): loss=0.1600071989290616\n",
      "Stochastic Gradient Descent(6834): loss=59.9116724555944\n",
      "Stochastic Gradient Descent(6835): loss=0.4340223061558473\n",
      "Stochastic Gradient Descent(6836): loss=0.21211917179993012\n",
      "Stochastic Gradient Descent(6837): loss=21.36216470874456\n",
      "Stochastic Gradient Descent(6838): loss=0.15134528959134871\n",
      "Stochastic Gradient Descent(6839): loss=4.294097269719544\n",
      "Stochastic Gradient Descent(6840): loss=9.945948050372984\n",
      "Stochastic Gradient Descent(6841): loss=0.020659324331836985\n",
      "Stochastic Gradient Descent(6842): loss=0.9332462752478313\n",
      "Stochastic Gradient Descent(6843): loss=41.78249146894589\n",
      "Stochastic Gradient Descent(6844): loss=2.913118429244593\n",
      "Stochastic Gradient Descent(6845): loss=9.763778726919686\n",
      "Stochastic Gradient Descent(6846): loss=0.5269843593734952\n",
      "Stochastic Gradient Descent(6847): loss=20.188895041805473\n",
      "Stochastic Gradient Descent(6848): loss=0.037379995833062625\n",
      "Stochastic Gradient Descent(6849): loss=2.882351977362691\n",
      "Stochastic Gradient Descent(6850): loss=0.0011364346506911795\n",
      "Stochastic Gradient Descent(6851): loss=0.8838171540453044\n",
      "Stochastic Gradient Descent(6852): loss=0.16248461985381704\n",
      "Stochastic Gradient Descent(6853): loss=4.399915082932665\n",
      "Stochastic Gradient Descent(6854): loss=0.4422525458448817\n",
      "Stochastic Gradient Descent(6855): loss=0.22737025757352042\n",
      "Stochastic Gradient Descent(6856): loss=0.5733808611638662\n",
      "Stochastic Gradient Descent(6857): loss=1.9713168362498201\n",
      "Stochastic Gradient Descent(6858): loss=2.977015951493611\n",
      "Stochastic Gradient Descent(6859): loss=7.120775750074945\n",
      "Stochastic Gradient Descent(6860): loss=9.328198899451964\n",
      "Stochastic Gradient Descent(6861): loss=0.5174757197465002\n",
      "Stochastic Gradient Descent(6862): loss=0.50054853024312\n",
      "Stochastic Gradient Descent(6863): loss=6.295961344917493\n",
      "Stochastic Gradient Descent(6864): loss=0.030967016619136433\n",
      "Stochastic Gradient Descent(6865): loss=0.17596666880530654\n",
      "Stochastic Gradient Descent(6866): loss=23.06322989831762\n",
      "Stochastic Gradient Descent(6867): loss=0.3223725686188373\n",
      "Stochastic Gradient Descent(6868): loss=9.504336116662223\n",
      "Stochastic Gradient Descent(6869): loss=1.9946068578355285\n",
      "Stochastic Gradient Descent(6870): loss=5.0962225123940055\n",
      "Stochastic Gradient Descent(6871): loss=7.567278049031232\n",
      "Stochastic Gradient Descent(6872): loss=0.12977347295960362\n",
      "Stochastic Gradient Descent(6873): loss=0.08601198651410492\n",
      "Stochastic Gradient Descent(6874): loss=20.527074159184757\n",
      "Stochastic Gradient Descent(6875): loss=9.555172428655448\n",
      "Stochastic Gradient Descent(6876): loss=3.548369371441557\n",
      "Stochastic Gradient Descent(6877): loss=0.15996333157076578\n",
      "Stochastic Gradient Descent(6878): loss=0.34145741044651357\n",
      "Stochastic Gradient Descent(6879): loss=0.4978148816185698\n",
      "Stochastic Gradient Descent(6880): loss=0.1463439578219448\n",
      "Stochastic Gradient Descent(6881): loss=11.315192492821856\n",
      "Stochastic Gradient Descent(6882): loss=11.76351641778548\n",
      "Stochastic Gradient Descent(6883): loss=12.88285540104634\n",
      "Stochastic Gradient Descent(6884): loss=0.42436914399694736\n",
      "Stochastic Gradient Descent(6885): loss=4.642120177818995\n",
      "Stochastic Gradient Descent(6886): loss=0.6908093338787624\n",
      "Stochastic Gradient Descent(6887): loss=1.344223487048699\n",
      "Stochastic Gradient Descent(6888): loss=0.12935142076662196\n",
      "Stochastic Gradient Descent(6889): loss=4.1590330376651465\n",
      "Stochastic Gradient Descent(6890): loss=1.5889794897663811\n",
      "Stochastic Gradient Descent(6891): loss=6.7067660713591275\n",
      "Stochastic Gradient Descent(6892): loss=0.376718577929952\n",
      "Stochastic Gradient Descent(6893): loss=1.9275378865916937\n",
      "Stochastic Gradient Descent(6894): loss=6.147183990547617\n",
      "Stochastic Gradient Descent(6895): loss=10.482491626414268\n",
      "Stochastic Gradient Descent(6896): loss=11.781984115721501\n",
      "Stochastic Gradient Descent(6897): loss=0.007386741889580876\n",
      "Stochastic Gradient Descent(6898): loss=12.595228264412416\n",
      "Stochastic Gradient Descent(6899): loss=0.010424895437827417\n",
      "Stochastic Gradient Descent(6900): loss=1.2312812768930297\n",
      "Stochastic Gradient Descent(6901): loss=3.1626779711072635\n",
      "Stochastic Gradient Descent(6902): loss=0.17455656590292495\n",
      "Stochastic Gradient Descent(6903): loss=6.962865322900558\n",
      "Stochastic Gradient Descent(6904): loss=34.67299159495146\n",
      "Stochastic Gradient Descent(6905): loss=22.76825253789713\n",
      "Stochastic Gradient Descent(6906): loss=4.108764575222275\n",
      "Stochastic Gradient Descent(6907): loss=16.449784752614526\n",
      "Stochastic Gradient Descent(6908): loss=1.6790041385525856\n",
      "Stochastic Gradient Descent(6909): loss=0.3613316237206128\n",
      "Stochastic Gradient Descent(6910): loss=5.834983623912409\n",
      "Stochastic Gradient Descent(6911): loss=1.2610752114168848\n",
      "Stochastic Gradient Descent(6912): loss=0.12055395533029593\n",
      "Stochastic Gradient Descent(6913): loss=6.8869202040568185\n",
      "Stochastic Gradient Descent(6914): loss=13.10767763589673\n",
      "Stochastic Gradient Descent(6915): loss=3.9152595855770826\n",
      "Stochastic Gradient Descent(6916): loss=0.9751281660606658\n",
      "Stochastic Gradient Descent(6917): loss=6.815997924473063\n",
      "Stochastic Gradient Descent(6918): loss=0.33878856567417814\n",
      "Stochastic Gradient Descent(6919): loss=0.34959848540888994\n",
      "Stochastic Gradient Descent(6920): loss=0.9986140603748977\n",
      "Stochastic Gradient Descent(6921): loss=6.755894608358267\n",
      "Stochastic Gradient Descent(6922): loss=0.2969207187947869\n",
      "Stochastic Gradient Descent(6923): loss=3.327745280568689\n",
      "Stochastic Gradient Descent(6924): loss=0.5673047052134871\n",
      "Stochastic Gradient Descent(6925): loss=5.451398913917897\n",
      "Stochastic Gradient Descent(6926): loss=4.772940926295487\n",
      "Stochastic Gradient Descent(6927): loss=4.443711898783912\n",
      "Stochastic Gradient Descent(6928): loss=13.156093258382178\n",
      "Stochastic Gradient Descent(6929): loss=0.01766019272262908\n",
      "Stochastic Gradient Descent(6930): loss=12.284555083530565\n",
      "Stochastic Gradient Descent(6931): loss=1.9841473233994569\n",
      "Stochastic Gradient Descent(6932): loss=1.3884346126051792\n",
      "Stochastic Gradient Descent(6933): loss=0.021015639108090137\n",
      "Stochastic Gradient Descent(6934): loss=0.7568336216389775\n",
      "Stochastic Gradient Descent(6935): loss=4.755825756141939\n",
      "Stochastic Gradient Descent(6936): loss=10.829742759230397\n",
      "Stochastic Gradient Descent(6937): loss=8.989305750102098\n",
      "Stochastic Gradient Descent(6938): loss=6.602239958353092\n",
      "Stochastic Gradient Descent(6939): loss=0.04630514463546224\n",
      "Stochastic Gradient Descent(6940): loss=1.9773476044069844\n",
      "Stochastic Gradient Descent(6941): loss=0.10581991877186808\n",
      "Stochastic Gradient Descent(6942): loss=0.659976056678323\n",
      "Stochastic Gradient Descent(6943): loss=11.271920521507198\n",
      "Stochastic Gradient Descent(6944): loss=0.0017202948365280305\n",
      "Stochastic Gradient Descent(6945): loss=0.05312913909951114\n",
      "Stochastic Gradient Descent(6946): loss=6.848386616757632\n",
      "Stochastic Gradient Descent(6947): loss=2.6120106422857217\n",
      "Stochastic Gradient Descent(6948): loss=1.628706440872927\n",
      "Stochastic Gradient Descent(6949): loss=0.6162708359468813\n",
      "Stochastic Gradient Descent(6950): loss=5.274432277759174\n",
      "Stochastic Gradient Descent(6951): loss=0.4173917318891872\n",
      "Stochastic Gradient Descent(6952): loss=2.3732939049857964\n",
      "Stochastic Gradient Descent(6953): loss=0.9495762324855355\n",
      "Stochastic Gradient Descent(6954): loss=7.797535858081145\n",
      "Stochastic Gradient Descent(6955): loss=7.252008392152137\n",
      "Stochastic Gradient Descent(6956): loss=0.98775507344572\n",
      "Stochastic Gradient Descent(6957): loss=0.15409243962234276\n",
      "Stochastic Gradient Descent(6958): loss=8.395682044578773\n",
      "Stochastic Gradient Descent(6959): loss=1.7916807608760155\n",
      "Stochastic Gradient Descent(6960): loss=1.8987034062077186\n",
      "Stochastic Gradient Descent(6961): loss=4.500334573735323\n",
      "Stochastic Gradient Descent(6962): loss=0.002194853597646376\n",
      "Stochastic Gradient Descent(6963): loss=0.19073141964187534\n",
      "Stochastic Gradient Descent(6964): loss=3.0774897447327354\n",
      "Stochastic Gradient Descent(6965): loss=14.190127131647406\n",
      "Stochastic Gradient Descent(6966): loss=3.1238167511927735\n",
      "Stochastic Gradient Descent(6967): loss=0.7809862918904141\n",
      "Stochastic Gradient Descent(6968): loss=8.23367763690058\n",
      "Stochastic Gradient Descent(6969): loss=4.893858492549342\n",
      "Stochastic Gradient Descent(6970): loss=9.224071919311873\n",
      "Stochastic Gradient Descent(6971): loss=3.2935753663412926\n",
      "Stochastic Gradient Descent(6972): loss=1.472534970531855\n",
      "Stochastic Gradient Descent(6973): loss=8.364893468170324\n",
      "Stochastic Gradient Descent(6974): loss=13.863733101733697\n",
      "Stochastic Gradient Descent(6975): loss=0.5554430550861191\n",
      "Stochastic Gradient Descent(6976): loss=2.6878623617308763\n",
      "Stochastic Gradient Descent(6977): loss=4.74951076536322\n",
      "Stochastic Gradient Descent(6978): loss=0.7880079398780268\n",
      "Stochastic Gradient Descent(6979): loss=5.9716668663700485\n",
      "Stochastic Gradient Descent(6980): loss=1.2768192183581286\n",
      "Stochastic Gradient Descent(6981): loss=0.009719990574723756\n",
      "Stochastic Gradient Descent(6982): loss=7.497895448288778\n",
      "Stochastic Gradient Descent(6983): loss=0.007017793932508291\n",
      "Stochastic Gradient Descent(6984): loss=0.6358800003941889\n",
      "Stochastic Gradient Descent(6985): loss=8.06461641020779\n",
      "Stochastic Gradient Descent(6986): loss=6.2779221185845895\n",
      "Stochastic Gradient Descent(6987): loss=1.198908850811952\n",
      "Stochastic Gradient Descent(6988): loss=3.457920733440548\n",
      "Stochastic Gradient Descent(6989): loss=0.07778551204176275\n",
      "Stochastic Gradient Descent(6990): loss=6.191738589605716\n",
      "Stochastic Gradient Descent(6991): loss=6.82624360004785\n",
      "Stochastic Gradient Descent(6992): loss=10.846236566564343\n",
      "Stochastic Gradient Descent(6993): loss=1.6392645615806856\n",
      "Stochastic Gradient Descent(6994): loss=6.19927360007451\n",
      "Stochastic Gradient Descent(6995): loss=13.18274885181181\n",
      "Stochastic Gradient Descent(6996): loss=15.800677359371836\n",
      "Stochastic Gradient Descent(6997): loss=11.756743771187827\n",
      "Stochastic Gradient Descent(6998): loss=4.015324506346098\n",
      "Stochastic Gradient Descent(6999): loss=1.903931396979189\n",
      "Stochastic Gradient Descent(7000): loss=6.84622434205987\n",
      "Stochastic Gradient Descent(7001): loss=1.00442552402056\n",
      "Stochastic Gradient Descent(7002): loss=7.543828185692464\n",
      "Stochastic Gradient Descent(7003): loss=3.3717247060195383\n",
      "Stochastic Gradient Descent(7004): loss=0.09415351824389895\n",
      "Stochastic Gradient Descent(7005): loss=0.0004994299398750849\n",
      "Stochastic Gradient Descent(7006): loss=1.1938425814168434\n",
      "Stochastic Gradient Descent(7007): loss=24.63650142117397\n",
      "Stochastic Gradient Descent(7008): loss=7.764359679954805\n",
      "Stochastic Gradient Descent(7009): loss=17.36044873637699\n",
      "Stochastic Gradient Descent(7010): loss=0.032092176974824554\n",
      "Stochastic Gradient Descent(7011): loss=3.091135836766375\n",
      "Stochastic Gradient Descent(7012): loss=0.4714092896434535\n",
      "Stochastic Gradient Descent(7013): loss=11.48477336727078\n",
      "Stochastic Gradient Descent(7014): loss=0.0197525845978028\n",
      "Stochastic Gradient Descent(7015): loss=0.2202902057222872\n",
      "Stochastic Gradient Descent(7016): loss=23.896753719736306\n",
      "Stochastic Gradient Descent(7017): loss=0.28487474459891005\n",
      "Stochastic Gradient Descent(7018): loss=4.869554745998694\n",
      "Stochastic Gradient Descent(7019): loss=0.41412728382569086\n",
      "Stochastic Gradient Descent(7020): loss=14.733506985893575\n",
      "Stochastic Gradient Descent(7021): loss=0.4445794422904819\n",
      "Stochastic Gradient Descent(7022): loss=0.9745261444299778\n",
      "Stochastic Gradient Descent(7023): loss=3.158276887024971\n",
      "Stochastic Gradient Descent(7024): loss=0.05140335045033197\n",
      "Stochastic Gradient Descent(7025): loss=2.642420597153571\n",
      "Stochastic Gradient Descent(7026): loss=1.1187243758905048\n",
      "Stochastic Gradient Descent(7027): loss=0.00591381312852114\n",
      "Stochastic Gradient Descent(7028): loss=0.03024386174873705\n",
      "Stochastic Gradient Descent(7029): loss=6.025722888842248\n",
      "Stochastic Gradient Descent(7030): loss=6.376500845186089\n",
      "Stochastic Gradient Descent(7031): loss=0.2068981062971168\n",
      "Stochastic Gradient Descent(7032): loss=0.6391672438676697\n",
      "Stochastic Gradient Descent(7033): loss=4.097945229614152\n",
      "Stochastic Gradient Descent(7034): loss=3.7607146580640403\n",
      "Stochastic Gradient Descent(7035): loss=2.9871205342106655\n",
      "Stochastic Gradient Descent(7036): loss=6.08962907539085\n",
      "Stochastic Gradient Descent(7037): loss=9.602891387122133\n",
      "Stochastic Gradient Descent(7038): loss=11.24750913393165\n",
      "Stochastic Gradient Descent(7039): loss=8.934157362405827\n",
      "Stochastic Gradient Descent(7040): loss=0.2380649986528313\n",
      "Stochastic Gradient Descent(7041): loss=1.6437860706686132\n",
      "Stochastic Gradient Descent(7042): loss=15.664532088401653\n",
      "Stochastic Gradient Descent(7043): loss=1.7660804387078932\n",
      "Stochastic Gradient Descent(7044): loss=0.1082004599984515\n",
      "Stochastic Gradient Descent(7045): loss=1.525222141493349\n",
      "Stochastic Gradient Descent(7046): loss=2.56035383934472\n",
      "Stochastic Gradient Descent(7047): loss=0.009398071687028598\n",
      "Stochastic Gradient Descent(7048): loss=0.09468062703775433\n",
      "Stochastic Gradient Descent(7049): loss=8.640178561249472\n",
      "Stochastic Gradient Descent(7050): loss=0.04372440717465949\n",
      "Stochastic Gradient Descent(7051): loss=0.005786674812581482\n",
      "Stochastic Gradient Descent(7052): loss=5.5578301979402225\n",
      "Stochastic Gradient Descent(7053): loss=8.684522336561107\n",
      "Stochastic Gradient Descent(7054): loss=4.586986828087906\n",
      "Stochastic Gradient Descent(7055): loss=2.6031294608499977\n",
      "Stochastic Gradient Descent(7056): loss=38.873228692921096\n",
      "Stochastic Gradient Descent(7057): loss=0.6371605812551494\n",
      "Stochastic Gradient Descent(7058): loss=41.91071075905534\n",
      "Stochastic Gradient Descent(7059): loss=44.09926902039644\n",
      "Stochastic Gradient Descent(7060): loss=0.4745650711817325\n",
      "Stochastic Gradient Descent(7061): loss=1.9745010884769558\n",
      "Stochastic Gradient Descent(7062): loss=4.493991170362222\n",
      "Stochastic Gradient Descent(7063): loss=19.689643046343097\n",
      "Stochastic Gradient Descent(7064): loss=0.6454730896942696\n",
      "Stochastic Gradient Descent(7065): loss=0.10751015907553847\n",
      "Stochastic Gradient Descent(7066): loss=0.269519131558948\n",
      "Stochastic Gradient Descent(7067): loss=2.245321447613715\n",
      "Stochastic Gradient Descent(7068): loss=6.5965848509994744\n",
      "Stochastic Gradient Descent(7069): loss=0.24443878664347832\n",
      "Stochastic Gradient Descent(7070): loss=13.76796991011135\n",
      "Stochastic Gradient Descent(7071): loss=25.40151135963034\n",
      "Stochastic Gradient Descent(7072): loss=0.4227790275522966\n",
      "Stochastic Gradient Descent(7073): loss=0.0050483455198004085\n",
      "Stochastic Gradient Descent(7074): loss=8.11335391565457\n",
      "Stochastic Gradient Descent(7075): loss=1.0081290709539816\n",
      "Stochastic Gradient Descent(7076): loss=7.339554497721847\n",
      "Stochastic Gradient Descent(7077): loss=4.901053940031076\n",
      "Stochastic Gradient Descent(7078): loss=30.355057853838034\n",
      "Stochastic Gradient Descent(7079): loss=4.716944385189477\n",
      "Stochastic Gradient Descent(7080): loss=1.247326735299628\n",
      "Stochastic Gradient Descent(7081): loss=0.17215834418787918\n",
      "Stochastic Gradient Descent(7082): loss=0.07517803656344418\n",
      "Stochastic Gradient Descent(7083): loss=0.521612148311438\n",
      "Stochastic Gradient Descent(7084): loss=1.2133501960381259\n",
      "Stochastic Gradient Descent(7085): loss=0.40004840365667804\n",
      "Stochastic Gradient Descent(7086): loss=1.5674427167537264\n",
      "Stochastic Gradient Descent(7087): loss=1.610607345113919\n",
      "Stochastic Gradient Descent(7088): loss=28.64834415071651\n",
      "Stochastic Gradient Descent(7089): loss=5.756764830673526\n",
      "Stochastic Gradient Descent(7090): loss=4.95379666459095\n",
      "Stochastic Gradient Descent(7091): loss=0.5990211830081507\n",
      "Stochastic Gradient Descent(7092): loss=5.5076690330951275\n",
      "Stochastic Gradient Descent(7093): loss=0.9232397947754676\n",
      "Stochastic Gradient Descent(7094): loss=5.1231166705394635\n",
      "Stochastic Gradient Descent(7095): loss=1.25545795732352\n",
      "Stochastic Gradient Descent(7096): loss=3.5495037034810464\n",
      "Stochastic Gradient Descent(7097): loss=4.31667285290844\n",
      "Stochastic Gradient Descent(7098): loss=0.42468752651027436\n",
      "Stochastic Gradient Descent(7099): loss=0.05735919550288596\n",
      "Stochastic Gradient Descent(7100): loss=6.740245252175122\n",
      "Stochastic Gradient Descent(7101): loss=0.039857376809498475\n",
      "Stochastic Gradient Descent(7102): loss=0.6276419131141723\n",
      "Stochastic Gradient Descent(7103): loss=5.928351651089087\n",
      "Stochastic Gradient Descent(7104): loss=0.0037261271136505413\n",
      "Stochastic Gradient Descent(7105): loss=1.5643317982932796\n",
      "Stochastic Gradient Descent(7106): loss=2.9651049737772173\n",
      "Stochastic Gradient Descent(7107): loss=3.816442103397409\n",
      "Stochastic Gradient Descent(7108): loss=1.3188502620646956\n",
      "Stochastic Gradient Descent(7109): loss=1.687980485037329\n",
      "Stochastic Gradient Descent(7110): loss=3.2886975807025607\n",
      "Stochastic Gradient Descent(7111): loss=1.046180007515004\n",
      "Stochastic Gradient Descent(7112): loss=4.579453786559511\n",
      "Stochastic Gradient Descent(7113): loss=3.7234435066301637\n",
      "Stochastic Gradient Descent(7114): loss=11.15930763765496\n",
      "Stochastic Gradient Descent(7115): loss=7.7479195480556315\n",
      "Stochastic Gradient Descent(7116): loss=23.261291494224093\n",
      "Stochastic Gradient Descent(7117): loss=19.013807250266918\n",
      "Stochastic Gradient Descent(7118): loss=10.826779592913267\n",
      "Stochastic Gradient Descent(7119): loss=2.4758777414833757\n",
      "Stochastic Gradient Descent(7120): loss=3.727820256521273\n",
      "Stochastic Gradient Descent(7121): loss=0.0924923836270743\n",
      "Stochastic Gradient Descent(7122): loss=1.9681772788045093\n",
      "Stochastic Gradient Descent(7123): loss=14.39436003693854\n",
      "Stochastic Gradient Descent(7124): loss=7.429001724959329\n",
      "Stochastic Gradient Descent(7125): loss=0.8303204801738486\n",
      "Stochastic Gradient Descent(7126): loss=0.10894516608048857\n",
      "Stochastic Gradient Descent(7127): loss=1.787684742937914\n",
      "Stochastic Gradient Descent(7128): loss=1.6413063744352514\n",
      "Stochastic Gradient Descent(7129): loss=0.1537314062967725\n",
      "Stochastic Gradient Descent(7130): loss=0.5995044587626603\n",
      "Stochastic Gradient Descent(7131): loss=20.86806731884282\n",
      "Stochastic Gradient Descent(7132): loss=23.495601052332344\n",
      "Stochastic Gradient Descent(7133): loss=5.998162640822553\n",
      "Stochastic Gradient Descent(7134): loss=1.617525843016712\n",
      "Stochastic Gradient Descent(7135): loss=4.917104083974757\n",
      "Stochastic Gradient Descent(7136): loss=0.6247189520334275\n",
      "Stochastic Gradient Descent(7137): loss=3.967150470611867\n",
      "Stochastic Gradient Descent(7138): loss=0.07735475379258372\n",
      "Stochastic Gradient Descent(7139): loss=1.3043689597230281\n",
      "Stochastic Gradient Descent(7140): loss=8.493192583900772\n",
      "Stochastic Gradient Descent(7141): loss=1.5786880947392565\n",
      "Stochastic Gradient Descent(7142): loss=2.753919806009115\n",
      "Stochastic Gradient Descent(7143): loss=0.2958457032969618\n",
      "Stochastic Gradient Descent(7144): loss=10.510064325752571\n",
      "Stochastic Gradient Descent(7145): loss=2.9821555424122312\n",
      "Stochastic Gradient Descent(7146): loss=0.04317911975319866\n",
      "Stochastic Gradient Descent(7147): loss=0.00252217711654166\n",
      "Stochastic Gradient Descent(7148): loss=0.6729901959467589\n",
      "Stochastic Gradient Descent(7149): loss=0.19246287896579445\n",
      "Stochastic Gradient Descent(7150): loss=0.006029307940993739\n",
      "Stochastic Gradient Descent(7151): loss=6.587483353802511e-06\n",
      "Stochastic Gradient Descent(7152): loss=31.86239983406945\n",
      "Stochastic Gradient Descent(7153): loss=6.490667471955519\n",
      "Stochastic Gradient Descent(7154): loss=21.513653284532573\n",
      "Stochastic Gradient Descent(7155): loss=1.8572855747057777\n",
      "Stochastic Gradient Descent(7156): loss=7.796102658202702\n",
      "Stochastic Gradient Descent(7157): loss=1.8686086505608566\n",
      "Stochastic Gradient Descent(7158): loss=6.335819963066255\n",
      "Stochastic Gradient Descent(7159): loss=3.4760273959785444\n",
      "Stochastic Gradient Descent(7160): loss=2.1791937011390807\n",
      "Stochastic Gradient Descent(7161): loss=0.9660209068445473\n",
      "Stochastic Gradient Descent(7162): loss=0.010208954906361879\n",
      "Stochastic Gradient Descent(7163): loss=5.287332966004181\n",
      "Stochastic Gradient Descent(7164): loss=9.210311182023338\n",
      "Stochastic Gradient Descent(7165): loss=0.28896402737407156\n",
      "Stochastic Gradient Descent(7166): loss=0.5970367161174809\n",
      "Stochastic Gradient Descent(7167): loss=0.019845020862046626\n",
      "Stochastic Gradient Descent(7168): loss=3.2654197626871553\n",
      "Stochastic Gradient Descent(7169): loss=4.209859958916288\n",
      "Stochastic Gradient Descent(7170): loss=1.4336790532487962\n",
      "Stochastic Gradient Descent(7171): loss=2.416130710709839\n",
      "Stochastic Gradient Descent(7172): loss=14.287295351115832\n",
      "Stochastic Gradient Descent(7173): loss=24.26405370087483\n",
      "Stochastic Gradient Descent(7174): loss=6.291704684440044\n",
      "Stochastic Gradient Descent(7175): loss=5.525413816250912\n",
      "Stochastic Gradient Descent(7176): loss=3.797356967149977\n",
      "Stochastic Gradient Descent(7177): loss=5.3260464978044055\n",
      "Stochastic Gradient Descent(7178): loss=0.0012884406304748952\n",
      "Stochastic Gradient Descent(7179): loss=0.030535779394590817\n",
      "Stochastic Gradient Descent(7180): loss=11.898692960204055\n",
      "Stochastic Gradient Descent(7181): loss=0.16724573922812158\n",
      "Stochastic Gradient Descent(7182): loss=3.600383862638396\n",
      "Stochastic Gradient Descent(7183): loss=0.05311902000670855\n",
      "Stochastic Gradient Descent(7184): loss=3.155633977521602\n",
      "Stochastic Gradient Descent(7185): loss=0.4255248166365009\n",
      "Stochastic Gradient Descent(7186): loss=13.943983949486146\n",
      "Stochastic Gradient Descent(7187): loss=1.160522580523844\n",
      "Stochastic Gradient Descent(7188): loss=0.8443564981954078\n",
      "Stochastic Gradient Descent(7189): loss=1.1503996572505437\n",
      "Stochastic Gradient Descent(7190): loss=10.640029467245647\n",
      "Stochastic Gradient Descent(7191): loss=0.9195278984672234\n",
      "Stochastic Gradient Descent(7192): loss=1.6291105353033508\n",
      "Stochastic Gradient Descent(7193): loss=0.8060392508511393\n",
      "Stochastic Gradient Descent(7194): loss=0.10197964471792682\n",
      "Stochastic Gradient Descent(7195): loss=0.27090509918098865\n",
      "Stochastic Gradient Descent(7196): loss=26.23537426432855\n",
      "Stochastic Gradient Descent(7197): loss=0.11432671489477438\n",
      "Stochastic Gradient Descent(7198): loss=21.99520970830524\n",
      "Stochastic Gradient Descent(7199): loss=1.937616519204121\n",
      "Stochastic Gradient Descent(7200): loss=0.28291860641705446\n",
      "Stochastic Gradient Descent(7201): loss=5.55650980602746\n",
      "Stochastic Gradient Descent(7202): loss=1.4347558554277695\n",
      "Stochastic Gradient Descent(7203): loss=7.091217530597273\n",
      "Stochastic Gradient Descent(7204): loss=1.377467182651294\n",
      "Stochastic Gradient Descent(7205): loss=0.7755979397574098\n",
      "Stochastic Gradient Descent(7206): loss=4.491102729738225\n",
      "Stochastic Gradient Descent(7207): loss=2.8039975334802825\n",
      "Stochastic Gradient Descent(7208): loss=0.909730869999264\n",
      "Stochastic Gradient Descent(7209): loss=1.928357261962764\n",
      "Stochastic Gradient Descent(7210): loss=0.3977451339306635\n",
      "Stochastic Gradient Descent(7211): loss=0.00021183449960200096\n",
      "Stochastic Gradient Descent(7212): loss=0.08322947838643124\n",
      "Stochastic Gradient Descent(7213): loss=5.961076726615038\n",
      "Stochastic Gradient Descent(7214): loss=7.8052268939665455\n",
      "Stochastic Gradient Descent(7215): loss=8.994855142390517\n",
      "Stochastic Gradient Descent(7216): loss=7.843966601195509\n",
      "Stochastic Gradient Descent(7217): loss=0.1205242465568214\n",
      "Stochastic Gradient Descent(7218): loss=29.46719539751334\n",
      "Stochastic Gradient Descent(7219): loss=24.55620405720642\n",
      "Stochastic Gradient Descent(7220): loss=0.007257482801044383\n",
      "Stochastic Gradient Descent(7221): loss=0.9327131811815029\n",
      "Stochastic Gradient Descent(7222): loss=0.1510053086021196\n",
      "Stochastic Gradient Descent(7223): loss=2.31906985865601\n",
      "Stochastic Gradient Descent(7224): loss=0.6845023463412977\n",
      "Stochastic Gradient Descent(7225): loss=0.24949396230359383\n",
      "Stochastic Gradient Descent(7226): loss=2.3290912099803136\n",
      "Stochastic Gradient Descent(7227): loss=0.4436182360129796\n",
      "Stochastic Gradient Descent(7228): loss=40.905784164215184\n",
      "Stochastic Gradient Descent(7229): loss=2.8708419829805827\n",
      "Stochastic Gradient Descent(7230): loss=22.597376393746092\n",
      "Stochastic Gradient Descent(7231): loss=24.14254031075209\n",
      "Stochastic Gradient Descent(7232): loss=1.2524842993721228\n",
      "Stochastic Gradient Descent(7233): loss=13.003095898437401\n",
      "Stochastic Gradient Descent(7234): loss=7.809021754266499\n",
      "Stochastic Gradient Descent(7235): loss=10.511289844024477\n",
      "Stochastic Gradient Descent(7236): loss=12.593947539916416\n",
      "Stochastic Gradient Descent(7237): loss=19.628775365051354\n",
      "Stochastic Gradient Descent(7238): loss=0.11943430140348789\n",
      "Stochastic Gradient Descent(7239): loss=4.7012949776479855\n",
      "Stochastic Gradient Descent(7240): loss=58.418994574223966\n",
      "Stochastic Gradient Descent(7241): loss=13.716111248954045\n",
      "Stochastic Gradient Descent(7242): loss=2.7386692549005076\n",
      "Stochastic Gradient Descent(7243): loss=9.749880725702727\n",
      "Stochastic Gradient Descent(7244): loss=17.498041960532046\n",
      "Stochastic Gradient Descent(7245): loss=25.68824565795829\n",
      "Stochastic Gradient Descent(7246): loss=10.585567021345708\n",
      "Stochastic Gradient Descent(7247): loss=3.0718460333809112\n",
      "Stochastic Gradient Descent(7248): loss=11.100732795627447\n",
      "Stochastic Gradient Descent(7249): loss=1.899836471924114\n",
      "Stochastic Gradient Descent(7250): loss=0.9008470154246979\n",
      "Stochastic Gradient Descent(7251): loss=0.17118971270868807\n",
      "Stochastic Gradient Descent(7252): loss=0.12524549557530537\n",
      "Stochastic Gradient Descent(7253): loss=0.8703157792322103\n",
      "Stochastic Gradient Descent(7254): loss=5.702251220386996\n",
      "Stochastic Gradient Descent(7255): loss=1.2175470492594211\n",
      "Stochastic Gradient Descent(7256): loss=19.692225167015376\n",
      "Stochastic Gradient Descent(7257): loss=4.604818757707812\n",
      "Stochastic Gradient Descent(7258): loss=5.9385970294007295\n",
      "Stochastic Gradient Descent(7259): loss=6.569193766731207\n",
      "Stochastic Gradient Descent(7260): loss=0.7631785299872942\n",
      "Stochastic Gradient Descent(7261): loss=0.6793287360002731\n",
      "Stochastic Gradient Descent(7262): loss=0.0023641643822824073\n",
      "Stochastic Gradient Descent(7263): loss=68.9932399501332\n",
      "Stochastic Gradient Descent(7264): loss=74.63918569010188\n",
      "Stochastic Gradient Descent(7265): loss=15.635747504629107\n",
      "Stochastic Gradient Descent(7266): loss=1.9683088522929735\n",
      "Stochastic Gradient Descent(7267): loss=46.12708353614646\n",
      "Stochastic Gradient Descent(7268): loss=38.52859230048842\n",
      "Stochastic Gradient Descent(7269): loss=5.33535343578424\n",
      "Stochastic Gradient Descent(7270): loss=2.829512370475759\n",
      "Stochastic Gradient Descent(7271): loss=9.092311487108743\n",
      "Stochastic Gradient Descent(7272): loss=3.8730346370904134\n",
      "Stochastic Gradient Descent(7273): loss=1.3297749892155066\n",
      "Stochastic Gradient Descent(7274): loss=19.49559952356779\n",
      "Stochastic Gradient Descent(7275): loss=0.877254118723885\n",
      "Stochastic Gradient Descent(7276): loss=4.7542485047218985\n",
      "Stochastic Gradient Descent(7277): loss=4.986533304669335\n",
      "Stochastic Gradient Descent(7278): loss=0.3776035329497662\n",
      "Stochastic Gradient Descent(7279): loss=11.953221734330645\n",
      "Stochastic Gradient Descent(7280): loss=3.6059362357845712\n",
      "Stochastic Gradient Descent(7281): loss=11.900012309015853\n",
      "Stochastic Gradient Descent(7282): loss=13.310981171644507\n",
      "Stochastic Gradient Descent(7283): loss=0.4137635494337524\n",
      "Stochastic Gradient Descent(7284): loss=26.727440343741435\n",
      "Stochastic Gradient Descent(7285): loss=13.40180873768485\n",
      "Stochastic Gradient Descent(7286): loss=5.715408102255583\n",
      "Stochastic Gradient Descent(7287): loss=20.21406865599922\n",
      "Stochastic Gradient Descent(7288): loss=0.6532702308247054\n",
      "Stochastic Gradient Descent(7289): loss=3.5407907938913996\n",
      "Stochastic Gradient Descent(7290): loss=9.35096289075091\n",
      "Stochastic Gradient Descent(7291): loss=5.0352905386912585\n",
      "Stochastic Gradient Descent(7292): loss=10.343405160570471\n",
      "Stochastic Gradient Descent(7293): loss=0.1936793523806248\n",
      "Stochastic Gradient Descent(7294): loss=4.857232811493209\n",
      "Stochastic Gradient Descent(7295): loss=36.56032293198356\n",
      "Stochastic Gradient Descent(7296): loss=0.3454754336281383\n",
      "Stochastic Gradient Descent(7297): loss=0.2991299839390554\n",
      "Stochastic Gradient Descent(7298): loss=0.6824774479996936\n",
      "Stochastic Gradient Descent(7299): loss=8.736139127475436\n",
      "Stochastic Gradient Descent(7300): loss=0.054024298751691284\n",
      "Stochastic Gradient Descent(7301): loss=12.766320532603343\n",
      "Stochastic Gradient Descent(7302): loss=2.9704957419390685\n",
      "Stochastic Gradient Descent(7303): loss=0.037814932020371526\n",
      "Stochastic Gradient Descent(7304): loss=0.19684495299909466\n",
      "Stochastic Gradient Descent(7305): loss=0.0019398414136877842\n",
      "Stochastic Gradient Descent(7306): loss=0.3458701300857083\n",
      "Stochastic Gradient Descent(7307): loss=9.18186009901202\n",
      "Stochastic Gradient Descent(7308): loss=8.884716136723455\n",
      "Stochastic Gradient Descent(7309): loss=0.14887355415949763\n",
      "Stochastic Gradient Descent(7310): loss=0.025589714635379547\n",
      "Stochastic Gradient Descent(7311): loss=8.745630871972235\n",
      "Stochastic Gradient Descent(7312): loss=48.15151454340748\n",
      "Stochastic Gradient Descent(7313): loss=1.3314786729203942\n",
      "Stochastic Gradient Descent(7314): loss=0.9618713734249121\n",
      "Stochastic Gradient Descent(7315): loss=6.095454016771324\n",
      "Stochastic Gradient Descent(7316): loss=12.367836573498847\n",
      "Stochastic Gradient Descent(7317): loss=39.97755508664104\n",
      "Stochastic Gradient Descent(7318): loss=4.172044903038236\n",
      "Stochastic Gradient Descent(7319): loss=4.719672131562295\n",
      "Stochastic Gradient Descent(7320): loss=3.2058473307167783\n",
      "Stochastic Gradient Descent(7321): loss=10.287810974716276\n",
      "Stochastic Gradient Descent(7322): loss=13.585311813829438\n",
      "Stochastic Gradient Descent(7323): loss=1.96498592232929\n",
      "Stochastic Gradient Descent(7324): loss=1.2788951638875725\n",
      "Stochastic Gradient Descent(7325): loss=0.8398654441140557\n",
      "Stochastic Gradient Descent(7326): loss=5.286119701647096\n",
      "Stochastic Gradient Descent(7327): loss=0.04953580104058661\n",
      "Stochastic Gradient Descent(7328): loss=4.15399528137998\n",
      "Stochastic Gradient Descent(7329): loss=9.339092639441748\n",
      "Stochastic Gradient Descent(7330): loss=4.0678977173513\n",
      "Stochastic Gradient Descent(7331): loss=0.7824690394425595\n",
      "Stochastic Gradient Descent(7332): loss=0.20315060226916953\n",
      "Stochastic Gradient Descent(7333): loss=2.4446648895349004\n",
      "Stochastic Gradient Descent(7334): loss=9.06513432602172\n",
      "Stochastic Gradient Descent(7335): loss=20.082469692126246\n",
      "Stochastic Gradient Descent(7336): loss=27.769210918565605\n",
      "Stochastic Gradient Descent(7337): loss=2.9550528076563034\n",
      "Stochastic Gradient Descent(7338): loss=3.435535477921265\n",
      "Stochastic Gradient Descent(7339): loss=7.926682381152454\n",
      "Stochastic Gradient Descent(7340): loss=0.7896416808919773\n",
      "Stochastic Gradient Descent(7341): loss=0.2589840147950271\n",
      "Stochastic Gradient Descent(7342): loss=4.883166807830303\n",
      "Stochastic Gradient Descent(7343): loss=32.71047318931362\n",
      "Stochastic Gradient Descent(7344): loss=3.5774908822699825\n",
      "Stochastic Gradient Descent(7345): loss=0.0038925273450278242\n",
      "Stochastic Gradient Descent(7346): loss=1.2974796362418697\n",
      "Stochastic Gradient Descent(7347): loss=0.47622224757517867\n",
      "Stochastic Gradient Descent(7348): loss=6.69070387155147\n",
      "Stochastic Gradient Descent(7349): loss=10.533763228562036\n",
      "Stochastic Gradient Descent(7350): loss=1.0497009032174234\n",
      "Stochastic Gradient Descent(7351): loss=4.3183216623570875\n",
      "Stochastic Gradient Descent(7352): loss=1.0857634829074265\n",
      "Stochastic Gradient Descent(7353): loss=5.335015781322395\n",
      "Stochastic Gradient Descent(7354): loss=24.53636302758076\n",
      "Stochastic Gradient Descent(7355): loss=1.2738087465284913\n",
      "Stochastic Gradient Descent(7356): loss=0.05304520016981667\n",
      "Stochastic Gradient Descent(7357): loss=20.709524094630925\n",
      "Stochastic Gradient Descent(7358): loss=10.146075375617672\n",
      "Stochastic Gradient Descent(7359): loss=1.1763356882925637\n",
      "Stochastic Gradient Descent(7360): loss=5.698599414045796\n",
      "Stochastic Gradient Descent(7361): loss=13.981634559029052\n",
      "Stochastic Gradient Descent(7362): loss=7.469089672001142\n",
      "Stochastic Gradient Descent(7363): loss=0.0010985082113845008\n",
      "Stochastic Gradient Descent(7364): loss=40.33429055448108\n",
      "Stochastic Gradient Descent(7365): loss=6.577051957513105\n",
      "Stochastic Gradient Descent(7366): loss=13.845009060988748\n",
      "Stochastic Gradient Descent(7367): loss=0.2613591367147588\n",
      "Stochastic Gradient Descent(7368): loss=0.36686606764253277\n",
      "Stochastic Gradient Descent(7369): loss=0.0019372558514238233\n",
      "Stochastic Gradient Descent(7370): loss=16.605787273318423\n",
      "Stochastic Gradient Descent(7371): loss=5.222573856477042\n",
      "Stochastic Gradient Descent(7372): loss=1.8465065261666287\n",
      "Stochastic Gradient Descent(7373): loss=0.014220971755915695\n",
      "Stochastic Gradient Descent(7374): loss=8.787547074219441\n",
      "Stochastic Gradient Descent(7375): loss=2.602575364193487\n",
      "Stochastic Gradient Descent(7376): loss=8.108481655964413\n",
      "Stochastic Gradient Descent(7377): loss=2.9987619514739006\n",
      "Stochastic Gradient Descent(7378): loss=0.006821381920974665\n",
      "Stochastic Gradient Descent(7379): loss=0.0018231523814056757\n",
      "Stochastic Gradient Descent(7380): loss=3.2160052239403427\n",
      "Stochastic Gradient Descent(7381): loss=0.007781832007648558\n",
      "Stochastic Gradient Descent(7382): loss=4.21738599268267\n",
      "Stochastic Gradient Descent(7383): loss=2.5396811716871306\n",
      "Stochastic Gradient Descent(7384): loss=1.6755290744369116\n",
      "Stochastic Gradient Descent(7385): loss=0.00712075510692787\n",
      "Stochastic Gradient Descent(7386): loss=1.9753084525078506\n",
      "Stochastic Gradient Descent(7387): loss=3.4204162066295636\n",
      "Stochastic Gradient Descent(7388): loss=3.8381293419407037\n",
      "Stochastic Gradient Descent(7389): loss=1.237201471480183\n",
      "Stochastic Gradient Descent(7390): loss=9.177482793927464\n",
      "Stochastic Gradient Descent(7391): loss=3.153525986312549\n",
      "Stochastic Gradient Descent(7392): loss=4.741343200901275\n",
      "Stochastic Gradient Descent(7393): loss=25.441886808036475\n",
      "Stochastic Gradient Descent(7394): loss=2.726343965461493\n",
      "Stochastic Gradient Descent(7395): loss=18.85600390815753\n",
      "Stochastic Gradient Descent(7396): loss=2.7844260940759935\n",
      "Stochastic Gradient Descent(7397): loss=3.6783418594963413\n",
      "Stochastic Gradient Descent(7398): loss=35.62042755704883\n",
      "Stochastic Gradient Descent(7399): loss=34.10197346572789\n",
      "Stochastic Gradient Descent(7400): loss=13.384828293251507\n",
      "Stochastic Gradient Descent(7401): loss=3.3298393882926547\n",
      "Stochastic Gradient Descent(7402): loss=4.139434345849594\n",
      "Stochastic Gradient Descent(7403): loss=7.956723083602628\n",
      "Stochastic Gradient Descent(7404): loss=6.2803890219515335\n",
      "Stochastic Gradient Descent(7405): loss=4.0731304919031155\n",
      "Stochastic Gradient Descent(7406): loss=1.5649627696187924\n",
      "Stochastic Gradient Descent(7407): loss=2.060204446902446\n",
      "Stochastic Gradient Descent(7408): loss=0.15506742725965292\n",
      "Stochastic Gradient Descent(7409): loss=1.7056160171560713\n",
      "Stochastic Gradient Descent(7410): loss=0.08471060780084219\n",
      "Stochastic Gradient Descent(7411): loss=0.5597559474689148\n",
      "Stochastic Gradient Descent(7412): loss=0.48585330105060265\n",
      "Stochastic Gradient Descent(7413): loss=9.084095725803886\n",
      "Stochastic Gradient Descent(7414): loss=0.7572538915841788\n",
      "Stochastic Gradient Descent(7415): loss=20.98835293842924\n",
      "Stochastic Gradient Descent(7416): loss=0.4017953350691193\n",
      "Stochastic Gradient Descent(7417): loss=13.572321469180153\n",
      "Stochastic Gradient Descent(7418): loss=0.08367843774965736\n",
      "Stochastic Gradient Descent(7419): loss=1.3811260865226413\n",
      "Stochastic Gradient Descent(7420): loss=0.026620788147160773\n",
      "Stochastic Gradient Descent(7421): loss=15.781694270102772\n",
      "Stochastic Gradient Descent(7422): loss=7.8583357486839525\n",
      "Stochastic Gradient Descent(7423): loss=1.3232256775694593\n",
      "Stochastic Gradient Descent(7424): loss=11.465372466385142\n",
      "Stochastic Gradient Descent(7425): loss=7.667762751591204\n",
      "Stochastic Gradient Descent(7426): loss=2.1318535123192306\n",
      "Stochastic Gradient Descent(7427): loss=0.5590588933906268\n",
      "Stochastic Gradient Descent(7428): loss=0.16060351850130702\n",
      "Stochastic Gradient Descent(7429): loss=0.4900216713095313\n",
      "Stochastic Gradient Descent(7430): loss=34.2295849250985\n",
      "Stochastic Gradient Descent(7431): loss=0.08725796270241129\n",
      "Stochastic Gradient Descent(7432): loss=0.36145627701207433\n",
      "Stochastic Gradient Descent(7433): loss=5.251863265590152\n",
      "Stochastic Gradient Descent(7434): loss=0.0555272090640559\n",
      "Stochastic Gradient Descent(7435): loss=15.699969723765664\n",
      "Stochastic Gradient Descent(7436): loss=0.11147496335386274\n",
      "Stochastic Gradient Descent(7437): loss=1.238501287371101\n",
      "Stochastic Gradient Descent(7438): loss=2.5075446762199127\n",
      "Stochastic Gradient Descent(7439): loss=2.1175359141010333\n",
      "Stochastic Gradient Descent(7440): loss=2.708781506564574\n",
      "Stochastic Gradient Descent(7441): loss=6.64911268348513\n",
      "Stochastic Gradient Descent(7442): loss=0.14302600937472604\n",
      "Stochastic Gradient Descent(7443): loss=0.8799886324681419\n",
      "Stochastic Gradient Descent(7444): loss=0.23345363882674205\n",
      "Stochastic Gradient Descent(7445): loss=1.9228779602740427\n",
      "Stochastic Gradient Descent(7446): loss=0.09567422239133729\n",
      "Stochastic Gradient Descent(7447): loss=5.55928096223121\n",
      "Stochastic Gradient Descent(7448): loss=0.09355148646082716\n",
      "Stochastic Gradient Descent(7449): loss=24.33398314313875\n",
      "Stochastic Gradient Descent(7450): loss=16.354784248632505\n",
      "Stochastic Gradient Descent(7451): loss=1.0574845875455814\n",
      "Stochastic Gradient Descent(7452): loss=0.6643630331993435\n",
      "Stochastic Gradient Descent(7453): loss=0.4284687071234101\n",
      "Stochastic Gradient Descent(7454): loss=0.9222870156835017\n",
      "Stochastic Gradient Descent(7455): loss=2.5347428361525233\n",
      "Stochastic Gradient Descent(7456): loss=0.15653756040575928\n",
      "Stochastic Gradient Descent(7457): loss=2.767922781418863\n",
      "Stochastic Gradient Descent(7458): loss=0.14425112572359572\n",
      "Stochastic Gradient Descent(7459): loss=34.28980124221452\n",
      "Stochastic Gradient Descent(7460): loss=6.47039302418937\n",
      "Stochastic Gradient Descent(7461): loss=1.8386557971545086\n",
      "Stochastic Gradient Descent(7462): loss=37.13993553436847\n",
      "Stochastic Gradient Descent(7463): loss=0.2605914613489528\n",
      "Stochastic Gradient Descent(7464): loss=0.2822509913449467\n",
      "Stochastic Gradient Descent(7465): loss=0.01599268094427873\n",
      "Stochastic Gradient Descent(7466): loss=7.065046762240686\n",
      "Stochastic Gradient Descent(7467): loss=10.764715329905707\n",
      "Stochastic Gradient Descent(7468): loss=4.443895929926734\n",
      "Stochastic Gradient Descent(7469): loss=39.245884775981644\n",
      "Stochastic Gradient Descent(7470): loss=0.03548934797253429\n",
      "Stochastic Gradient Descent(7471): loss=5.259765260813212\n",
      "Stochastic Gradient Descent(7472): loss=0.09106136719330496\n",
      "Stochastic Gradient Descent(7473): loss=5.07933431988752\n",
      "Stochastic Gradient Descent(7474): loss=1.5502793874407252\n",
      "Stochastic Gradient Descent(7475): loss=0.20483243987201813\n",
      "Stochastic Gradient Descent(7476): loss=2.024404533310576\n",
      "Stochastic Gradient Descent(7477): loss=10.891222738896124\n",
      "Stochastic Gradient Descent(7478): loss=4.883789100204498\n",
      "Stochastic Gradient Descent(7479): loss=6.357737937368508\n",
      "Stochastic Gradient Descent(7480): loss=33.51914237342079\n",
      "Stochastic Gradient Descent(7481): loss=50.72913411892275\n",
      "Stochastic Gradient Descent(7482): loss=0.3911902378836156\n",
      "Stochastic Gradient Descent(7483): loss=70.43499047523639\n",
      "Stochastic Gradient Descent(7484): loss=0.6869374809437765\n",
      "Stochastic Gradient Descent(7485): loss=0.04446257332489956\n",
      "Stochastic Gradient Descent(7486): loss=1.498546962358234\n",
      "Stochastic Gradient Descent(7487): loss=5.046722667955007\n",
      "Stochastic Gradient Descent(7488): loss=4.429984276351112\n",
      "Stochastic Gradient Descent(7489): loss=1.2587742290801027\n",
      "Stochastic Gradient Descent(7490): loss=30.806062386776993\n",
      "Stochastic Gradient Descent(7491): loss=1.8318106346044647\n",
      "Stochastic Gradient Descent(7492): loss=2.057567562790453\n",
      "Stochastic Gradient Descent(7493): loss=1.9705885675688921\n",
      "Stochastic Gradient Descent(7494): loss=2.283195502425843\n",
      "Stochastic Gradient Descent(7495): loss=13.420071510879465\n",
      "Stochastic Gradient Descent(7496): loss=2.6522178845864084\n",
      "Stochastic Gradient Descent(7497): loss=0.14336022379676128\n",
      "Stochastic Gradient Descent(7498): loss=0.11174910867565424\n",
      "Stochastic Gradient Descent(7499): loss=4.649593345774279\n",
      "Stochastic Gradient Descent(7500): loss=32.95777665338937\n",
      "Stochastic Gradient Descent(7501): loss=0.027347618631921512\n",
      "Stochastic Gradient Descent(7502): loss=7.856891668529846\n",
      "Stochastic Gradient Descent(7503): loss=2.6801811541123786\n",
      "Stochastic Gradient Descent(7504): loss=6.305654686993819\n",
      "Stochastic Gradient Descent(7505): loss=0.0023861903522638173\n",
      "Stochastic Gradient Descent(7506): loss=0.18842934027883831\n",
      "Stochastic Gradient Descent(7507): loss=0.7603026165258137\n",
      "Stochastic Gradient Descent(7508): loss=3.3456307733521853\n",
      "Stochastic Gradient Descent(7509): loss=3.7241939925446763\n",
      "Stochastic Gradient Descent(7510): loss=17.644788311639577\n",
      "Stochastic Gradient Descent(7511): loss=1.0642272892336242\n",
      "Stochastic Gradient Descent(7512): loss=2.0691419827411703\n",
      "Stochastic Gradient Descent(7513): loss=0.1573702927260716\n",
      "Stochastic Gradient Descent(7514): loss=2.7732508619749874\n",
      "Stochastic Gradient Descent(7515): loss=2.024014479376143\n",
      "Stochastic Gradient Descent(7516): loss=0.11107441668889137\n",
      "Stochastic Gradient Descent(7517): loss=6.123522837508266\n",
      "Stochastic Gradient Descent(7518): loss=2.6794345604752343\n",
      "Stochastic Gradient Descent(7519): loss=14.633197793890577\n",
      "Stochastic Gradient Descent(7520): loss=0.049286027025565185\n",
      "Stochastic Gradient Descent(7521): loss=2.7912985462062987\n",
      "Stochastic Gradient Descent(7522): loss=1.6579700959022214\n",
      "Stochastic Gradient Descent(7523): loss=0.0010162893247700829\n",
      "Stochastic Gradient Descent(7524): loss=8.87868916500678\n",
      "Stochastic Gradient Descent(7525): loss=0.010896531447407124\n",
      "Stochastic Gradient Descent(7526): loss=7.11069779212117\n",
      "Stochastic Gradient Descent(7527): loss=2.847385987219153\n",
      "Stochastic Gradient Descent(7528): loss=0.37613142676244044\n",
      "Stochastic Gradient Descent(7529): loss=1.235634930690578\n",
      "Stochastic Gradient Descent(7530): loss=1.832859477512927\n",
      "Stochastic Gradient Descent(7531): loss=2.5022692910198403\n",
      "Stochastic Gradient Descent(7532): loss=1.3453872564613198\n",
      "Stochastic Gradient Descent(7533): loss=17.524821938008476\n",
      "Stochastic Gradient Descent(7534): loss=0.7053640763098875\n",
      "Stochastic Gradient Descent(7535): loss=0.20541546166044647\n",
      "Stochastic Gradient Descent(7536): loss=0.8405641679967717\n",
      "Stochastic Gradient Descent(7537): loss=2.9017579467140107\n",
      "Stochastic Gradient Descent(7538): loss=5.217094163554851\n",
      "Stochastic Gradient Descent(7539): loss=0.24516708154812036\n",
      "Stochastic Gradient Descent(7540): loss=0.10270213441982169\n",
      "Stochastic Gradient Descent(7541): loss=0.07326703615523499\n",
      "Stochastic Gradient Descent(7542): loss=0.10083045968076539\n",
      "Stochastic Gradient Descent(7543): loss=0.11716203167207705\n",
      "Stochastic Gradient Descent(7544): loss=2.34368444892107\n",
      "Stochastic Gradient Descent(7545): loss=22.034191825033236\n",
      "Stochastic Gradient Descent(7546): loss=40.84425121694471\n",
      "Stochastic Gradient Descent(7547): loss=2.2641628088278756\n",
      "Stochastic Gradient Descent(7548): loss=1.8327400131880622\n",
      "Stochastic Gradient Descent(7549): loss=19.41428006470631\n",
      "Stochastic Gradient Descent(7550): loss=8.002311218086588\n",
      "Stochastic Gradient Descent(7551): loss=13.381864248635582\n",
      "Stochastic Gradient Descent(7552): loss=1.2074673768766035\n",
      "Stochastic Gradient Descent(7553): loss=0.4392992323147456\n",
      "Stochastic Gradient Descent(7554): loss=0.09591871762979369\n",
      "Stochastic Gradient Descent(7555): loss=1.4187543499428739\n",
      "Stochastic Gradient Descent(7556): loss=1.373189516023138\n",
      "Stochastic Gradient Descent(7557): loss=2.820297591984847\n",
      "Stochastic Gradient Descent(7558): loss=60.259665956274326\n",
      "Stochastic Gradient Descent(7559): loss=1.8245586607558488\n",
      "Stochastic Gradient Descent(7560): loss=0.05980701327096145\n",
      "Stochastic Gradient Descent(7561): loss=1.1365668231620758\n",
      "Stochastic Gradient Descent(7562): loss=1.3183019463096612\n",
      "Stochastic Gradient Descent(7563): loss=13.105604841274426\n",
      "Stochastic Gradient Descent(7564): loss=5.534558535843324\n",
      "Stochastic Gradient Descent(7565): loss=2.0778309253799923\n",
      "Stochastic Gradient Descent(7566): loss=26.012083020575442\n",
      "Stochastic Gradient Descent(7567): loss=7.392191069415473\n",
      "Stochastic Gradient Descent(7568): loss=13.681462237088361\n",
      "Stochastic Gradient Descent(7569): loss=10.065411371705599\n",
      "Stochastic Gradient Descent(7570): loss=0.17710391063216765\n",
      "Stochastic Gradient Descent(7571): loss=4.082643174788251\n",
      "Stochastic Gradient Descent(7572): loss=15.98915412550405\n",
      "Stochastic Gradient Descent(7573): loss=1.3755408769736488\n",
      "Stochastic Gradient Descent(7574): loss=0.1032781232510808\n",
      "Stochastic Gradient Descent(7575): loss=1.3482256622643112\n",
      "Stochastic Gradient Descent(7576): loss=1.1497613917052694\n",
      "Stochastic Gradient Descent(7577): loss=3.293404000084737\n",
      "Stochastic Gradient Descent(7578): loss=16.04043069345486\n",
      "Stochastic Gradient Descent(7579): loss=0.6566233468981914\n",
      "Stochastic Gradient Descent(7580): loss=0.053115755683842386\n",
      "Stochastic Gradient Descent(7581): loss=7.0170012335426835\n",
      "Stochastic Gradient Descent(7582): loss=22.544042632792156\n",
      "Stochastic Gradient Descent(7583): loss=0.6968298621309198\n",
      "Stochastic Gradient Descent(7584): loss=4.09010507823723\n",
      "Stochastic Gradient Descent(7585): loss=1.200030715589573\n",
      "Stochastic Gradient Descent(7586): loss=0.29659952085002816\n",
      "Stochastic Gradient Descent(7587): loss=4.15816448500515\n",
      "Stochastic Gradient Descent(7588): loss=3.80777035881122\n",
      "Stochastic Gradient Descent(7589): loss=1.7129812898417995\n",
      "Stochastic Gradient Descent(7590): loss=21.64932527042833\n",
      "Stochastic Gradient Descent(7591): loss=23.45122048840099\n",
      "Stochastic Gradient Descent(7592): loss=15.044473942173436\n",
      "Stochastic Gradient Descent(7593): loss=6.418270916728635\n",
      "Stochastic Gradient Descent(7594): loss=3.653117942374458\n",
      "Stochastic Gradient Descent(7595): loss=10.06841555088295\n",
      "Stochastic Gradient Descent(7596): loss=2.1870504850752472\n",
      "Stochastic Gradient Descent(7597): loss=18.323870793823808\n",
      "Stochastic Gradient Descent(7598): loss=3.46815490959069\n",
      "Stochastic Gradient Descent(7599): loss=0.7371963963676018\n",
      "Stochastic Gradient Descent(7600): loss=1.0411728586219462\n",
      "Stochastic Gradient Descent(7601): loss=5.084953394589207\n",
      "Stochastic Gradient Descent(7602): loss=4.950801989083818\n",
      "Stochastic Gradient Descent(7603): loss=15.429497579817102\n",
      "Stochastic Gradient Descent(7604): loss=18.293505821629683\n",
      "Stochastic Gradient Descent(7605): loss=1.2342848972669989\n",
      "Stochastic Gradient Descent(7606): loss=8.846169329138815\n",
      "Stochastic Gradient Descent(7607): loss=11.825316608624936\n",
      "Stochastic Gradient Descent(7608): loss=0.7108283517527639\n",
      "Stochastic Gradient Descent(7609): loss=1.525220710775766\n",
      "Stochastic Gradient Descent(7610): loss=5.2534453275324955\n",
      "Stochastic Gradient Descent(7611): loss=2.0822190451434617\n",
      "Stochastic Gradient Descent(7612): loss=0.30423113292253134\n",
      "Stochastic Gradient Descent(7613): loss=10.426271843097915\n",
      "Stochastic Gradient Descent(7614): loss=0.16531814963743438\n",
      "Stochastic Gradient Descent(7615): loss=1.9574575320657364\n",
      "Stochastic Gradient Descent(7616): loss=36.55679359370203\n",
      "Stochastic Gradient Descent(7617): loss=7.444601600023594\n",
      "Stochastic Gradient Descent(7618): loss=0.05509042344063211\n",
      "Stochastic Gradient Descent(7619): loss=24.66088954869599\n",
      "Stochastic Gradient Descent(7620): loss=6.490848448026628\n",
      "Stochastic Gradient Descent(7621): loss=1.2785795550290158\n",
      "Stochastic Gradient Descent(7622): loss=0.6451746786404732\n",
      "Stochastic Gradient Descent(7623): loss=1.2817475304561825\n",
      "Stochastic Gradient Descent(7624): loss=0.0013888781780758872\n",
      "Stochastic Gradient Descent(7625): loss=10.988261855443264\n",
      "Stochastic Gradient Descent(7626): loss=11.88366121215211\n",
      "Stochastic Gradient Descent(7627): loss=0.9001267646999719\n",
      "Stochastic Gradient Descent(7628): loss=46.56712875934861\n",
      "Stochastic Gradient Descent(7629): loss=0.38847865425374145\n",
      "Stochastic Gradient Descent(7630): loss=2.311633733739411\n",
      "Stochastic Gradient Descent(7631): loss=1.783929412695929\n",
      "Stochastic Gradient Descent(7632): loss=59.69394035158024\n",
      "Stochastic Gradient Descent(7633): loss=44.66268682738501\n",
      "Stochastic Gradient Descent(7634): loss=1.5852122298557563\n",
      "Stochastic Gradient Descent(7635): loss=1.0013834517459244\n",
      "Stochastic Gradient Descent(7636): loss=0.4461764074706457\n",
      "Stochastic Gradient Descent(7637): loss=0.37969629598351434\n",
      "Stochastic Gradient Descent(7638): loss=0.6918699089383478\n",
      "Stochastic Gradient Descent(7639): loss=0.3697723485411163\n",
      "Stochastic Gradient Descent(7640): loss=0.6308944727046619\n",
      "Stochastic Gradient Descent(7641): loss=4.721299483602226\n",
      "Stochastic Gradient Descent(7642): loss=11.643894854737956\n",
      "Stochastic Gradient Descent(7643): loss=8.186615352736291\n",
      "Stochastic Gradient Descent(7644): loss=8.546387914966202\n",
      "Stochastic Gradient Descent(7645): loss=1.354438105712694\n",
      "Stochastic Gradient Descent(7646): loss=0.12444648196256766\n",
      "Stochastic Gradient Descent(7647): loss=0.16563768560007686\n",
      "Stochastic Gradient Descent(7648): loss=1.0182854792464633\n",
      "Stochastic Gradient Descent(7649): loss=5.034795423610237\n",
      "Stochastic Gradient Descent(7650): loss=4.52469306545567\n",
      "Stochastic Gradient Descent(7651): loss=2.8402905291161122\n",
      "Stochastic Gradient Descent(7652): loss=18.220756036012105\n",
      "Stochastic Gradient Descent(7653): loss=3.544629272335091\n",
      "Stochastic Gradient Descent(7654): loss=1.4885550605657374\n",
      "Stochastic Gradient Descent(7655): loss=13.619303969350714\n",
      "Stochastic Gradient Descent(7656): loss=0.04437389537147875\n",
      "Stochastic Gradient Descent(7657): loss=0.36652631899606647\n",
      "Stochastic Gradient Descent(7658): loss=2.170697831514118\n",
      "Stochastic Gradient Descent(7659): loss=9.080659138430416\n",
      "Stochastic Gradient Descent(7660): loss=0.0351885140995758\n",
      "Stochastic Gradient Descent(7661): loss=0.10463703024043158\n",
      "Stochastic Gradient Descent(7662): loss=3.8692665784433276\n",
      "Stochastic Gradient Descent(7663): loss=22.061451599698465\n",
      "Stochastic Gradient Descent(7664): loss=4.729263954790149\n",
      "Stochastic Gradient Descent(7665): loss=16.85677561734229\n",
      "Stochastic Gradient Descent(7666): loss=2.01367987503337\n",
      "Stochastic Gradient Descent(7667): loss=4.593860575677057\n",
      "Stochastic Gradient Descent(7668): loss=0.967795791171306\n",
      "Stochastic Gradient Descent(7669): loss=4.876649730488635\n",
      "Stochastic Gradient Descent(7670): loss=9.940899379808778\n",
      "Stochastic Gradient Descent(7671): loss=2.429316442598589\n",
      "Stochastic Gradient Descent(7672): loss=11.438811344097816\n",
      "Stochastic Gradient Descent(7673): loss=1.4588202516983044\n",
      "Stochastic Gradient Descent(7674): loss=0.8482358348189375\n",
      "Stochastic Gradient Descent(7675): loss=5.006046156913715\n",
      "Stochastic Gradient Descent(7676): loss=23.909205437563816\n",
      "Stochastic Gradient Descent(7677): loss=4.326554186849444\n",
      "Stochastic Gradient Descent(7678): loss=2.380082054995459\n",
      "Stochastic Gradient Descent(7679): loss=8.336838603227735\n",
      "Stochastic Gradient Descent(7680): loss=12.391519033269644\n",
      "Stochastic Gradient Descent(7681): loss=0.18373721806290166\n",
      "Stochastic Gradient Descent(7682): loss=0.18043009944388785\n",
      "Stochastic Gradient Descent(7683): loss=0.8899394131280836\n",
      "Stochastic Gradient Descent(7684): loss=0.16499155862265533\n",
      "Stochastic Gradient Descent(7685): loss=2.1051264348407512\n",
      "Stochastic Gradient Descent(7686): loss=0.027817735916490848\n",
      "Stochastic Gradient Descent(7687): loss=5.002272105914054\n",
      "Stochastic Gradient Descent(7688): loss=16.87033436006972\n",
      "Stochastic Gradient Descent(7689): loss=9.455205378447449\n",
      "Stochastic Gradient Descent(7690): loss=4.946347109089644\n",
      "Stochastic Gradient Descent(7691): loss=1.1864259315350831\n",
      "Stochastic Gradient Descent(7692): loss=8.000881415617311\n",
      "Stochastic Gradient Descent(7693): loss=0.5174620949558627\n",
      "Stochastic Gradient Descent(7694): loss=19.73571216503748\n",
      "Stochastic Gradient Descent(7695): loss=1.2664882774491215\n",
      "Stochastic Gradient Descent(7696): loss=0.7980685251667633\n",
      "Stochastic Gradient Descent(7697): loss=4.38471067717079\n",
      "Stochastic Gradient Descent(7698): loss=13.482157810472067\n",
      "Stochastic Gradient Descent(7699): loss=10.53690959693998\n",
      "Stochastic Gradient Descent(7700): loss=1.9708903465907897\n",
      "Stochastic Gradient Descent(7701): loss=0.6371655323874572\n",
      "Stochastic Gradient Descent(7702): loss=14.713660440844126\n",
      "Stochastic Gradient Descent(7703): loss=6.4669970642486145\n",
      "Stochastic Gradient Descent(7704): loss=0.019660052207980255\n",
      "Stochastic Gradient Descent(7705): loss=0.42680132533063486\n",
      "Stochastic Gradient Descent(7706): loss=0.939999199586662\n",
      "Stochastic Gradient Descent(7707): loss=3.233506316168382\n",
      "Stochastic Gradient Descent(7708): loss=1.3609191402895708\n",
      "Stochastic Gradient Descent(7709): loss=4.580310257960486\n",
      "Stochastic Gradient Descent(7710): loss=6.496664040354068\n",
      "Stochastic Gradient Descent(7711): loss=0.4418093295778046\n",
      "Stochastic Gradient Descent(7712): loss=0.006734743665867734\n",
      "Stochastic Gradient Descent(7713): loss=0.9551715448672449\n",
      "Stochastic Gradient Descent(7714): loss=3.0682496739566063\n",
      "Stochastic Gradient Descent(7715): loss=2.842245011808801\n",
      "Stochastic Gradient Descent(7716): loss=0.4133029557396565\n",
      "Stochastic Gradient Descent(7717): loss=5.215328140462074\n",
      "Stochastic Gradient Descent(7718): loss=3.7356493263475716\n",
      "Stochastic Gradient Descent(7719): loss=0.023982566477035658\n",
      "Stochastic Gradient Descent(7720): loss=0.00763417585021849\n",
      "Stochastic Gradient Descent(7721): loss=6.412969496316737\n",
      "Stochastic Gradient Descent(7722): loss=0.061480493012807357\n",
      "Stochastic Gradient Descent(7723): loss=0.7970773901973981\n",
      "Stochastic Gradient Descent(7724): loss=0.6307440868854893\n",
      "Stochastic Gradient Descent(7725): loss=6.656286283929955\n",
      "Stochastic Gradient Descent(7726): loss=11.348225345956111\n",
      "Stochastic Gradient Descent(7727): loss=0.18118358435252005\n",
      "Stochastic Gradient Descent(7728): loss=1.7063769924167458\n",
      "Stochastic Gradient Descent(7729): loss=6.597584709923261\n",
      "Stochastic Gradient Descent(7730): loss=29.54021003825128\n",
      "Stochastic Gradient Descent(7731): loss=0.7845048735288235\n",
      "Stochastic Gradient Descent(7732): loss=31.70538210019698\n",
      "Stochastic Gradient Descent(7733): loss=7.207153557052231\n",
      "Stochastic Gradient Descent(7734): loss=0.11792589118928293\n",
      "Stochastic Gradient Descent(7735): loss=15.128818533013494\n",
      "Stochastic Gradient Descent(7736): loss=3.346967491795359\n",
      "Stochastic Gradient Descent(7737): loss=1.6971291487060378\n",
      "Stochastic Gradient Descent(7738): loss=3.4070872240979986\n",
      "Stochastic Gradient Descent(7739): loss=0.8898729999091116\n",
      "Stochastic Gradient Descent(7740): loss=96.89960617737908\n",
      "Stochastic Gradient Descent(7741): loss=23.53675455595832\n",
      "Stochastic Gradient Descent(7742): loss=0.013542782951398177\n",
      "Stochastic Gradient Descent(7743): loss=4.8224902478933895\n",
      "Stochastic Gradient Descent(7744): loss=0.08013045486072666\n",
      "Stochastic Gradient Descent(7745): loss=3.506380416843812\n",
      "Stochastic Gradient Descent(7746): loss=5.393190418803473\n",
      "Stochastic Gradient Descent(7747): loss=2.0051751537966607\n",
      "Stochastic Gradient Descent(7748): loss=2.084363478114233\n",
      "Stochastic Gradient Descent(7749): loss=1.2978842207055408\n",
      "Stochastic Gradient Descent(7750): loss=5.584331838085699\n",
      "Stochastic Gradient Descent(7751): loss=0.34933268278968427\n",
      "Stochastic Gradient Descent(7752): loss=0.02167932746322327\n",
      "Stochastic Gradient Descent(7753): loss=9.214316064293731\n",
      "Stochastic Gradient Descent(7754): loss=0.7071867788144364\n",
      "Stochastic Gradient Descent(7755): loss=0.1583731387608192\n",
      "Stochastic Gradient Descent(7756): loss=0.07708303432570841\n",
      "Stochastic Gradient Descent(7757): loss=34.598609660834164\n",
      "Stochastic Gradient Descent(7758): loss=22.146100478191702\n",
      "Stochastic Gradient Descent(7759): loss=0.1813697744016268\n",
      "Stochastic Gradient Descent(7760): loss=0.40232108226451585\n",
      "Stochastic Gradient Descent(7761): loss=1.0785180205091283\n",
      "Stochastic Gradient Descent(7762): loss=1.42145165728905\n",
      "Stochastic Gradient Descent(7763): loss=4.453447107261201\n",
      "Stochastic Gradient Descent(7764): loss=6.515112654816026\n",
      "Stochastic Gradient Descent(7765): loss=4.395381634578754\n",
      "Stochastic Gradient Descent(7766): loss=0.041128019991023515\n",
      "Stochastic Gradient Descent(7767): loss=5.308274759068935\n",
      "Stochastic Gradient Descent(7768): loss=2.2063454984673343\n",
      "Stochastic Gradient Descent(7769): loss=0.09677252333547645\n",
      "Stochastic Gradient Descent(7770): loss=8.662506719048137\n",
      "Stochastic Gradient Descent(7771): loss=23.816798811695843\n",
      "Stochastic Gradient Descent(7772): loss=22.36894164205728\n",
      "Stochastic Gradient Descent(7773): loss=13.157344705074797\n",
      "Stochastic Gradient Descent(7774): loss=0.06337806165667342\n",
      "Stochastic Gradient Descent(7775): loss=1.1910645941167637\n",
      "Stochastic Gradient Descent(7776): loss=0.4323413098959607\n",
      "Stochastic Gradient Descent(7777): loss=2.2819316077643705\n",
      "Stochastic Gradient Descent(7778): loss=4.861722737118475\n",
      "Stochastic Gradient Descent(7779): loss=0.010305866172376162\n",
      "Stochastic Gradient Descent(7780): loss=3.474822526743924\n",
      "Stochastic Gradient Descent(7781): loss=2.909887068226654\n",
      "Stochastic Gradient Descent(7782): loss=1.595915578060103\n",
      "Stochastic Gradient Descent(7783): loss=6.267223325933484\n",
      "Stochastic Gradient Descent(7784): loss=0.5045505419292828\n",
      "Stochastic Gradient Descent(7785): loss=0.5119185708742123\n",
      "Stochastic Gradient Descent(7786): loss=0.10407284465061334\n",
      "Stochastic Gradient Descent(7787): loss=21.192079062037834\n",
      "Stochastic Gradient Descent(7788): loss=0.14952372549437531\n",
      "Stochastic Gradient Descent(7789): loss=3.462158448729595\n",
      "Stochastic Gradient Descent(7790): loss=0.5262662752544739\n",
      "Stochastic Gradient Descent(7791): loss=0.07253506634613387\n",
      "Stochastic Gradient Descent(7792): loss=0.04587531559052311\n",
      "Stochastic Gradient Descent(7793): loss=2.7777359372827264\n",
      "Stochastic Gradient Descent(7794): loss=2.1508308762981274\n",
      "Stochastic Gradient Descent(7795): loss=0.30552477576601683\n",
      "Stochastic Gradient Descent(7796): loss=6.5081575854384175\n",
      "Stochastic Gradient Descent(7797): loss=0.796857555376218\n",
      "Stochastic Gradient Descent(7798): loss=1.4474480540030512\n",
      "Stochastic Gradient Descent(7799): loss=1.6560709629485204\n",
      "Stochastic Gradient Descent(7800): loss=13.97222576134585\n",
      "Stochastic Gradient Descent(7801): loss=3.287069472321563\n",
      "Stochastic Gradient Descent(7802): loss=13.575014438862388\n",
      "Stochastic Gradient Descent(7803): loss=2.365225105163093\n",
      "Stochastic Gradient Descent(7804): loss=1.2548280456102414\n",
      "Stochastic Gradient Descent(7805): loss=0.8996857126775045\n",
      "Stochastic Gradient Descent(7806): loss=5.604921391184226\n",
      "Stochastic Gradient Descent(7807): loss=10.276114461965511\n",
      "Stochastic Gradient Descent(7808): loss=2.5011775708407544\n",
      "Stochastic Gradient Descent(7809): loss=0.3643179720906287\n",
      "Stochastic Gradient Descent(7810): loss=6.077216437766659\n",
      "Stochastic Gradient Descent(7811): loss=9.388153582600737\n",
      "Stochastic Gradient Descent(7812): loss=2.2341089970068313\n",
      "Stochastic Gradient Descent(7813): loss=18.164527258217205\n",
      "Stochastic Gradient Descent(7814): loss=0.3333428810676597\n",
      "Stochastic Gradient Descent(7815): loss=0.0757454366481622\n",
      "Stochastic Gradient Descent(7816): loss=0.10515682067810288\n",
      "Stochastic Gradient Descent(7817): loss=7.578110104567886\n",
      "Stochastic Gradient Descent(7818): loss=0.4349789388616388\n",
      "Stochastic Gradient Descent(7819): loss=2.2096497424824855\n",
      "Stochastic Gradient Descent(7820): loss=25.210082404904583\n",
      "Stochastic Gradient Descent(7821): loss=0.5951750735262868\n",
      "Stochastic Gradient Descent(7822): loss=4.350181070072398\n",
      "Stochastic Gradient Descent(7823): loss=2.707026712789344\n",
      "Stochastic Gradient Descent(7824): loss=0.17203108423479968\n",
      "Stochastic Gradient Descent(7825): loss=11.033990274272385\n",
      "Stochastic Gradient Descent(7826): loss=4.468607286476715\n",
      "Stochastic Gradient Descent(7827): loss=7.828363742920093\n",
      "Stochastic Gradient Descent(7828): loss=6.274013565842381\n",
      "Stochastic Gradient Descent(7829): loss=0.06002101564207986\n",
      "Stochastic Gradient Descent(7830): loss=23.801691068621643\n",
      "Stochastic Gradient Descent(7831): loss=18.394625294786227\n",
      "Stochastic Gradient Descent(7832): loss=0.14410345433418778\n",
      "Stochastic Gradient Descent(7833): loss=0.7877927732586261\n",
      "Stochastic Gradient Descent(7834): loss=7.840519922832993\n",
      "Stochastic Gradient Descent(7835): loss=2.3337048077078744\n",
      "Stochastic Gradient Descent(7836): loss=13.69549950138917\n",
      "Stochastic Gradient Descent(7837): loss=0.9612816214176563\n",
      "Stochastic Gradient Descent(7838): loss=1.5839647681232092\n",
      "Stochastic Gradient Descent(7839): loss=7.550696528191574\n",
      "Stochastic Gradient Descent(7840): loss=4.05020692730928\n",
      "Stochastic Gradient Descent(7841): loss=7.419662683330104\n",
      "Stochastic Gradient Descent(7842): loss=11.30105784064379\n",
      "Stochastic Gradient Descent(7843): loss=1.1350230378500132\n",
      "Stochastic Gradient Descent(7844): loss=0.6009975178495798\n",
      "Stochastic Gradient Descent(7845): loss=1.086811210892612\n",
      "Stochastic Gradient Descent(7846): loss=7.637363209488866\n",
      "Stochastic Gradient Descent(7847): loss=0.002850058511908963\n",
      "Stochastic Gradient Descent(7848): loss=0.47800726961816087\n",
      "Stochastic Gradient Descent(7849): loss=0.05878500296186786\n",
      "Stochastic Gradient Descent(7850): loss=4.897705713916834\n",
      "Stochastic Gradient Descent(7851): loss=1.061054668708019\n",
      "Stochastic Gradient Descent(7852): loss=20.438579298872252\n",
      "Stochastic Gradient Descent(7853): loss=3.598497153175305\n",
      "Stochastic Gradient Descent(7854): loss=2.6526814862703234\n",
      "Stochastic Gradient Descent(7855): loss=10.147226692067434\n",
      "Stochastic Gradient Descent(7856): loss=1.0818237827979573\n",
      "Stochastic Gradient Descent(7857): loss=0.008064260393684965\n",
      "Stochastic Gradient Descent(7858): loss=4.887956994190968\n",
      "Stochastic Gradient Descent(7859): loss=5.637562542604579\n",
      "Stochastic Gradient Descent(7860): loss=1.9114101734411364\n",
      "Stochastic Gradient Descent(7861): loss=15.40340530966554\n",
      "Stochastic Gradient Descent(7862): loss=3.450188410036323\n",
      "Stochastic Gradient Descent(7863): loss=0.7627320981391187\n",
      "Stochastic Gradient Descent(7864): loss=0.044907094465233784\n",
      "Stochastic Gradient Descent(7865): loss=0.32694110409037386\n",
      "Stochastic Gradient Descent(7866): loss=0.6906750352590504\n",
      "Stochastic Gradient Descent(7867): loss=4.088882895799565\n",
      "Stochastic Gradient Descent(7868): loss=0.9081693575297607\n",
      "Stochastic Gradient Descent(7869): loss=1.4932715878457845\n",
      "Stochastic Gradient Descent(7870): loss=0.7907355511055523\n",
      "Stochastic Gradient Descent(7871): loss=12.2679941554498\n",
      "Stochastic Gradient Descent(7872): loss=0.40946894772762615\n",
      "Stochastic Gradient Descent(7873): loss=8.13355975584968\n",
      "Stochastic Gradient Descent(7874): loss=0.04837024150143597\n",
      "Stochastic Gradient Descent(7875): loss=0.06044344739962854\n",
      "Stochastic Gradient Descent(7876): loss=0.7705272953897283\n",
      "Stochastic Gradient Descent(7877): loss=2.5526256616631806\n",
      "Stochastic Gradient Descent(7878): loss=3.6769503000946657\n",
      "Stochastic Gradient Descent(7879): loss=9.859767180240402\n",
      "Stochastic Gradient Descent(7880): loss=0.0011938822939788342\n",
      "Stochastic Gradient Descent(7881): loss=0.1629664597471012\n",
      "Stochastic Gradient Descent(7882): loss=12.949802361570512\n",
      "Stochastic Gradient Descent(7883): loss=0.11586217868085674\n",
      "Stochastic Gradient Descent(7884): loss=0.22736554174778859\n",
      "Stochastic Gradient Descent(7885): loss=3.2303721730279475\n",
      "Stochastic Gradient Descent(7886): loss=7.401756737558473\n",
      "Stochastic Gradient Descent(7887): loss=0.15786261878235694\n",
      "Stochastic Gradient Descent(7888): loss=2.0926173716511176\n",
      "Stochastic Gradient Descent(7889): loss=4.447656957451958\n",
      "Stochastic Gradient Descent(7890): loss=0.061242080786767364\n",
      "Stochastic Gradient Descent(7891): loss=0.20344538855603672\n",
      "Stochastic Gradient Descent(7892): loss=0.2966481364471118\n",
      "Stochastic Gradient Descent(7893): loss=10.620871283465297\n",
      "Stochastic Gradient Descent(7894): loss=0.2978589185192789\n",
      "Stochastic Gradient Descent(7895): loss=15.745128466738512\n",
      "Stochastic Gradient Descent(7896): loss=58.35647369486526\n",
      "Stochastic Gradient Descent(7897): loss=21.25059049764945\n",
      "Stochastic Gradient Descent(7898): loss=9.531230598176425\n",
      "Stochastic Gradient Descent(7899): loss=22.493579526751677\n",
      "Stochastic Gradient Descent(7900): loss=1.3940966737856637\n",
      "Stochastic Gradient Descent(7901): loss=1.2076043849575029\n",
      "Stochastic Gradient Descent(7902): loss=0.361419440097914\n",
      "Stochastic Gradient Descent(7903): loss=0.47485225269264997\n",
      "Stochastic Gradient Descent(7904): loss=0.08617574105912686\n",
      "Stochastic Gradient Descent(7905): loss=3.556289819879616\n",
      "Stochastic Gradient Descent(7906): loss=0.29042157516709316\n",
      "Stochastic Gradient Descent(7907): loss=48.800560368615116\n",
      "Stochastic Gradient Descent(7908): loss=1.3226693694451832\n",
      "Stochastic Gradient Descent(7909): loss=1.977062952849997\n",
      "Stochastic Gradient Descent(7910): loss=24.27764128569146\n",
      "Stochastic Gradient Descent(7911): loss=10.127360869570651\n",
      "Stochastic Gradient Descent(7912): loss=26.871568515331198\n",
      "Stochastic Gradient Descent(7913): loss=3.0141272863361293\n",
      "Stochastic Gradient Descent(7914): loss=3.95448960734055\n",
      "Stochastic Gradient Descent(7915): loss=5.544566882626666\n",
      "Stochastic Gradient Descent(7916): loss=0.05667478035425741\n",
      "Stochastic Gradient Descent(7917): loss=7.407234737684568\n",
      "Stochastic Gradient Descent(7918): loss=11.479487959180856\n",
      "Stochastic Gradient Descent(7919): loss=0.006731310489939881\n",
      "Stochastic Gradient Descent(7920): loss=13.924665101862983\n",
      "Stochastic Gradient Descent(7921): loss=11.659841115260669\n",
      "Stochastic Gradient Descent(7922): loss=3.660628156094125\n",
      "Stochastic Gradient Descent(7923): loss=5.428406454555736\n",
      "Stochastic Gradient Descent(7924): loss=0.8267067324882742\n",
      "Stochastic Gradient Descent(7925): loss=0.10759196612266973\n",
      "Stochastic Gradient Descent(7926): loss=3.8354136916660506\n",
      "Stochastic Gradient Descent(7927): loss=0.08099430253674995\n",
      "Stochastic Gradient Descent(7928): loss=2.9722259053136266\n",
      "Stochastic Gradient Descent(7929): loss=0.17564209566253933\n",
      "Stochastic Gradient Descent(7930): loss=5.28940472041529\n",
      "Stochastic Gradient Descent(7931): loss=0.014558672450691052\n",
      "Stochastic Gradient Descent(7932): loss=0.12516985660197358\n",
      "Stochastic Gradient Descent(7933): loss=79.7010128789681\n",
      "Stochastic Gradient Descent(7934): loss=3.647059620960236\n",
      "Stochastic Gradient Descent(7935): loss=61.93224534913394\n",
      "Stochastic Gradient Descent(7936): loss=0.6941580403850267\n",
      "Stochastic Gradient Descent(7937): loss=1.5064760600743967\n",
      "Stochastic Gradient Descent(7938): loss=10.775397565080697\n",
      "Stochastic Gradient Descent(7939): loss=0.08688243684984767\n",
      "Stochastic Gradient Descent(7940): loss=0.0011767187585895965\n",
      "Stochastic Gradient Descent(7941): loss=0.0018117071529441292\n",
      "Stochastic Gradient Descent(7942): loss=4.615585242128219\n",
      "Stochastic Gradient Descent(7943): loss=0.6144290363244962\n",
      "Stochastic Gradient Descent(7944): loss=0.22555850543733497\n",
      "Stochastic Gradient Descent(7945): loss=8.192831022968901\n",
      "Stochastic Gradient Descent(7946): loss=0.23463901976464233\n",
      "Stochastic Gradient Descent(7947): loss=1.8267847961909203\n",
      "Stochastic Gradient Descent(7948): loss=9.797416142803362\n",
      "Stochastic Gradient Descent(7949): loss=0.020530045892067493\n",
      "Stochastic Gradient Descent(7950): loss=1.3820564783870417\n",
      "Stochastic Gradient Descent(7951): loss=18.625280964462597\n",
      "Stochastic Gradient Descent(7952): loss=16.80969311982505\n",
      "Stochastic Gradient Descent(7953): loss=0.3440770524067544\n",
      "Stochastic Gradient Descent(7954): loss=19.646281738525182\n",
      "Stochastic Gradient Descent(7955): loss=0.11954640776653547\n",
      "Stochastic Gradient Descent(7956): loss=3.330989400371206\n",
      "Stochastic Gradient Descent(7957): loss=6.7573898926692575\n",
      "Stochastic Gradient Descent(7958): loss=1.2486672616124523\n",
      "Stochastic Gradient Descent(7959): loss=0.7812679857717053\n",
      "Stochastic Gradient Descent(7960): loss=0.0014496905801521165\n",
      "Stochastic Gradient Descent(7961): loss=0.5151590271241739\n",
      "Stochastic Gradient Descent(7962): loss=1.4324567233674868\n",
      "Stochastic Gradient Descent(7963): loss=3.5307176362477537\n",
      "Stochastic Gradient Descent(7964): loss=21.16800935087651\n",
      "Stochastic Gradient Descent(7965): loss=0.3332777452010085\n",
      "Stochastic Gradient Descent(7966): loss=1.6405172502652579\n",
      "Stochastic Gradient Descent(7967): loss=5.35967732834241\n",
      "Stochastic Gradient Descent(7968): loss=7.86691737076929\n",
      "Stochastic Gradient Descent(7969): loss=1.8355605444681864\n",
      "Stochastic Gradient Descent(7970): loss=1.9318260588622793\n",
      "Stochastic Gradient Descent(7971): loss=0.03837452145593189\n",
      "Stochastic Gradient Descent(7972): loss=0.008547419460423384\n",
      "Stochastic Gradient Descent(7973): loss=11.105957352540438\n",
      "Stochastic Gradient Descent(7974): loss=6.145298395497337\n",
      "Stochastic Gradient Descent(7975): loss=0.3466645325746326\n",
      "Stochastic Gradient Descent(7976): loss=0.010254696558619013\n",
      "Stochastic Gradient Descent(7977): loss=61.31774930036241\n",
      "Stochastic Gradient Descent(7978): loss=7.559235694867366\n",
      "Stochastic Gradient Descent(7979): loss=3.7992242535164675\n",
      "Stochastic Gradient Descent(7980): loss=20.214960655368042\n",
      "Stochastic Gradient Descent(7981): loss=0.1602628160123279\n",
      "Stochastic Gradient Descent(7982): loss=5.795198669429027\n",
      "Stochastic Gradient Descent(7983): loss=7.931211756223279\n",
      "Stochastic Gradient Descent(7984): loss=0.27623808456622506\n",
      "Stochastic Gradient Descent(7985): loss=0.16779085417314682\n",
      "Stochastic Gradient Descent(7986): loss=23.702000473325622\n",
      "Stochastic Gradient Descent(7987): loss=0.00021073623089102152\n",
      "Stochastic Gradient Descent(7988): loss=23.673814262379636\n",
      "Stochastic Gradient Descent(7989): loss=0.005590326872402839\n",
      "Stochastic Gradient Descent(7990): loss=15.27065555738219\n",
      "Stochastic Gradient Descent(7991): loss=11.29741154718711\n",
      "Stochastic Gradient Descent(7992): loss=18.92878501509976\n",
      "Stochastic Gradient Descent(7993): loss=30.319093010893994\n",
      "Stochastic Gradient Descent(7994): loss=9.977215401973517\n",
      "Stochastic Gradient Descent(7995): loss=0.5082591326607544\n",
      "Stochastic Gradient Descent(7996): loss=5.472687689851641\n",
      "Stochastic Gradient Descent(7997): loss=40.53566484580964\n",
      "Stochastic Gradient Descent(7998): loss=14.660529435884614\n",
      "Stochastic Gradient Descent(7999): loss=0.864247580569024\n",
      "Stochastic Gradient Descent(8000): loss=0.09033774706702828\n",
      "Stochastic Gradient Descent(8001): loss=23.887551489954753\n",
      "Stochastic Gradient Descent(8002): loss=2.113843876648503\n",
      "Stochastic Gradient Descent(8003): loss=12.454813721990293\n",
      "Stochastic Gradient Descent(8004): loss=24.507003130693295\n",
      "Stochastic Gradient Descent(8005): loss=18.690544916268877\n",
      "Stochastic Gradient Descent(8006): loss=0.5737576996726588\n",
      "Stochastic Gradient Descent(8007): loss=13.156400989076756\n",
      "Stochastic Gradient Descent(8008): loss=4.244074889786189\n",
      "Stochastic Gradient Descent(8009): loss=10.977681923851865\n",
      "Stochastic Gradient Descent(8010): loss=2.1266637841269103\n",
      "Stochastic Gradient Descent(8011): loss=8.286606467569568\n",
      "Stochastic Gradient Descent(8012): loss=3.2663326569201407\n",
      "Stochastic Gradient Descent(8013): loss=0.23672091321052852\n",
      "Stochastic Gradient Descent(8014): loss=1.380042214251618\n",
      "Stochastic Gradient Descent(8015): loss=5.504389597494966\n",
      "Stochastic Gradient Descent(8016): loss=4.012314865550137\n",
      "Stochastic Gradient Descent(8017): loss=2.548408777987435\n",
      "Stochastic Gradient Descent(8018): loss=7.598058226856551\n",
      "Stochastic Gradient Descent(8019): loss=4.433888034643945\n",
      "Stochastic Gradient Descent(8020): loss=0.36528342807402997\n",
      "Stochastic Gradient Descent(8021): loss=3.194950669309629\n",
      "Stochastic Gradient Descent(8022): loss=17.618096245192753\n",
      "Stochastic Gradient Descent(8023): loss=0.5749040405919307\n",
      "Stochastic Gradient Descent(8024): loss=0.0043827748374576665\n",
      "Stochastic Gradient Descent(8025): loss=11.61507152948689\n",
      "Stochastic Gradient Descent(8026): loss=6.147636050743315\n",
      "Stochastic Gradient Descent(8027): loss=8.577134669742817\n",
      "Stochastic Gradient Descent(8028): loss=14.338175206978649\n",
      "Stochastic Gradient Descent(8029): loss=0.1489286001699873\n",
      "Stochastic Gradient Descent(8030): loss=1.439955481613328\n",
      "Stochastic Gradient Descent(8031): loss=0.6673583012883154\n",
      "Stochastic Gradient Descent(8032): loss=59.66106192175033\n",
      "Stochastic Gradient Descent(8033): loss=39.457909207627395\n",
      "Stochastic Gradient Descent(8034): loss=0.7746709328130666\n",
      "Stochastic Gradient Descent(8035): loss=1.1293508800845586\n",
      "Stochastic Gradient Descent(8036): loss=12.65086693777953\n",
      "Stochastic Gradient Descent(8037): loss=0.5021146113442652\n",
      "Stochastic Gradient Descent(8038): loss=94.01525823727638\n",
      "Stochastic Gradient Descent(8039): loss=11.19782959217591\n",
      "Stochastic Gradient Descent(8040): loss=17.89278389576559\n",
      "Stochastic Gradient Descent(8041): loss=11.608292259925788\n",
      "Stochastic Gradient Descent(8042): loss=0.4479918976369243\n",
      "Stochastic Gradient Descent(8043): loss=0.1942259090536497\n",
      "Stochastic Gradient Descent(8044): loss=0.16770910185175408\n",
      "Stochastic Gradient Descent(8045): loss=0.001942431897353824\n",
      "Stochastic Gradient Descent(8046): loss=0.07838541019122432\n",
      "Stochastic Gradient Descent(8047): loss=2.61204404941247\n",
      "Stochastic Gradient Descent(8048): loss=0.02252245838400238\n",
      "Stochastic Gradient Descent(8049): loss=17.27560011270749\n",
      "Stochastic Gradient Descent(8050): loss=54.54130506169569\n",
      "Stochastic Gradient Descent(8051): loss=26.50227216187962\n",
      "Stochastic Gradient Descent(8052): loss=0.15301286495885982\n",
      "Stochastic Gradient Descent(8053): loss=3.386768729169045\n",
      "Stochastic Gradient Descent(8054): loss=21.978428317025\n",
      "Stochastic Gradient Descent(8055): loss=4.195141427960924\n",
      "Stochastic Gradient Descent(8056): loss=1.16567040900377\n",
      "Stochastic Gradient Descent(8057): loss=5.906528808773259\n",
      "Stochastic Gradient Descent(8058): loss=3.273543499573283\n",
      "Stochastic Gradient Descent(8059): loss=1.498631259253441\n",
      "Stochastic Gradient Descent(8060): loss=5.068718467091532\n",
      "Stochastic Gradient Descent(8061): loss=3.184942285698539\n",
      "Stochastic Gradient Descent(8062): loss=1.7444220225695524\n",
      "Stochastic Gradient Descent(8063): loss=4.8455677770213\n",
      "Stochastic Gradient Descent(8064): loss=3.586252392129399\n",
      "Stochastic Gradient Descent(8065): loss=6.789196087285968\n",
      "Stochastic Gradient Descent(8066): loss=3.3937442772483712\n",
      "Stochastic Gradient Descent(8067): loss=24.90747799928179\n",
      "Stochastic Gradient Descent(8068): loss=5.421396787420449\n",
      "Stochastic Gradient Descent(8069): loss=10.515730102232064\n",
      "Stochastic Gradient Descent(8070): loss=4.049222607152746\n",
      "Stochastic Gradient Descent(8071): loss=24.74268816963503\n",
      "Stochastic Gradient Descent(8072): loss=1.1911257701949953\n",
      "Stochastic Gradient Descent(8073): loss=0.7776652090826311\n",
      "Stochastic Gradient Descent(8074): loss=0.7558530994870943\n",
      "Stochastic Gradient Descent(8075): loss=1.3763424575068868\n",
      "Stochastic Gradient Descent(8076): loss=4.952139476989546\n",
      "Stochastic Gradient Descent(8077): loss=16.83810627429985\n",
      "Stochastic Gradient Descent(8078): loss=2.0636181959507613\n",
      "Stochastic Gradient Descent(8079): loss=10.978534353453638\n",
      "Stochastic Gradient Descent(8080): loss=6.998243945599476\n",
      "Stochastic Gradient Descent(8081): loss=0.003530810104749026\n",
      "Stochastic Gradient Descent(8082): loss=2.6259922140813683\n",
      "Stochastic Gradient Descent(8083): loss=5.278362793803259\n",
      "Stochastic Gradient Descent(8084): loss=0.36167614836531226\n",
      "Stochastic Gradient Descent(8085): loss=64.80773241310793\n",
      "Stochastic Gradient Descent(8086): loss=9.770415799020427\n",
      "Stochastic Gradient Descent(8087): loss=6.309671882404272\n",
      "Stochastic Gradient Descent(8088): loss=9.896289718450971\n",
      "Stochastic Gradient Descent(8089): loss=0.8687897801361467\n",
      "Stochastic Gradient Descent(8090): loss=0.7964918196019889\n",
      "Stochastic Gradient Descent(8091): loss=3.837354564154652\n",
      "Stochastic Gradient Descent(8092): loss=0.06450680672684837\n",
      "Stochastic Gradient Descent(8093): loss=0.07913028739655437\n",
      "Stochastic Gradient Descent(8094): loss=4.48209223374754\n",
      "Stochastic Gradient Descent(8095): loss=1.6278022389980833\n",
      "Stochastic Gradient Descent(8096): loss=2.82491789171317\n",
      "Stochastic Gradient Descent(8097): loss=0.419498923452821\n",
      "Stochastic Gradient Descent(8098): loss=3.9246329654426186\n",
      "Stochastic Gradient Descent(8099): loss=3.0087821702601136\n",
      "Stochastic Gradient Descent(8100): loss=3.028892896829255\n",
      "Stochastic Gradient Descent(8101): loss=0.0004321324158939247\n",
      "Stochastic Gradient Descent(8102): loss=2.292123584133197\n",
      "Stochastic Gradient Descent(8103): loss=4.379049605731829\n",
      "Stochastic Gradient Descent(8104): loss=9.490176041200524\n",
      "Stochastic Gradient Descent(8105): loss=0.00017493655411498986\n",
      "Stochastic Gradient Descent(8106): loss=10.432981821535476\n",
      "Stochastic Gradient Descent(8107): loss=2.8809166314736903\n",
      "Stochastic Gradient Descent(8108): loss=5.362471342065972\n",
      "Stochastic Gradient Descent(8109): loss=0.08176918543683329\n",
      "Stochastic Gradient Descent(8110): loss=9.08299450341409\n",
      "Stochastic Gradient Descent(8111): loss=0.88543809036557\n",
      "Stochastic Gradient Descent(8112): loss=0.3258226145553192\n",
      "Stochastic Gradient Descent(8113): loss=1.4719193427064436\n",
      "Stochastic Gradient Descent(8114): loss=0.7350061603503966\n",
      "Stochastic Gradient Descent(8115): loss=0.6797235033098739\n",
      "Stochastic Gradient Descent(8116): loss=1.4565045491623958\n",
      "Stochastic Gradient Descent(8117): loss=1.8909097990165658\n",
      "Stochastic Gradient Descent(8118): loss=14.409969582023013\n",
      "Stochastic Gradient Descent(8119): loss=0.1421508606031079\n",
      "Stochastic Gradient Descent(8120): loss=0.05324480344033159\n",
      "Stochastic Gradient Descent(8121): loss=0.384030971063193\n",
      "Stochastic Gradient Descent(8122): loss=10.493702795448195\n",
      "Stochastic Gradient Descent(8123): loss=1.3421684938988698\n",
      "Stochastic Gradient Descent(8124): loss=2.689384393713097\n",
      "Stochastic Gradient Descent(8125): loss=4.53036666335827\n",
      "Stochastic Gradient Descent(8126): loss=0.3024487016539453\n",
      "Stochastic Gradient Descent(8127): loss=15.440142619399339\n",
      "Stochastic Gradient Descent(8128): loss=8.84478281426677\n",
      "Stochastic Gradient Descent(8129): loss=14.845001980222664\n",
      "Stochastic Gradient Descent(8130): loss=1.1083000090754296\n",
      "Stochastic Gradient Descent(8131): loss=3.4730435670241424\n",
      "Stochastic Gradient Descent(8132): loss=0.021171379396434684\n",
      "Stochastic Gradient Descent(8133): loss=0.21877333360604073\n",
      "Stochastic Gradient Descent(8134): loss=0.4894243224156507\n",
      "Stochastic Gradient Descent(8135): loss=5.943179245496729\n",
      "Stochastic Gradient Descent(8136): loss=1.471600838077827\n",
      "Stochastic Gradient Descent(8137): loss=11.990588332692946\n",
      "Stochastic Gradient Descent(8138): loss=0.0023570825490340256\n",
      "Stochastic Gradient Descent(8139): loss=11.205250659488033\n",
      "Stochastic Gradient Descent(8140): loss=11.137706178686011\n",
      "Stochastic Gradient Descent(8141): loss=5.620447525523653\n",
      "Stochastic Gradient Descent(8142): loss=2.905365934596398\n",
      "Stochastic Gradient Descent(8143): loss=7.198951645574793\n",
      "Stochastic Gradient Descent(8144): loss=2.9395546628286184\n",
      "Stochastic Gradient Descent(8145): loss=3.8535372722172436\n",
      "Stochastic Gradient Descent(8146): loss=2.432180192766033\n",
      "Stochastic Gradient Descent(8147): loss=0.0020108319049453667\n",
      "Stochastic Gradient Descent(8148): loss=1.0606471576361305\n",
      "Stochastic Gradient Descent(8149): loss=4.15095261335058\n",
      "Stochastic Gradient Descent(8150): loss=3.654495472118733\n",
      "Stochastic Gradient Descent(8151): loss=4.691338383955373\n",
      "Stochastic Gradient Descent(8152): loss=2.3686333839897604\n",
      "Stochastic Gradient Descent(8153): loss=0.3827587145285239\n",
      "Stochastic Gradient Descent(8154): loss=0.8781604537120855\n",
      "Stochastic Gradient Descent(8155): loss=8.199571734953633\n",
      "Stochastic Gradient Descent(8156): loss=0.052507460811769456\n",
      "Stochastic Gradient Descent(8157): loss=0.05758462115945912\n",
      "Stochastic Gradient Descent(8158): loss=14.097072508561398\n",
      "Stochastic Gradient Descent(8159): loss=3.1947745988920992\n",
      "Stochastic Gradient Descent(8160): loss=0.2863538867578022\n",
      "Stochastic Gradient Descent(8161): loss=9.199876953655988\n",
      "Stochastic Gradient Descent(8162): loss=1.4513042586262628\n",
      "Stochastic Gradient Descent(8163): loss=0.44438937093975167\n",
      "Stochastic Gradient Descent(8164): loss=0.9993395716046793\n",
      "Stochastic Gradient Descent(8165): loss=21.962111247211524\n",
      "Stochastic Gradient Descent(8166): loss=0.1983420380903801\n",
      "Stochastic Gradient Descent(8167): loss=1.782833130881362\n",
      "Stochastic Gradient Descent(8168): loss=7.682735581861288\n",
      "Stochastic Gradient Descent(8169): loss=0.11941438521659338\n",
      "Stochastic Gradient Descent(8170): loss=0.5913646010788635\n",
      "Stochastic Gradient Descent(8171): loss=0.44128828662125097\n",
      "Stochastic Gradient Descent(8172): loss=5.495363669775966\n",
      "Stochastic Gradient Descent(8173): loss=0.049973000345323496\n",
      "Stochastic Gradient Descent(8174): loss=0.8802897052923963\n",
      "Stochastic Gradient Descent(8175): loss=0.7227319338099507\n",
      "Stochastic Gradient Descent(8176): loss=7.118363306102522\n",
      "Stochastic Gradient Descent(8177): loss=2.2656160043648597\n",
      "Stochastic Gradient Descent(8178): loss=10.891429647111375\n",
      "Stochastic Gradient Descent(8179): loss=12.091258634381022\n",
      "Stochastic Gradient Descent(8180): loss=0.0007942018483893907\n",
      "Stochastic Gradient Descent(8181): loss=1.4442302018586755\n",
      "Stochastic Gradient Descent(8182): loss=0.0018323651867950788\n",
      "Stochastic Gradient Descent(8183): loss=5.181447122383256\n",
      "Stochastic Gradient Descent(8184): loss=8.912579194668183\n",
      "Stochastic Gradient Descent(8185): loss=7.461203289873889\n",
      "Stochastic Gradient Descent(8186): loss=20.3112913915352\n",
      "Stochastic Gradient Descent(8187): loss=0.5193981504358596\n",
      "Stochastic Gradient Descent(8188): loss=0.8636904878371565\n",
      "Stochastic Gradient Descent(8189): loss=7.798259478467016\n",
      "Stochastic Gradient Descent(8190): loss=27.02219095482631\n",
      "Stochastic Gradient Descent(8191): loss=0.1475315228190312\n",
      "Stochastic Gradient Descent(8192): loss=1.1935604516629672\n",
      "Stochastic Gradient Descent(8193): loss=1.377682384406732\n",
      "Stochastic Gradient Descent(8194): loss=1.8414041409173838\n",
      "Stochastic Gradient Descent(8195): loss=4.174554379404332\n",
      "Stochastic Gradient Descent(8196): loss=16.75020723940326\n",
      "Stochastic Gradient Descent(8197): loss=2.9153410733924785\n",
      "Stochastic Gradient Descent(8198): loss=1.0417579085193818\n",
      "Stochastic Gradient Descent(8199): loss=4.099905649266281\n",
      "Stochastic Gradient Descent(8200): loss=0.003948193848425606\n",
      "Stochastic Gradient Descent(8201): loss=2.26403739834139\n",
      "Stochastic Gradient Descent(8202): loss=4.658499463635441\n",
      "Stochastic Gradient Descent(8203): loss=5.145525798540731\n",
      "Stochastic Gradient Descent(8204): loss=0.0021488247327428986\n",
      "Stochastic Gradient Descent(8205): loss=4.115565542440474\n",
      "Stochastic Gradient Descent(8206): loss=28.39952201783415\n",
      "Stochastic Gradient Descent(8207): loss=27.297310234251427\n",
      "Stochastic Gradient Descent(8208): loss=0.42269188864733426\n",
      "Stochastic Gradient Descent(8209): loss=0.25790707210115715\n",
      "Stochastic Gradient Descent(8210): loss=5.674717322283068\n",
      "Stochastic Gradient Descent(8211): loss=0.003259778470562221\n",
      "Stochastic Gradient Descent(8212): loss=0.027101362490937135\n",
      "Stochastic Gradient Descent(8213): loss=0.02431313044337193\n",
      "Stochastic Gradient Descent(8214): loss=0.028793332002490173\n",
      "Stochastic Gradient Descent(8215): loss=3.3842724641746633\n",
      "Stochastic Gradient Descent(8216): loss=1.0328778347322283\n",
      "Stochastic Gradient Descent(8217): loss=13.300374225209625\n",
      "Stochastic Gradient Descent(8218): loss=3.084161897226675\n",
      "Stochastic Gradient Descent(8219): loss=0.5976826302513031\n",
      "Stochastic Gradient Descent(8220): loss=2.9187106225497885\n",
      "Stochastic Gradient Descent(8221): loss=4.132322917159458\n",
      "Stochastic Gradient Descent(8222): loss=2.338960854891746\n",
      "Stochastic Gradient Descent(8223): loss=7.86235941190906\n",
      "Stochastic Gradient Descent(8224): loss=28.86387644645271\n",
      "Stochastic Gradient Descent(8225): loss=4.154699112100992\n",
      "Stochastic Gradient Descent(8226): loss=20.350314912332216\n",
      "Stochastic Gradient Descent(8227): loss=1.564525603676918\n",
      "Stochastic Gradient Descent(8228): loss=0.2780253550636882\n",
      "Stochastic Gradient Descent(8229): loss=7.194449809999992\n",
      "Stochastic Gradient Descent(8230): loss=5.009032706129293\n",
      "Stochastic Gradient Descent(8231): loss=2.771077570806125\n",
      "Stochastic Gradient Descent(8232): loss=0.16420805886318393\n",
      "Stochastic Gradient Descent(8233): loss=2.4761914344470055\n",
      "Stochastic Gradient Descent(8234): loss=1.4716112863032416\n",
      "Stochastic Gradient Descent(8235): loss=0.033086615783936624\n",
      "Stochastic Gradient Descent(8236): loss=0.1009654413714792\n",
      "Stochastic Gradient Descent(8237): loss=4.441797517928466\n",
      "Stochastic Gradient Descent(8238): loss=5.565170980930825\n",
      "Stochastic Gradient Descent(8239): loss=1.9597347534971312\n",
      "Stochastic Gradient Descent(8240): loss=0.5870898730031351\n",
      "Stochastic Gradient Descent(8241): loss=0.0777911656754685\n",
      "Stochastic Gradient Descent(8242): loss=0.04633123026592608\n",
      "Stochastic Gradient Descent(8243): loss=13.148552389064788\n",
      "Stochastic Gradient Descent(8244): loss=0.3365404148243819\n",
      "Stochastic Gradient Descent(8245): loss=0.0002799326872335106\n",
      "Stochastic Gradient Descent(8246): loss=2.8324255302779573\n",
      "Stochastic Gradient Descent(8247): loss=0.02392379701757863\n",
      "Stochastic Gradient Descent(8248): loss=2.6849636305703326\n",
      "Stochastic Gradient Descent(8249): loss=1.125228918809529\n",
      "Stochastic Gradient Descent(8250): loss=0.4780289737418643\n",
      "Stochastic Gradient Descent(8251): loss=8.36932587530885\n",
      "Stochastic Gradient Descent(8252): loss=22.4632895433279\n",
      "Stochastic Gradient Descent(8253): loss=0.782722542119781\n",
      "Stochastic Gradient Descent(8254): loss=8.589142344402958\n",
      "Stochastic Gradient Descent(8255): loss=2.8528234888132005\n",
      "Stochastic Gradient Descent(8256): loss=2.5657636158877017\n",
      "Stochastic Gradient Descent(8257): loss=6.543013434430941\n",
      "Stochastic Gradient Descent(8258): loss=0.24546528871751364\n",
      "Stochastic Gradient Descent(8259): loss=0.7009543761385315\n",
      "Stochastic Gradient Descent(8260): loss=7.463926578738974e-05\n",
      "Stochastic Gradient Descent(8261): loss=3.375276140344959\n",
      "Stochastic Gradient Descent(8262): loss=0.018506351955194757\n",
      "Stochastic Gradient Descent(8263): loss=0.8108447967899857\n",
      "Stochastic Gradient Descent(8264): loss=2.2909343327151444\n",
      "Stochastic Gradient Descent(8265): loss=0.08991598192600257\n",
      "Stochastic Gradient Descent(8266): loss=8.414875215628513\n",
      "Stochastic Gradient Descent(8267): loss=6.442211102080419\n",
      "Stochastic Gradient Descent(8268): loss=9.226503460182471\n",
      "Stochastic Gradient Descent(8269): loss=0.3007588766635238\n",
      "Stochastic Gradient Descent(8270): loss=2.3454110994839112\n",
      "Stochastic Gradient Descent(8271): loss=0.8765659110249255\n",
      "Stochastic Gradient Descent(8272): loss=0.3984004840583078\n",
      "Stochastic Gradient Descent(8273): loss=37.89568440466967\n",
      "Stochastic Gradient Descent(8274): loss=19.636009417415917\n",
      "Stochastic Gradient Descent(8275): loss=15.513339432794734\n",
      "Stochastic Gradient Descent(8276): loss=0.2934203285244512\n",
      "Stochastic Gradient Descent(8277): loss=5.918459810422547\n",
      "Stochastic Gradient Descent(8278): loss=0.11620339814777492\n",
      "Stochastic Gradient Descent(8279): loss=2.4675752735787566\n",
      "Stochastic Gradient Descent(8280): loss=2.9386802840527544\n",
      "Stochastic Gradient Descent(8281): loss=0.002144679358055741\n",
      "Stochastic Gradient Descent(8282): loss=20.887388963654324\n",
      "Stochastic Gradient Descent(8283): loss=0.06465912645775339\n",
      "Stochastic Gradient Descent(8284): loss=0.4728800096709532\n",
      "Stochastic Gradient Descent(8285): loss=3.114872805818141\n",
      "Stochastic Gradient Descent(8286): loss=24.54082321064539\n",
      "Stochastic Gradient Descent(8287): loss=2.3101232737650457\n",
      "Stochastic Gradient Descent(8288): loss=13.174090761518261\n",
      "Stochastic Gradient Descent(8289): loss=3.8703706108967704\n",
      "Stochastic Gradient Descent(8290): loss=0.5563811514904187\n",
      "Stochastic Gradient Descent(8291): loss=14.403648939365505\n",
      "Stochastic Gradient Descent(8292): loss=2.9855891993982775\n",
      "Stochastic Gradient Descent(8293): loss=2.8316428666861735\n",
      "Stochastic Gradient Descent(8294): loss=0.726758381204217\n",
      "Stochastic Gradient Descent(8295): loss=0.30665198510377173\n",
      "Stochastic Gradient Descent(8296): loss=2.4813970440952215\n",
      "Stochastic Gradient Descent(8297): loss=2.7107659788858234\n",
      "Stochastic Gradient Descent(8298): loss=0.3280718507973931\n",
      "Stochastic Gradient Descent(8299): loss=28.785392384234022\n",
      "Stochastic Gradient Descent(8300): loss=4.735059699518595\n",
      "Stochastic Gradient Descent(8301): loss=0.005764288417949091\n",
      "Stochastic Gradient Descent(8302): loss=0.4915381998604818\n",
      "Stochastic Gradient Descent(8303): loss=0.0025969195966822184\n",
      "Stochastic Gradient Descent(8304): loss=2.7564188980717175\n",
      "Stochastic Gradient Descent(8305): loss=12.457871300009675\n",
      "Stochastic Gradient Descent(8306): loss=8.654947618166657\n",
      "Stochastic Gradient Descent(8307): loss=0.10875789458018603\n",
      "Stochastic Gradient Descent(8308): loss=0.14616233488897573\n",
      "Stochastic Gradient Descent(8309): loss=0.584182960469935\n",
      "Stochastic Gradient Descent(8310): loss=16.66078631557288\n",
      "Stochastic Gradient Descent(8311): loss=8.684351961250952\n",
      "Stochastic Gradient Descent(8312): loss=1.4196994118570327\n",
      "Stochastic Gradient Descent(8313): loss=1.4950360478803812\n",
      "Stochastic Gradient Descent(8314): loss=0.06678397597415776\n",
      "Stochastic Gradient Descent(8315): loss=0.9816788818491304\n",
      "Stochastic Gradient Descent(8316): loss=9.881111391531089\n",
      "Stochastic Gradient Descent(8317): loss=0.019434588261795078\n",
      "Stochastic Gradient Descent(8318): loss=0.1269014075884393\n",
      "Stochastic Gradient Descent(8319): loss=1.0306836927367857\n",
      "Stochastic Gradient Descent(8320): loss=5.458564484559998\n",
      "Stochastic Gradient Descent(8321): loss=16.551193174141908\n",
      "Stochastic Gradient Descent(8322): loss=3.4302494147297784\n",
      "Stochastic Gradient Descent(8323): loss=63.349904760058685\n",
      "Stochastic Gradient Descent(8324): loss=23.978900262525098\n",
      "Stochastic Gradient Descent(8325): loss=0.0014723578822512995\n",
      "Stochastic Gradient Descent(8326): loss=3.7715486264971307\n",
      "Stochastic Gradient Descent(8327): loss=8.899093628716466\n",
      "Stochastic Gradient Descent(8328): loss=0.0836466011991275\n",
      "Stochastic Gradient Descent(8329): loss=20.35299041333081\n",
      "Stochastic Gradient Descent(8330): loss=1.7708543382864894\n",
      "Stochastic Gradient Descent(8331): loss=20.975006876394747\n",
      "Stochastic Gradient Descent(8332): loss=0.06979122616591286\n",
      "Stochastic Gradient Descent(8333): loss=3.3821973026348062\n",
      "Stochastic Gradient Descent(8334): loss=32.30669837084243\n",
      "Stochastic Gradient Descent(8335): loss=4.2221848348830555\n",
      "Stochastic Gradient Descent(8336): loss=0.02893522972898317\n",
      "Stochastic Gradient Descent(8337): loss=0.08147005663754041\n",
      "Stochastic Gradient Descent(8338): loss=7.700338032103386\n",
      "Stochastic Gradient Descent(8339): loss=8.82191799085435\n",
      "Stochastic Gradient Descent(8340): loss=3.259617366305533\n",
      "Stochastic Gradient Descent(8341): loss=0.7251050833209689\n",
      "Stochastic Gradient Descent(8342): loss=6.2336052694332516\n",
      "Stochastic Gradient Descent(8343): loss=19.444830693967024\n",
      "Stochastic Gradient Descent(8344): loss=0.03536815795008187\n",
      "Stochastic Gradient Descent(8345): loss=0.6961828500769716\n",
      "Stochastic Gradient Descent(8346): loss=0.23645520667456552\n",
      "Stochastic Gradient Descent(8347): loss=6.003711747683463\n",
      "Stochastic Gradient Descent(8348): loss=0.08045796138192637\n",
      "Stochastic Gradient Descent(8349): loss=1.284106340920233\n",
      "Stochastic Gradient Descent(8350): loss=14.539453215672003\n",
      "Stochastic Gradient Descent(8351): loss=0.1518066584467564\n",
      "Stochastic Gradient Descent(8352): loss=0.3388488861874254\n",
      "Stochastic Gradient Descent(8353): loss=1.0160492600381081\n",
      "Stochastic Gradient Descent(8354): loss=10.355431010754957\n",
      "Stochastic Gradient Descent(8355): loss=5.427819560420073\n",
      "Stochastic Gradient Descent(8356): loss=1.148537877699542\n",
      "Stochastic Gradient Descent(8357): loss=5.429179412606462\n",
      "Stochastic Gradient Descent(8358): loss=14.589101567424212\n",
      "Stochastic Gradient Descent(8359): loss=0.6398677361367967\n",
      "Stochastic Gradient Descent(8360): loss=2.104750315525767\n",
      "Stochastic Gradient Descent(8361): loss=48.094105025473816\n",
      "Stochastic Gradient Descent(8362): loss=2.6316913469352734\n",
      "Stochastic Gradient Descent(8363): loss=5.0501193397098305\n",
      "Stochastic Gradient Descent(8364): loss=2.4454181776256596\n",
      "Stochastic Gradient Descent(8365): loss=1.75936065983833\n",
      "Stochastic Gradient Descent(8366): loss=5.107468842742641\n",
      "Stochastic Gradient Descent(8367): loss=21.395139087538922\n",
      "Stochastic Gradient Descent(8368): loss=1.5463701217693793\n",
      "Stochastic Gradient Descent(8369): loss=2.4903076948588696\n",
      "Stochastic Gradient Descent(8370): loss=1.1062222523593115\n",
      "Stochastic Gradient Descent(8371): loss=2.8565812742607255\n",
      "Stochastic Gradient Descent(8372): loss=4.743217101566871\n",
      "Stochastic Gradient Descent(8373): loss=16.744373397867907\n",
      "Stochastic Gradient Descent(8374): loss=48.69198573842907\n",
      "Stochastic Gradient Descent(8375): loss=0.019685111485963348\n",
      "Stochastic Gradient Descent(8376): loss=14.623518070758083\n",
      "Stochastic Gradient Descent(8377): loss=20.80940905227584\n",
      "Stochastic Gradient Descent(8378): loss=18.299527253852844\n",
      "Stochastic Gradient Descent(8379): loss=2.5180855659601487\n",
      "Stochastic Gradient Descent(8380): loss=13.26557318019908\n",
      "Stochastic Gradient Descent(8381): loss=1.3939956107178664\n",
      "Stochastic Gradient Descent(8382): loss=13.081339513371644\n",
      "Stochastic Gradient Descent(8383): loss=16.178448052481095\n",
      "Stochastic Gradient Descent(8384): loss=1.285064996678214\n",
      "Stochastic Gradient Descent(8385): loss=3.165356450037623\n",
      "Stochastic Gradient Descent(8386): loss=24.022999553069496\n",
      "Stochastic Gradient Descent(8387): loss=0.643127414941699\n",
      "Stochastic Gradient Descent(8388): loss=6.472248292121268\n",
      "Stochastic Gradient Descent(8389): loss=0.03299884192154765\n",
      "Stochastic Gradient Descent(8390): loss=2.4630451960498663\n",
      "Stochastic Gradient Descent(8391): loss=2.7216348803829775\n",
      "Stochastic Gradient Descent(8392): loss=11.54430163103142\n",
      "Stochastic Gradient Descent(8393): loss=3.8655508941890675\n",
      "Stochastic Gradient Descent(8394): loss=9.589932454306025\n",
      "Stochastic Gradient Descent(8395): loss=0.009884105822675132\n",
      "Stochastic Gradient Descent(8396): loss=7.0587321434192\n",
      "Stochastic Gradient Descent(8397): loss=0.06141265649220546\n",
      "Stochastic Gradient Descent(8398): loss=0.3814208469937141\n",
      "Stochastic Gradient Descent(8399): loss=1.734334441976144\n",
      "Stochastic Gradient Descent(8400): loss=1.328983811914337\n",
      "Stochastic Gradient Descent(8401): loss=7.696898734809449\n",
      "Stochastic Gradient Descent(8402): loss=0.981158477764041\n",
      "Stochastic Gradient Descent(8403): loss=17.42387974729424\n",
      "Stochastic Gradient Descent(8404): loss=0.6117419142012716\n",
      "Stochastic Gradient Descent(8405): loss=12.090506844008392\n",
      "Stochastic Gradient Descent(8406): loss=22.306659779701892\n",
      "Stochastic Gradient Descent(8407): loss=6.841567409487797\n",
      "Stochastic Gradient Descent(8408): loss=11.360654444403368\n",
      "Stochastic Gradient Descent(8409): loss=0.1890386909083339\n",
      "Stochastic Gradient Descent(8410): loss=18.270688999038452\n",
      "Stochastic Gradient Descent(8411): loss=1.4000222991891105\n",
      "Stochastic Gradient Descent(8412): loss=0.0023063141923538935\n",
      "Stochastic Gradient Descent(8413): loss=1.7825646624817264\n",
      "Stochastic Gradient Descent(8414): loss=2.1749640721947885\n",
      "Stochastic Gradient Descent(8415): loss=22.124172715414602\n",
      "Stochastic Gradient Descent(8416): loss=0.26564092683111507\n",
      "Stochastic Gradient Descent(8417): loss=0.43610752982565226\n",
      "Stochastic Gradient Descent(8418): loss=4.557411211451512\n",
      "Stochastic Gradient Descent(8419): loss=0.39293879493802936\n",
      "Stochastic Gradient Descent(8420): loss=26.627172096864143\n",
      "Stochastic Gradient Descent(8421): loss=0.3098947409775837\n",
      "Stochastic Gradient Descent(8422): loss=4.603025454548654\n",
      "Stochastic Gradient Descent(8423): loss=1.6468977155554259\n",
      "Stochastic Gradient Descent(8424): loss=0.03979747751822437\n",
      "Stochastic Gradient Descent(8425): loss=9.933762795386304\n",
      "Stochastic Gradient Descent(8426): loss=14.99570308695482\n",
      "Stochastic Gradient Descent(8427): loss=166.57863501446454\n",
      "Stochastic Gradient Descent(8428): loss=1.0136457688073885\n",
      "Stochastic Gradient Descent(8429): loss=14.330856977836753\n",
      "Stochastic Gradient Descent(8430): loss=17.463767539847236\n",
      "Stochastic Gradient Descent(8431): loss=32.12953191555243\n",
      "Stochastic Gradient Descent(8432): loss=37.82071701690927\n",
      "Stochastic Gradient Descent(8433): loss=0.0648970807525219\n",
      "Stochastic Gradient Descent(8434): loss=0.2412776758743843\n",
      "Stochastic Gradient Descent(8435): loss=3.9783265446599363\n",
      "Stochastic Gradient Descent(8436): loss=1.6068347925822475\n",
      "Stochastic Gradient Descent(8437): loss=2.526858626322109\n",
      "Stochastic Gradient Descent(8438): loss=0.11923982730064556\n",
      "Stochastic Gradient Descent(8439): loss=26.63533501903844\n",
      "Stochastic Gradient Descent(8440): loss=1.779225367635262\n",
      "Stochastic Gradient Descent(8441): loss=18.266340303379657\n",
      "Stochastic Gradient Descent(8442): loss=3.0622012099539293\n",
      "Stochastic Gradient Descent(8443): loss=0.023816085164914107\n",
      "Stochastic Gradient Descent(8444): loss=0.9347408420020891\n",
      "Stochastic Gradient Descent(8445): loss=48.92451153571677\n",
      "Stochastic Gradient Descent(8446): loss=2.1406795045044618\n",
      "Stochastic Gradient Descent(8447): loss=0.3532395251874418\n",
      "Stochastic Gradient Descent(8448): loss=0.016097512733174644\n",
      "Stochastic Gradient Descent(8449): loss=1.8125763107692319\n",
      "Stochastic Gradient Descent(8450): loss=0.9315788583835487\n",
      "Stochastic Gradient Descent(8451): loss=2.5108114447025884\n",
      "Stochastic Gradient Descent(8452): loss=9.753949868235761\n",
      "Stochastic Gradient Descent(8453): loss=3.531993591771682\n",
      "Stochastic Gradient Descent(8454): loss=0.8973090504772296\n",
      "Stochastic Gradient Descent(8455): loss=7.189966926114331\n",
      "Stochastic Gradient Descent(8456): loss=10.141374179760483\n",
      "Stochastic Gradient Descent(8457): loss=11.60378797199519\n",
      "Stochastic Gradient Descent(8458): loss=0.12246563971400566\n",
      "Stochastic Gradient Descent(8459): loss=1.3199282868369782\n",
      "Stochastic Gradient Descent(8460): loss=4.0847840224580585\n",
      "Stochastic Gradient Descent(8461): loss=13.748736664834757\n",
      "Stochastic Gradient Descent(8462): loss=1.1666868251439328\n",
      "Stochastic Gradient Descent(8463): loss=9.11177096030382\n",
      "Stochastic Gradient Descent(8464): loss=0.485757662381522\n",
      "Stochastic Gradient Descent(8465): loss=0.023143541342671818\n",
      "Stochastic Gradient Descent(8466): loss=0.18612641046013118\n",
      "Stochastic Gradient Descent(8467): loss=1.037912318916204\n",
      "Stochastic Gradient Descent(8468): loss=11.809220818158183\n",
      "Stochastic Gradient Descent(8469): loss=2.0120298343021767\n",
      "Stochastic Gradient Descent(8470): loss=26.60610248573315\n",
      "Stochastic Gradient Descent(8471): loss=10.565547641281976\n",
      "Stochastic Gradient Descent(8472): loss=0.05156583708932639\n",
      "Stochastic Gradient Descent(8473): loss=2.3443983266385606\n",
      "Stochastic Gradient Descent(8474): loss=1.2527465455901035\n",
      "Stochastic Gradient Descent(8475): loss=0.2501215887729564\n",
      "Stochastic Gradient Descent(8476): loss=3.0583167675622813\n",
      "Stochastic Gradient Descent(8477): loss=1.9091537213315157\n",
      "Stochastic Gradient Descent(8478): loss=0.3613854779562073\n",
      "Stochastic Gradient Descent(8479): loss=1.6962043243175813\n",
      "Stochastic Gradient Descent(8480): loss=0.4425882943084135\n",
      "Stochastic Gradient Descent(8481): loss=0.17927131964285975\n",
      "Stochastic Gradient Descent(8482): loss=6.803230416634845\n",
      "Stochastic Gradient Descent(8483): loss=53.662316374820776\n",
      "Stochastic Gradient Descent(8484): loss=1.8488088517906236\n",
      "Stochastic Gradient Descent(8485): loss=1.194616857484861\n",
      "Stochastic Gradient Descent(8486): loss=0.6159182841214252\n",
      "Stochastic Gradient Descent(8487): loss=22.02537154113896\n",
      "Stochastic Gradient Descent(8488): loss=10.272644311030406\n",
      "Stochastic Gradient Descent(8489): loss=12.90226219991899\n",
      "Stochastic Gradient Descent(8490): loss=0.6887869744345305\n",
      "Stochastic Gradient Descent(8491): loss=39.67120587140818\n",
      "Stochastic Gradient Descent(8492): loss=5.781707334235291\n",
      "Stochastic Gradient Descent(8493): loss=0.2002361595205608\n",
      "Stochastic Gradient Descent(8494): loss=2.1056936276714464\n",
      "Stochastic Gradient Descent(8495): loss=0.02889078645732563\n",
      "Stochastic Gradient Descent(8496): loss=0.0008621755261312218\n",
      "Stochastic Gradient Descent(8497): loss=10.766941227982318\n",
      "Stochastic Gradient Descent(8498): loss=1.1378280949935584\n",
      "Stochastic Gradient Descent(8499): loss=1.51098814865948\n",
      "Stochastic Gradient Descent(8500): loss=3.232153645675183\n",
      "Stochastic Gradient Descent(8501): loss=3.626866159040589\n",
      "Stochastic Gradient Descent(8502): loss=9.578523803084387\n",
      "Stochastic Gradient Descent(8503): loss=1.4780835563632335\n",
      "Stochastic Gradient Descent(8504): loss=12.240061961298201\n",
      "Stochastic Gradient Descent(8505): loss=5.070058569094435\n",
      "Stochastic Gradient Descent(8506): loss=6.06563562847366\n",
      "Stochastic Gradient Descent(8507): loss=5.33229331513138\n",
      "Stochastic Gradient Descent(8508): loss=0.7833161192618577\n",
      "Stochastic Gradient Descent(8509): loss=33.94429614543243\n",
      "Stochastic Gradient Descent(8510): loss=1.9309405399426443\n",
      "Stochastic Gradient Descent(8511): loss=1.3565482466844702e-05\n",
      "Stochastic Gradient Descent(8512): loss=1.372140539324786\n",
      "Stochastic Gradient Descent(8513): loss=0.5974774770250715\n",
      "Stochastic Gradient Descent(8514): loss=0.9367113155761881\n",
      "Stochastic Gradient Descent(8515): loss=1.4751331888143164\n",
      "Stochastic Gradient Descent(8516): loss=0.7899665678564414\n",
      "Stochastic Gradient Descent(8517): loss=1.1722886790161609\n",
      "Stochastic Gradient Descent(8518): loss=0.02302745282447184\n",
      "Stochastic Gradient Descent(8519): loss=0.15986274456012892\n",
      "Stochastic Gradient Descent(8520): loss=0.17311408527728814\n",
      "Stochastic Gradient Descent(8521): loss=1.7809879025694269\n",
      "Stochastic Gradient Descent(8522): loss=4.518957073363444\n",
      "Stochastic Gradient Descent(8523): loss=2.11774558261909\n",
      "Stochastic Gradient Descent(8524): loss=7.250892226225228\n",
      "Stochastic Gradient Descent(8525): loss=1.3879156872215157\n",
      "Stochastic Gradient Descent(8526): loss=19.389027949055983\n",
      "Stochastic Gradient Descent(8527): loss=4.383193730272253\n",
      "Stochastic Gradient Descent(8528): loss=4.023717220034723\n",
      "Stochastic Gradient Descent(8529): loss=1.2428814534264316\n",
      "Stochastic Gradient Descent(8530): loss=4.155403558233799\n",
      "Stochastic Gradient Descent(8531): loss=5.4084788737657625\n",
      "Stochastic Gradient Descent(8532): loss=4.078730309693777\n",
      "Stochastic Gradient Descent(8533): loss=0.13564033835737024\n",
      "Stochastic Gradient Descent(8534): loss=0.09179169846988865\n",
      "Stochastic Gradient Descent(8535): loss=6.999128119495449\n",
      "Stochastic Gradient Descent(8536): loss=0.07177125826236712\n",
      "Stochastic Gradient Descent(8537): loss=21.704154878759752\n",
      "Stochastic Gradient Descent(8538): loss=79.46081781861477\n",
      "Stochastic Gradient Descent(8539): loss=15.46551918576957\n",
      "Stochastic Gradient Descent(8540): loss=27.645068637990885\n",
      "Stochastic Gradient Descent(8541): loss=18.90453319671955\n",
      "Stochastic Gradient Descent(8542): loss=8.357189491508896\n",
      "Stochastic Gradient Descent(8543): loss=5.956564219280099\n",
      "Stochastic Gradient Descent(8544): loss=3.2816534365355734\n",
      "Stochastic Gradient Descent(8545): loss=10.594415978722084\n",
      "Stochastic Gradient Descent(8546): loss=16.213898950793443\n",
      "Stochastic Gradient Descent(8547): loss=1.6155935520879383\n",
      "Stochastic Gradient Descent(8548): loss=6.159940219629012\n",
      "Stochastic Gradient Descent(8549): loss=21.223314131390183\n",
      "Stochastic Gradient Descent(8550): loss=2.397963125478903\n",
      "Stochastic Gradient Descent(8551): loss=7.277255285085518\n",
      "Stochastic Gradient Descent(8552): loss=17.887318724675676\n",
      "Stochastic Gradient Descent(8553): loss=0.005349207492277067\n",
      "Stochastic Gradient Descent(8554): loss=9.344414462418996\n",
      "Stochastic Gradient Descent(8555): loss=0.6256669932390568\n",
      "Stochastic Gradient Descent(8556): loss=2.3391810526762087\n",
      "Stochastic Gradient Descent(8557): loss=7.084941065667248\n",
      "Stochastic Gradient Descent(8558): loss=23.677452604119395\n",
      "Stochastic Gradient Descent(8559): loss=0.8367387522557089\n",
      "Stochastic Gradient Descent(8560): loss=0.2901228148286614\n",
      "Stochastic Gradient Descent(8561): loss=0.88822358301672\n",
      "Stochastic Gradient Descent(8562): loss=4.014987746289876\n",
      "Stochastic Gradient Descent(8563): loss=3.3298171550250877\n",
      "Stochastic Gradient Descent(8564): loss=6.084204097733544\n",
      "Stochastic Gradient Descent(8565): loss=0.014247628580719316\n",
      "Stochastic Gradient Descent(8566): loss=0.4336048665124406\n",
      "Stochastic Gradient Descent(8567): loss=0.006932697727263907\n",
      "Stochastic Gradient Descent(8568): loss=3.270367969085726\n",
      "Stochastic Gradient Descent(8569): loss=15.609311107469262\n",
      "Stochastic Gradient Descent(8570): loss=14.561130047974645\n",
      "Stochastic Gradient Descent(8571): loss=1.0311065374569488\n",
      "Stochastic Gradient Descent(8572): loss=1.1999455227819789\n",
      "Stochastic Gradient Descent(8573): loss=0.8238513105087725\n",
      "Stochastic Gradient Descent(8574): loss=39.656063877963675\n",
      "Stochastic Gradient Descent(8575): loss=0.34098478092859685\n",
      "Stochastic Gradient Descent(8576): loss=1.7231337930842598\n",
      "Stochastic Gradient Descent(8577): loss=0.44299310346800863\n",
      "Stochastic Gradient Descent(8578): loss=2.652771161889617\n",
      "Stochastic Gradient Descent(8579): loss=9.37471164435597\n",
      "Stochastic Gradient Descent(8580): loss=4.623345945088561\n",
      "Stochastic Gradient Descent(8581): loss=121.70648053914653\n",
      "Stochastic Gradient Descent(8582): loss=6.56057088260929\n",
      "Stochastic Gradient Descent(8583): loss=16.07765718305692\n",
      "Stochastic Gradient Descent(8584): loss=12.60623578785742\n",
      "Stochastic Gradient Descent(8585): loss=3.6318505493844095\n",
      "Stochastic Gradient Descent(8586): loss=0.07009774344330179\n",
      "Stochastic Gradient Descent(8587): loss=2.411136541301713\n",
      "Stochastic Gradient Descent(8588): loss=1.1352023248996619\n",
      "Stochastic Gradient Descent(8589): loss=1.2106844336036064\n",
      "Stochastic Gradient Descent(8590): loss=1.0325760749728048\n",
      "Stochastic Gradient Descent(8591): loss=4.252289623040618\n",
      "Stochastic Gradient Descent(8592): loss=16.56529783803339\n",
      "Stochastic Gradient Descent(8593): loss=19.95202947226284\n",
      "Stochastic Gradient Descent(8594): loss=0.23850021108526026\n",
      "Stochastic Gradient Descent(8595): loss=0.000770391785602563\n",
      "Stochastic Gradient Descent(8596): loss=1.4023590092844755\n",
      "Stochastic Gradient Descent(8597): loss=6.053798763088275\n",
      "Stochastic Gradient Descent(8598): loss=63.121105574308466\n",
      "Stochastic Gradient Descent(8599): loss=62.89575719808973\n",
      "Stochastic Gradient Descent(8600): loss=36.908232087358535\n",
      "Stochastic Gradient Descent(8601): loss=0.014960099679898652\n",
      "Stochastic Gradient Descent(8602): loss=1.5687145759870877\n",
      "Stochastic Gradient Descent(8603): loss=1.6312867145553676\n",
      "Stochastic Gradient Descent(8604): loss=2.077005863855889\n",
      "Stochastic Gradient Descent(8605): loss=0.040876877897385165\n",
      "Stochastic Gradient Descent(8606): loss=1.2045971286287436\n",
      "Stochastic Gradient Descent(8607): loss=22.757084619676537\n",
      "Stochastic Gradient Descent(8608): loss=0.5850530367821877\n",
      "Stochastic Gradient Descent(8609): loss=5.233799627363897\n",
      "Stochastic Gradient Descent(8610): loss=2.139020809127792\n",
      "Stochastic Gradient Descent(8611): loss=6.117105075594443\n",
      "Stochastic Gradient Descent(8612): loss=4.1580095805479305\n",
      "Stochastic Gradient Descent(8613): loss=2.9107554035132273\n",
      "Stochastic Gradient Descent(8614): loss=80.06539779030732\n",
      "Stochastic Gradient Descent(8615): loss=20.911314040707467\n",
      "Stochastic Gradient Descent(8616): loss=148.99688754359326\n",
      "Stochastic Gradient Descent(8617): loss=4.262757123791909\n",
      "Stochastic Gradient Descent(8618): loss=9.971235542464268e-05\n",
      "Stochastic Gradient Descent(8619): loss=0.416078270091238\n",
      "Stochastic Gradient Descent(8620): loss=26.003885629541887\n",
      "Stochastic Gradient Descent(8621): loss=29.567805408698685\n",
      "Stochastic Gradient Descent(8622): loss=0.8454751626808434\n",
      "Stochastic Gradient Descent(8623): loss=4.829427357761541\n",
      "Stochastic Gradient Descent(8624): loss=35.02916969185116\n",
      "Stochastic Gradient Descent(8625): loss=2.114958826435927\n",
      "Stochastic Gradient Descent(8626): loss=0.6242304461076618\n",
      "Stochastic Gradient Descent(8627): loss=1.268795087827019\n",
      "Stochastic Gradient Descent(8628): loss=23.77654658825928\n",
      "Stochastic Gradient Descent(8629): loss=0.4109110591888204\n",
      "Stochastic Gradient Descent(8630): loss=3.0856420445686763\n",
      "Stochastic Gradient Descent(8631): loss=5.737410808500162\n",
      "Stochastic Gradient Descent(8632): loss=0.006640392311684167\n",
      "Stochastic Gradient Descent(8633): loss=2.7148905220574546\n",
      "Stochastic Gradient Descent(8634): loss=0.08219769716891254\n",
      "Stochastic Gradient Descent(8635): loss=4.916878277115218\n",
      "Stochastic Gradient Descent(8636): loss=0.10360003801610243\n",
      "Stochastic Gradient Descent(8637): loss=1.1759563500320633\n",
      "Stochastic Gradient Descent(8638): loss=10.795532212385492\n",
      "Stochastic Gradient Descent(8639): loss=0.5140140586211994\n",
      "Stochastic Gradient Descent(8640): loss=7.94903943602305\n",
      "Stochastic Gradient Descent(8641): loss=2.2639879943588994\n",
      "Stochastic Gradient Descent(8642): loss=0.0988013144185171\n",
      "Stochastic Gradient Descent(8643): loss=0.03611515394380965\n",
      "Stochastic Gradient Descent(8644): loss=0.17416982353041038\n",
      "Stochastic Gradient Descent(8645): loss=1.7298113925021814\n",
      "Stochastic Gradient Descent(8646): loss=3.5965616945980052\n",
      "Stochastic Gradient Descent(8647): loss=0.059136574433070584\n",
      "Stochastic Gradient Descent(8648): loss=2.1137615126420704\n",
      "Stochastic Gradient Descent(8649): loss=0.7543534675250042\n",
      "Stochastic Gradient Descent(8650): loss=1.7392069990776555\n",
      "Stochastic Gradient Descent(8651): loss=0.31269149537985885\n",
      "Stochastic Gradient Descent(8652): loss=166.96573408958818\n",
      "Stochastic Gradient Descent(8653): loss=31.290612190341054\n",
      "Stochastic Gradient Descent(8654): loss=46.866310683284325\n",
      "Stochastic Gradient Descent(8655): loss=46.96297884909092\n",
      "Stochastic Gradient Descent(8656): loss=18.888173925736297\n",
      "Stochastic Gradient Descent(8657): loss=64.12400966729103\n",
      "Stochastic Gradient Descent(8658): loss=8.061476995905714\n",
      "Stochastic Gradient Descent(8659): loss=1.6130002183202534\n",
      "Stochastic Gradient Descent(8660): loss=28.897744302978566\n",
      "Stochastic Gradient Descent(8661): loss=27.587496674143157\n",
      "Stochastic Gradient Descent(8662): loss=2.8672008871278147\n",
      "Stochastic Gradient Descent(8663): loss=7.668775809293599\n",
      "Stochastic Gradient Descent(8664): loss=0.13361403982047546\n",
      "Stochastic Gradient Descent(8665): loss=3.3783703671827414\n",
      "Stochastic Gradient Descent(8666): loss=0.7535628452941614\n",
      "Stochastic Gradient Descent(8667): loss=0.12044368706029843\n",
      "Stochastic Gradient Descent(8668): loss=0.750970326046503\n",
      "Stochastic Gradient Descent(8669): loss=10.937305163914738\n",
      "Stochastic Gradient Descent(8670): loss=3.449005011326362\n",
      "Stochastic Gradient Descent(8671): loss=8.688564353186525\n",
      "Stochastic Gradient Descent(8672): loss=1.1625700469951536\n",
      "Stochastic Gradient Descent(8673): loss=0.12412979960906687\n",
      "Stochastic Gradient Descent(8674): loss=0.005807181275994555\n",
      "Stochastic Gradient Descent(8675): loss=3.309329093452872\n",
      "Stochastic Gradient Descent(8676): loss=0.7493918445373723\n",
      "Stochastic Gradient Descent(8677): loss=7.760134224245293\n",
      "Stochastic Gradient Descent(8678): loss=0.8291042174685094\n",
      "Stochastic Gradient Descent(8679): loss=4.754482443454457\n",
      "Stochastic Gradient Descent(8680): loss=16.06510978328501\n",
      "Stochastic Gradient Descent(8681): loss=6.06183956730379\n",
      "Stochastic Gradient Descent(8682): loss=1.4651396574234257\n",
      "Stochastic Gradient Descent(8683): loss=1.059295609268521\n",
      "Stochastic Gradient Descent(8684): loss=3.201142068146342\n",
      "Stochastic Gradient Descent(8685): loss=6.955076998567185\n",
      "Stochastic Gradient Descent(8686): loss=0.29439145841513586\n",
      "Stochastic Gradient Descent(8687): loss=3.913680414962901\n",
      "Stochastic Gradient Descent(8688): loss=3.6943841831070214\n",
      "Stochastic Gradient Descent(8689): loss=15.022650700935836\n",
      "Stochastic Gradient Descent(8690): loss=7.9441418359181295\n",
      "Stochastic Gradient Descent(8691): loss=3.190368105344182\n",
      "Stochastic Gradient Descent(8692): loss=2.723447139558485\n",
      "Stochastic Gradient Descent(8693): loss=0.030327753827721522\n",
      "Stochastic Gradient Descent(8694): loss=0.018344056319852645\n",
      "Stochastic Gradient Descent(8695): loss=0.06381556888496803\n",
      "Stochastic Gradient Descent(8696): loss=4.2072114683476345\n",
      "Stochastic Gradient Descent(8697): loss=25.3752310306648\n",
      "Stochastic Gradient Descent(8698): loss=0.3678191617344502\n",
      "Stochastic Gradient Descent(8699): loss=1.205312109642338\n",
      "Stochastic Gradient Descent(8700): loss=5.735640608051976\n",
      "Stochastic Gradient Descent(8701): loss=0.21596752641457803\n",
      "Stochastic Gradient Descent(8702): loss=21.19741016819517\n",
      "Stochastic Gradient Descent(8703): loss=0.5084184804952256\n",
      "Stochastic Gradient Descent(8704): loss=5.228783587515037\n",
      "Stochastic Gradient Descent(8705): loss=7.046861790873867\n",
      "Stochastic Gradient Descent(8706): loss=1.155081092931683\n",
      "Stochastic Gradient Descent(8707): loss=3.802615747429147\n",
      "Stochastic Gradient Descent(8708): loss=10.471919163542706\n",
      "Stochastic Gradient Descent(8709): loss=13.396356515158407\n",
      "Stochastic Gradient Descent(8710): loss=0.5519477254628136\n",
      "Stochastic Gradient Descent(8711): loss=3.230694014915211\n",
      "Stochastic Gradient Descent(8712): loss=0.22910703735523003\n",
      "Stochastic Gradient Descent(8713): loss=1.7000779822050527\n",
      "Stochastic Gradient Descent(8714): loss=0.6591645984687287\n",
      "Stochastic Gradient Descent(8715): loss=0.18865323262864536\n",
      "Stochastic Gradient Descent(8716): loss=18.898028854267213\n",
      "Stochastic Gradient Descent(8717): loss=19.786195352985235\n",
      "Stochastic Gradient Descent(8718): loss=0.22315765104261814\n",
      "Stochastic Gradient Descent(8719): loss=0.007688702774038434\n",
      "Stochastic Gradient Descent(8720): loss=5.126462538987362\n",
      "Stochastic Gradient Descent(8721): loss=8.15824107973681\n",
      "Stochastic Gradient Descent(8722): loss=10.980001103675521\n",
      "Stochastic Gradient Descent(8723): loss=64.4144681839349\n",
      "Stochastic Gradient Descent(8724): loss=26.511072449895295\n",
      "Stochastic Gradient Descent(8725): loss=18.422127493017207\n",
      "Stochastic Gradient Descent(8726): loss=17.11638060340133\n",
      "Stochastic Gradient Descent(8727): loss=0.12937750651976054\n",
      "Stochastic Gradient Descent(8728): loss=10.313424406609595\n",
      "Stochastic Gradient Descent(8729): loss=3.5277647242317167\n",
      "Stochastic Gradient Descent(8730): loss=2.7704927768475116\n",
      "Stochastic Gradient Descent(8731): loss=1.1677698678123307\n",
      "Stochastic Gradient Descent(8732): loss=2.344550886188073\n",
      "Stochastic Gradient Descent(8733): loss=2.463095736453418\n",
      "Stochastic Gradient Descent(8734): loss=3.6486845236282544\n",
      "Stochastic Gradient Descent(8735): loss=0.003608497930858734\n",
      "Stochastic Gradient Descent(8736): loss=2.0496311337852853\n",
      "Stochastic Gradient Descent(8737): loss=3.5188415346695425\n",
      "Stochastic Gradient Descent(8738): loss=11.960688147727435\n",
      "Stochastic Gradient Descent(8739): loss=7.270163959480561\n",
      "Stochastic Gradient Descent(8740): loss=1.5979632476804395\n",
      "Stochastic Gradient Descent(8741): loss=11.654764190017191\n",
      "Stochastic Gradient Descent(8742): loss=18.723488836492145\n",
      "Stochastic Gradient Descent(8743): loss=1.3413735374316935\n",
      "Stochastic Gradient Descent(8744): loss=8.801570930449767\n",
      "Stochastic Gradient Descent(8745): loss=0.7282350656688535\n",
      "Stochastic Gradient Descent(8746): loss=0.08682939267106023\n",
      "Stochastic Gradient Descent(8747): loss=5.18146207151941\n",
      "Stochastic Gradient Descent(8748): loss=0.08418623253305185\n",
      "Stochastic Gradient Descent(8749): loss=1.743539372992652\n",
      "Stochastic Gradient Descent(8750): loss=1.4989356047703835\n",
      "Stochastic Gradient Descent(8751): loss=0.858087038497052\n",
      "Stochastic Gradient Descent(8752): loss=2.9370655770528957\n",
      "Stochastic Gradient Descent(8753): loss=1.3774957592919668\n",
      "Stochastic Gradient Descent(8754): loss=0.08320061117749869\n",
      "Stochastic Gradient Descent(8755): loss=0.3105842624425556\n",
      "Stochastic Gradient Descent(8756): loss=0.25571291451454214\n",
      "Stochastic Gradient Descent(8757): loss=14.819665998115529\n",
      "Stochastic Gradient Descent(8758): loss=0.006879305084652412\n",
      "Stochastic Gradient Descent(8759): loss=8.98701490989935\n",
      "Stochastic Gradient Descent(8760): loss=0.06272678394443605\n",
      "Stochastic Gradient Descent(8761): loss=0.04781784068044373\n",
      "Stochastic Gradient Descent(8762): loss=4.249529442876218\n",
      "Stochastic Gradient Descent(8763): loss=0.006659084791720854\n",
      "Stochastic Gradient Descent(8764): loss=0.36947412200776764\n",
      "Stochastic Gradient Descent(8765): loss=11.985377287690557\n",
      "Stochastic Gradient Descent(8766): loss=3.342817962608463\n",
      "Stochastic Gradient Descent(8767): loss=9.683833035362653\n",
      "Stochastic Gradient Descent(8768): loss=1.050505300818911\n",
      "Stochastic Gradient Descent(8769): loss=1.559824728406883\n",
      "Stochastic Gradient Descent(8770): loss=0.28285271337843165\n",
      "Stochastic Gradient Descent(8771): loss=25.100560568820207\n",
      "Stochastic Gradient Descent(8772): loss=0.9769488357318219\n",
      "Stochastic Gradient Descent(8773): loss=8.552573096277646\n",
      "Stochastic Gradient Descent(8774): loss=0.0004216146108001083\n",
      "Stochastic Gradient Descent(8775): loss=0.5714419750307365\n",
      "Stochastic Gradient Descent(8776): loss=3.050732600487996\n",
      "Stochastic Gradient Descent(8777): loss=9.015927323991063\n",
      "Stochastic Gradient Descent(8778): loss=15.385562716244456\n",
      "Stochastic Gradient Descent(8779): loss=0.310427604559086\n",
      "Stochastic Gradient Descent(8780): loss=6.6540037216231935\n",
      "Stochastic Gradient Descent(8781): loss=14.47855567355818\n",
      "Stochastic Gradient Descent(8782): loss=4.543859717717533\n",
      "Stochastic Gradient Descent(8783): loss=0.08580373692553915\n",
      "Stochastic Gradient Descent(8784): loss=2.957812788379542\n",
      "Stochastic Gradient Descent(8785): loss=0.09216842971866201\n",
      "Stochastic Gradient Descent(8786): loss=5.552394235648197\n",
      "Stochastic Gradient Descent(8787): loss=1.1203400894422617\n",
      "Stochastic Gradient Descent(8788): loss=1.8708802138721767\n",
      "Stochastic Gradient Descent(8789): loss=15.244981308113951\n",
      "Stochastic Gradient Descent(8790): loss=3.7328808238107882\n",
      "Stochastic Gradient Descent(8791): loss=0.10567450769422446\n",
      "Stochastic Gradient Descent(8792): loss=15.086992257730007\n",
      "Stochastic Gradient Descent(8793): loss=11.18508498474617\n",
      "Stochastic Gradient Descent(8794): loss=0.2516963204432636\n",
      "Stochastic Gradient Descent(8795): loss=0.024982659655347572\n",
      "Stochastic Gradient Descent(8796): loss=2.4128936570543487\n",
      "Stochastic Gradient Descent(8797): loss=0.43136875464209473\n",
      "Stochastic Gradient Descent(8798): loss=45.47823458701457\n",
      "Stochastic Gradient Descent(8799): loss=10.561803377141624\n",
      "Stochastic Gradient Descent(8800): loss=1.9886484732831502\n",
      "Stochastic Gradient Descent(8801): loss=6.58751935818032\n",
      "Stochastic Gradient Descent(8802): loss=3.681476528889906\n",
      "Stochastic Gradient Descent(8803): loss=7.190405964961011\n",
      "Stochastic Gradient Descent(8804): loss=3.016648803894699\n",
      "Stochastic Gradient Descent(8805): loss=0.11720843663740832\n",
      "Stochastic Gradient Descent(8806): loss=0.6496410765977488\n",
      "Stochastic Gradient Descent(8807): loss=3.1767354323299295\n",
      "Stochastic Gradient Descent(8808): loss=5.406608010001828\n",
      "Stochastic Gradient Descent(8809): loss=0.4571256748493756\n",
      "Stochastic Gradient Descent(8810): loss=0.6800036313920089\n",
      "Stochastic Gradient Descent(8811): loss=0.059144644219205886\n",
      "Stochastic Gradient Descent(8812): loss=0.15440780838392126\n",
      "Stochastic Gradient Descent(8813): loss=0.03612691687775827\n",
      "Stochastic Gradient Descent(8814): loss=0.679855894485366\n",
      "Stochastic Gradient Descent(8815): loss=0.5009477522630948\n",
      "Stochastic Gradient Descent(8816): loss=5.04902986290979\n",
      "Stochastic Gradient Descent(8817): loss=0.0011050929497447774\n",
      "Stochastic Gradient Descent(8818): loss=0.014054513986823917\n",
      "Stochastic Gradient Descent(8819): loss=12.121469345339325\n",
      "Stochastic Gradient Descent(8820): loss=9.140126631196468\n",
      "Stochastic Gradient Descent(8821): loss=9.120859123169284\n",
      "Stochastic Gradient Descent(8822): loss=5.787720831875647\n",
      "Stochastic Gradient Descent(8823): loss=4.816320921662886\n",
      "Stochastic Gradient Descent(8824): loss=0.7697296122995063\n",
      "Stochastic Gradient Descent(8825): loss=11.090834426610643\n",
      "Stochastic Gradient Descent(8826): loss=3.449519558078519\n",
      "Stochastic Gradient Descent(8827): loss=40.15165244307341\n",
      "Stochastic Gradient Descent(8828): loss=17.23949322770477\n",
      "Stochastic Gradient Descent(8829): loss=18.934776761209527\n",
      "Stochastic Gradient Descent(8830): loss=2.3371421700742903\n",
      "Stochastic Gradient Descent(8831): loss=0.9658347654220812\n",
      "Stochastic Gradient Descent(8832): loss=0.08443454430985599\n",
      "Stochastic Gradient Descent(8833): loss=0.7461173667641319\n",
      "Stochastic Gradient Descent(8834): loss=0.15503330917478322\n",
      "Stochastic Gradient Descent(8835): loss=0.03168591437429447\n",
      "Stochastic Gradient Descent(8836): loss=6.2503374822227595\n",
      "Stochastic Gradient Descent(8837): loss=1.5179104412537643\n",
      "Stochastic Gradient Descent(8838): loss=0.060935489668872043\n",
      "Stochastic Gradient Descent(8839): loss=6.270307008007876\n",
      "Stochastic Gradient Descent(8840): loss=0.011879718992519394\n",
      "Stochastic Gradient Descent(8841): loss=9.94679267480494\n",
      "Stochastic Gradient Descent(8842): loss=0.3087911826020998\n",
      "Stochastic Gradient Descent(8843): loss=1.7270501407087648\n",
      "Stochastic Gradient Descent(8844): loss=2.4223297189261337\n",
      "Stochastic Gradient Descent(8845): loss=2.3657689141851916\n",
      "Stochastic Gradient Descent(8846): loss=0.8817595184616458\n",
      "Stochastic Gradient Descent(8847): loss=1.0156790384578849\n",
      "Stochastic Gradient Descent(8848): loss=25.21635858026481\n",
      "Stochastic Gradient Descent(8849): loss=19.213948729313312\n",
      "Stochastic Gradient Descent(8850): loss=4.459336011865317\n",
      "Stochastic Gradient Descent(8851): loss=0.6245646424047726\n",
      "Stochastic Gradient Descent(8852): loss=2.8851099080412883\n",
      "Stochastic Gradient Descent(8853): loss=7.891949860969218\n",
      "Stochastic Gradient Descent(8854): loss=0.34846751874122045\n",
      "Stochastic Gradient Descent(8855): loss=1.1752167034893672\n",
      "Stochastic Gradient Descent(8856): loss=10.894723894235275\n",
      "Stochastic Gradient Descent(8857): loss=11.642191124106326\n",
      "Stochastic Gradient Descent(8858): loss=13.944386668645578\n",
      "Stochastic Gradient Descent(8859): loss=6.439233941064188\n",
      "Stochastic Gradient Descent(8860): loss=1.6484480877195413\n",
      "Stochastic Gradient Descent(8861): loss=31.551204027779395\n",
      "Stochastic Gradient Descent(8862): loss=0.0009940774432179867\n",
      "Stochastic Gradient Descent(8863): loss=3.9794934484684314\n",
      "Stochastic Gradient Descent(8864): loss=2.713985142410001\n",
      "Stochastic Gradient Descent(8865): loss=25.017456937506296\n",
      "Stochastic Gradient Descent(8866): loss=18.631860348632753\n",
      "Stochastic Gradient Descent(8867): loss=0.8198338579585606\n",
      "Stochastic Gradient Descent(8868): loss=10.611187259436885\n",
      "Stochastic Gradient Descent(8869): loss=1.2201409003272594\n",
      "Stochastic Gradient Descent(8870): loss=7.170680234847529\n",
      "Stochastic Gradient Descent(8871): loss=12.773105748472796\n",
      "Stochastic Gradient Descent(8872): loss=2.8507042884068285\n",
      "Stochastic Gradient Descent(8873): loss=0.3408851529366283\n",
      "Stochastic Gradient Descent(8874): loss=0.013102876507507442\n",
      "Stochastic Gradient Descent(8875): loss=0.7486275738500687\n",
      "Stochastic Gradient Descent(8876): loss=1.89854576589976\n",
      "Stochastic Gradient Descent(8877): loss=0.07330078191052983\n",
      "Stochastic Gradient Descent(8878): loss=3.992494910132249\n",
      "Stochastic Gradient Descent(8879): loss=6.963373568587801\n",
      "Stochastic Gradient Descent(8880): loss=0.5124111130375827\n",
      "Stochastic Gradient Descent(8881): loss=11.502644595546409\n",
      "Stochastic Gradient Descent(8882): loss=7.930780157148854\n",
      "Stochastic Gradient Descent(8883): loss=0.2001920593846424\n",
      "Stochastic Gradient Descent(8884): loss=0.006308939625296881\n",
      "Stochastic Gradient Descent(8885): loss=3.7836143943667793\n",
      "Stochastic Gradient Descent(8886): loss=9.236876086541505\n",
      "Stochastic Gradient Descent(8887): loss=0.19887459310607775\n",
      "Stochastic Gradient Descent(8888): loss=3.859159908268683\n",
      "Stochastic Gradient Descent(8889): loss=15.150144211207815\n",
      "Stochastic Gradient Descent(8890): loss=0.035499996144835906\n",
      "Stochastic Gradient Descent(8891): loss=0.09506502799022956\n",
      "Stochastic Gradient Descent(8892): loss=1.720531928412693\n",
      "Stochastic Gradient Descent(8893): loss=3.883337618350838\n",
      "Stochastic Gradient Descent(8894): loss=0.7615531196621318\n",
      "Stochastic Gradient Descent(8895): loss=0.9124502136899334\n",
      "Stochastic Gradient Descent(8896): loss=0.07228354180896442\n",
      "Stochastic Gradient Descent(8897): loss=2.195659440843178\n",
      "Stochastic Gradient Descent(8898): loss=4.947663142664685\n",
      "Stochastic Gradient Descent(8899): loss=24.901608630462817\n",
      "Stochastic Gradient Descent(8900): loss=2.7598895416589735\n",
      "Stochastic Gradient Descent(8901): loss=35.68619052297189\n",
      "Stochastic Gradient Descent(8902): loss=0.028777150617678274\n",
      "Stochastic Gradient Descent(8903): loss=4.036388817747333\n",
      "Stochastic Gradient Descent(8904): loss=1.9823527669765\n",
      "Stochastic Gradient Descent(8905): loss=14.4519847287192\n",
      "Stochastic Gradient Descent(8906): loss=1.9712290755634954\n",
      "Stochastic Gradient Descent(8907): loss=1.7944797686907359\n",
      "Stochastic Gradient Descent(8908): loss=2.1523996369391183\n",
      "Stochastic Gradient Descent(8909): loss=12.846006995691287\n",
      "Stochastic Gradient Descent(8910): loss=0.01408940370647914\n",
      "Stochastic Gradient Descent(8911): loss=3.6110628862622858\n",
      "Stochastic Gradient Descent(8912): loss=1.6102637904485224\n",
      "Stochastic Gradient Descent(8913): loss=3.8826495304054074\n",
      "Stochastic Gradient Descent(8914): loss=1.1671820442196739\n",
      "Stochastic Gradient Descent(8915): loss=4.877161423179461\n",
      "Stochastic Gradient Descent(8916): loss=0.10895229657151338\n",
      "Stochastic Gradient Descent(8917): loss=2.3276930367004773\n",
      "Stochastic Gradient Descent(8918): loss=0.053051892115667944\n",
      "Stochastic Gradient Descent(8919): loss=2.403878611974377\n",
      "Stochastic Gradient Descent(8920): loss=0.16795236749194895\n",
      "Stochastic Gradient Descent(8921): loss=0.356798796790108\n",
      "Stochastic Gradient Descent(8922): loss=0.0016631847335144697\n",
      "Stochastic Gradient Descent(8923): loss=1.818686287597184\n",
      "Stochastic Gradient Descent(8924): loss=2.498435892726007\n",
      "Stochastic Gradient Descent(8925): loss=2.4844116719708973\n",
      "Stochastic Gradient Descent(8926): loss=14.148118712770573\n",
      "Stochastic Gradient Descent(8927): loss=0.3329920128904706\n",
      "Stochastic Gradient Descent(8928): loss=0.990375212886068\n",
      "Stochastic Gradient Descent(8929): loss=3.86801414457514e-05\n",
      "Stochastic Gradient Descent(8930): loss=12.469185633956933\n",
      "Stochastic Gradient Descent(8931): loss=6.167119931457337\n",
      "Stochastic Gradient Descent(8932): loss=0.33413420344407685\n",
      "Stochastic Gradient Descent(8933): loss=7.182031469442442\n",
      "Stochastic Gradient Descent(8934): loss=27.376296362417012\n",
      "Stochastic Gradient Descent(8935): loss=0.15260698807231848\n",
      "Stochastic Gradient Descent(8936): loss=1.9746168404115758\n",
      "Stochastic Gradient Descent(8937): loss=0.268651556785919\n",
      "Stochastic Gradient Descent(8938): loss=0.002250742009385061\n",
      "Stochastic Gradient Descent(8939): loss=6.236046814843697\n",
      "Stochastic Gradient Descent(8940): loss=32.52190993980442\n",
      "Stochastic Gradient Descent(8941): loss=0.45564698391921016\n",
      "Stochastic Gradient Descent(8942): loss=1.6234600768708036\n",
      "Stochastic Gradient Descent(8943): loss=9.508196988022558\n",
      "Stochastic Gradient Descent(8944): loss=3.399979013780167\n",
      "Stochastic Gradient Descent(8945): loss=0.44298291884671426\n",
      "Stochastic Gradient Descent(8946): loss=0.23907061025994072\n",
      "Stochastic Gradient Descent(8947): loss=1.6106616873613373\n",
      "Stochastic Gradient Descent(8948): loss=8.280574529111398\n",
      "Stochastic Gradient Descent(8949): loss=2.175460545357635\n",
      "Stochastic Gradient Descent(8950): loss=0.14830468491992904\n",
      "Stochastic Gradient Descent(8951): loss=6.496648420311187\n",
      "Stochastic Gradient Descent(8952): loss=0.02453022953925292\n",
      "Stochastic Gradient Descent(8953): loss=4.46589976866043\n",
      "Stochastic Gradient Descent(8954): loss=0.41209454186222333\n",
      "Stochastic Gradient Descent(8955): loss=3.6159493268924594\n",
      "Stochastic Gradient Descent(8956): loss=1.0018300188754683\n",
      "Stochastic Gradient Descent(8957): loss=4.152810807460752\n",
      "Stochastic Gradient Descent(8958): loss=1.3293292375506105\n",
      "Stochastic Gradient Descent(8959): loss=0.0018873742441817651\n",
      "Stochastic Gradient Descent(8960): loss=5.256200715912779\n",
      "Stochastic Gradient Descent(8961): loss=0.0027057181406535866\n",
      "Stochastic Gradient Descent(8962): loss=0.4966798324281775\n",
      "Stochastic Gradient Descent(8963): loss=2.5050358813394222\n",
      "Stochastic Gradient Descent(8964): loss=1.0527641425695549\n",
      "Stochastic Gradient Descent(8965): loss=0.00031059121189562126\n",
      "Stochastic Gradient Descent(8966): loss=4.230753429907903\n",
      "Stochastic Gradient Descent(8967): loss=3.0606261307208187\n",
      "Stochastic Gradient Descent(8968): loss=18.840643056435916\n",
      "Stochastic Gradient Descent(8969): loss=1.4859047674382897\n",
      "Stochastic Gradient Descent(8970): loss=1.2124421856468146\n",
      "Stochastic Gradient Descent(8971): loss=0.05743414633279831\n",
      "Stochastic Gradient Descent(8972): loss=6.341647920521562\n",
      "Stochastic Gradient Descent(8973): loss=0.8898758185443721\n",
      "Stochastic Gradient Descent(8974): loss=1.2300776818717984\n",
      "Stochastic Gradient Descent(8975): loss=0.0001884403638215746\n",
      "Stochastic Gradient Descent(8976): loss=5.528555892259243\n",
      "Stochastic Gradient Descent(8977): loss=4.100442774534202\n",
      "Stochastic Gradient Descent(8978): loss=0.4030390070292937\n",
      "Stochastic Gradient Descent(8979): loss=0.5131116645288243\n",
      "Stochastic Gradient Descent(8980): loss=3.5589027249586507\n",
      "Stochastic Gradient Descent(8981): loss=2.095133287485073\n",
      "Stochastic Gradient Descent(8982): loss=9.120433970172902\n",
      "Stochastic Gradient Descent(8983): loss=1.6971876250670939\n",
      "Stochastic Gradient Descent(8984): loss=0.03387046467661428\n",
      "Stochastic Gradient Descent(8985): loss=3.042023080929477\n",
      "Stochastic Gradient Descent(8986): loss=0.007696577881712499\n",
      "Stochastic Gradient Descent(8987): loss=0.7035574450386807\n",
      "Stochastic Gradient Descent(8988): loss=2.4741197301849756\n",
      "Stochastic Gradient Descent(8989): loss=11.412044977242246\n",
      "Stochastic Gradient Descent(8990): loss=2.4555986652918183\n",
      "Stochastic Gradient Descent(8991): loss=1.2373246244072054\n",
      "Stochastic Gradient Descent(8992): loss=6.517371965528021\n",
      "Stochastic Gradient Descent(8993): loss=3.3204064758731606\n",
      "Stochastic Gradient Descent(8994): loss=0.6478261368176845\n",
      "Stochastic Gradient Descent(8995): loss=11.844918733187486\n",
      "Stochastic Gradient Descent(8996): loss=2.889121130691439\n",
      "Stochastic Gradient Descent(8997): loss=1.0720439545138765\n",
      "Stochastic Gradient Descent(8998): loss=3.8311002353284223\n",
      "Stochastic Gradient Descent(8999): loss=18.419842846276094\n",
      "Stochastic Gradient Descent(9000): loss=1.930043967022645\n",
      "Stochastic Gradient Descent(9001): loss=0.5166505906539423\n",
      "Stochastic Gradient Descent(9002): loss=12.26288387977771\n",
      "Stochastic Gradient Descent(9003): loss=0.14607750784157295\n",
      "Stochastic Gradient Descent(9004): loss=2.137039670141933\n",
      "Stochastic Gradient Descent(9005): loss=2.7010494435734125\n",
      "Stochastic Gradient Descent(9006): loss=0.3775061165314592\n",
      "Stochastic Gradient Descent(9007): loss=11.670867319364376\n",
      "Stochastic Gradient Descent(9008): loss=2.5856065799303116\n",
      "Stochastic Gradient Descent(9009): loss=8.834936335047747\n",
      "Stochastic Gradient Descent(9010): loss=4.754132251399798\n",
      "Stochastic Gradient Descent(9011): loss=25.315580024756294\n",
      "Stochastic Gradient Descent(9012): loss=0.6540944924890302\n",
      "Stochastic Gradient Descent(9013): loss=1.7176079008729874\n",
      "Stochastic Gradient Descent(9014): loss=10.68535387207355\n",
      "Stochastic Gradient Descent(9015): loss=0.47441300650496926\n",
      "Stochastic Gradient Descent(9016): loss=1.2840109307562753\n",
      "Stochastic Gradient Descent(9017): loss=3.8996599126688367\n",
      "Stochastic Gradient Descent(9018): loss=2.403321596972568\n",
      "Stochastic Gradient Descent(9019): loss=4.777810096282076\n",
      "Stochastic Gradient Descent(9020): loss=6.6008595154185565\n",
      "Stochastic Gradient Descent(9021): loss=1.006559103171283\n",
      "Stochastic Gradient Descent(9022): loss=6.938248839571728\n",
      "Stochastic Gradient Descent(9023): loss=7.040814573049371\n",
      "Stochastic Gradient Descent(9024): loss=0.2577253361270677\n",
      "Stochastic Gradient Descent(9025): loss=1.0620548548041977\n",
      "Stochastic Gradient Descent(9026): loss=12.391715138741457\n",
      "Stochastic Gradient Descent(9027): loss=0.5012699727123346\n",
      "Stochastic Gradient Descent(9028): loss=0.20181217064032506\n",
      "Stochastic Gradient Descent(9029): loss=2.9350755786494065\n",
      "Stochastic Gradient Descent(9030): loss=0.023290562988256523\n",
      "Stochastic Gradient Descent(9031): loss=0.6612678247968973\n",
      "Stochastic Gradient Descent(9032): loss=0.14599314492384952\n",
      "Stochastic Gradient Descent(9033): loss=0.7752743559240155\n",
      "Stochastic Gradient Descent(9034): loss=0.14731583723374878\n",
      "Stochastic Gradient Descent(9035): loss=0.144760959072825\n",
      "Stochastic Gradient Descent(9036): loss=0.0046992957412856085\n",
      "Stochastic Gradient Descent(9037): loss=1.3276945441349692\n",
      "Stochastic Gradient Descent(9038): loss=22.899719806484597\n",
      "Stochastic Gradient Descent(9039): loss=0.36121469524377414\n",
      "Stochastic Gradient Descent(9040): loss=7.428138053116747\n",
      "Stochastic Gradient Descent(9041): loss=1.3516409036863999\n",
      "Stochastic Gradient Descent(9042): loss=9.322420593807268\n",
      "Stochastic Gradient Descent(9043): loss=2.2749712375547433\n",
      "Stochastic Gradient Descent(9044): loss=0.6418078426289997\n",
      "Stochastic Gradient Descent(9045): loss=0.8267262153259286\n",
      "Stochastic Gradient Descent(9046): loss=1.6217404053510307\n",
      "Stochastic Gradient Descent(9047): loss=5.388704261377656\n",
      "Stochastic Gradient Descent(9048): loss=1.613134889226643\n",
      "Stochastic Gradient Descent(9049): loss=0.32076204454209234\n",
      "Stochastic Gradient Descent(9050): loss=1.8052907297047331\n",
      "Stochastic Gradient Descent(9051): loss=0.004559194785436281\n",
      "Stochastic Gradient Descent(9052): loss=0.16201208393547137\n",
      "Stochastic Gradient Descent(9053): loss=12.019252263145878\n",
      "Stochastic Gradient Descent(9054): loss=0.01614885645450188\n",
      "Stochastic Gradient Descent(9055): loss=3.376352149619043\n",
      "Stochastic Gradient Descent(9056): loss=0.789356478441301\n",
      "Stochastic Gradient Descent(9057): loss=22.515879328802317\n",
      "Stochastic Gradient Descent(9058): loss=4.300001097478532\n",
      "Stochastic Gradient Descent(9059): loss=0.18559109296581552\n",
      "Stochastic Gradient Descent(9060): loss=9.097771632655677\n",
      "Stochastic Gradient Descent(9061): loss=0.04705511007032509\n",
      "Stochastic Gradient Descent(9062): loss=8.228805839278207\n",
      "Stochastic Gradient Descent(9063): loss=0.12634457875429445\n",
      "Stochastic Gradient Descent(9064): loss=8.585663827898635\n",
      "Stochastic Gradient Descent(9065): loss=3.0504233021910068\n",
      "Stochastic Gradient Descent(9066): loss=21.48786774889676\n",
      "Stochastic Gradient Descent(9067): loss=25.819278425651646\n",
      "Stochastic Gradient Descent(9068): loss=0.29653026234079133\n",
      "Stochastic Gradient Descent(9069): loss=13.395156714445951\n",
      "Stochastic Gradient Descent(9070): loss=2.1449562011443515\n",
      "Stochastic Gradient Descent(9071): loss=1.7081388570771496\n",
      "Stochastic Gradient Descent(9072): loss=0.08204305416608441\n",
      "Stochastic Gradient Descent(9073): loss=39.26648120544872\n",
      "Stochastic Gradient Descent(9074): loss=0.1393008235254063\n",
      "Stochastic Gradient Descent(9075): loss=40.753538065578915\n",
      "Stochastic Gradient Descent(9076): loss=11.345774199210354\n",
      "Stochastic Gradient Descent(9077): loss=3.738766904735255\n",
      "Stochastic Gradient Descent(9078): loss=6.436860923991958\n",
      "Stochastic Gradient Descent(9079): loss=0.008498004764897637\n",
      "Stochastic Gradient Descent(9080): loss=0.142139126335762\n",
      "Stochastic Gradient Descent(9081): loss=3.522799353955011\n",
      "Stochastic Gradient Descent(9082): loss=46.77811230184003\n",
      "Stochastic Gradient Descent(9083): loss=0.9565328645279593\n",
      "Stochastic Gradient Descent(9084): loss=0.5478251678246165\n",
      "Stochastic Gradient Descent(9085): loss=1.325356627224084\n",
      "Stochastic Gradient Descent(9086): loss=1.2139071284224405\n",
      "Stochastic Gradient Descent(9087): loss=5.5705228713551955\n",
      "Stochastic Gradient Descent(9088): loss=12.261450344840387\n",
      "Stochastic Gradient Descent(9089): loss=14.923335229923767\n",
      "Stochastic Gradient Descent(9090): loss=1.6808026708223385\n",
      "Stochastic Gradient Descent(9091): loss=0.37894950302797037\n",
      "Stochastic Gradient Descent(9092): loss=0.9180430888432888\n",
      "Stochastic Gradient Descent(9093): loss=0.08595461583155646\n",
      "Stochastic Gradient Descent(9094): loss=9.816471742943275\n",
      "Stochastic Gradient Descent(9095): loss=10.401701109549872\n",
      "Stochastic Gradient Descent(9096): loss=8.845741555928798\n",
      "Stochastic Gradient Descent(9097): loss=3.782582646537003\n",
      "Stochastic Gradient Descent(9098): loss=0.2997896336325318\n",
      "Stochastic Gradient Descent(9099): loss=1.3975395739058774\n",
      "Stochastic Gradient Descent(9100): loss=1.782441297025839\n",
      "Stochastic Gradient Descent(9101): loss=2.649340158129984\n",
      "Stochastic Gradient Descent(9102): loss=0.30864932258083483\n",
      "Stochastic Gradient Descent(9103): loss=8.854177840170642\n",
      "Stochastic Gradient Descent(9104): loss=3.752541366391006\n",
      "Stochastic Gradient Descent(9105): loss=1.205026799289817\n",
      "Stochastic Gradient Descent(9106): loss=1.3206703942154552\n",
      "Stochastic Gradient Descent(9107): loss=5.65960475124607\n",
      "Stochastic Gradient Descent(9108): loss=0.4137186345736263\n",
      "Stochastic Gradient Descent(9109): loss=4.660169168153455\n",
      "Stochastic Gradient Descent(9110): loss=7.953491350653079\n",
      "Stochastic Gradient Descent(9111): loss=2.0047671138529677\n",
      "Stochastic Gradient Descent(9112): loss=4.233794221180115\n",
      "Stochastic Gradient Descent(9113): loss=3.6999762308131574\n",
      "Stochastic Gradient Descent(9114): loss=2.043771424414675\n",
      "Stochastic Gradient Descent(9115): loss=9.227116072951642\n",
      "Stochastic Gradient Descent(9116): loss=4.148245189987825\n",
      "Stochastic Gradient Descent(9117): loss=0.8995365916781066\n",
      "Stochastic Gradient Descent(9118): loss=2.310514886593359\n",
      "Stochastic Gradient Descent(9119): loss=0.3880555997599405\n",
      "Stochastic Gradient Descent(9120): loss=0.7026060816353582\n",
      "Stochastic Gradient Descent(9121): loss=0.22635309931798833\n",
      "Stochastic Gradient Descent(9122): loss=0.10977795034385641\n",
      "Stochastic Gradient Descent(9123): loss=2.4060329540429564\n",
      "Stochastic Gradient Descent(9124): loss=6.356166956565888\n",
      "Stochastic Gradient Descent(9125): loss=8.794796185755086\n",
      "Stochastic Gradient Descent(9126): loss=0.6793374558870713\n",
      "Stochastic Gradient Descent(9127): loss=2.8550798376145834\n",
      "Stochastic Gradient Descent(9128): loss=0.5416011146782005\n",
      "Stochastic Gradient Descent(9129): loss=2.3943884408130294\n",
      "Stochastic Gradient Descent(9130): loss=19.50293726097059\n",
      "Stochastic Gradient Descent(9131): loss=2.772685247667234\n",
      "Stochastic Gradient Descent(9132): loss=19.49182908047088\n",
      "Stochastic Gradient Descent(9133): loss=0.5027519348799347\n",
      "Stochastic Gradient Descent(9134): loss=1.4491019268254954\n",
      "Stochastic Gradient Descent(9135): loss=3.373274020085523\n",
      "Stochastic Gradient Descent(9136): loss=0.4919147814741118\n",
      "Stochastic Gradient Descent(9137): loss=13.812402549969464\n",
      "Stochastic Gradient Descent(9138): loss=0.13247455255188312\n",
      "Stochastic Gradient Descent(9139): loss=0.8530316665401515\n",
      "Stochastic Gradient Descent(9140): loss=0.8683830700143345\n",
      "Stochastic Gradient Descent(9141): loss=10.176340016611416\n",
      "Stochastic Gradient Descent(9142): loss=5.137938348245809\n",
      "Stochastic Gradient Descent(9143): loss=2.2064854028183576\n",
      "Stochastic Gradient Descent(9144): loss=20.83377276237681\n",
      "Stochastic Gradient Descent(9145): loss=0.5845315037626563\n",
      "Stochastic Gradient Descent(9146): loss=0.33856129084143016\n",
      "Stochastic Gradient Descent(9147): loss=1.0897531021092408\n",
      "Stochastic Gradient Descent(9148): loss=0.12755309783613267\n",
      "Stochastic Gradient Descent(9149): loss=2.2826765041151287\n",
      "Stochastic Gradient Descent(9150): loss=0.03239310568366112\n",
      "Stochastic Gradient Descent(9151): loss=1.3519965928270845\n",
      "Stochastic Gradient Descent(9152): loss=0.2213385783190506\n",
      "Stochastic Gradient Descent(9153): loss=0.03945706071422618\n",
      "Stochastic Gradient Descent(9154): loss=0.9278552338923663\n",
      "Stochastic Gradient Descent(9155): loss=3.0839330157927747\n",
      "Stochastic Gradient Descent(9156): loss=1.0023636259257405\n",
      "Stochastic Gradient Descent(9157): loss=0.06642414796914956\n",
      "Stochastic Gradient Descent(9158): loss=2.5168953711668\n",
      "Stochastic Gradient Descent(9159): loss=39.60068582591408\n",
      "Stochastic Gradient Descent(9160): loss=7.647281240509792\n",
      "Stochastic Gradient Descent(9161): loss=0.636894120339804\n",
      "Stochastic Gradient Descent(9162): loss=0.13608292078633116\n",
      "Stochastic Gradient Descent(9163): loss=1.5471011230026577\n",
      "Stochastic Gradient Descent(9164): loss=0.4566187488289402\n",
      "Stochastic Gradient Descent(9165): loss=2.2154171518821366\n",
      "Stochastic Gradient Descent(9166): loss=0.31455207407524\n",
      "Stochastic Gradient Descent(9167): loss=13.494867665167453\n",
      "Stochastic Gradient Descent(9168): loss=2.6909953012957866\n",
      "Stochastic Gradient Descent(9169): loss=0.3876729564986786\n",
      "Stochastic Gradient Descent(9170): loss=2.2772509315694496\n",
      "Stochastic Gradient Descent(9171): loss=13.05236826325746\n",
      "Stochastic Gradient Descent(9172): loss=0.3913784131990528\n",
      "Stochastic Gradient Descent(9173): loss=3.310064702130774\n",
      "Stochastic Gradient Descent(9174): loss=1.6293312046010422\n",
      "Stochastic Gradient Descent(9175): loss=19.738852518523164\n",
      "Stochastic Gradient Descent(9176): loss=5.431641227359822\n",
      "Stochastic Gradient Descent(9177): loss=2.8225777352096606\n",
      "Stochastic Gradient Descent(9178): loss=0.25053462527647885\n",
      "Stochastic Gradient Descent(9179): loss=0.7049342970754338\n",
      "Stochastic Gradient Descent(9180): loss=0.015461895810512185\n",
      "Stochastic Gradient Descent(9181): loss=2.2282180209815436\n",
      "Stochastic Gradient Descent(9182): loss=0.36569401642330296\n",
      "Stochastic Gradient Descent(9183): loss=7.687134203484019\n",
      "Stochastic Gradient Descent(9184): loss=0.10536622395447921\n",
      "Stochastic Gradient Descent(9185): loss=0.2609108845581404\n",
      "Stochastic Gradient Descent(9186): loss=5.172051022865271\n",
      "Stochastic Gradient Descent(9187): loss=1.0472568982638686\n",
      "Stochastic Gradient Descent(9188): loss=3.323424195454965\n",
      "Stochastic Gradient Descent(9189): loss=25.506776005420548\n",
      "Stochastic Gradient Descent(9190): loss=0.006636479682275566\n",
      "Stochastic Gradient Descent(9191): loss=17.708390861562183\n",
      "Stochastic Gradient Descent(9192): loss=3.7234510398819163\n",
      "Stochastic Gradient Descent(9193): loss=4.735522959844226\n",
      "Stochastic Gradient Descent(9194): loss=0.06396524266786544\n",
      "Stochastic Gradient Descent(9195): loss=0.0022712573780261994\n",
      "Stochastic Gradient Descent(9196): loss=0.004911640742151636\n",
      "Stochastic Gradient Descent(9197): loss=20.34326575503757\n",
      "Stochastic Gradient Descent(9198): loss=0.03634029327402453\n",
      "Stochastic Gradient Descent(9199): loss=0.9567846635325119\n",
      "Stochastic Gradient Descent(9200): loss=0.09413013574454016\n",
      "Stochastic Gradient Descent(9201): loss=9.673951495004594\n",
      "Stochastic Gradient Descent(9202): loss=7.214865277998268\n",
      "Stochastic Gradient Descent(9203): loss=3.5857165636661232\n",
      "Stochastic Gradient Descent(9204): loss=1.2923046465272015\n",
      "Stochastic Gradient Descent(9205): loss=9.315458128394736\n",
      "Stochastic Gradient Descent(9206): loss=8.048447530295006\n",
      "Stochastic Gradient Descent(9207): loss=11.221920517500228\n",
      "Stochastic Gradient Descent(9208): loss=1.5486127656481645\n",
      "Stochastic Gradient Descent(9209): loss=4.234686704487523\n",
      "Stochastic Gradient Descent(9210): loss=0.5044653869432485\n",
      "Stochastic Gradient Descent(9211): loss=0.2962428216488581\n",
      "Stochastic Gradient Descent(9212): loss=6.790967277815062\n",
      "Stochastic Gradient Descent(9213): loss=1.9478802606995014\n",
      "Stochastic Gradient Descent(9214): loss=8.581270066735714\n",
      "Stochastic Gradient Descent(9215): loss=45.67359973161499\n",
      "Stochastic Gradient Descent(9216): loss=29.472715555917407\n",
      "Stochastic Gradient Descent(9217): loss=0.05404244487916407\n",
      "Stochastic Gradient Descent(9218): loss=33.85113227740476\n",
      "Stochastic Gradient Descent(9219): loss=4.247906456446591\n",
      "Stochastic Gradient Descent(9220): loss=3.579107287238041\n",
      "Stochastic Gradient Descent(9221): loss=0.000530897247563255\n",
      "Stochastic Gradient Descent(9222): loss=16.849981426540626\n",
      "Stochastic Gradient Descent(9223): loss=3.369645147734154\n",
      "Stochastic Gradient Descent(9224): loss=1.5123636280607882\n",
      "Stochastic Gradient Descent(9225): loss=1.7950945813719015\n",
      "Stochastic Gradient Descent(9226): loss=0.32256008077770865\n",
      "Stochastic Gradient Descent(9227): loss=0.6535674301982536\n",
      "Stochastic Gradient Descent(9228): loss=0.23488982505704933\n",
      "Stochastic Gradient Descent(9229): loss=5.836691889494831\n",
      "Stochastic Gradient Descent(9230): loss=8.82604603725401\n",
      "Stochastic Gradient Descent(9231): loss=18.01838314293448\n",
      "Stochastic Gradient Descent(9232): loss=6.368619135795029\n",
      "Stochastic Gradient Descent(9233): loss=3.46909146502264\n",
      "Stochastic Gradient Descent(9234): loss=2.5673623172242075\n",
      "Stochastic Gradient Descent(9235): loss=0.05567301842545055\n",
      "Stochastic Gradient Descent(9236): loss=1.2861151237867414\n",
      "Stochastic Gradient Descent(9237): loss=0.6110178943603783\n",
      "Stochastic Gradient Descent(9238): loss=3.189192795987108\n",
      "Stochastic Gradient Descent(9239): loss=0.2236102254246658\n",
      "Stochastic Gradient Descent(9240): loss=0.9924604147525505\n",
      "Stochastic Gradient Descent(9241): loss=5.110027273511708\n",
      "Stochastic Gradient Descent(9242): loss=0.9748289841034422\n",
      "Stochastic Gradient Descent(9243): loss=0.0399898827588017\n",
      "Stochastic Gradient Descent(9244): loss=4.838408520628682\n",
      "Stochastic Gradient Descent(9245): loss=0.3621560526431919\n",
      "Stochastic Gradient Descent(9246): loss=3.1675643978451893\n",
      "Stochastic Gradient Descent(9247): loss=2.621586688104938\n",
      "Stochastic Gradient Descent(9248): loss=11.16275807001242\n",
      "Stochastic Gradient Descent(9249): loss=0.6166911015176443\n",
      "Stochastic Gradient Descent(9250): loss=4.4013350883670865\n",
      "Stochastic Gradient Descent(9251): loss=3.252022572993449\n",
      "Stochastic Gradient Descent(9252): loss=0.8846795230547521\n",
      "Stochastic Gradient Descent(9253): loss=11.913922830266882\n",
      "Stochastic Gradient Descent(9254): loss=2.9706146887686065\n",
      "Stochastic Gradient Descent(9255): loss=0.23774773414893466\n",
      "Stochastic Gradient Descent(9256): loss=3.881920448042859\n",
      "Stochastic Gradient Descent(9257): loss=18.239834311800053\n",
      "Stochastic Gradient Descent(9258): loss=0.15025659286655246\n",
      "Stochastic Gradient Descent(9259): loss=6.4087459244149265\n",
      "Stochastic Gradient Descent(9260): loss=6.585214986908564\n",
      "Stochastic Gradient Descent(9261): loss=0.07422824143918517\n",
      "Stochastic Gradient Descent(9262): loss=0.2586092204135717\n",
      "Stochastic Gradient Descent(9263): loss=6.11024358594325\n",
      "Stochastic Gradient Descent(9264): loss=5.731947229926415\n",
      "Stochastic Gradient Descent(9265): loss=9.987765343290055\n",
      "Stochastic Gradient Descent(9266): loss=5.1085095327051775\n",
      "Stochastic Gradient Descent(9267): loss=0.0673298108384467\n",
      "Stochastic Gradient Descent(9268): loss=3.5868950361579954\n",
      "Stochastic Gradient Descent(9269): loss=0.6896932371840668\n",
      "Stochastic Gradient Descent(9270): loss=3.114853618347183\n",
      "Stochastic Gradient Descent(9271): loss=9.68695867973416\n",
      "Stochastic Gradient Descent(9272): loss=13.814784980595286\n",
      "Stochastic Gradient Descent(9273): loss=9.569020210388032\n",
      "Stochastic Gradient Descent(9274): loss=1.78659271868819\n",
      "Stochastic Gradient Descent(9275): loss=19.81851237133455\n",
      "Stochastic Gradient Descent(9276): loss=0.01579523254296695\n",
      "Stochastic Gradient Descent(9277): loss=0.27479794483044656\n",
      "Stochastic Gradient Descent(9278): loss=1.9509408550270164\n",
      "Stochastic Gradient Descent(9279): loss=3.158202379272893\n",
      "Stochastic Gradient Descent(9280): loss=0.5582382390292239\n",
      "Stochastic Gradient Descent(9281): loss=16.647684344480037\n",
      "Stochastic Gradient Descent(9282): loss=0.2394838831049658\n",
      "Stochastic Gradient Descent(9283): loss=14.868377554286306\n",
      "Stochastic Gradient Descent(9284): loss=11.396043598299622\n",
      "Stochastic Gradient Descent(9285): loss=8.581168753396124\n",
      "Stochastic Gradient Descent(9286): loss=6.267586161052886\n",
      "Stochastic Gradient Descent(9287): loss=0.5935859719695555\n",
      "Stochastic Gradient Descent(9288): loss=2.572860317257534\n",
      "Stochastic Gradient Descent(9289): loss=0.010515856092927073\n",
      "Stochastic Gradient Descent(9290): loss=21.189547301587265\n",
      "Stochastic Gradient Descent(9291): loss=6.318924528597227\n",
      "Stochastic Gradient Descent(9292): loss=0.10200491994301561\n",
      "Stochastic Gradient Descent(9293): loss=6.227022457936372\n",
      "Stochastic Gradient Descent(9294): loss=0.32352267294648157\n",
      "Stochastic Gradient Descent(9295): loss=22.9343846669455\n",
      "Stochastic Gradient Descent(9296): loss=2.7681542765989655\n",
      "Stochastic Gradient Descent(9297): loss=8.548906380413552\n",
      "Stochastic Gradient Descent(9298): loss=14.09805500571013\n",
      "Stochastic Gradient Descent(9299): loss=0.4271949960070687\n",
      "Stochastic Gradient Descent(9300): loss=10.976273879251979\n",
      "Stochastic Gradient Descent(9301): loss=63.75237686059549\n",
      "Stochastic Gradient Descent(9302): loss=0.17434152231099345\n",
      "Stochastic Gradient Descent(9303): loss=0.5017456750805305\n",
      "Stochastic Gradient Descent(9304): loss=1.2040904232819727\n",
      "Stochastic Gradient Descent(9305): loss=12.53600691615047\n",
      "Stochastic Gradient Descent(9306): loss=42.76293635846932\n",
      "Stochastic Gradient Descent(9307): loss=0.22750978139761696\n",
      "Stochastic Gradient Descent(9308): loss=15.387817369793611\n",
      "Stochastic Gradient Descent(9309): loss=12.394749835329911\n",
      "Stochastic Gradient Descent(9310): loss=6.883262937824517\n",
      "Stochastic Gradient Descent(9311): loss=3.9802892244875467\n",
      "Stochastic Gradient Descent(9312): loss=4.118568778522324\n",
      "Stochastic Gradient Descent(9313): loss=0.4369669197241108\n",
      "Stochastic Gradient Descent(9314): loss=0.010020127597597264\n",
      "Stochastic Gradient Descent(9315): loss=0.46095690856635807\n",
      "Stochastic Gradient Descent(9316): loss=8.448100595142364\n",
      "Stochastic Gradient Descent(9317): loss=0.8324398795094541\n",
      "Stochastic Gradient Descent(9318): loss=1.130013081917133\n",
      "Stochastic Gradient Descent(9319): loss=0.4484608881295814\n",
      "Stochastic Gradient Descent(9320): loss=0.08211537840265061\n",
      "Stochastic Gradient Descent(9321): loss=1.5669716363279982\n",
      "Stochastic Gradient Descent(9322): loss=16.093065237492198\n",
      "Stochastic Gradient Descent(9323): loss=2.8330678625100925\n",
      "Stochastic Gradient Descent(9324): loss=4.816817490259004\n",
      "Stochastic Gradient Descent(9325): loss=12.993517644178247\n",
      "Stochastic Gradient Descent(9326): loss=5.389767589733793\n",
      "Stochastic Gradient Descent(9327): loss=0.7787293545977935\n",
      "Stochastic Gradient Descent(9328): loss=0.5249529284293691\n",
      "Stochastic Gradient Descent(9329): loss=6.095058152642418e-05\n",
      "Stochastic Gradient Descent(9330): loss=0.6980133704817979\n",
      "Stochastic Gradient Descent(9331): loss=0.0011388578331495657\n",
      "Stochastic Gradient Descent(9332): loss=7.8211513108603405\n",
      "Stochastic Gradient Descent(9333): loss=0.010688413829334723\n",
      "Stochastic Gradient Descent(9334): loss=0.18963967302824897\n",
      "Stochastic Gradient Descent(9335): loss=4.655029575620533\n",
      "Stochastic Gradient Descent(9336): loss=3.436982309110377\n",
      "Stochastic Gradient Descent(9337): loss=3.2562864148980446\n",
      "Stochastic Gradient Descent(9338): loss=1.022814905539733\n",
      "Stochastic Gradient Descent(9339): loss=0.07577741290072144\n",
      "Stochastic Gradient Descent(9340): loss=0.9843035573628592\n",
      "Stochastic Gradient Descent(9341): loss=7.885994402988945\n",
      "Stochastic Gradient Descent(9342): loss=4.536580571355494\n",
      "Stochastic Gradient Descent(9343): loss=11.880891479002084\n",
      "Stochastic Gradient Descent(9344): loss=50.93844578255877\n",
      "Stochastic Gradient Descent(9345): loss=0.2140565708224504\n",
      "Stochastic Gradient Descent(9346): loss=0.018330228322278916\n",
      "Stochastic Gradient Descent(9347): loss=10.200432190588177\n",
      "Stochastic Gradient Descent(9348): loss=24.488618639347454\n",
      "Stochastic Gradient Descent(9349): loss=0.5808238832172569\n",
      "Stochastic Gradient Descent(9350): loss=0.6212824359789694\n",
      "Stochastic Gradient Descent(9351): loss=1.7305161365791926\n",
      "Stochastic Gradient Descent(9352): loss=8.292773474690934\n",
      "Stochastic Gradient Descent(9353): loss=4.158137808403046\n",
      "Stochastic Gradient Descent(9354): loss=0.2983333821584783\n",
      "Stochastic Gradient Descent(9355): loss=4.71794374842634\n",
      "Stochastic Gradient Descent(9356): loss=0.003925275697626584\n",
      "Stochastic Gradient Descent(9357): loss=0.508861038089688\n",
      "Stochastic Gradient Descent(9358): loss=4.075547896396877\n",
      "Stochastic Gradient Descent(9359): loss=4.719941171874687\n",
      "Stochastic Gradient Descent(9360): loss=1.0767687893136142\n",
      "Stochastic Gradient Descent(9361): loss=0.1771897231221799\n",
      "Stochastic Gradient Descent(9362): loss=2.6170944345474383\n",
      "Stochastic Gradient Descent(9363): loss=0.24369353615397832\n",
      "Stochastic Gradient Descent(9364): loss=3.604811111513168\n",
      "Stochastic Gradient Descent(9365): loss=2.452460692200069\n",
      "Stochastic Gradient Descent(9366): loss=0.29252327207055245\n",
      "Stochastic Gradient Descent(9367): loss=1.7292323319553833\n",
      "Stochastic Gradient Descent(9368): loss=2.009833768057847\n",
      "Stochastic Gradient Descent(9369): loss=0.20151276489452363\n",
      "Stochastic Gradient Descent(9370): loss=6.7797625900519405\n",
      "Stochastic Gradient Descent(9371): loss=0.8709491454204918\n",
      "Stochastic Gradient Descent(9372): loss=24.379133210555803\n",
      "Stochastic Gradient Descent(9373): loss=2.3796117554765077\n",
      "Stochastic Gradient Descent(9374): loss=16.42402372335939\n",
      "Stochastic Gradient Descent(9375): loss=31.80325918579855\n",
      "Stochastic Gradient Descent(9376): loss=1.422151026959269\n",
      "Stochastic Gradient Descent(9377): loss=2.6992939048334086\n",
      "Stochastic Gradient Descent(9378): loss=0.34123341710245053\n",
      "Stochastic Gradient Descent(9379): loss=0.0918076765885889\n",
      "Stochastic Gradient Descent(9380): loss=0.9050792625513432\n",
      "Stochastic Gradient Descent(9381): loss=0.030489210797739825\n",
      "Stochastic Gradient Descent(9382): loss=2.913855591956716\n",
      "Stochastic Gradient Descent(9383): loss=0.0019250808443795028\n",
      "Stochastic Gradient Descent(9384): loss=4.777340629281237\n",
      "Stochastic Gradient Descent(9385): loss=6.498915131605265e-05\n",
      "Stochastic Gradient Descent(9386): loss=1.046766106404726\n",
      "Stochastic Gradient Descent(9387): loss=0.44493854955963935\n",
      "Stochastic Gradient Descent(9388): loss=0.00479531249008344\n",
      "Stochastic Gradient Descent(9389): loss=8.739072137704287\n",
      "Stochastic Gradient Descent(9390): loss=0.17966872991152247\n",
      "Stochastic Gradient Descent(9391): loss=0.08981022165102771\n",
      "Stochastic Gradient Descent(9392): loss=1.7788885936211603\n",
      "Stochastic Gradient Descent(9393): loss=0.6310301552359854\n",
      "Stochastic Gradient Descent(9394): loss=1.4599653481832418\n",
      "Stochastic Gradient Descent(9395): loss=0.7019871476763523\n",
      "Stochastic Gradient Descent(9396): loss=0.8713555976521096\n",
      "Stochastic Gradient Descent(9397): loss=0.004196814057584744\n",
      "Stochastic Gradient Descent(9398): loss=67.67254273651868\n",
      "Stochastic Gradient Descent(9399): loss=2.70790072902903\n",
      "Stochastic Gradient Descent(9400): loss=43.80175311446005\n",
      "Stochastic Gradient Descent(9401): loss=69.23701086373772\n",
      "Stochastic Gradient Descent(9402): loss=14.708799188082812\n",
      "Stochastic Gradient Descent(9403): loss=0.27939484271705817\n",
      "Stochastic Gradient Descent(9404): loss=4.614542219517743\n",
      "Stochastic Gradient Descent(9405): loss=8.335049012614276\n",
      "Stochastic Gradient Descent(9406): loss=4.9591409913926965\n",
      "Stochastic Gradient Descent(9407): loss=2.7360465335566952\n",
      "Stochastic Gradient Descent(9408): loss=0.013393400572849348\n",
      "Stochastic Gradient Descent(9409): loss=0.08586825038898999\n",
      "Stochastic Gradient Descent(9410): loss=0.36658440373190265\n",
      "Stochastic Gradient Descent(9411): loss=3.7055597785422294\n",
      "Stochastic Gradient Descent(9412): loss=11.037559920420929\n",
      "Stochastic Gradient Descent(9413): loss=14.182457982870057\n",
      "Stochastic Gradient Descent(9414): loss=4.760472642877768\n",
      "Stochastic Gradient Descent(9415): loss=9.910040844300262\n",
      "Stochastic Gradient Descent(9416): loss=0.006575045342609476\n",
      "Stochastic Gradient Descent(9417): loss=2.571495783564255\n",
      "Stochastic Gradient Descent(9418): loss=14.077104375316178\n",
      "Stochastic Gradient Descent(9419): loss=4.071484355978749\n",
      "Stochastic Gradient Descent(9420): loss=1.4622477478594058\n",
      "Stochastic Gradient Descent(9421): loss=2.6888414759636667\n",
      "Stochastic Gradient Descent(9422): loss=0.7095912773347253\n",
      "Stochastic Gradient Descent(9423): loss=20.519966923807747\n",
      "Stochastic Gradient Descent(9424): loss=1.8107840387638072\n",
      "Stochastic Gradient Descent(9425): loss=2.251185214783306\n",
      "Stochastic Gradient Descent(9426): loss=0.5163235327073639\n",
      "Stochastic Gradient Descent(9427): loss=0.42106985938808045\n",
      "Stochastic Gradient Descent(9428): loss=3.4912538454107915\n",
      "Stochastic Gradient Descent(9429): loss=10.472860212239002\n",
      "Stochastic Gradient Descent(9430): loss=0.8089169939839511\n",
      "Stochastic Gradient Descent(9431): loss=1.210780485576172\n",
      "Stochastic Gradient Descent(9432): loss=1.320965282817445\n",
      "Stochastic Gradient Descent(9433): loss=0.6493110470453387\n",
      "Stochastic Gradient Descent(9434): loss=12.352112060659996\n",
      "Stochastic Gradient Descent(9435): loss=30.894339093995928\n",
      "Stochastic Gradient Descent(9436): loss=1.4905527370096114\n",
      "Stochastic Gradient Descent(9437): loss=10.409747692563435\n",
      "Stochastic Gradient Descent(9438): loss=8.864222150520641\n",
      "Stochastic Gradient Descent(9439): loss=6.093798781121144\n",
      "Stochastic Gradient Descent(9440): loss=17.092720953178315\n",
      "Stochastic Gradient Descent(9441): loss=0.9453405374648804\n",
      "Stochastic Gradient Descent(9442): loss=1.048247878815315\n",
      "Stochastic Gradient Descent(9443): loss=0.9608246006238965\n",
      "Stochastic Gradient Descent(9444): loss=0.12784147999894183\n",
      "Stochastic Gradient Descent(9445): loss=1.4036664655630247\n",
      "Stochastic Gradient Descent(9446): loss=0.7166925175733543\n",
      "Stochastic Gradient Descent(9447): loss=22.25879828795168\n",
      "Stochastic Gradient Descent(9448): loss=16.021751701774985\n",
      "Stochastic Gradient Descent(9449): loss=6.797118812451826\n",
      "Stochastic Gradient Descent(9450): loss=4.5607102739775245\n",
      "Stochastic Gradient Descent(9451): loss=3.887977783331662\n",
      "Stochastic Gradient Descent(9452): loss=0.03644938581550997\n",
      "Stochastic Gradient Descent(9453): loss=32.72345998593642\n",
      "Stochastic Gradient Descent(9454): loss=0.07725606842347932\n",
      "Stochastic Gradient Descent(9455): loss=1.732371800220675\n",
      "Stochastic Gradient Descent(9456): loss=9.733926708571929\n",
      "Stochastic Gradient Descent(9457): loss=8.48861266583559\n",
      "Stochastic Gradient Descent(9458): loss=6.272083376769396\n",
      "Stochastic Gradient Descent(9459): loss=2.0011479430334926\n",
      "Stochastic Gradient Descent(9460): loss=0.4006474991933494\n",
      "Stochastic Gradient Descent(9461): loss=3.7433404372916925\n",
      "Stochastic Gradient Descent(9462): loss=13.92005675384335\n",
      "Stochastic Gradient Descent(9463): loss=10.678407721509577\n",
      "Stochastic Gradient Descent(9464): loss=0.841051247955466\n",
      "Stochastic Gradient Descent(9465): loss=0.0052858750922270695\n",
      "Stochastic Gradient Descent(9466): loss=0.05738715888253166\n",
      "Stochastic Gradient Descent(9467): loss=6.3542564813245646\n",
      "Stochastic Gradient Descent(9468): loss=0.6233770072120167\n",
      "Stochastic Gradient Descent(9469): loss=0.042352451812411034\n",
      "Stochastic Gradient Descent(9470): loss=7.85173427930897\n",
      "Stochastic Gradient Descent(9471): loss=2.2513735932499273e-05\n",
      "Stochastic Gradient Descent(9472): loss=43.850498531961165\n",
      "Stochastic Gradient Descent(9473): loss=29.551615525353412\n",
      "Stochastic Gradient Descent(9474): loss=8.172232682096299\n",
      "Stochastic Gradient Descent(9475): loss=9.200965917041186\n",
      "Stochastic Gradient Descent(9476): loss=5.163229532856358\n",
      "Stochastic Gradient Descent(9477): loss=96.12184862396575\n",
      "Stochastic Gradient Descent(9478): loss=74.00505234063266\n",
      "Stochastic Gradient Descent(9479): loss=0.026027352371902644\n",
      "Stochastic Gradient Descent(9480): loss=0.009144158432993835\n",
      "Stochastic Gradient Descent(9481): loss=5.798985150485509\n",
      "Stochastic Gradient Descent(9482): loss=6.139661686943054\n",
      "Stochastic Gradient Descent(9483): loss=12.793664798876701\n",
      "Stochastic Gradient Descent(9484): loss=9.984622050091469\n",
      "Stochastic Gradient Descent(9485): loss=0.25458770659186053\n",
      "Stochastic Gradient Descent(9486): loss=143.54098439447364\n",
      "Stochastic Gradient Descent(9487): loss=8.297176450923478\n",
      "Stochastic Gradient Descent(9488): loss=5.097333875461413\n",
      "Stochastic Gradient Descent(9489): loss=5.840949948652882\n",
      "Stochastic Gradient Descent(9490): loss=0.6490623354355236\n",
      "Stochastic Gradient Descent(9491): loss=3.147334683253336\n",
      "Stochastic Gradient Descent(9492): loss=5.315348924290492\n",
      "Stochastic Gradient Descent(9493): loss=0.46846116744409644\n",
      "Stochastic Gradient Descent(9494): loss=0.2938530922425409\n",
      "Stochastic Gradient Descent(9495): loss=10.498288528147794\n",
      "Stochastic Gradient Descent(9496): loss=8.903244305419134\n",
      "Stochastic Gradient Descent(9497): loss=1.731208204899256\n",
      "Stochastic Gradient Descent(9498): loss=2.6921602241762135\n",
      "Stochastic Gradient Descent(9499): loss=1.1368240532887448\n",
      "Stochastic Gradient Descent(9500): loss=2.9378799186329667\n",
      "Stochastic Gradient Descent(9501): loss=18.49891718660484\n",
      "Stochastic Gradient Descent(9502): loss=0.6652797448846014\n",
      "Stochastic Gradient Descent(9503): loss=9.724619746379322\n",
      "Stochastic Gradient Descent(9504): loss=0.04402033748831485\n",
      "Stochastic Gradient Descent(9505): loss=3.1384938761636314\n",
      "Stochastic Gradient Descent(9506): loss=4.4638954686571495\n",
      "Stochastic Gradient Descent(9507): loss=1.6216680055573112\n",
      "Stochastic Gradient Descent(9508): loss=0.11035561730577126\n",
      "Stochastic Gradient Descent(9509): loss=11.7837066033332\n",
      "Stochastic Gradient Descent(9510): loss=0.10211455490651064\n",
      "Stochastic Gradient Descent(9511): loss=0.8664343053860732\n",
      "Stochastic Gradient Descent(9512): loss=0.08320836358284003\n",
      "Stochastic Gradient Descent(9513): loss=2.4312589540582668\n",
      "Stochastic Gradient Descent(9514): loss=14.018839031927246\n",
      "Stochastic Gradient Descent(9515): loss=1.402709910290106\n",
      "Stochastic Gradient Descent(9516): loss=0.12085117846698543\n",
      "Stochastic Gradient Descent(9517): loss=0.5320495052233822\n",
      "Stochastic Gradient Descent(9518): loss=2.7325451365561926\n",
      "Stochastic Gradient Descent(9519): loss=2.405713069080291\n",
      "Stochastic Gradient Descent(9520): loss=0.020816012522718155\n",
      "Stochastic Gradient Descent(9521): loss=0.3038768547488119\n",
      "Stochastic Gradient Descent(9522): loss=5.895434360733552\n",
      "Stochastic Gradient Descent(9523): loss=0.1668774108055375\n",
      "Stochastic Gradient Descent(9524): loss=14.605838551732722\n",
      "Stochastic Gradient Descent(9525): loss=0.1201839549126226\n",
      "Stochastic Gradient Descent(9526): loss=15.149306926349054\n",
      "Stochastic Gradient Descent(9527): loss=1.1192518875191713\n",
      "Stochastic Gradient Descent(9528): loss=0.2413497923864714\n",
      "Stochastic Gradient Descent(9529): loss=0.05022153881365301\n",
      "Stochastic Gradient Descent(9530): loss=1.640676605029888\n",
      "Stochastic Gradient Descent(9531): loss=1.0213387905084206\n",
      "Stochastic Gradient Descent(9532): loss=7.268980626639019\n",
      "Stochastic Gradient Descent(9533): loss=5.328457478524409\n",
      "Stochastic Gradient Descent(9534): loss=4.336274188294087\n",
      "Stochastic Gradient Descent(9535): loss=4.490184560253233\n",
      "Stochastic Gradient Descent(9536): loss=42.18982427260441\n",
      "Stochastic Gradient Descent(9537): loss=4.016124584209818\n",
      "Stochastic Gradient Descent(9538): loss=9.169516713634055\n",
      "Stochastic Gradient Descent(9539): loss=0.23769761719626506\n",
      "Stochastic Gradient Descent(9540): loss=0.2103907910279421\n",
      "Stochastic Gradient Descent(9541): loss=7.3513506829818045\n",
      "Stochastic Gradient Descent(9542): loss=2.0339675667715977\n",
      "Stochastic Gradient Descent(9543): loss=10.323370861012238\n",
      "Stochastic Gradient Descent(9544): loss=4.23839747178809\n",
      "Stochastic Gradient Descent(9545): loss=2.810811684251052\n",
      "Stochastic Gradient Descent(9546): loss=26.926402927504586\n",
      "Stochastic Gradient Descent(9547): loss=0.1988594823175228\n",
      "Stochastic Gradient Descent(9548): loss=6.146268255889001\n",
      "Stochastic Gradient Descent(9549): loss=13.676923635394727\n",
      "Stochastic Gradient Descent(9550): loss=0.12643140872392547\n",
      "Stochastic Gradient Descent(9551): loss=0.12264756677110501\n",
      "Stochastic Gradient Descent(9552): loss=3.0406135121797786\n",
      "Stochastic Gradient Descent(9553): loss=22.153454862757886\n",
      "Stochastic Gradient Descent(9554): loss=1.0631147476627727\n",
      "Stochastic Gradient Descent(9555): loss=3.1613013138494277\n",
      "Stochastic Gradient Descent(9556): loss=1.21227345114518\n",
      "Stochastic Gradient Descent(9557): loss=0.13468055054046132\n",
      "Stochastic Gradient Descent(9558): loss=0.4949322244693619\n",
      "Stochastic Gradient Descent(9559): loss=1.4366291496945653\n",
      "Stochastic Gradient Descent(9560): loss=4.803787023691879\n",
      "Stochastic Gradient Descent(9561): loss=1.9088745451140399\n",
      "Stochastic Gradient Descent(9562): loss=1.1187370977691344\n",
      "Stochastic Gradient Descent(9563): loss=2.9620559594686897\n",
      "Stochastic Gradient Descent(9564): loss=0.7157171144455257\n",
      "Stochastic Gradient Descent(9565): loss=23.810272608984203\n",
      "Stochastic Gradient Descent(9566): loss=2.8258456529329723\n",
      "Stochastic Gradient Descent(9567): loss=0.7634024149604917\n",
      "Stochastic Gradient Descent(9568): loss=0.24777192491919628\n",
      "Stochastic Gradient Descent(9569): loss=1.9096686211867089\n",
      "Stochastic Gradient Descent(9570): loss=2.534081424080585\n",
      "Stochastic Gradient Descent(9571): loss=0.0063244970413485185\n",
      "Stochastic Gradient Descent(9572): loss=0.26957531166406923\n",
      "Stochastic Gradient Descent(9573): loss=2.428543996139009\n",
      "Stochastic Gradient Descent(9574): loss=24.175509224656253\n",
      "Stochastic Gradient Descent(9575): loss=3.7424189989810572\n",
      "Stochastic Gradient Descent(9576): loss=3.7657761703655366\n",
      "Stochastic Gradient Descent(9577): loss=0.11825171987502146\n",
      "Stochastic Gradient Descent(9578): loss=0.4080533118872201\n",
      "Stochastic Gradient Descent(9579): loss=0.4044546472806372\n",
      "Stochastic Gradient Descent(9580): loss=41.71172047229099\n",
      "Stochastic Gradient Descent(9581): loss=3.0090377979361724\n",
      "Stochastic Gradient Descent(9582): loss=216.69259271202657\n",
      "Stochastic Gradient Descent(9583): loss=670.6128192129618\n",
      "Stochastic Gradient Descent(9584): loss=8.025783523662819\n",
      "Stochastic Gradient Descent(9585): loss=22.158425743173545\n",
      "Stochastic Gradient Descent(9586): loss=4.751346487175603\n",
      "Stochastic Gradient Descent(9587): loss=3.5279741090668852\n",
      "Stochastic Gradient Descent(9588): loss=0.10837107512896978\n",
      "Stochastic Gradient Descent(9589): loss=0.0925999057339716\n",
      "Stochastic Gradient Descent(9590): loss=3.88853506146026\n",
      "Stochastic Gradient Descent(9591): loss=12.079036046500363\n",
      "Stochastic Gradient Descent(9592): loss=5.8389242156904375\n",
      "Stochastic Gradient Descent(9593): loss=0.22294974971509593\n",
      "Stochastic Gradient Descent(9594): loss=0.015531112217630667\n",
      "Stochastic Gradient Descent(9595): loss=23.660388884141046\n",
      "Stochastic Gradient Descent(9596): loss=23.51551205232183\n",
      "Stochastic Gradient Descent(9597): loss=13.229185299256127\n",
      "Stochastic Gradient Descent(9598): loss=1.59068355681646\n",
      "Stochastic Gradient Descent(9599): loss=10.242890467089074\n",
      "Stochastic Gradient Descent(9600): loss=4.327569650704451\n",
      "Stochastic Gradient Descent(9601): loss=5.76349611930183\n",
      "Stochastic Gradient Descent(9602): loss=2.7304116297150967\n",
      "Stochastic Gradient Descent(9603): loss=20.396342008804808\n",
      "Stochastic Gradient Descent(9604): loss=0.009816144688048714\n",
      "Stochastic Gradient Descent(9605): loss=1.877280562938093\n",
      "Stochastic Gradient Descent(9606): loss=2.030818354598762\n",
      "Stochastic Gradient Descent(9607): loss=5.390274619593228\n",
      "Stochastic Gradient Descent(9608): loss=7.982079730207017\n",
      "Stochastic Gradient Descent(9609): loss=0.02552242289587814\n",
      "Stochastic Gradient Descent(9610): loss=10.98264270607336\n",
      "Stochastic Gradient Descent(9611): loss=3.9871797761171703\n",
      "Stochastic Gradient Descent(9612): loss=2.0888132641204487\n",
      "Stochastic Gradient Descent(9613): loss=4.3844471830244665\n",
      "Stochastic Gradient Descent(9614): loss=0.4833708020246859\n",
      "Stochastic Gradient Descent(9615): loss=6.2217549293493875\n",
      "Stochastic Gradient Descent(9616): loss=2.452148071143634\n",
      "Stochastic Gradient Descent(9617): loss=0.8298710581092131\n",
      "Stochastic Gradient Descent(9618): loss=15.677029353104826\n",
      "Stochastic Gradient Descent(9619): loss=1.919897864639394\n",
      "Stochastic Gradient Descent(9620): loss=1.788100168954732\n",
      "Stochastic Gradient Descent(9621): loss=0.8790374355243233\n",
      "Stochastic Gradient Descent(9622): loss=4.215130532787449\n",
      "Stochastic Gradient Descent(9623): loss=4.8971741637576995\n",
      "Stochastic Gradient Descent(9624): loss=0.7972741152921733\n",
      "Stochastic Gradient Descent(9625): loss=0.4401312066926161\n",
      "Stochastic Gradient Descent(9626): loss=17.427872060081267\n",
      "Stochastic Gradient Descent(9627): loss=0.44466779551236235\n",
      "Stochastic Gradient Descent(9628): loss=0.019164288599774763\n",
      "Stochastic Gradient Descent(9629): loss=0.012342806918627686\n",
      "Stochastic Gradient Descent(9630): loss=20.68776919169361\n",
      "Stochastic Gradient Descent(9631): loss=3.9376527990100922\n",
      "Stochastic Gradient Descent(9632): loss=3.1887930734830716\n",
      "Stochastic Gradient Descent(9633): loss=2.3115123063091545\n",
      "Stochastic Gradient Descent(9634): loss=10.098623018947597\n",
      "Stochastic Gradient Descent(9635): loss=0.11898549980769996\n",
      "Stochastic Gradient Descent(9636): loss=6.2296735544935515\n",
      "Stochastic Gradient Descent(9637): loss=8.554580580333168\n",
      "Stochastic Gradient Descent(9638): loss=1.8293777567732796\n",
      "Stochastic Gradient Descent(9639): loss=0.07263086865513003\n",
      "Stochastic Gradient Descent(9640): loss=9.26597248485145\n",
      "Stochastic Gradient Descent(9641): loss=1.5507771029652224\n",
      "Stochastic Gradient Descent(9642): loss=3.3859788212160984\n",
      "Stochastic Gradient Descent(9643): loss=14.849171758263248\n",
      "Stochastic Gradient Descent(9644): loss=4.795632692071491\n",
      "Stochastic Gradient Descent(9645): loss=0.04425929626581506\n",
      "Stochastic Gradient Descent(9646): loss=18.119159319391127\n",
      "Stochastic Gradient Descent(9647): loss=3.1617559870024787\n",
      "Stochastic Gradient Descent(9648): loss=0.41581456778855697\n",
      "Stochastic Gradient Descent(9649): loss=0.49488594607604247\n",
      "Stochastic Gradient Descent(9650): loss=0.8015419775494778\n",
      "Stochastic Gradient Descent(9651): loss=0.35889025616494524\n",
      "Stochastic Gradient Descent(9652): loss=0.0006266757147600487\n",
      "Stochastic Gradient Descent(9653): loss=26.72419749788245\n",
      "Stochastic Gradient Descent(9654): loss=23.40286716585144\n",
      "Stochastic Gradient Descent(9655): loss=2.2546483277411853\n",
      "Stochastic Gradient Descent(9656): loss=15.346157209631098\n",
      "Stochastic Gradient Descent(9657): loss=4.7300486589253685\n",
      "Stochastic Gradient Descent(9658): loss=8.262242001597944\n",
      "Stochastic Gradient Descent(9659): loss=0.19612862191040292\n",
      "Stochastic Gradient Descent(9660): loss=14.493602758678708\n",
      "Stochastic Gradient Descent(9661): loss=11.592063995472117\n",
      "Stochastic Gradient Descent(9662): loss=0.9643528527109916\n",
      "Stochastic Gradient Descent(9663): loss=4.146025017959689\n",
      "Stochastic Gradient Descent(9664): loss=8.109024207772254\n",
      "Stochastic Gradient Descent(9665): loss=1.3911592897560718\n",
      "Stochastic Gradient Descent(9666): loss=0.13761057260842643\n",
      "Stochastic Gradient Descent(9667): loss=2.249797099848316\n",
      "Stochastic Gradient Descent(9668): loss=1.5544598669003107\n",
      "Stochastic Gradient Descent(9669): loss=0.5872227513533214\n",
      "Stochastic Gradient Descent(9670): loss=0.1587730674063942\n",
      "Stochastic Gradient Descent(9671): loss=13.076393819764375\n",
      "Stochastic Gradient Descent(9672): loss=0.9655149671091732\n",
      "Stochastic Gradient Descent(9673): loss=84.44762773386329\n",
      "Stochastic Gradient Descent(9674): loss=18.97902060460732\n",
      "Stochastic Gradient Descent(9675): loss=9.810223720428327\n",
      "Stochastic Gradient Descent(9676): loss=15.024558245641227\n",
      "Stochastic Gradient Descent(9677): loss=2.6780597456608435\n",
      "Stochastic Gradient Descent(9678): loss=18.67727054829623\n",
      "Stochastic Gradient Descent(9679): loss=5.870628913557054\n",
      "Stochastic Gradient Descent(9680): loss=1.2086977328929813\n",
      "Stochastic Gradient Descent(9681): loss=5.62068157083987\n",
      "Stochastic Gradient Descent(9682): loss=0.00041945143800245786\n",
      "Stochastic Gradient Descent(9683): loss=1.2931400878355108\n",
      "Stochastic Gradient Descent(9684): loss=0.4791048020667318\n",
      "Stochastic Gradient Descent(9685): loss=3.594213518376616\n",
      "Stochastic Gradient Descent(9686): loss=40.79660349237726\n",
      "Stochastic Gradient Descent(9687): loss=3.3408367318834284\n",
      "Stochastic Gradient Descent(9688): loss=0.03497448039720439\n",
      "Stochastic Gradient Descent(9689): loss=0.07045057958298862\n",
      "Stochastic Gradient Descent(9690): loss=1.5157896180854251\n",
      "Stochastic Gradient Descent(9691): loss=3.0680163738713233\n",
      "Stochastic Gradient Descent(9692): loss=0.7499470937638059\n",
      "Stochastic Gradient Descent(9693): loss=0.01135360226713256\n",
      "Stochastic Gradient Descent(9694): loss=0.08050086796130847\n",
      "Stochastic Gradient Descent(9695): loss=3.7698595455512365\n",
      "Stochastic Gradient Descent(9696): loss=0.09307744332730768\n",
      "Stochastic Gradient Descent(9697): loss=12.058520346697994\n",
      "Stochastic Gradient Descent(9698): loss=4.006303270957854\n",
      "Stochastic Gradient Descent(9699): loss=0.006176594659948375\n",
      "Stochastic Gradient Descent(9700): loss=19.051182991929693\n",
      "Stochastic Gradient Descent(9701): loss=1.6274882990826363\n",
      "Stochastic Gradient Descent(9702): loss=8.140461196830737\n",
      "Stochastic Gradient Descent(9703): loss=5.840615207670827\n",
      "Stochastic Gradient Descent(9704): loss=9.828741854108886\n",
      "Stochastic Gradient Descent(9705): loss=7.635361137206489\n",
      "Stochastic Gradient Descent(9706): loss=0.5307812484145105\n",
      "Stochastic Gradient Descent(9707): loss=1.2731586244938007\n",
      "Stochastic Gradient Descent(9708): loss=1.080149304173749\n",
      "Stochastic Gradient Descent(9709): loss=2.69572264217323\n",
      "Stochastic Gradient Descent(9710): loss=0.9373521228612289\n",
      "Stochastic Gradient Descent(9711): loss=0.6721948427226583\n",
      "Stochastic Gradient Descent(9712): loss=0.3819815102176125\n",
      "Stochastic Gradient Descent(9713): loss=1.4282456761567826\n",
      "Stochastic Gradient Descent(9714): loss=0.559765155281966\n",
      "Stochastic Gradient Descent(9715): loss=0.22119742284279928\n",
      "Stochastic Gradient Descent(9716): loss=0.26422299719252795\n",
      "Stochastic Gradient Descent(9717): loss=1.434725201077352\n",
      "Stochastic Gradient Descent(9718): loss=8.377543357468175\n",
      "Stochastic Gradient Descent(9719): loss=0.287419422957995\n",
      "Stochastic Gradient Descent(9720): loss=2.527336301221307\n",
      "Stochastic Gradient Descent(9721): loss=1.93694607761178\n",
      "Stochastic Gradient Descent(9722): loss=0.060017431786071315\n",
      "Stochastic Gradient Descent(9723): loss=0.00038265107146828893\n",
      "Stochastic Gradient Descent(9724): loss=0.11210086374639901\n",
      "Stochastic Gradient Descent(9725): loss=0.12555238017853743\n",
      "Stochastic Gradient Descent(9726): loss=0.001330261221667752\n",
      "Stochastic Gradient Descent(9727): loss=4.165820068913585\n",
      "Stochastic Gradient Descent(9728): loss=3.1762178252217477\n",
      "Stochastic Gradient Descent(9729): loss=0.49862376980811757\n",
      "Stochastic Gradient Descent(9730): loss=26.195234935430314\n",
      "Stochastic Gradient Descent(9731): loss=20.808325302213177\n",
      "Stochastic Gradient Descent(9732): loss=2.0508712238402618\n",
      "Stochastic Gradient Descent(9733): loss=0.0007777441228128742\n",
      "Stochastic Gradient Descent(9734): loss=5.59405806425088\n",
      "Stochastic Gradient Descent(9735): loss=8.659951565804132\n",
      "Stochastic Gradient Descent(9736): loss=0.09303479922891261\n",
      "Stochastic Gradient Descent(9737): loss=0.18709130668258972\n",
      "Stochastic Gradient Descent(9738): loss=4.057276416943329\n",
      "Stochastic Gradient Descent(9739): loss=3.4982809382727598\n",
      "Stochastic Gradient Descent(9740): loss=29.371666835665167\n",
      "Stochastic Gradient Descent(9741): loss=4.003120011610176\n",
      "Stochastic Gradient Descent(9742): loss=9.984490235471732\n",
      "Stochastic Gradient Descent(9743): loss=2.7137003699298\n",
      "Stochastic Gradient Descent(9744): loss=0.6880960235888431\n",
      "Stochastic Gradient Descent(9745): loss=1.2171071420904063\n",
      "Stochastic Gradient Descent(9746): loss=1.2561304580993753\n",
      "Stochastic Gradient Descent(9747): loss=9.220276833353307\n",
      "Stochastic Gradient Descent(9748): loss=17.18076858377193\n",
      "Stochastic Gradient Descent(9749): loss=0.43156439873033414\n",
      "Stochastic Gradient Descent(9750): loss=1.0605404248697408\n",
      "Stochastic Gradient Descent(9751): loss=6.1124791681516015\n",
      "Stochastic Gradient Descent(9752): loss=1.2155217141605505\n",
      "Stochastic Gradient Descent(9753): loss=1.2744742780878726\n",
      "Stochastic Gradient Descent(9754): loss=3.964429945205412\n",
      "Stochastic Gradient Descent(9755): loss=1.531880179437104\n",
      "Stochastic Gradient Descent(9756): loss=9.079540275625865\n",
      "Stochastic Gradient Descent(9757): loss=4.167768145443812\n",
      "Stochastic Gradient Descent(9758): loss=0.2717540396649519\n",
      "Stochastic Gradient Descent(9759): loss=8.751707681129801\n",
      "Stochastic Gradient Descent(9760): loss=0.16431962406025977\n",
      "Stochastic Gradient Descent(9761): loss=0.291077248190236\n",
      "Stochastic Gradient Descent(9762): loss=58.344802799031434\n",
      "Stochastic Gradient Descent(9763): loss=0.988748035924999\n",
      "Stochastic Gradient Descent(9764): loss=10.611366083232395\n",
      "Stochastic Gradient Descent(9765): loss=5.374224400368647\n",
      "Stochastic Gradient Descent(9766): loss=35.22896767782459\n",
      "Stochastic Gradient Descent(9767): loss=0.3422076045886389\n",
      "Stochastic Gradient Descent(9768): loss=6.2299145087063925\n",
      "Stochastic Gradient Descent(9769): loss=3.6609368260079367\n",
      "Stochastic Gradient Descent(9770): loss=0.00013651150111562992\n",
      "Stochastic Gradient Descent(9771): loss=0.2950374090541967\n",
      "Stochastic Gradient Descent(9772): loss=0.017706516296790313\n",
      "Stochastic Gradient Descent(9773): loss=0.2108383296518562\n",
      "Stochastic Gradient Descent(9774): loss=11.6593372309731\n",
      "Stochastic Gradient Descent(9775): loss=0.9038529930697857\n",
      "Stochastic Gradient Descent(9776): loss=2.120024258827629\n",
      "Stochastic Gradient Descent(9777): loss=0.4874787223240158\n",
      "Stochastic Gradient Descent(9778): loss=5.101899826337936\n",
      "Stochastic Gradient Descent(9779): loss=16.52709432964713\n",
      "Stochastic Gradient Descent(9780): loss=0.011987572527571022\n",
      "Stochastic Gradient Descent(9781): loss=15.59457228949045\n",
      "Stochastic Gradient Descent(9782): loss=0.8534188803218228\n",
      "Stochastic Gradient Descent(9783): loss=2.6317960811952616\n",
      "Stochastic Gradient Descent(9784): loss=0.9065325691552978\n",
      "Stochastic Gradient Descent(9785): loss=0.6219209917616174\n",
      "Stochastic Gradient Descent(9786): loss=0.29059769603723984\n",
      "Stochastic Gradient Descent(9787): loss=5.237908902134341\n",
      "Stochastic Gradient Descent(9788): loss=1.413261454594431\n",
      "Stochastic Gradient Descent(9789): loss=2.5157614249740106\n",
      "Stochastic Gradient Descent(9790): loss=7.8374480935048965\n",
      "Stochastic Gradient Descent(9791): loss=0.8281956370358146\n",
      "Stochastic Gradient Descent(9792): loss=1.2173308176798399\n",
      "Stochastic Gradient Descent(9793): loss=7.59374686429927\n",
      "Stochastic Gradient Descent(9794): loss=0.1319069394015933\n",
      "Stochastic Gradient Descent(9795): loss=0.7657406076765995\n",
      "Stochastic Gradient Descent(9796): loss=1.7301567256713912\n",
      "Stochastic Gradient Descent(9797): loss=5.73242070074429\n",
      "Stochastic Gradient Descent(9798): loss=2.6716166479290497\n",
      "Stochastic Gradient Descent(9799): loss=19.42630564772188\n",
      "Stochastic Gradient Descent(9800): loss=0.01934629936793419\n",
      "Stochastic Gradient Descent(9801): loss=12.974159795763162\n",
      "Stochastic Gradient Descent(9802): loss=7.126956658285694\n",
      "Stochastic Gradient Descent(9803): loss=1.920510036496785\n",
      "Stochastic Gradient Descent(9804): loss=2.7557127527032548\n",
      "Stochastic Gradient Descent(9805): loss=0.0051033891334776186\n",
      "Stochastic Gradient Descent(9806): loss=0.22836040948079792\n",
      "Stochastic Gradient Descent(9807): loss=4.7621496077884595\n",
      "Stochastic Gradient Descent(9808): loss=0.2492033801868191\n",
      "Stochastic Gradient Descent(9809): loss=4.259451839712966\n",
      "Stochastic Gradient Descent(9810): loss=0.6708062012751944\n",
      "Stochastic Gradient Descent(9811): loss=3.985453037657468\n",
      "Stochastic Gradient Descent(9812): loss=0.2821244588120613\n",
      "Stochastic Gradient Descent(9813): loss=3.3393515561884852\n",
      "Stochastic Gradient Descent(9814): loss=2.023639662547768\n",
      "Stochastic Gradient Descent(9815): loss=3.0512936128206523\n",
      "Stochastic Gradient Descent(9816): loss=5.01743388434738\n",
      "Stochastic Gradient Descent(9817): loss=3.811774942288273\n",
      "Stochastic Gradient Descent(9818): loss=28.32140401975045\n",
      "Stochastic Gradient Descent(9819): loss=0.5193944018911811\n",
      "Stochastic Gradient Descent(9820): loss=4.012183417934686\n",
      "Stochastic Gradient Descent(9821): loss=6.341040768104387\n",
      "Stochastic Gradient Descent(9822): loss=0.19109981236506826\n",
      "Stochastic Gradient Descent(9823): loss=7.700819765084738\n",
      "Stochastic Gradient Descent(9824): loss=5.855885234477772\n",
      "Stochastic Gradient Descent(9825): loss=7.8331919259091904\n",
      "Stochastic Gradient Descent(9826): loss=9.46991658826507\n",
      "Stochastic Gradient Descent(9827): loss=4.0562768162461245\n",
      "Stochastic Gradient Descent(9828): loss=0.09401685376397376\n",
      "Stochastic Gradient Descent(9829): loss=0.6630830586748176\n",
      "Stochastic Gradient Descent(9830): loss=0.7261199977543449\n",
      "Stochastic Gradient Descent(9831): loss=5.4531499715849385\n",
      "Stochastic Gradient Descent(9832): loss=3.3619818349496566\n",
      "Stochastic Gradient Descent(9833): loss=0.6763958099323301\n",
      "Stochastic Gradient Descent(9834): loss=0.32285697922679324\n",
      "Stochastic Gradient Descent(9835): loss=7.287171719478137\n",
      "Stochastic Gradient Descent(9836): loss=0.5763851050037977\n",
      "Stochastic Gradient Descent(9837): loss=1.7252945979880479\n",
      "Stochastic Gradient Descent(9838): loss=7.485921362363272\n",
      "Stochastic Gradient Descent(9839): loss=32.5335989391508\n",
      "Stochastic Gradient Descent(9840): loss=0.005312708292071901\n",
      "Stochastic Gradient Descent(9841): loss=0.8243459751294697\n",
      "Stochastic Gradient Descent(9842): loss=2.931112525109991\n",
      "Stochastic Gradient Descent(9843): loss=0.0909296445207699\n",
      "Stochastic Gradient Descent(9844): loss=1.0634865342144801\n",
      "Stochastic Gradient Descent(9845): loss=0.18111517568396066\n",
      "Stochastic Gradient Descent(9846): loss=3.105828910836284\n",
      "Stochastic Gradient Descent(9847): loss=0.5054213173744873\n",
      "Stochastic Gradient Descent(9848): loss=0.7267952174872763\n",
      "Stochastic Gradient Descent(9849): loss=4.219193114329006\n",
      "Stochastic Gradient Descent(9850): loss=1.0115357526456374\n",
      "Stochastic Gradient Descent(9851): loss=0.9308599695410784\n",
      "Stochastic Gradient Descent(9852): loss=1.6921491153815105\n",
      "Stochastic Gradient Descent(9853): loss=2.802086710245494\n",
      "Stochastic Gradient Descent(9854): loss=0.0007927472173098763\n",
      "Stochastic Gradient Descent(9855): loss=10.6938024162016\n",
      "Stochastic Gradient Descent(9856): loss=4.851299228213909\n",
      "Stochastic Gradient Descent(9857): loss=6.661830375233418\n",
      "Stochastic Gradient Descent(9858): loss=1.655151779969098\n",
      "Stochastic Gradient Descent(9859): loss=1.2751645977964696\n",
      "Stochastic Gradient Descent(9860): loss=2.39774154173268\n",
      "Stochastic Gradient Descent(9861): loss=3.434207714150879\n",
      "Stochastic Gradient Descent(9862): loss=1.1567892306971337\n",
      "Stochastic Gradient Descent(9863): loss=1.7977543345346485\n",
      "Stochastic Gradient Descent(9864): loss=33.94746405540356\n",
      "Stochastic Gradient Descent(9865): loss=4.558662518665235\n",
      "Stochastic Gradient Descent(9866): loss=0.1168191072224195\n",
      "Stochastic Gradient Descent(9867): loss=0.1374372697323083\n",
      "Stochastic Gradient Descent(9868): loss=1.2810262949036615\n",
      "Stochastic Gradient Descent(9869): loss=5.4568474171277375\n",
      "Stochastic Gradient Descent(9870): loss=11.506089884667771\n",
      "Stochastic Gradient Descent(9871): loss=0.3877890925647297\n",
      "Stochastic Gradient Descent(9872): loss=0.6073764089804533\n",
      "Stochastic Gradient Descent(9873): loss=1.80557998945303\n",
      "Stochastic Gradient Descent(9874): loss=6.079402081441206\n",
      "Stochastic Gradient Descent(9875): loss=2.926315748953362\n",
      "Stochastic Gradient Descent(9876): loss=0.10409021551301838\n",
      "Stochastic Gradient Descent(9877): loss=0.6309092611800104\n",
      "Stochastic Gradient Descent(9878): loss=2.0846285329446292\n",
      "Stochastic Gradient Descent(9879): loss=14.753008157666859\n",
      "Stochastic Gradient Descent(9880): loss=0.45405452828632176\n",
      "Stochastic Gradient Descent(9881): loss=9.068149065206965\n",
      "Stochastic Gradient Descent(9882): loss=2.7811190827710783\n",
      "Stochastic Gradient Descent(9883): loss=5.993914004366237\n",
      "Stochastic Gradient Descent(9884): loss=5.514664098083296\n",
      "Stochastic Gradient Descent(9885): loss=5.680769551922294\n",
      "Stochastic Gradient Descent(9886): loss=0.9416860164715126\n",
      "Stochastic Gradient Descent(9887): loss=0.0785593796220371\n",
      "Stochastic Gradient Descent(9888): loss=1.381911033935601\n",
      "Stochastic Gradient Descent(9889): loss=10.271125516782956\n",
      "Stochastic Gradient Descent(9890): loss=0.00043060174349712977\n",
      "Stochastic Gradient Descent(9891): loss=0.9823472349479111\n",
      "Stochastic Gradient Descent(9892): loss=6.55800516928201\n",
      "Stochastic Gradient Descent(9893): loss=5.1351489609638135\n",
      "Stochastic Gradient Descent(9894): loss=5.912025441730389\n",
      "Stochastic Gradient Descent(9895): loss=3.9618308538481486\n",
      "Stochastic Gradient Descent(9896): loss=0.47923111605002766\n",
      "Stochastic Gradient Descent(9897): loss=3.442804867119405\n",
      "Stochastic Gradient Descent(9898): loss=2.9872698714467725\n",
      "Stochastic Gradient Descent(9899): loss=0.9703743676494341\n",
      "Stochastic Gradient Descent(9900): loss=8.116713638952067\n",
      "Stochastic Gradient Descent(9901): loss=0.26009195944115837\n",
      "Stochastic Gradient Descent(9902): loss=7.433080551810188\n",
      "Stochastic Gradient Descent(9903): loss=21.024740698256597\n",
      "Stochastic Gradient Descent(9904): loss=0.7466835209001129\n",
      "Stochastic Gradient Descent(9905): loss=1.3866754778913923\n",
      "Stochastic Gradient Descent(9906): loss=0.2186730593264924\n",
      "Stochastic Gradient Descent(9907): loss=0.8702713065415655\n",
      "Stochastic Gradient Descent(9908): loss=0.24314954812655795\n",
      "Stochastic Gradient Descent(9909): loss=2.0466191177307125\n",
      "Stochastic Gradient Descent(9910): loss=17.26128576287276\n",
      "Stochastic Gradient Descent(9911): loss=6.8207558793976215\n",
      "Stochastic Gradient Descent(9912): loss=10.718545738475706\n",
      "Stochastic Gradient Descent(9913): loss=3.3263346881367086\n",
      "Stochastic Gradient Descent(9914): loss=1.2683612192464602\n",
      "Stochastic Gradient Descent(9915): loss=12.444322234992809\n",
      "Stochastic Gradient Descent(9916): loss=1.3936131379469154\n",
      "Stochastic Gradient Descent(9917): loss=9.187777355775047\n",
      "Stochastic Gradient Descent(9918): loss=3.3714940829258047\n",
      "Stochastic Gradient Descent(9919): loss=1.6779790733753508\n",
      "Stochastic Gradient Descent(9920): loss=4.209747333720902\n",
      "Stochastic Gradient Descent(9921): loss=2.1965182546738578\n",
      "Stochastic Gradient Descent(9922): loss=31.36460490138272\n",
      "Stochastic Gradient Descent(9923): loss=18.703918206826845\n",
      "Stochastic Gradient Descent(9924): loss=2.534384968626905\n",
      "Stochastic Gradient Descent(9925): loss=3.14476960139715\n",
      "Stochastic Gradient Descent(9926): loss=7.583114941140285\n",
      "Stochastic Gradient Descent(9927): loss=2.311979103949042\n",
      "Stochastic Gradient Descent(9928): loss=1.9428643600075213\n",
      "Stochastic Gradient Descent(9929): loss=11.665880930586034\n",
      "Stochastic Gradient Descent(9930): loss=1.5560586858059045\n",
      "Stochastic Gradient Descent(9931): loss=0.5789320123418319\n",
      "Stochastic Gradient Descent(9932): loss=18.216790463539635\n",
      "Stochastic Gradient Descent(9933): loss=4.719303719779617\n",
      "Stochastic Gradient Descent(9934): loss=2.3174845890629956\n",
      "Stochastic Gradient Descent(9935): loss=9.638352219419035\n",
      "Stochastic Gradient Descent(9936): loss=3.2370426730769983\n",
      "Stochastic Gradient Descent(9937): loss=2.874824697568125\n",
      "Stochastic Gradient Descent(9938): loss=2.9329368750226936\n",
      "Stochastic Gradient Descent(9939): loss=2.5063497592105977\n",
      "Stochastic Gradient Descent(9940): loss=0.09376195615926637\n",
      "Stochastic Gradient Descent(9941): loss=29.06451624809196\n",
      "Stochastic Gradient Descent(9942): loss=12.434295710611849\n",
      "Stochastic Gradient Descent(9943): loss=2.8685478301424174\n",
      "Stochastic Gradient Descent(9944): loss=9.697926966140331\n",
      "Stochastic Gradient Descent(9945): loss=32.56594417949974\n",
      "Stochastic Gradient Descent(9946): loss=2.057936573249157\n",
      "Stochastic Gradient Descent(9947): loss=0.07658606946383775\n",
      "Stochastic Gradient Descent(9948): loss=2.17128725666353\n",
      "Stochastic Gradient Descent(9949): loss=2.1685382394326838\n",
      "Stochastic Gradient Descent(9950): loss=7.999467038226008\n",
      "Stochastic Gradient Descent(9951): loss=0.4111708021347159\n",
      "Stochastic Gradient Descent(9952): loss=0.339992980089048\n",
      "Stochastic Gradient Descent(9953): loss=24.320903521504345\n",
      "Stochastic Gradient Descent(9954): loss=5.998498643902617\n",
      "Stochastic Gradient Descent(9955): loss=1.4365732694007274\n",
      "Stochastic Gradient Descent(9956): loss=0.03638158097650488\n",
      "Stochastic Gradient Descent(9957): loss=9.217463499388131\n",
      "Stochastic Gradient Descent(9958): loss=23.27400600709893\n",
      "Stochastic Gradient Descent(9959): loss=0.16345897233499634\n",
      "Stochastic Gradient Descent(9960): loss=12.964211913247489\n",
      "Stochastic Gradient Descent(9961): loss=2.5252275049648705\n",
      "Stochastic Gradient Descent(9962): loss=0.7097061139441395\n",
      "Stochastic Gradient Descent(9963): loss=5.142776376670198\n",
      "Stochastic Gradient Descent(9964): loss=0.44293032623642414\n",
      "Stochastic Gradient Descent(9965): loss=1.7112488992702728\n",
      "Stochastic Gradient Descent(9966): loss=1.2043239890636643\n",
      "Stochastic Gradient Descent(9967): loss=29.831172471485047\n",
      "Stochastic Gradient Descent(9968): loss=21.42573735352295\n",
      "Stochastic Gradient Descent(9969): loss=1.711587274453102\n",
      "Stochastic Gradient Descent(9970): loss=1.802370402402365\n",
      "Stochastic Gradient Descent(9971): loss=21.60324103929404\n",
      "Stochastic Gradient Descent(9972): loss=6.010645806929053\n",
      "Stochastic Gradient Descent(9973): loss=19.893512620900797\n",
      "Stochastic Gradient Descent(9974): loss=7.425151840744081\n",
      "Stochastic Gradient Descent(9975): loss=1.3502731157229737\n",
      "Stochastic Gradient Descent(9976): loss=11.163858686330144\n",
      "Stochastic Gradient Descent(9977): loss=1.933966115632015\n",
      "Stochastic Gradient Descent(9978): loss=7.946866428149524\n",
      "Stochastic Gradient Descent(9979): loss=0.9425575026024674\n",
      "Stochastic Gradient Descent(9980): loss=19.711637944609535\n",
      "Stochastic Gradient Descent(9981): loss=1.6076624381789648\n",
      "Stochastic Gradient Descent(9982): loss=0.5775700726655227\n",
      "Stochastic Gradient Descent(9983): loss=1.2429525269998973\n",
      "Stochastic Gradient Descent(9984): loss=14.132652510421877\n",
      "Stochastic Gradient Descent(9985): loss=0.5063482317706572\n",
      "Stochastic Gradient Descent(9986): loss=5.465690324693978\n",
      "Stochastic Gradient Descent(9987): loss=6.7960362114174275\n",
      "Stochastic Gradient Descent(9988): loss=4.471942452109714\n",
      "Stochastic Gradient Descent(9989): loss=0.30413650270198134\n",
      "Stochastic Gradient Descent(9990): loss=0.01780583151351601\n",
      "Stochastic Gradient Descent(9991): loss=1.7236994363369775\n",
      "Stochastic Gradient Descent(9992): loss=4.163053342706342\n",
      "Stochastic Gradient Descent(9993): loss=2.21181051137452\n",
      "Stochastic Gradient Descent(9994): loss=0.3677914737430639\n",
      "Stochastic Gradient Descent(9995): loss=8.292829340957798\n",
      "Stochastic Gradient Descent(9996): loss=2.9100046377002964\n",
      "Stochastic Gradient Descent(9997): loss=9.781669024662207\n",
      "Stochastic Gradient Descent(9998): loss=0.37329750683257984\n",
      "Stochastic Gradient Descent(9999): loss=6.241777896471192\n",
      "Stochastic Gradient Descent(10000): loss=1.9278380676762759\n",
      "Stochastic Gradient Descent(10001): loss=3.5217338016309925\n",
      "Stochastic Gradient Descent(10002): loss=0.21100435517520383\n",
      "Stochastic Gradient Descent(10003): loss=24.632332941341495\n",
      "Stochastic Gradient Descent(10004): loss=3.525309508733948\n",
      "Stochastic Gradient Descent(10005): loss=7.200058419368854\n",
      "Stochastic Gradient Descent(10006): loss=16.511868070372287\n",
      "Stochastic Gradient Descent(10007): loss=5.3898308317946215\n",
      "Stochastic Gradient Descent(10008): loss=0.01684381442943765\n",
      "Stochastic Gradient Descent(10009): loss=3.9158508327797357\n",
      "Stochastic Gradient Descent(10010): loss=0.011736861327320266\n",
      "Stochastic Gradient Descent(10011): loss=21.79341187928896\n",
      "Stochastic Gradient Descent(10012): loss=14.935143475972735\n",
      "Stochastic Gradient Descent(10013): loss=1.9980057241013587\n",
      "Stochastic Gradient Descent(10014): loss=0.3074297723239401\n",
      "Stochastic Gradient Descent(10015): loss=1.4542069523120642\n",
      "Stochastic Gradient Descent(10016): loss=24.92276426031357\n",
      "Stochastic Gradient Descent(10017): loss=24.21206937799614\n",
      "Stochastic Gradient Descent(10018): loss=0.19983371822128132\n",
      "Stochastic Gradient Descent(10019): loss=4.446552522590328\n",
      "Stochastic Gradient Descent(10020): loss=12.731552521653162\n",
      "Stochastic Gradient Descent(10021): loss=3.9633127682573233\n",
      "Stochastic Gradient Descent(10022): loss=0.4853261239481493\n",
      "Stochastic Gradient Descent(10023): loss=0.2360941811498884\n",
      "Stochastic Gradient Descent(10024): loss=8.042802027023486\n",
      "Stochastic Gradient Descent(10025): loss=0.3495208650746393\n",
      "Stochastic Gradient Descent(10026): loss=3.1752922513940116\n",
      "Stochastic Gradient Descent(10027): loss=1.5690515604718493\n",
      "Stochastic Gradient Descent(10028): loss=20.804939048883693\n",
      "Stochastic Gradient Descent(10029): loss=0.3951836260206076\n",
      "Stochastic Gradient Descent(10030): loss=1.4297940470088235\n",
      "Stochastic Gradient Descent(10031): loss=10.90896750146205\n",
      "Stochastic Gradient Descent(10032): loss=0.11713269059139858\n",
      "Stochastic Gradient Descent(10033): loss=0.2718679927274903\n",
      "Stochastic Gradient Descent(10034): loss=0.11294475127166355\n",
      "Stochastic Gradient Descent(10035): loss=7.740299996295401\n",
      "Stochastic Gradient Descent(10036): loss=0.6361603430469664\n",
      "Stochastic Gradient Descent(10037): loss=1.8184004302922019\n",
      "Stochastic Gradient Descent(10038): loss=5.6140281278297195\n",
      "Stochastic Gradient Descent(10039): loss=5.866176865118174\n",
      "Stochastic Gradient Descent(10040): loss=0.2858984220912177\n",
      "Stochastic Gradient Descent(10041): loss=0.8572723413322055\n",
      "Stochastic Gradient Descent(10042): loss=0.1510090273116192\n",
      "Stochastic Gradient Descent(10043): loss=6.876387468760209\n",
      "Stochastic Gradient Descent(10044): loss=0.5415935822511856\n",
      "Stochastic Gradient Descent(10045): loss=9.11495362140154\n",
      "Stochastic Gradient Descent(10046): loss=20.64466803628663\n",
      "Stochastic Gradient Descent(10047): loss=0.013888864911500405\n",
      "Stochastic Gradient Descent(10048): loss=1.023968117126844\n",
      "Stochastic Gradient Descent(10049): loss=0.02126879789306746\n",
      "Stochastic Gradient Descent(10050): loss=0.08712917967279402\n",
      "Stochastic Gradient Descent(10051): loss=5.029097898958063\n",
      "Stochastic Gradient Descent(10052): loss=7.284295456766203\n",
      "Stochastic Gradient Descent(10053): loss=3.112986250805885\n",
      "Stochastic Gradient Descent(10054): loss=12.205815261898957\n",
      "Stochastic Gradient Descent(10055): loss=0.03570992020062456\n",
      "Stochastic Gradient Descent(10056): loss=1.351306713527474\n",
      "Stochastic Gradient Descent(10057): loss=0.06857859147933149\n",
      "Stochastic Gradient Descent(10058): loss=0.07644095155165809\n",
      "Stochastic Gradient Descent(10059): loss=1.5568095851162094\n",
      "Stochastic Gradient Descent(10060): loss=0.22752927280549132\n",
      "Stochastic Gradient Descent(10061): loss=0.2377705077613099\n",
      "Stochastic Gradient Descent(10062): loss=5.031538365553417\n",
      "Stochastic Gradient Descent(10063): loss=1.0712794218558803\n",
      "Stochastic Gradient Descent(10064): loss=0.056815304520679864\n",
      "Stochastic Gradient Descent(10065): loss=31.36666311000125\n",
      "Stochastic Gradient Descent(10066): loss=3.5203243218977764\n",
      "Stochastic Gradient Descent(10067): loss=3.4851700586361454\n",
      "Stochastic Gradient Descent(10068): loss=1.4478904573266276\n",
      "Stochastic Gradient Descent(10069): loss=0.45592265219612516\n",
      "Stochastic Gradient Descent(10070): loss=3.2159872017573385\n",
      "Stochastic Gradient Descent(10071): loss=0.008420945560483785\n",
      "Stochastic Gradient Descent(10072): loss=3.048609456793618\n",
      "Stochastic Gradient Descent(10073): loss=8.653705541264571\n",
      "Stochastic Gradient Descent(10074): loss=1.048812400353971\n",
      "Stochastic Gradient Descent(10075): loss=3.99340662405506\n",
      "Stochastic Gradient Descent(10076): loss=63.795045333881\n",
      "Stochastic Gradient Descent(10077): loss=12.652386378338006\n",
      "Stochastic Gradient Descent(10078): loss=1.6603044493286943\n",
      "Stochastic Gradient Descent(10079): loss=9.522067138381871\n",
      "Stochastic Gradient Descent(10080): loss=0.03335707999000479\n",
      "Stochastic Gradient Descent(10081): loss=9.062922458495322\n",
      "Stochastic Gradient Descent(10082): loss=9.08781358383637\n",
      "Stochastic Gradient Descent(10083): loss=0.34216764767635527\n",
      "Stochastic Gradient Descent(10084): loss=0.2057010922568544\n",
      "Stochastic Gradient Descent(10085): loss=5.3176905623334525\n",
      "Stochastic Gradient Descent(10086): loss=62.82989938238672\n",
      "Stochastic Gradient Descent(10087): loss=0.8525346536623506\n",
      "Stochastic Gradient Descent(10088): loss=0.06453998289344398\n",
      "Stochastic Gradient Descent(10089): loss=0.3114286855982876\n",
      "Stochastic Gradient Descent(10090): loss=7.554698580825326\n",
      "Stochastic Gradient Descent(10091): loss=0.17930911485966755\n",
      "Stochastic Gradient Descent(10092): loss=0.656943790123709\n",
      "Stochastic Gradient Descent(10093): loss=1.9095058556332416\n",
      "Stochastic Gradient Descent(10094): loss=35.51575398029925\n",
      "Stochastic Gradient Descent(10095): loss=1.3528497865059137\n",
      "Stochastic Gradient Descent(10096): loss=2.5520153803286374\n",
      "Stochastic Gradient Descent(10097): loss=4.231841674744279\n",
      "Stochastic Gradient Descent(10098): loss=7.661061207652842\n",
      "Stochastic Gradient Descent(10099): loss=8.777187034882312\n",
      "Stochastic Gradient Descent(10100): loss=8.712002916898454\n",
      "Stochastic Gradient Descent(10101): loss=0.22938452283283176\n",
      "Stochastic Gradient Descent(10102): loss=3.4383598489030103\n",
      "Stochastic Gradient Descent(10103): loss=2.815598454586955\n",
      "Stochastic Gradient Descent(10104): loss=0.6104487904931563\n",
      "Stochastic Gradient Descent(10105): loss=4.945234235958639\n",
      "Stochastic Gradient Descent(10106): loss=0.7165187830848001\n",
      "Stochastic Gradient Descent(10107): loss=7.436975437591449\n",
      "Stochastic Gradient Descent(10108): loss=5.7024864330225\n",
      "Stochastic Gradient Descent(10109): loss=7.400143151154154\n",
      "Stochastic Gradient Descent(10110): loss=21.897402412570052\n",
      "Stochastic Gradient Descent(10111): loss=80.78089994477438\n",
      "Stochastic Gradient Descent(10112): loss=3.1170945107271293\n",
      "Stochastic Gradient Descent(10113): loss=1.733861410031236\n",
      "Stochastic Gradient Descent(10114): loss=2.2221163699379822\n",
      "Stochastic Gradient Descent(10115): loss=5.180558669035783\n",
      "Stochastic Gradient Descent(10116): loss=16.4777610804578\n",
      "Stochastic Gradient Descent(10117): loss=0.1971057149641546\n",
      "Stochastic Gradient Descent(10118): loss=1.9212672231632373\n",
      "Stochastic Gradient Descent(10119): loss=0.8412110129212309\n",
      "Stochastic Gradient Descent(10120): loss=0.19801225643460793\n",
      "Stochastic Gradient Descent(10121): loss=14.907733972066595\n",
      "Stochastic Gradient Descent(10122): loss=3.6104480006669792\n",
      "Stochastic Gradient Descent(10123): loss=0.0001299271099561837\n",
      "Stochastic Gradient Descent(10124): loss=13.556932139296746\n",
      "Stochastic Gradient Descent(10125): loss=0.7295085513396746\n",
      "Stochastic Gradient Descent(10126): loss=6.105727342726028\n",
      "Stochastic Gradient Descent(10127): loss=3.826250214432949\n",
      "Stochastic Gradient Descent(10128): loss=10.76049386612368\n",
      "Stochastic Gradient Descent(10129): loss=0.4326065491211158\n",
      "Stochastic Gradient Descent(10130): loss=0.08719735917822014\n",
      "Stochastic Gradient Descent(10131): loss=0.06396772729558882\n",
      "Stochastic Gradient Descent(10132): loss=0.41570415902017493\n",
      "Stochastic Gradient Descent(10133): loss=8.804957056674859\n",
      "Stochastic Gradient Descent(10134): loss=0.00038932885781035534\n",
      "Stochastic Gradient Descent(10135): loss=0.22909674923142992\n",
      "Stochastic Gradient Descent(10136): loss=3.994156774052931\n",
      "Stochastic Gradient Descent(10137): loss=35.12388850985044\n",
      "Stochastic Gradient Descent(10138): loss=7.916878419097541\n",
      "Stochastic Gradient Descent(10139): loss=0.4148798256813144\n",
      "Stochastic Gradient Descent(10140): loss=1.1821932167674947\n",
      "Stochastic Gradient Descent(10141): loss=14.461443693403126\n",
      "Stochastic Gradient Descent(10142): loss=2.6891121045921174\n",
      "Stochastic Gradient Descent(10143): loss=0.04989490013420809\n",
      "Stochastic Gradient Descent(10144): loss=7.404433619229392\n",
      "Stochastic Gradient Descent(10145): loss=6.154142200661928\n",
      "Stochastic Gradient Descent(10146): loss=0.8893222324516883\n",
      "Stochastic Gradient Descent(10147): loss=0.14339691786821937\n",
      "Stochastic Gradient Descent(10148): loss=0.7005128790088646\n",
      "Stochastic Gradient Descent(10149): loss=0.11070631983707623\n",
      "Stochastic Gradient Descent(10150): loss=0.02092044526950144\n",
      "Stochastic Gradient Descent(10151): loss=4.8183685219907195\n",
      "Stochastic Gradient Descent(10152): loss=2.5768183188173954\n",
      "Stochastic Gradient Descent(10153): loss=0.6973434306756577\n",
      "Stochastic Gradient Descent(10154): loss=2.040330789270643\n",
      "Stochastic Gradient Descent(10155): loss=0.14841022145580082\n",
      "Stochastic Gradient Descent(10156): loss=0.6549291038958576\n",
      "Stochastic Gradient Descent(10157): loss=2.493312922201889\n",
      "Stochastic Gradient Descent(10158): loss=17.66481557504567\n",
      "Stochastic Gradient Descent(10159): loss=21.301700979307746\n",
      "Stochastic Gradient Descent(10160): loss=1.8368314377226016\n",
      "Stochastic Gradient Descent(10161): loss=35.03135816427564\n",
      "Stochastic Gradient Descent(10162): loss=2.4887757342892374\n",
      "Stochastic Gradient Descent(10163): loss=4.182216097449859\n",
      "Stochastic Gradient Descent(10164): loss=9.508490557212202\n",
      "Stochastic Gradient Descent(10165): loss=2.70846215551982\n",
      "Stochastic Gradient Descent(10166): loss=2.278759855174599\n",
      "Stochastic Gradient Descent(10167): loss=9.07899067661335\n",
      "Stochastic Gradient Descent(10168): loss=5.144554092346413\n",
      "Stochastic Gradient Descent(10169): loss=0.9127155698141606\n",
      "Stochastic Gradient Descent(10170): loss=6.093269353594633\n",
      "Stochastic Gradient Descent(10171): loss=8.089010193793019\n",
      "Stochastic Gradient Descent(10172): loss=1.464512837611445\n",
      "Stochastic Gradient Descent(10173): loss=1.6024753097991002\n",
      "Stochastic Gradient Descent(10174): loss=0.12660982145878463\n",
      "Stochastic Gradient Descent(10175): loss=4.656033696731661\n",
      "Stochastic Gradient Descent(10176): loss=0.29934159375486913\n",
      "Stochastic Gradient Descent(10177): loss=1.785959201382425\n",
      "Stochastic Gradient Descent(10178): loss=14.572009459661198\n",
      "Stochastic Gradient Descent(10179): loss=2.5170844666922667\n",
      "Stochastic Gradient Descent(10180): loss=11.316049345308254\n",
      "Stochastic Gradient Descent(10181): loss=3.024709396139003\n",
      "Stochastic Gradient Descent(10182): loss=1.0572325236638718\n",
      "Stochastic Gradient Descent(10183): loss=0.8523414779728454\n",
      "Stochastic Gradient Descent(10184): loss=1.851406531949368\n",
      "Stochastic Gradient Descent(10185): loss=0.5480267145447061\n",
      "Stochastic Gradient Descent(10186): loss=5.169910650878448\n",
      "Stochastic Gradient Descent(10187): loss=0.5157590859090366\n",
      "Stochastic Gradient Descent(10188): loss=10.672190412581287\n",
      "Stochastic Gradient Descent(10189): loss=3.7606554726098484\n",
      "Stochastic Gradient Descent(10190): loss=0.024509960427324177\n",
      "Stochastic Gradient Descent(10191): loss=2.051912352173122\n",
      "Stochastic Gradient Descent(10192): loss=2.2239143359695444\n",
      "Stochastic Gradient Descent(10193): loss=9.52265991754995\n",
      "Stochastic Gradient Descent(10194): loss=12.800018210323394\n",
      "Stochastic Gradient Descent(10195): loss=7.769046214035749\n",
      "Stochastic Gradient Descent(10196): loss=7.320666757758136\n",
      "Stochastic Gradient Descent(10197): loss=0.00401001821713156\n",
      "Stochastic Gradient Descent(10198): loss=16.592816657871182\n",
      "Stochastic Gradient Descent(10199): loss=5.353728431275599\n",
      "Stochastic Gradient Descent(10200): loss=0.005179320683279943\n",
      "Stochastic Gradient Descent(10201): loss=0.0001054567655236594\n",
      "Stochastic Gradient Descent(10202): loss=6.786400516137426\n",
      "Stochastic Gradient Descent(10203): loss=4.055543795101834\n",
      "Stochastic Gradient Descent(10204): loss=0.3189974778991101\n",
      "Stochastic Gradient Descent(10205): loss=3.524964944959928\n",
      "Stochastic Gradient Descent(10206): loss=10.356520110084746\n",
      "Stochastic Gradient Descent(10207): loss=19.843185909613826\n",
      "Stochastic Gradient Descent(10208): loss=24.378971566381058\n",
      "Stochastic Gradient Descent(10209): loss=4.007715379155228\n",
      "Stochastic Gradient Descent(10210): loss=0.5572722900472843\n",
      "Stochastic Gradient Descent(10211): loss=3.790102123895866\n",
      "Stochastic Gradient Descent(10212): loss=0.4228764246382884\n",
      "Stochastic Gradient Descent(10213): loss=1.137088836179892\n",
      "Stochastic Gradient Descent(10214): loss=0.0018638877032744012\n",
      "Stochastic Gradient Descent(10215): loss=8.431824855655591\n",
      "Stochastic Gradient Descent(10216): loss=4.063632370122895\n",
      "Stochastic Gradient Descent(10217): loss=0.8474511565401497\n",
      "Stochastic Gradient Descent(10218): loss=2.1576532938550708\n",
      "Stochastic Gradient Descent(10219): loss=4.006943761874056\n",
      "Stochastic Gradient Descent(10220): loss=0.0286020302924029\n",
      "Stochastic Gradient Descent(10221): loss=4.199826790649053\n",
      "Stochastic Gradient Descent(10222): loss=3.933953101588176\n",
      "Stochastic Gradient Descent(10223): loss=12.243725918665133\n",
      "Stochastic Gradient Descent(10224): loss=4.578171030111564\n",
      "Stochastic Gradient Descent(10225): loss=0.4125919347515212\n",
      "Stochastic Gradient Descent(10226): loss=0.16583165118711365\n",
      "Stochastic Gradient Descent(10227): loss=0.037024791140329645\n",
      "Stochastic Gradient Descent(10228): loss=6.677664895928554\n",
      "Stochastic Gradient Descent(10229): loss=6.598206220714319\n",
      "Stochastic Gradient Descent(10230): loss=11.783984894740808\n",
      "Stochastic Gradient Descent(10231): loss=14.952244724154854\n",
      "Stochastic Gradient Descent(10232): loss=31.319211952413845\n",
      "Stochastic Gradient Descent(10233): loss=9.628476895809891\n",
      "Stochastic Gradient Descent(10234): loss=1.0138117324883584\n",
      "Stochastic Gradient Descent(10235): loss=1.066702084084742\n",
      "Stochastic Gradient Descent(10236): loss=2.674110036674356\n",
      "Stochastic Gradient Descent(10237): loss=1.139170434575633\n",
      "Stochastic Gradient Descent(10238): loss=0.2100139994434602\n",
      "Stochastic Gradient Descent(10239): loss=0.002584101020836058\n",
      "Stochastic Gradient Descent(10240): loss=4.533371498668397\n",
      "Stochastic Gradient Descent(10241): loss=19.96050596552055\n",
      "Stochastic Gradient Descent(10242): loss=2.749934309212549\n",
      "Stochastic Gradient Descent(10243): loss=1.6434523670475645\n",
      "Stochastic Gradient Descent(10244): loss=5.930619675266658\n",
      "Stochastic Gradient Descent(10245): loss=1.5580736441935152\n",
      "Stochastic Gradient Descent(10246): loss=0.49311323958340336\n",
      "Stochastic Gradient Descent(10247): loss=0.3265628803131091\n",
      "Stochastic Gradient Descent(10248): loss=0.0694162711429992\n",
      "Stochastic Gradient Descent(10249): loss=0.45549223036164965\n",
      "Stochastic Gradient Descent(10250): loss=14.560036504675372\n",
      "Stochastic Gradient Descent(10251): loss=6.744907037284049\n",
      "Stochastic Gradient Descent(10252): loss=14.277456305045542\n",
      "Stochastic Gradient Descent(10253): loss=6.905171890095038\n",
      "Stochastic Gradient Descent(10254): loss=0.2082172701959044\n",
      "Stochastic Gradient Descent(10255): loss=0.004234954038506464\n",
      "Stochastic Gradient Descent(10256): loss=0.25683724525535995\n",
      "Stochastic Gradient Descent(10257): loss=0.6828449053323823\n",
      "Stochastic Gradient Descent(10258): loss=1.4077118798124157\n",
      "Stochastic Gradient Descent(10259): loss=0.16767984384123394\n",
      "Stochastic Gradient Descent(10260): loss=0.052234454594086856\n",
      "Stochastic Gradient Descent(10261): loss=7.557489679750054\n",
      "Stochastic Gradient Descent(10262): loss=2.838701107856981\n",
      "Stochastic Gradient Descent(10263): loss=1.3560785476901431\n",
      "Stochastic Gradient Descent(10264): loss=9.105716227880862\n",
      "Stochastic Gradient Descent(10265): loss=9.53790189738795\n",
      "Stochastic Gradient Descent(10266): loss=0.6725115385059693\n",
      "Stochastic Gradient Descent(10267): loss=13.069900434804861\n",
      "Stochastic Gradient Descent(10268): loss=1.139409552368399\n",
      "Stochastic Gradient Descent(10269): loss=3.07730108532565\n",
      "Stochastic Gradient Descent(10270): loss=28.617313231708113\n",
      "Stochastic Gradient Descent(10271): loss=1.8535366131283368\n",
      "Stochastic Gradient Descent(10272): loss=0.559888462642325\n",
      "Stochastic Gradient Descent(10273): loss=1.881052424167565\n",
      "Stochastic Gradient Descent(10274): loss=10.931080124465028\n",
      "Stochastic Gradient Descent(10275): loss=14.868900586927504\n",
      "Stochastic Gradient Descent(10276): loss=4.716627563765011\n",
      "Stochastic Gradient Descent(10277): loss=0.23525004163947608\n",
      "Stochastic Gradient Descent(10278): loss=0.09401089217432362\n",
      "Stochastic Gradient Descent(10279): loss=3.0255653496154156\n",
      "Stochastic Gradient Descent(10280): loss=1.6210110700994627\n",
      "Stochastic Gradient Descent(10281): loss=0.3953415652370632\n",
      "Stochastic Gradient Descent(10282): loss=1.3947542344397894\n",
      "Stochastic Gradient Descent(10283): loss=1.7945472735065355\n",
      "Stochastic Gradient Descent(10284): loss=27.110143749547177\n",
      "Stochastic Gradient Descent(10285): loss=12.868199240042912\n",
      "Stochastic Gradient Descent(10286): loss=0.5188766800850979\n",
      "Stochastic Gradient Descent(10287): loss=4.9826022402326995\n",
      "Stochastic Gradient Descent(10288): loss=0.17915254706892392\n",
      "Stochastic Gradient Descent(10289): loss=1.9780803143746286\n",
      "Stochastic Gradient Descent(10290): loss=0.1985482230261889\n",
      "Stochastic Gradient Descent(10291): loss=1.9946334031753932\n",
      "Stochastic Gradient Descent(10292): loss=1.3699729194454524\n",
      "Stochastic Gradient Descent(10293): loss=25.99472397312381\n",
      "Stochastic Gradient Descent(10294): loss=0.18892552449331776\n",
      "Stochastic Gradient Descent(10295): loss=10.023814747421845\n",
      "Stochastic Gradient Descent(10296): loss=0.11990829004774425\n",
      "Stochastic Gradient Descent(10297): loss=0.8289205280148458\n",
      "Stochastic Gradient Descent(10298): loss=24.57710260976485\n",
      "Stochastic Gradient Descent(10299): loss=0.22163571037113922\n",
      "Stochastic Gradient Descent(10300): loss=0.1560597741033223\n",
      "Stochastic Gradient Descent(10301): loss=6.95464883704016\n",
      "Stochastic Gradient Descent(10302): loss=2.947287695229453\n",
      "Stochastic Gradient Descent(10303): loss=0.11518992911966548\n",
      "Stochastic Gradient Descent(10304): loss=2.4549527889177005\n",
      "Stochastic Gradient Descent(10305): loss=0.357180091904539\n",
      "Stochastic Gradient Descent(10306): loss=0.0003480950925537739\n",
      "Stochastic Gradient Descent(10307): loss=9.770781395025649\n",
      "Stochastic Gradient Descent(10308): loss=2.8126734982335178\n",
      "Stochastic Gradient Descent(10309): loss=0.010962116149312192\n",
      "Stochastic Gradient Descent(10310): loss=0.8771014103884694\n",
      "Stochastic Gradient Descent(10311): loss=3.559567505703296\n",
      "Stochastic Gradient Descent(10312): loss=1.2431160297454213\n",
      "Stochastic Gradient Descent(10313): loss=26.504176076008317\n",
      "Stochastic Gradient Descent(10314): loss=0.8498476673712054\n",
      "Stochastic Gradient Descent(10315): loss=6.470643538701264\n",
      "Stochastic Gradient Descent(10316): loss=0.5553913253694933\n",
      "Stochastic Gradient Descent(10317): loss=1.4208748061242533\n",
      "Stochastic Gradient Descent(10318): loss=0.7548458931742309\n",
      "Stochastic Gradient Descent(10319): loss=0.09867473699474752\n",
      "Stochastic Gradient Descent(10320): loss=1.2456908298278353\n",
      "Stochastic Gradient Descent(10321): loss=0.4259685579408259\n",
      "Stochastic Gradient Descent(10322): loss=7.188910437747199\n",
      "Stochastic Gradient Descent(10323): loss=2.2023072910341326\n",
      "Stochastic Gradient Descent(10324): loss=2.206983794802663\n",
      "Stochastic Gradient Descent(10325): loss=0.24117095882821596\n",
      "Stochastic Gradient Descent(10326): loss=2.710519887894196\n",
      "Stochastic Gradient Descent(10327): loss=2.123640918106472\n",
      "Stochastic Gradient Descent(10328): loss=0.0685626484068338\n",
      "Stochastic Gradient Descent(10329): loss=2.8507593968349223\n",
      "Stochastic Gradient Descent(10330): loss=0.37205924713146504\n",
      "Stochastic Gradient Descent(10331): loss=15.000881187991927\n",
      "Stochastic Gradient Descent(10332): loss=0.03186269749790037\n",
      "Stochastic Gradient Descent(10333): loss=1.5236426554039193\n",
      "Stochastic Gradient Descent(10334): loss=2.3621147095001245\n",
      "Stochastic Gradient Descent(10335): loss=9.4808249241337\n",
      "Stochastic Gradient Descent(10336): loss=0.03388105288660067\n",
      "Stochastic Gradient Descent(10337): loss=25.443830217865415\n",
      "Stochastic Gradient Descent(10338): loss=0.23441049172674053\n",
      "Stochastic Gradient Descent(10339): loss=2.5083013103367175\n",
      "Stochastic Gradient Descent(10340): loss=0.13144923053997365\n",
      "Stochastic Gradient Descent(10341): loss=1.0883396729862995\n",
      "Stochastic Gradient Descent(10342): loss=0.09386910258072312\n",
      "Stochastic Gradient Descent(10343): loss=25.309667827331012\n",
      "Stochastic Gradient Descent(10344): loss=2.2990755832668595\n",
      "Stochastic Gradient Descent(10345): loss=2.699363929211799\n",
      "Stochastic Gradient Descent(10346): loss=0.13514771198012066\n",
      "Stochastic Gradient Descent(10347): loss=8.018305090957563\n",
      "Stochastic Gradient Descent(10348): loss=2.9706537355823612\n",
      "Stochastic Gradient Descent(10349): loss=2.205784996819075\n",
      "Stochastic Gradient Descent(10350): loss=0.09433541680415976\n",
      "Stochastic Gradient Descent(10351): loss=1.9708017861312106\n",
      "Stochastic Gradient Descent(10352): loss=35.65605091096936\n",
      "Stochastic Gradient Descent(10353): loss=14.176804809519947\n",
      "Stochastic Gradient Descent(10354): loss=4.3829339315324525\n",
      "Stochastic Gradient Descent(10355): loss=5.095735290862625\n",
      "Stochastic Gradient Descent(10356): loss=0.06696410350846511\n",
      "Stochastic Gradient Descent(10357): loss=4.164200897070464\n",
      "Stochastic Gradient Descent(10358): loss=14.470069910097136\n",
      "Stochastic Gradient Descent(10359): loss=0.21834644235354178\n",
      "Stochastic Gradient Descent(10360): loss=4.058768336125971\n",
      "Stochastic Gradient Descent(10361): loss=0.0013587702152879062\n",
      "Stochastic Gradient Descent(10362): loss=0.12019608318179227\n",
      "Stochastic Gradient Descent(10363): loss=0.8765419039540285\n",
      "Stochastic Gradient Descent(10364): loss=7.648028497072965\n",
      "Stochastic Gradient Descent(10365): loss=9.712412609548657\n",
      "Stochastic Gradient Descent(10366): loss=8.518984989259017\n",
      "Stochastic Gradient Descent(10367): loss=0.12265986517410618\n",
      "Stochastic Gradient Descent(10368): loss=7.849007518056393\n",
      "Stochastic Gradient Descent(10369): loss=0.8199716958601377\n",
      "Stochastic Gradient Descent(10370): loss=1.4573615543078062\n",
      "Stochastic Gradient Descent(10371): loss=45.82417386325554\n",
      "Stochastic Gradient Descent(10372): loss=61.134976873652626\n",
      "Stochastic Gradient Descent(10373): loss=7.479941755758965\n",
      "Stochastic Gradient Descent(10374): loss=0.7355022952488296\n",
      "Stochastic Gradient Descent(10375): loss=3.6328655504230807\n",
      "Stochastic Gradient Descent(10376): loss=0.013661144614122859\n",
      "Stochastic Gradient Descent(10377): loss=15.571568091531775\n",
      "Stochastic Gradient Descent(10378): loss=1.2837236248419575\n",
      "Stochastic Gradient Descent(10379): loss=0.54261349418963\n",
      "Stochastic Gradient Descent(10380): loss=0.8045112567860805\n",
      "Stochastic Gradient Descent(10381): loss=11.56473954377658\n",
      "Stochastic Gradient Descent(10382): loss=50.005616488684375\n",
      "Stochastic Gradient Descent(10383): loss=10.931423768996144\n",
      "Stochastic Gradient Descent(10384): loss=13.532227151158324\n",
      "Stochastic Gradient Descent(10385): loss=9.811948071490182\n",
      "Stochastic Gradient Descent(10386): loss=1.7530702188056877\n",
      "Stochastic Gradient Descent(10387): loss=0.07166545236734408\n",
      "Stochastic Gradient Descent(10388): loss=0.04441870466567246\n",
      "Stochastic Gradient Descent(10389): loss=2.0505845947461667\n",
      "Stochastic Gradient Descent(10390): loss=21.468731619092797\n",
      "Stochastic Gradient Descent(10391): loss=11.758037780495153\n",
      "Stochastic Gradient Descent(10392): loss=2.9161028697623714\n",
      "Stochastic Gradient Descent(10393): loss=4.159253628106291\n",
      "Stochastic Gradient Descent(10394): loss=0.4367157979384572\n",
      "Stochastic Gradient Descent(10395): loss=0.3682475703783889\n",
      "Stochastic Gradient Descent(10396): loss=0.0032611681659246838\n",
      "Stochastic Gradient Descent(10397): loss=14.63489141871958\n",
      "Stochastic Gradient Descent(10398): loss=12.94516340785736\n",
      "Stochastic Gradient Descent(10399): loss=14.885567544871162\n",
      "Stochastic Gradient Descent(10400): loss=2.2101107504264754\n",
      "Stochastic Gradient Descent(10401): loss=189.57399774894503\n",
      "Stochastic Gradient Descent(10402): loss=35.38009104531902\n",
      "Stochastic Gradient Descent(10403): loss=99.11903313308207\n",
      "Stochastic Gradient Descent(10404): loss=100.2733426655024\n",
      "Stochastic Gradient Descent(10405): loss=8.314335221397288\n",
      "Stochastic Gradient Descent(10406): loss=25.3057738875694\n",
      "Stochastic Gradient Descent(10407): loss=199.4954156839743\n",
      "Stochastic Gradient Descent(10408): loss=6.608942345042804\n",
      "Stochastic Gradient Descent(10409): loss=20.777901172093657\n",
      "Stochastic Gradient Descent(10410): loss=0.17692810935288988\n",
      "Stochastic Gradient Descent(10411): loss=4.370030105111961\n",
      "Stochastic Gradient Descent(10412): loss=8.94409870797798\n",
      "Stochastic Gradient Descent(10413): loss=6.84345851924082\n",
      "Stochastic Gradient Descent(10414): loss=43.05969657718092\n",
      "Stochastic Gradient Descent(10415): loss=5.542470027243234\n",
      "Stochastic Gradient Descent(10416): loss=0.01189663502038306\n",
      "Stochastic Gradient Descent(10417): loss=2.068664698381187\n",
      "Stochastic Gradient Descent(10418): loss=18.029548379311642\n",
      "Stochastic Gradient Descent(10419): loss=0.0005836997947841856\n",
      "Stochastic Gradient Descent(10420): loss=28.944271567096205\n",
      "Stochastic Gradient Descent(10421): loss=15.759342036510775\n",
      "Stochastic Gradient Descent(10422): loss=2.81901032100715\n",
      "Stochastic Gradient Descent(10423): loss=6.166485213958152\n",
      "Stochastic Gradient Descent(10424): loss=0.2635067764118658\n",
      "Stochastic Gradient Descent(10425): loss=5.202397783341684\n",
      "Stochastic Gradient Descent(10426): loss=4.597955091794049\n",
      "Stochastic Gradient Descent(10427): loss=0.17619631411963804\n",
      "Stochastic Gradient Descent(10428): loss=0.028438028452059134\n",
      "Stochastic Gradient Descent(10429): loss=5.486230909824656\n",
      "Stochastic Gradient Descent(10430): loss=42.10516812501946\n",
      "Stochastic Gradient Descent(10431): loss=4.186335184581922\n",
      "Stochastic Gradient Descent(10432): loss=1.979947465260463\n",
      "Stochastic Gradient Descent(10433): loss=2.32601335712749\n",
      "Stochastic Gradient Descent(10434): loss=0.43085195663534576\n",
      "Stochastic Gradient Descent(10435): loss=6.0099256574148745\n",
      "Stochastic Gradient Descent(10436): loss=3.1243162155583035\n",
      "Stochastic Gradient Descent(10437): loss=1.3019094099334703\n",
      "Stochastic Gradient Descent(10438): loss=3.5969099374307327\n",
      "Stochastic Gradient Descent(10439): loss=2.8814145511497835\n",
      "Stochastic Gradient Descent(10440): loss=4.72157581157492\n",
      "Stochastic Gradient Descent(10441): loss=0.6045980195396549\n",
      "Stochastic Gradient Descent(10442): loss=0.23840176640136626\n",
      "Stochastic Gradient Descent(10443): loss=8.645098767414321\n",
      "Stochastic Gradient Descent(10444): loss=6.688808380535618\n",
      "Stochastic Gradient Descent(10445): loss=5.439348791201111\n",
      "Stochastic Gradient Descent(10446): loss=2.280974540799227\n",
      "Stochastic Gradient Descent(10447): loss=8.030463144233227\n",
      "Stochastic Gradient Descent(10448): loss=3.162462390924186\n",
      "Stochastic Gradient Descent(10449): loss=0.5665282777708976\n",
      "Stochastic Gradient Descent(10450): loss=2.5431790057851535\n",
      "Stochastic Gradient Descent(10451): loss=0.9574300811739872\n",
      "Stochastic Gradient Descent(10452): loss=1.2263857642437557\n",
      "Stochastic Gradient Descent(10453): loss=3.554954776612988\n",
      "Stochastic Gradient Descent(10454): loss=3.360036262611751\n",
      "Stochastic Gradient Descent(10455): loss=0.4868043905843172\n",
      "Stochastic Gradient Descent(10456): loss=0.06631901451435272\n",
      "Stochastic Gradient Descent(10457): loss=0.010041446041648415\n",
      "Stochastic Gradient Descent(10458): loss=10.432910509011641\n",
      "Stochastic Gradient Descent(10459): loss=9.182910629599167\n",
      "Stochastic Gradient Descent(10460): loss=1.7171008459346282\n",
      "Stochastic Gradient Descent(10461): loss=1.8436250574225617\n",
      "Stochastic Gradient Descent(10462): loss=12.832099637947517\n",
      "Stochastic Gradient Descent(10463): loss=33.30502950018229\n",
      "Stochastic Gradient Descent(10464): loss=1.2162831372282477\n",
      "Stochastic Gradient Descent(10465): loss=2.326678008529571\n",
      "Stochastic Gradient Descent(10466): loss=5.649504694380886\n",
      "Stochastic Gradient Descent(10467): loss=3.5287084034937526\n",
      "Stochastic Gradient Descent(10468): loss=9.993929162951098\n",
      "Stochastic Gradient Descent(10469): loss=1.6258521172335125\n",
      "Stochastic Gradient Descent(10470): loss=0.014333178760238506\n",
      "Stochastic Gradient Descent(10471): loss=8.34516920948272\n",
      "Stochastic Gradient Descent(10472): loss=0.17111607087401634\n",
      "Stochastic Gradient Descent(10473): loss=6.392161130947517\n",
      "Stochastic Gradient Descent(10474): loss=1.1103547118454111\n",
      "Stochastic Gradient Descent(10475): loss=0.7218688788741122\n",
      "Stochastic Gradient Descent(10476): loss=0.02999014629777882\n",
      "Stochastic Gradient Descent(10477): loss=0.3007153167121523\n",
      "Stochastic Gradient Descent(10478): loss=3.297845682733687\n",
      "Stochastic Gradient Descent(10479): loss=1.1146744158429398\n",
      "Stochastic Gradient Descent(10480): loss=8.491579956480408\n",
      "Stochastic Gradient Descent(10481): loss=0.7833160703810106\n",
      "Stochastic Gradient Descent(10482): loss=0.0415771861252071\n",
      "Stochastic Gradient Descent(10483): loss=0.15346140181384324\n",
      "Stochastic Gradient Descent(10484): loss=14.013145943441836\n",
      "Stochastic Gradient Descent(10485): loss=7.598792074300381\n",
      "Stochastic Gradient Descent(10486): loss=0.905564033105769\n",
      "Stochastic Gradient Descent(10487): loss=10.743965697893744\n",
      "Stochastic Gradient Descent(10488): loss=0.45839371371842774\n",
      "Stochastic Gradient Descent(10489): loss=8.845383606635052\n",
      "Stochastic Gradient Descent(10490): loss=0.5410599903707468\n",
      "Stochastic Gradient Descent(10491): loss=13.445422695666828\n",
      "Stochastic Gradient Descent(10492): loss=0.22599510221053287\n",
      "Stochastic Gradient Descent(10493): loss=0.032845787948347895\n",
      "Stochastic Gradient Descent(10494): loss=1.5711860242545406\n",
      "Stochastic Gradient Descent(10495): loss=7.620783474081011\n",
      "Stochastic Gradient Descent(10496): loss=0.7278464463077706\n",
      "Stochastic Gradient Descent(10497): loss=14.116627215655319\n",
      "Stochastic Gradient Descent(10498): loss=4.460021834492886\n",
      "Stochastic Gradient Descent(10499): loss=1.5518252602180058\n",
      "Stochastic Gradient Descent(10500): loss=0.16191376555790965\n",
      "Stochastic Gradient Descent(10501): loss=0.11101122798898712\n",
      "Stochastic Gradient Descent(10502): loss=12.553985296570215\n",
      "Stochastic Gradient Descent(10503): loss=1.7071058587556225\n",
      "Stochastic Gradient Descent(10504): loss=12.519408648141674\n",
      "Stochastic Gradient Descent(10505): loss=2.9061361954976457\n",
      "Stochastic Gradient Descent(10506): loss=18.22526357212529\n",
      "Stochastic Gradient Descent(10507): loss=7.655815992266516\n",
      "Stochastic Gradient Descent(10508): loss=6.694882386333121\n",
      "Stochastic Gradient Descent(10509): loss=5.514000995624985\n",
      "Stochastic Gradient Descent(10510): loss=0.1573842826057975\n",
      "Stochastic Gradient Descent(10511): loss=0.12285346930185152\n",
      "Stochastic Gradient Descent(10512): loss=8.211729671498032\n",
      "Stochastic Gradient Descent(10513): loss=24.912171952733676\n",
      "Stochastic Gradient Descent(10514): loss=0.8934727706094442\n",
      "Stochastic Gradient Descent(10515): loss=4.9049950810784475\n",
      "Stochastic Gradient Descent(10516): loss=1.3713219994272754\n",
      "Stochastic Gradient Descent(10517): loss=6.564119479257139\n",
      "Stochastic Gradient Descent(10518): loss=0.8315838452131314\n",
      "Stochastic Gradient Descent(10519): loss=1.0778803156140229\n",
      "Stochastic Gradient Descent(10520): loss=6.2711117932222304\n",
      "Stochastic Gradient Descent(10521): loss=0.0022682219163478595\n",
      "Stochastic Gradient Descent(10522): loss=4.633791787792344\n",
      "Stochastic Gradient Descent(10523): loss=0.07815450856192936\n",
      "Stochastic Gradient Descent(10524): loss=0.06912107090997728\n",
      "Stochastic Gradient Descent(10525): loss=8.023659180281673\n",
      "Stochastic Gradient Descent(10526): loss=50.660466013913904\n",
      "Stochastic Gradient Descent(10527): loss=0.18180687410463722\n",
      "Stochastic Gradient Descent(10528): loss=10.21157072282318\n",
      "Stochastic Gradient Descent(10529): loss=11.951992395096594\n",
      "Stochastic Gradient Descent(10530): loss=5.194520792267544\n",
      "Stochastic Gradient Descent(10531): loss=3.6938141002753486\n",
      "Stochastic Gradient Descent(10532): loss=13.864974113372089\n",
      "Stochastic Gradient Descent(10533): loss=2.515733997344566\n",
      "Stochastic Gradient Descent(10534): loss=11.48632649822318\n",
      "Stochastic Gradient Descent(10535): loss=2.044838555560047\n",
      "Stochastic Gradient Descent(10536): loss=1.2608407734845368\n",
      "Stochastic Gradient Descent(10537): loss=75.39230006207649\n",
      "Stochastic Gradient Descent(10538): loss=7.747252981027406\n",
      "Stochastic Gradient Descent(10539): loss=25.88072324532839\n",
      "Stochastic Gradient Descent(10540): loss=0.17812426017063918\n",
      "Stochastic Gradient Descent(10541): loss=0.21062840788256718\n",
      "Stochastic Gradient Descent(10542): loss=0.8655842381326635\n",
      "Stochastic Gradient Descent(10543): loss=9.465016047032895\n",
      "Stochastic Gradient Descent(10544): loss=5.675746260476108\n",
      "Stochastic Gradient Descent(10545): loss=5.369782262112629\n",
      "Stochastic Gradient Descent(10546): loss=0.7000846553879296\n",
      "Stochastic Gradient Descent(10547): loss=26.129785974601603\n",
      "Stochastic Gradient Descent(10548): loss=3.598219222883127\n",
      "Stochastic Gradient Descent(10549): loss=9.819142821540279\n",
      "Stochastic Gradient Descent(10550): loss=5.3549044997578825\n",
      "Stochastic Gradient Descent(10551): loss=30.403872977661692\n",
      "Stochastic Gradient Descent(10552): loss=0.15723305145752156\n",
      "Stochastic Gradient Descent(10553): loss=1.8105485932248486\n",
      "Stochastic Gradient Descent(10554): loss=20.79406351009899\n",
      "Stochastic Gradient Descent(10555): loss=15.009128581888545\n",
      "Stochastic Gradient Descent(10556): loss=1.233165212205835\n",
      "Stochastic Gradient Descent(10557): loss=0.0014094025612087187\n",
      "Stochastic Gradient Descent(10558): loss=0.02687674751777391\n",
      "Stochastic Gradient Descent(10559): loss=0.23697605545749728\n",
      "Stochastic Gradient Descent(10560): loss=8.252432772509662\n",
      "Stochastic Gradient Descent(10561): loss=2.371007709844032\n",
      "Stochastic Gradient Descent(10562): loss=1.9855317093999605\n",
      "Stochastic Gradient Descent(10563): loss=35.06124537841258\n",
      "Stochastic Gradient Descent(10564): loss=0.7128169944685816\n",
      "Stochastic Gradient Descent(10565): loss=5.8938385962167485\n",
      "Stochastic Gradient Descent(10566): loss=0.4141723943014015\n",
      "Stochastic Gradient Descent(10567): loss=11.39227487590079\n",
      "Stochastic Gradient Descent(10568): loss=0.14778255710618973\n",
      "Stochastic Gradient Descent(10569): loss=0.19813032733633498\n",
      "Stochastic Gradient Descent(10570): loss=1.2572675834641547\n",
      "Stochastic Gradient Descent(10571): loss=1.353783000852378\n",
      "Stochastic Gradient Descent(10572): loss=6.530636186245825\n",
      "Stochastic Gradient Descent(10573): loss=6.407839226732208\n",
      "Stochastic Gradient Descent(10574): loss=5.245283210372116\n",
      "Stochastic Gradient Descent(10575): loss=5.896705630083307\n",
      "Stochastic Gradient Descent(10576): loss=0.3359142655860607\n",
      "Stochastic Gradient Descent(10577): loss=0.08478654599667439\n",
      "Stochastic Gradient Descent(10578): loss=8.364047586602837\n",
      "Stochastic Gradient Descent(10579): loss=2.0587411285362065\n",
      "Stochastic Gradient Descent(10580): loss=0.9416409474308889\n",
      "Stochastic Gradient Descent(10581): loss=4.521387987234514\n",
      "Stochastic Gradient Descent(10582): loss=1.0058013591336403\n",
      "Stochastic Gradient Descent(10583): loss=3.550020500941984\n",
      "Stochastic Gradient Descent(10584): loss=0.044799399940123226\n",
      "Stochastic Gradient Descent(10585): loss=0.07912781415897906\n",
      "Stochastic Gradient Descent(10586): loss=0.023209229198130017\n",
      "Stochastic Gradient Descent(10587): loss=3.932389163979543\n",
      "Stochastic Gradient Descent(10588): loss=1.7599660032430111\n",
      "Stochastic Gradient Descent(10589): loss=0.2133924983245401\n",
      "Stochastic Gradient Descent(10590): loss=0.05003789569606977\n",
      "Stochastic Gradient Descent(10591): loss=6.227697937007802e-05\n",
      "Stochastic Gradient Descent(10592): loss=2.0471076998984294\n",
      "Stochastic Gradient Descent(10593): loss=1.2270481550838468\n",
      "Stochastic Gradient Descent(10594): loss=0.4684449817937384\n",
      "Stochastic Gradient Descent(10595): loss=0.7473699220774418\n",
      "Stochastic Gradient Descent(10596): loss=1.5062902260051956\n",
      "Stochastic Gradient Descent(10597): loss=6.682230920023978\n",
      "Stochastic Gradient Descent(10598): loss=1.8209587814892116\n",
      "Stochastic Gradient Descent(10599): loss=4.312689736750027\n",
      "Stochastic Gradient Descent(10600): loss=2.0396644855932244\n",
      "Stochastic Gradient Descent(10601): loss=0.06911985839644448\n",
      "Stochastic Gradient Descent(10602): loss=0.9073772367777191\n",
      "Stochastic Gradient Descent(10603): loss=1.305094707430072\n",
      "Stochastic Gradient Descent(10604): loss=0.07651664930169981\n",
      "Stochastic Gradient Descent(10605): loss=0.001092145671919311\n",
      "Stochastic Gradient Descent(10606): loss=0.06624463443662094\n",
      "Stochastic Gradient Descent(10607): loss=2.054699479365584\n",
      "Stochastic Gradient Descent(10608): loss=14.776153103966642\n",
      "Stochastic Gradient Descent(10609): loss=0.9555687249900696\n",
      "Stochastic Gradient Descent(10610): loss=1.5766278887959995\n",
      "Stochastic Gradient Descent(10611): loss=1.4899624939448401\n",
      "Stochastic Gradient Descent(10612): loss=0.2826992447086249\n",
      "Stochastic Gradient Descent(10613): loss=8.37122325738276\n",
      "Stochastic Gradient Descent(10614): loss=1.1332927538034108\n",
      "Stochastic Gradient Descent(10615): loss=0.4820258641552404\n",
      "Stochastic Gradient Descent(10616): loss=1.2972217645279718\n",
      "Stochastic Gradient Descent(10617): loss=3.417070222981876\n",
      "Stochastic Gradient Descent(10618): loss=2.6440206654514276\n",
      "Stochastic Gradient Descent(10619): loss=6.276218415171296\n",
      "Stochastic Gradient Descent(10620): loss=0.7831115328797065\n",
      "Stochastic Gradient Descent(10621): loss=0.001207937806541919\n",
      "Stochastic Gradient Descent(10622): loss=20.18635696761493\n",
      "Stochastic Gradient Descent(10623): loss=0.14968011795624714\n",
      "Stochastic Gradient Descent(10624): loss=12.30213986013333\n",
      "Stochastic Gradient Descent(10625): loss=17.872830613182117\n",
      "Stochastic Gradient Descent(10626): loss=0.9429372452450312\n",
      "Stochastic Gradient Descent(10627): loss=0.28150944990580323\n",
      "Stochastic Gradient Descent(10628): loss=1.745998824929079\n",
      "Stochastic Gradient Descent(10629): loss=0.7035897822448581\n",
      "Stochastic Gradient Descent(10630): loss=1.4082881037528505\n",
      "Stochastic Gradient Descent(10631): loss=0.1723340617739044\n",
      "Stochastic Gradient Descent(10632): loss=0.14450626319285842\n",
      "Stochastic Gradient Descent(10633): loss=1.9247013701540356\n",
      "Stochastic Gradient Descent(10634): loss=6.066218785827926\n",
      "Stochastic Gradient Descent(10635): loss=0.27532377446695044\n",
      "Stochastic Gradient Descent(10636): loss=0.09063925571222248\n",
      "Stochastic Gradient Descent(10637): loss=0.01686675255585323\n",
      "Stochastic Gradient Descent(10638): loss=5.275683208079806\n",
      "Stochastic Gradient Descent(10639): loss=5.210640953615297\n",
      "Stochastic Gradient Descent(10640): loss=7.90012328128649\n",
      "Stochastic Gradient Descent(10641): loss=3.3837537974682537\n",
      "Stochastic Gradient Descent(10642): loss=11.403984827060183\n",
      "Stochastic Gradient Descent(10643): loss=0.5157316737717407\n",
      "Stochastic Gradient Descent(10644): loss=11.357097409046705\n",
      "Stochastic Gradient Descent(10645): loss=2.2199048773753822\n",
      "Stochastic Gradient Descent(10646): loss=4.588950272896179\n",
      "Stochastic Gradient Descent(10647): loss=4.981597264539293\n",
      "Stochastic Gradient Descent(10648): loss=0.17327578113174621\n",
      "Stochastic Gradient Descent(10649): loss=2.5609286005301057\n",
      "Stochastic Gradient Descent(10650): loss=0.934377140233332\n",
      "Stochastic Gradient Descent(10651): loss=5.405741265950405\n",
      "Stochastic Gradient Descent(10652): loss=0.3190018441748321\n",
      "Stochastic Gradient Descent(10653): loss=0.3433213179713661\n",
      "Stochastic Gradient Descent(10654): loss=86.44245042971431\n",
      "Stochastic Gradient Descent(10655): loss=2.453630838721619\n",
      "Stochastic Gradient Descent(10656): loss=4.093786713002403\n",
      "Stochastic Gradient Descent(10657): loss=9.637124940245291\n",
      "Stochastic Gradient Descent(10658): loss=12.253404270337725\n",
      "Stochastic Gradient Descent(10659): loss=0.3995315377103042\n",
      "Stochastic Gradient Descent(10660): loss=0.00394600883192837\n",
      "Stochastic Gradient Descent(10661): loss=15.13753241272613\n",
      "Stochastic Gradient Descent(10662): loss=0.04219856866433942\n",
      "Stochastic Gradient Descent(10663): loss=3.19833712539148\n",
      "Stochastic Gradient Descent(10664): loss=14.709425866411795\n",
      "Stochastic Gradient Descent(10665): loss=0.4860943721826676\n",
      "Stochastic Gradient Descent(10666): loss=1.4367343975911793\n",
      "Stochastic Gradient Descent(10667): loss=0.40526717733698797\n",
      "Stochastic Gradient Descent(10668): loss=0.9378845945663306\n",
      "Stochastic Gradient Descent(10669): loss=1.831922702530219\n",
      "Stochastic Gradient Descent(10670): loss=0.03511237959944688\n",
      "Stochastic Gradient Descent(10671): loss=15.564437448363517\n",
      "Stochastic Gradient Descent(10672): loss=11.234076455360498\n",
      "Stochastic Gradient Descent(10673): loss=2.385572104009985\n",
      "Stochastic Gradient Descent(10674): loss=4.3547907130404784e-05\n",
      "Stochastic Gradient Descent(10675): loss=0.06407835138569005\n",
      "Stochastic Gradient Descent(10676): loss=0.00013161984621506138\n",
      "Stochastic Gradient Descent(10677): loss=12.227047966406388\n",
      "Stochastic Gradient Descent(10678): loss=8.220013392351742\n",
      "Stochastic Gradient Descent(10679): loss=13.997695727920213\n",
      "Stochastic Gradient Descent(10680): loss=4.701686315605821\n",
      "Stochastic Gradient Descent(10681): loss=7.6333710799186285\n",
      "Stochastic Gradient Descent(10682): loss=2.7164135903109865\n",
      "Stochastic Gradient Descent(10683): loss=2.4037849269588167\n",
      "Stochastic Gradient Descent(10684): loss=0.1369528755093031\n",
      "Stochastic Gradient Descent(10685): loss=8.022603324545377\n",
      "Stochastic Gradient Descent(10686): loss=0.12865354718516264\n",
      "Stochastic Gradient Descent(10687): loss=1.1024612472332775\n",
      "Stochastic Gradient Descent(10688): loss=0.02789103447678079\n",
      "Stochastic Gradient Descent(10689): loss=1.0285895098084388\n",
      "Stochastic Gradient Descent(10690): loss=0.4951453344038658\n",
      "Stochastic Gradient Descent(10691): loss=3.272146474094697\n",
      "Stochastic Gradient Descent(10692): loss=0.7450459346004412\n",
      "Stochastic Gradient Descent(10693): loss=0.590578376796027\n",
      "Stochastic Gradient Descent(10694): loss=0.10362793698476049\n",
      "Stochastic Gradient Descent(10695): loss=18.651632992025313\n",
      "Stochastic Gradient Descent(10696): loss=14.065329426263995\n",
      "Stochastic Gradient Descent(10697): loss=0.04248226473312164\n",
      "Stochastic Gradient Descent(10698): loss=2.6833176785983373\n",
      "Stochastic Gradient Descent(10699): loss=5.162540803100894\n",
      "Stochastic Gradient Descent(10700): loss=3.333474036975755\n",
      "Stochastic Gradient Descent(10701): loss=0.01710214020504029\n",
      "Stochastic Gradient Descent(10702): loss=0.408395679743179\n",
      "Stochastic Gradient Descent(10703): loss=4.077035133611448\n",
      "Stochastic Gradient Descent(10704): loss=0.02381960934899618\n",
      "Stochastic Gradient Descent(10705): loss=0.12134986164430266\n",
      "Stochastic Gradient Descent(10706): loss=9.082466501011321\n",
      "Stochastic Gradient Descent(10707): loss=0.5042065711310195\n",
      "Stochastic Gradient Descent(10708): loss=0.5525684835880428\n",
      "Stochastic Gradient Descent(10709): loss=1.9998170292438955\n",
      "Stochastic Gradient Descent(10710): loss=13.190890229636157\n",
      "Stochastic Gradient Descent(10711): loss=19.293361764213163\n",
      "Stochastic Gradient Descent(10712): loss=2.2588953006284855\n",
      "Stochastic Gradient Descent(10713): loss=9.682749272430849\n",
      "Stochastic Gradient Descent(10714): loss=0.9248764995188617\n",
      "Stochastic Gradient Descent(10715): loss=0.5664758868698798\n",
      "Stochastic Gradient Descent(10716): loss=4.583908836401151\n",
      "Stochastic Gradient Descent(10717): loss=13.965980372506769\n",
      "Stochastic Gradient Descent(10718): loss=0.6440713791629202\n",
      "Stochastic Gradient Descent(10719): loss=2.509299552885721\n",
      "Stochastic Gradient Descent(10720): loss=0.20687982020815532\n",
      "Stochastic Gradient Descent(10721): loss=1.628244644302029\n",
      "Stochastic Gradient Descent(10722): loss=0.7008458194624388\n",
      "Stochastic Gradient Descent(10723): loss=21.992975239954514\n",
      "Stochastic Gradient Descent(10724): loss=1.0032508082118112\n",
      "Stochastic Gradient Descent(10725): loss=0.21686901779331197\n",
      "Stochastic Gradient Descent(10726): loss=4.784818275623251\n",
      "Stochastic Gradient Descent(10727): loss=0.026631885163010252\n",
      "Stochastic Gradient Descent(10728): loss=39.10027798066406\n",
      "Stochastic Gradient Descent(10729): loss=6.717285316578129\n",
      "Stochastic Gradient Descent(10730): loss=1.7647918882543134\n",
      "Stochastic Gradient Descent(10731): loss=1.0479247652497345\n",
      "Stochastic Gradient Descent(10732): loss=2.3517725341172993\n",
      "Stochastic Gradient Descent(10733): loss=5.769752282497235\n",
      "Stochastic Gradient Descent(10734): loss=6.870353840933237\n",
      "Stochastic Gradient Descent(10735): loss=3.561595158680256\n",
      "Stochastic Gradient Descent(10736): loss=0.190475861848485\n",
      "Stochastic Gradient Descent(10737): loss=3.478017495139538\n",
      "Stochastic Gradient Descent(10738): loss=7.817069770629777\n",
      "Stochastic Gradient Descent(10739): loss=1.4355975527970573\n",
      "Stochastic Gradient Descent(10740): loss=3.640824675954988\n",
      "Stochastic Gradient Descent(10741): loss=11.541038071056013\n",
      "Stochastic Gradient Descent(10742): loss=9.096280001045768\n",
      "Stochastic Gradient Descent(10743): loss=12.819193620869076\n",
      "Stochastic Gradient Descent(10744): loss=2.090006144987812\n",
      "Stochastic Gradient Descent(10745): loss=0.8767495464760545\n",
      "Stochastic Gradient Descent(10746): loss=6.62851107768127\n",
      "Stochastic Gradient Descent(10747): loss=0.014479347194370485\n",
      "Stochastic Gradient Descent(10748): loss=6.326511717816484\n",
      "Stochastic Gradient Descent(10749): loss=4.2889322029219965\n",
      "Stochastic Gradient Descent(10750): loss=0.804161542001323\n",
      "Stochastic Gradient Descent(10751): loss=8.1771191219533\n",
      "Stochastic Gradient Descent(10752): loss=0.44347249799221855\n",
      "Stochastic Gradient Descent(10753): loss=15.000162146825396\n",
      "Stochastic Gradient Descent(10754): loss=0.13838315169706536\n",
      "Stochastic Gradient Descent(10755): loss=2.4993941316609027\n",
      "Stochastic Gradient Descent(10756): loss=0.4077764448173568\n",
      "Stochastic Gradient Descent(10757): loss=0.6783664097063953\n",
      "Stochastic Gradient Descent(10758): loss=0.660239718308088\n",
      "Stochastic Gradient Descent(10759): loss=2.9782486755074533\n",
      "Stochastic Gradient Descent(10760): loss=1.4812785764283267\n",
      "Stochastic Gradient Descent(10761): loss=0.15428846619101563\n",
      "Stochastic Gradient Descent(10762): loss=0.09177305882801484\n",
      "Stochastic Gradient Descent(10763): loss=2.262313722567861\n",
      "Stochastic Gradient Descent(10764): loss=1.3393582109991462\n",
      "Stochastic Gradient Descent(10765): loss=8.775753585186976\n",
      "Stochastic Gradient Descent(10766): loss=0.2930435280155766\n",
      "Stochastic Gradient Descent(10767): loss=0.43250577625720277\n",
      "Stochastic Gradient Descent(10768): loss=2.4818126749467178\n",
      "Stochastic Gradient Descent(10769): loss=4.222148402006712\n",
      "Stochastic Gradient Descent(10770): loss=0.7027686806697776\n",
      "Stochastic Gradient Descent(10771): loss=0.8040994215767856\n",
      "Stochastic Gradient Descent(10772): loss=0.7400241338263107\n",
      "Stochastic Gradient Descent(10773): loss=6.4453741590690194\n",
      "Stochastic Gradient Descent(10774): loss=15.855044766278928\n",
      "Stochastic Gradient Descent(10775): loss=0.06516320536483074\n",
      "Stochastic Gradient Descent(10776): loss=8.371828314442906\n",
      "Stochastic Gradient Descent(10777): loss=12.76596508663487\n",
      "Stochastic Gradient Descent(10778): loss=34.591007514042516\n",
      "Stochastic Gradient Descent(10779): loss=27.867862530779583\n",
      "Stochastic Gradient Descent(10780): loss=5.056636552582626\n",
      "Stochastic Gradient Descent(10781): loss=1.1918999762273106\n",
      "Stochastic Gradient Descent(10782): loss=1.762890688436943\n",
      "Stochastic Gradient Descent(10783): loss=1.8683717071147583\n",
      "Stochastic Gradient Descent(10784): loss=21.96900604067708\n",
      "Stochastic Gradient Descent(10785): loss=0.07552591113394493\n",
      "Stochastic Gradient Descent(10786): loss=2.7170448672693652\n",
      "Stochastic Gradient Descent(10787): loss=0.5174517701173463\n",
      "Stochastic Gradient Descent(10788): loss=0.9513473291623373\n",
      "Stochastic Gradient Descent(10789): loss=1.7509553782699954\n",
      "Stochastic Gradient Descent(10790): loss=0.14338004033956575\n",
      "Stochastic Gradient Descent(10791): loss=0.4450118666258786\n",
      "Stochastic Gradient Descent(10792): loss=16.997253757722007\n",
      "Stochastic Gradient Descent(10793): loss=0.5860818085900596\n",
      "Stochastic Gradient Descent(10794): loss=0.14288875994056144\n",
      "Stochastic Gradient Descent(10795): loss=0.02705631850528643\n",
      "Stochastic Gradient Descent(10796): loss=0.006766911493617084\n",
      "Stochastic Gradient Descent(10797): loss=10.008623038524595\n",
      "Stochastic Gradient Descent(10798): loss=4.200695291994352\n",
      "Stochastic Gradient Descent(10799): loss=15.359830221665803\n",
      "Stochastic Gradient Descent(10800): loss=2.4776035559614495\n",
      "Stochastic Gradient Descent(10801): loss=1.7469764769154252\n",
      "Stochastic Gradient Descent(10802): loss=5.251845259625035\n",
      "Stochastic Gradient Descent(10803): loss=0.8523895088932961\n",
      "Stochastic Gradient Descent(10804): loss=5.151750562443346\n",
      "Stochastic Gradient Descent(10805): loss=0.23747383209820655\n",
      "Stochastic Gradient Descent(10806): loss=10.743211359382991\n",
      "Stochastic Gradient Descent(10807): loss=0.057196961416966216\n",
      "Stochastic Gradient Descent(10808): loss=0.12381586925996516\n",
      "Stochastic Gradient Descent(10809): loss=2.5309499473638075\n",
      "Stochastic Gradient Descent(10810): loss=14.592548445700878\n",
      "Stochastic Gradient Descent(10811): loss=24.009163408614864\n",
      "Stochastic Gradient Descent(10812): loss=2.9118823671297225\n",
      "Stochastic Gradient Descent(10813): loss=29.272261093792068\n",
      "Stochastic Gradient Descent(10814): loss=3.102940608831423\n",
      "Stochastic Gradient Descent(10815): loss=0.8471442954706624\n",
      "Stochastic Gradient Descent(10816): loss=0.4676524442621685\n",
      "Stochastic Gradient Descent(10817): loss=0.8258954838722543\n",
      "Stochastic Gradient Descent(10818): loss=9.326351721059142\n",
      "Stochastic Gradient Descent(10819): loss=1.52016322908876\n",
      "Stochastic Gradient Descent(10820): loss=12.845636564885805\n",
      "Stochastic Gradient Descent(10821): loss=25.600484096311554\n",
      "Stochastic Gradient Descent(10822): loss=5.236432519431187\n",
      "Stochastic Gradient Descent(10823): loss=9.785247112496688\n",
      "Stochastic Gradient Descent(10824): loss=11.44627133898409\n",
      "Stochastic Gradient Descent(10825): loss=8.598890631713566\n",
      "Stochastic Gradient Descent(10826): loss=9.476332109896578\n",
      "Stochastic Gradient Descent(10827): loss=5.429108415050244\n",
      "Stochastic Gradient Descent(10828): loss=8.971596185518354\n",
      "Stochastic Gradient Descent(10829): loss=0.0072154694138171615\n",
      "Stochastic Gradient Descent(10830): loss=4.962374401380673\n",
      "Stochastic Gradient Descent(10831): loss=2.0974978134996327\n",
      "Stochastic Gradient Descent(10832): loss=3.9072196617852506\n",
      "Stochastic Gradient Descent(10833): loss=1.1294378416725597\n",
      "Stochastic Gradient Descent(10834): loss=0.0005230088876437401\n",
      "Stochastic Gradient Descent(10835): loss=5.1028767793507654\n",
      "Stochastic Gradient Descent(10836): loss=3.4535135324954997\n",
      "Stochastic Gradient Descent(10837): loss=21.475193597929565\n",
      "Stochastic Gradient Descent(10838): loss=2.3403979474677077\n",
      "Stochastic Gradient Descent(10839): loss=0.42062672796164885\n",
      "Stochastic Gradient Descent(10840): loss=1.8724227204978354\n",
      "Stochastic Gradient Descent(10841): loss=0.02138929042429372\n",
      "Stochastic Gradient Descent(10842): loss=0.11729503175209981\n",
      "Stochastic Gradient Descent(10843): loss=0.7127101960323373\n",
      "Stochastic Gradient Descent(10844): loss=1.0332824391613036\n",
      "Stochastic Gradient Descent(10845): loss=1.9330435825232783\n",
      "Stochastic Gradient Descent(10846): loss=30.21861983769947\n",
      "Stochastic Gradient Descent(10847): loss=0.2973224924523712\n",
      "Stochastic Gradient Descent(10848): loss=6.200880238082836\n",
      "Stochastic Gradient Descent(10849): loss=4.49153877626028\n",
      "Stochastic Gradient Descent(10850): loss=0.0006514917414926537\n",
      "Stochastic Gradient Descent(10851): loss=3.4494040045478145\n",
      "Stochastic Gradient Descent(10852): loss=0.8574535716452081\n",
      "Stochastic Gradient Descent(10853): loss=0.03370648071081921\n",
      "Stochastic Gradient Descent(10854): loss=0.010086237915888691\n",
      "Stochastic Gradient Descent(10855): loss=1.7819324842773425\n",
      "Stochastic Gradient Descent(10856): loss=36.26877398079549\n",
      "Stochastic Gradient Descent(10857): loss=3.515296433822691\n",
      "Stochastic Gradient Descent(10858): loss=6.069472136174991\n",
      "Stochastic Gradient Descent(10859): loss=2.4436777933409126\n",
      "Stochastic Gradient Descent(10860): loss=2.6434103455866307\n",
      "Stochastic Gradient Descent(10861): loss=0.3147619737064314\n",
      "Stochastic Gradient Descent(10862): loss=0.018256149300154307\n",
      "Stochastic Gradient Descent(10863): loss=10.849199049267149\n",
      "Stochastic Gradient Descent(10864): loss=9.718662631465397\n",
      "Stochastic Gradient Descent(10865): loss=0.02246784053068862\n",
      "Stochastic Gradient Descent(10866): loss=2.898645532059039\n",
      "Stochastic Gradient Descent(10867): loss=4.513613327691759\n",
      "Stochastic Gradient Descent(10868): loss=4.442244666323182\n",
      "Stochastic Gradient Descent(10869): loss=15.721161409180949\n",
      "Stochastic Gradient Descent(10870): loss=0.8584949641469588\n",
      "Stochastic Gradient Descent(10871): loss=12.097281989535512\n",
      "Stochastic Gradient Descent(10872): loss=3.4544214175232897\n",
      "Stochastic Gradient Descent(10873): loss=13.7794108587898\n",
      "Stochastic Gradient Descent(10874): loss=0.011330583407376353\n",
      "Stochastic Gradient Descent(10875): loss=18.63118364971906\n",
      "Stochastic Gradient Descent(10876): loss=0.24106964599204775\n",
      "Stochastic Gradient Descent(10877): loss=3.466514292169896\n",
      "Stochastic Gradient Descent(10878): loss=1.2944362108347922\n",
      "Stochastic Gradient Descent(10879): loss=8.645739808680815\n",
      "Stochastic Gradient Descent(10880): loss=0.15120813619319315\n",
      "Stochastic Gradient Descent(10881): loss=1.4079279732838454\n",
      "Stochastic Gradient Descent(10882): loss=1.7417245461164157\n",
      "Stochastic Gradient Descent(10883): loss=6.937157789754856\n",
      "Stochastic Gradient Descent(10884): loss=1.4980669436758205\n",
      "Stochastic Gradient Descent(10885): loss=0.03342839767685932\n",
      "Stochastic Gradient Descent(10886): loss=0.27140429704004326\n",
      "Stochastic Gradient Descent(10887): loss=0.6902859437770931\n",
      "Stochastic Gradient Descent(10888): loss=7.028024460054136\n",
      "Stochastic Gradient Descent(10889): loss=0.2319508844008485\n",
      "Stochastic Gradient Descent(10890): loss=0.3385389451227655\n",
      "Stochastic Gradient Descent(10891): loss=0.39285013465228735\n",
      "Stochastic Gradient Descent(10892): loss=5.087806140917205\n",
      "Stochastic Gradient Descent(10893): loss=4.833163491042081\n",
      "Stochastic Gradient Descent(10894): loss=0.05913054343629946\n",
      "Stochastic Gradient Descent(10895): loss=0.5294107708946264\n",
      "Stochastic Gradient Descent(10896): loss=5.195829009385718\n",
      "Stochastic Gradient Descent(10897): loss=7.445674987777281\n",
      "Stochastic Gradient Descent(10898): loss=3.5471536559347507\n",
      "Stochastic Gradient Descent(10899): loss=3.466421376193908\n",
      "Stochastic Gradient Descent(10900): loss=3.4336185974954514\n",
      "Stochastic Gradient Descent(10901): loss=7.997159895097831\n",
      "Stochastic Gradient Descent(10902): loss=10.821303015189075\n",
      "Stochastic Gradient Descent(10903): loss=1.98106499075162\n",
      "Stochastic Gradient Descent(10904): loss=2.2702860781459484\n",
      "Stochastic Gradient Descent(10905): loss=0.1499669403958848\n",
      "Stochastic Gradient Descent(10906): loss=4.105079471723479\n",
      "Stochastic Gradient Descent(10907): loss=0.4966016240293887\n",
      "Stochastic Gradient Descent(10908): loss=0.2816178367852333\n",
      "Stochastic Gradient Descent(10909): loss=11.217979183417496\n",
      "Stochastic Gradient Descent(10910): loss=0.06326014437011045\n",
      "Stochastic Gradient Descent(10911): loss=0.6246416016655519\n",
      "Stochastic Gradient Descent(10912): loss=4.50647384709685\n",
      "Stochastic Gradient Descent(10913): loss=0.2660214623551627\n",
      "Stochastic Gradient Descent(10914): loss=0.45556490231193714\n",
      "Stochastic Gradient Descent(10915): loss=17.00845160000346\n",
      "Stochastic Gradient Descent(10916): loss=4.793336155674919\n",
      "Stochastic Gradient Descent(10917): loss=0.10364158226514067\n",
      "Stochastic Gradient Descent(10918): loss=4.871515644967528\n",
      "Stochastic Gradient Descent(10919): loss=21.020461988891583\n",
      "Stochastic Gradient Descent(10920): loss=1.1090838449935911\n",
      "Stochastic Gradient Descent(10921): loss=11.94428216638602\n",
      "Stochastic Gradient Descent(10922): loss=0.26089810902593624\n",
      "Stochastic Gradient Descent(10923): loss=11.40955556617917\n",
      "Stochastic Gradient Descent(10924): loss=10.452892387958585\n",
      "Stochastic Gradient Descent(10925): loss=0.20659322872640506\n",
      "Stochastic Gradient Descent(10926): loss=0.42440693666706464\n",
      "Stochastic Gradient Descent(10927): loss=4.37403515836638\n",
      "Stochastic Gradient Descent(10928): loss=0.7071006255079944\n",
      "Stochastic Gradient Descent(10929): loss=3.3137019772207394\n",
      "Stochastic Gradient Descent(10930): loss=0.380291218466791\n",
      "Stochastic Gradient Descent(10931): loss=11.852777621835559\n",
      "Stochastic Gradient Descent(10932): loss=0.024618124891105514\n",
      "Stochastic Gradient Descent(10933): loss=0.11600230733953311\n",
      "Stochastic Gradient Descent(10934): loss=0.63035734806842\n",
      "Stochastic Gradient Descent(10935): loss=5.523540668573231\n",
      "Stochastic Gradient Descent(10936): loss=3.316800952542129\n",
      "Stochastic Gradient Descent(10937): loss=2.658761282245418\n",
      "Stochastic Gradient Descent(10938): loss=0.10555444849834697\n",
      "Stochastic Gradient Descent(10939): loss=1.367047752200882\n",
      "Stochastic Gradient Descent(10940): loss=0.4288768961329686\n",
      "Stochastic Gradient Descent(10941): loss=4.230026816056427\n",
      "Stochastic Gradient Descent(10942): loss=3.2568908938374737\n",
      "Stochastic Gradient Descent(10943): loss=0.026246850242450145\n",
      "Stochastic Gradient Descent(10944): loss=0.024380355162906146\n",
      "Stochastic Gradient Descent(10945): loss=9.467602269918853\n",
      "Stochastic Gradient Descent(10946): loss=0.028093909214251565\n",
      "Stochastic Gradient Descent(10947): loss=3.519222478908118\n",
      "Stochastic Gradient Descent(10948): loss=5.064782568272073\n",
      "Stochastic Gradient Descent(10949): loss=0.02719060774079144\n",
      "Stochastic Gradient Descent(10950): loss=29.45285380608594\n",
      "Stochastic Gradient Descent(10951): loss=1.4806429607040197\n",
      "Stochastic Gradient Descent(10952): loss=9.538178226571395\n",
      "Stochastic Gradient Descent(10953): loss=7.168388971208096\n",
      "Stochastic Gradient Descent(10954): loss=0.23378274231594945\n",
      "Stochastic Gradient Descent(10955): loss=0.37226740605338837\n",
      "Stochastic Gradient Descent(10956): loss=3.815482241393227\n",
      "Stochastic Gradient Descent(10957): loss=1.809276968257287\n",
      "Stochastic Gradient Descent(10958): loss=0.20023389381670534\n",
      "Stochastic Gradient Descent(10959): loss=2.4071410028154623\n",
      "Stochastic Gradient Descent(10960): loss=0.03828944697172197\n",
      "Stochastic Gradient Descent(10961): loss=0.17982589148742725\n",
      "Stochastic Gradient Descent(10962): loss=0.7393487456274647\n",
      "Stochastic Gradient Descent(10963): loss=0.6016933554767749\n",
      "Stochastic Gradient Descent(10964): loss=10.577952442606104\n",
      "Stochastic Gradient Descent(10965): loss=2.214072013301797\n",
      "Stochastic Gradient Descent(10966): loss=6.762272209286688\n",
      "Stochastic Gradient Descent(10967): loss=11.141853733653628\n",
      "Stochastic Gradient Descent(10968): loss=0.18488553072494474\n",
      "Stochastic Gradient Descent(10969): loss=0.6569211032287365\n",
      "Stochastic Gradient Descent(10970): loss=1.071187746088614\n",
      "Stochastic Gradient Descent(10971): loss=0.01547780314143409\n",
      "Stochastic Gradient Descent(10972): loss=16.204235838709938\n",
      "Stochastic Gradient Descent(10973): loss=1.1140431480571569\n",
      "Stochastic Gradient Descent(10974): loss=14.856894803610368\n",
      "Stochastic Gradient Descent(10975): loss=28.998923089575456\n",
      "Stochastic Gradient Descent(10976): loss=1.4575839964875767\n",
      "Stochastic Gradient Descent(10977): loss=9.797088645090687\n",
      "Stochastic Gradient Descent(10978): loss=0.523418413298231\n",
      "Stochastic Gradient Descent(10979): loss=9.58791220600226e-07\n",
      "Stochastic Gradient Descent(10980): loss=15.574539715761821\n",
      "Stochastic Gradient Descent(10981): loss=0.09342221867594648\n",
      "Stochastic Gradient Descent(10982): loss=3.062348215415761\n",
      "Stochastic Gradient Descent(10983): loss=8.941673826443017\n",
      "Stochastic Gradient Descent(10984): loss=12.650778307745933\n",
      "Stochastic Gradient Descent(10985): loss=11.41284192627124\n",
      "Stochastic Gradient Descent(10986): loss=3.028145214917547\n",
      "Stochastic Gradient Descent(10987): loss=0.48683078746706826\n",
      "Stochastic Gradient Descent(10988): loss=0.48790912393632896\n",
      "Stochastic Gradient Descent(10989): loss=22.818322687428555\n",
      "Stochastic Gradient Descent(10990): loss=7.661783508289671\n",
      "Stochastic Gradient Descent(10991): loss=2.2378256637732297\n",
      "Stochastic Gradient Descent(10992): loss=0.33310014238484215\n",
      "Stochastic Gradient Descent(10993): loss=0.3293215112184667\n",
      "Stochastic Gradient Descent(10994): loss=0.07971176377936436\n",
      "Stochastic Gradient Descent(10995): loss=0.5760902712410678\n",
      "Stochastic Gradient Descent(10996): loss=2.5249034630062615\n",
      "Stochastic Gradient Descent(10997): loss=10.501304741863848\n",
      "Stochastic Gradient Descent(10998): loss=3.983870362630165\n",
      "Stochastic Gradient Descent(10999): loss=5.10280224006823\n",
      "Stochastic Gradient Descent(11000): loss=18.91298379633323\n",
      "Stochastic Gradient Descent(11001): loss=0.46774405241018313\n",
      "Stochastic Gradient Descent(11002): loss=0.3084495586842056\n",
      "Stochastic Gradient Descent(11003): loss=0.09127598390338225\n",
      "Stochastic Gradient Descent(11004): loss=0.07665246073502029\n",
      "Stochastic Gradient Descent(11005): loss=3.9454625446027545\n",
      "Stochastic Gradient Descent(11006): loss=0.1684375691371724\n",
      "Stochastic Gradient Descent(11007): loss=0.0787403210604482\n",
      "Stochastic Gradient Descent(11008): loss=0.3636601769138924\n",
      "Stochastic Gradient Descent(11009): loss=9.975643841254051\n",
      "Stochastic Gradient Descent(11010): loss=0.076912160312326\n",
      "Stochastic Gradient Descent(11011): loss=0.18076731438280946\n",
      "Stochastic Gradient Descent(11012): loss=6.577020479698389\n",
      "Stochastic Gradient Descent(11013): loss=10.25511057899672\n",
      "Stochastic Gradient Descent(11014): loss=4.82758019263772\n",
      "Stochastic Gradient Descent(11015): loss=11.860715490954897\n",
      "Stochastic Gradient Descent(11016): loss=0.1337538536506119\n",
      "Stochastic Gradient Descent(11017): loss=1.387717875315913\n",
      "Stochastic Gradient Descent(11018): loss=10.714978756065879\n",
      "Stochastic Gradient Descent(11019): loss=0.00696062687385024\n",
      "Stochastic Gradient Descent(11020): loss=3.1213025128404546\n",
      "Stochastic Gradient Descent(11021): loss=6.032999439977768\n",
      "Stochastic Gradient Descent(11022): loss=11.538822630875357\n",
      "Stochastic Gradient Descent(11023): loss=18.64579485277594\n",
      "Stochastic Gradient Descent(11024): loss=19.977940219921905\n",
      "Stochastic Gradient Descent(11025): loss=9.585387423375817\n",
      "Stochastic Gradient Descent(11026): loss=0.39563508499952393\n",
      "Stochastic Gradient Descent(11027): loss=10.102881325457329\n",
      "Stochastic Gradient Descent(11028): loss=0.034288249928775175\n",
      "Stochastic Gradient Descent(11029): loss=2.687075364287188\n",
      "Stochastic Gradient Descent(11030): loss=13.104707604566626\n",
      "Stochastic Gradient Descent(11031): loss=25.063344179301822\n",
      "Stochastic Gradient Descent(11032): loss=0.2883441315054272\n",
      "Stochastic Gradient Descent(11033): loss=1.3200818422064324\n",
      "Stochastic Gradient Descent(11034): loss=2.1510758642958274\n",
      "Stochastic Gradient Descent(11035): loss=8.680862730628332\n",
      "Stochastic Gradient Descent(11036): loss=4.487627979620855\n",
      "Stochastic Gradient Descent(11037): loss=16.812419000606404\n",
      "Stochastic Gradient Descent(11038): loss=4.364564161684389\n",
      "Stochastic Gradient Descent(11039): loss=4.668916180354683\n",
      "Stochastic Gradient Descent(11040): loss=2.86322585500751\n",
      "Stochastic Gradient Descent(11041): loss=0.11690827743199142\n",
      "Stochastic Gradient Descent(11042): loss=5.849498375981888\n",
      "Stochastic Gradient Descent(11043): loss=0.04815632961199382\n",
      "Stochastic Gradient Descent(11044): loss=3.7077534340124885\n",
      "Stochastic Gradient Descent(11045): loss=1.597268813928473\n",
      "Stochastic Gradient Descent(11046): loss=6.960390704062051\n",
      "Stochastic Gradient Descent(11047): loss=21.200668858663953\n",
      "Stochastic Gradient Descent(11048): loss=2.201049836653634\n",
      "Stochastic Gradient Descent(11049): loss=4.989345076353456\n",
      "Stochastic Gradient Descent(11050): loss=0.2617991955296923\n",
      "Stochastic Gradient Descent(11051): loss=13.603481823985991\n",
      "Stochastic Gradient Descent(11052): loss=0.0009130465503460707\n",
      "Stochastic Gradient Descent(11053): loss=0.18175606047941606\n",
      "Stochastic Gradient Descent(11054): loss=0.505054629629352\n",
      "Stochastic Gradient Descent(11055): loss=4.180470930085592\n",
      "Stochastic Gradient Descent(11056): loss=1.688720454294225\n",
      "Stochastic Gradient Descent(11057): loss=0.2447816683932394\n",
      "Stochastic Gradient Descent(11058): loss=0.08837215846220337\n",
      "Stochastic Gradient Descent(11059): loss=3.8015553317467545\n",
      "Stochastic Gradient Descent(11060): loss=7.574721293826523\n",
      "Stochastic Gradient Descent(11061): loss=0.16243538488165235\n",
      "Stochastic Gradient Descent(11062): loss=0.0754125545900986\n",
      "Stochastic Gradient Descent(11063): loss=2.595680216167507\n",
      "Stochastic Gradient Descent(11064): loss=0.38128142674577536\n",
      "Stochastic Gradient Descent(11065): loss=1.0135408944928215\n",
      "Stochastic Gradient Descent(11066): loss=2.082110668111748\n",
      "Stochastic Gradient Descent(11067): loss=5.584858592926952\n",
      "Stochastic Gradient Descent(11068): loss=7.452932401263224\n",
      "Stochastic Gradient Descent(11069): loss=19.218641872278244\n",
      "Stochastic Gradient Descent(11070): loss=3.357484095753499\n",
      "Stochastic Gradient Descent(11071): loss=2.1133223198587148\n",
      "Stochastic Gradient Descent(11072): loss=6.8604697846401\n",
      "Stochastic Gradient Descent(11073): loss=2.3897300088455187\n",
      "Stochastic Gradient Descent(11074): loss=2.917369490437657\n",
      "Stochastic Gradient Descent(11075): loss=9.507301177770168\n",
      "Stochastic Gradient Descent(11076): loss=1.837640611603453\n",
      "Stochastic Gradient Descent(11077): loss=6.804967576728916\n",
      "Stochastic Gradient Descent(11078): loss=23.35433129648969\n",
      "Stochastic Gradient Descent(11079): loss=0.2622875408044126\n",
      "Stochastic Gradient Descent(11080): loss=12.229286121827354\n",
      "Stochastic Gradient Descent(11081): loss=3.6443655485985293\n",
      "Stochastic Gradient Descent(11082): loss=0.16688249089924748\n",
      "Stochastic Gradient Descent(11083): loss=2.8446551154796147\n",
      "Stochastic Gradient Descent(11084): loss=0.6271506312463822\n",
      "Stochastic Gradient Descent(11085): loss=5.666407313940729\n",
      "Stochastic Gradient Descent(11086): loss=0.00047544714491408544\n",
      "Stochastic Gradient Descent(11087): loss=0.17696353295171816\n",
      "Stochastic Gradient Descent(11088): loss=0.08650708147535953\n",
      "Stochastic Gradient Descent(11089): loss=1.584326494684727\n",
      "Stochastic Gradient Descent(11090): loss=0.9292647690797347\n",
      "Stochastic Gradient Descent(11091): loss=0.002666902983019156\n",
      "Stochastic Gradient Descent(11092): loss=2.2809578841363654\n",
      "Stochastic Gradient Descent(11093): loss=0.6952743438182344\n",
      "Stochastic Gradient Descent(11094): loss=0.03901636076267058\n",
      "Stochastic Gradient Descent(11095): loss=1.8818697669151285\n",
      "Stochastic Gradient Descent(11096): loss=0.0023954838003296904\n",
      "Stochastic Gradient Descent(11097): loss=0.4870862561967031\n",
      "Stochastic Gradient Descent(11098): loss=0.28361193059064094\n",
      "Stochastic Gradient Descent(11099): loss=3.5951321896224844\n",
      "Stochastic Gradient Descent(11100): loss=3.1301036795357686\n",
      "Stochastic Gradient Descent(11101): loss=2.0571955615294115\n",
      "Stochastic Gradient Descent(11102): loss=4.709615417909661\n",
      "Stochastic Gradient Descent(11103): loss=0.17668903887450454\n",
      "Stochastic Gradient Descent(11104): loss=5.8599024716496215\n",
      "Stochastic Gradient Descent(11105): loss=2.2462200655342035\n",
      "Stochastic Gradient Descent(11106): loss=5.859895049518228\n",
      "Stochastic Gradient Descent(11107): loss=0.47820653682979786\n",
      "Stochastic Gradient Descent(11108): loss=0.9568194201320283\n",
      "Stochastic Gradient Descent(11109): loss=8.967907195640352\n",
      "Stochastic Gradient Descent(11110): loss=21.739423285584326\n",
      "Stochastic Gradient Descent(11111): loss=0.528729663182341\n",
      "Stochastic Gradient Descent(11112): loss=1.3660668187649874\n",
      "Stochastic Gradient Descent(11113): loss=7.899804400265525\n",
      "Stochastic Gradient Descent(11114): loss=0.9598466806642111\n",
      "Stochastic Gradient Descent(11115): loss=19.438803179338745\n",
      "Stochastic Gradient Descent(11116): loss=4.226470455695193\n",
      "Stochastic Gradient Descent(11117): loss=2.4757461970693564\n",
      "Stochastic Gradient Descent(11118): loss=0.5580678676532954\n",
      "Stochastic Gradient Descent(11119): loss=2.7309600749051235\n",
      "Stochastic Gradient Descent(11120): loss=3.975421476324755\n",
      "Stochastic Gradient Descent(11121): loss=0.06762266703498758\n",
      "Stochastic Gradient Descent(11122): loss=6.543643041383722\n",
      "Stochastic Gradient Descent(11123): loss=0.0017190589413874288\n",
      "Stochastic Gradient Descent(11124): loss=20.556423669037674\n",
      "Stochastic Gradient Descent(11125): loss=0.13231151583281536\n",
      "Stochastic Gradient Descent(11126): loss=7.686981663656788\n",
      "Stochastic Gradient Descent(11127): loss=5.740529687656523\n",
      "Stochastic Gradient Descent(11128): loss=9.146998416318302\n",
      "Stochastic Gradient Descent(11129): loss=7.332006860908197\n",
      "Stochastic Gradient Descent(11130): loss=0.13293609010074714\n",
      "Stochastic Gradient Descent(11131): loss=11.885644301948059\n",
      "Stochastic Gradient Descent(11132): loss=11.466540754224651\n",
      "Stochastic Gradient Descent(11133): loss=12.649671315832437\n",
      "Stochastic Gradient Descent(11134): loss=0.008061322168007852\n",
      "Stochastic Gradient Descent(11135): loss=8.793659508628956\n",
      "Stochastic Gradient Descent(11136): loss=25.69832993690096\n",
      "Stochastic Gradient Descent(11137): loss=37.138167052709115\n",
      "Stochastic Gradient Descent(11138): loss=0.7882798169695333\n",
      "Stochastic Gradient Descent(11139): loss=8.786710238025448\n",
      "Stochastic Gradient Descent(11140): loss=1.8435650554919016\n",
      "Stochastic Gradient Descent(11141): loss=4.435445108797474\n",
      "Stochastic Gradient Descent(11142): loss=1.7289651655790583\n",
      "Stochastic Gradient Descent(11143): loss=2.2037236477601034\n",
      "Stochastic Gradient Descent(11144): loss=0.005649363880359086\n",
      "Stochastic Gradient Descent(11145): loss=0.04183820813118544\n",
      "Stochastic Gradient Descent(11146): loss=4.013352234318782\n",
      "Stochastic Gradient Descent(11147): loss=5.849477260683225\n",
      "Stochastic Gradient Descent(11148): loss=72.1207192587767\n",
      "Stochastic Gradient Descent(11149): loss=20.41474652402968\n",
      "Stochastic Gradient Descent(11150): loss=8.88977458819118\n",
      "Stochastic Gradient Descent(11151): loss=0.6443058206802723\n",
      "Stochastic Gradient Descent(11152): loss=12.724594530433695\n",
      "Stochastic Gradient Descent(11153): loss=9.148072708943136\n",
      "Stochastic Gradient Descent(11154): loss=0.5840727175078884\n",
      "Stochastic Gradient Descent(11155): loss=0.09068516887988902\n",
      "Stochastic Gradient Descent(11156): loss=0.22901100597831994\n",
      "Stochastic Gradient Descent(11157): loss=14.231575188802797\n",
      "Stochastic Gradient Descent(11158): loss=2.3809826453860214\n",
      "Stochastic Gradient Descent(11159): loss=1.6154876220517835\n",
      "Stochastic Gradient Descent(11160): loss=4.326762515789119\n",
      "Stochastic Gradient Descent(11161): loss=7.887844081932662\n",
      "Stochastic Gradient Descent(11162): loss=0.11198165577902545\n",
      "Stochastic Gradient Descent(11163): loss=4.94375623262787\n",
      "Stochastic Gradient Descent(11164): loss=1.1115191121816297\n",
      "Stochastic Gradient Descent(11165): loss=0.367779236357674\n",
      "Stochastic Gradient Descent(11166): loss=2.948776623211606\n",
      "Stochastic Gradient Descent(11167): loss=2.631048366440373\n",
      "Stochastic Gradient Descent(11168): loss=1.9355526870763289\n",
      "Stochastic Gradient Descent(11169): loss=8.604962782558722\n",
      "Stochastic Gradient Descent(11170): loss=4.994515217572858\n",
      "Stochastic Gradient Descent(11171): loss=2.6672968569991733\n",
      "Stochastic Gradient Descent(11172): loss=0.004971516934289637\n",
      "Stochastic Gradient Descent(11173): loss=0.17073642405470496\n",
      "Stochastic Gradient Descent(11174): loss=0.3735835373551839\n",
      "Stochastic Gradient Descent(11175): loss=4.278244468391378\n",
      "Stochastic Gradient Descent(11176): loss=9.496719993784826\n",
      "Stochastic Gradient Descent(11177): loss=1.6331642073178287\n",
      "Stochastic Gradient Descent(11178): loss=4.071966685335137\n",
      "Stochastic Gradient Descent(11179): loss=2.936633128435346\n",
      "Stochastic Gradient Descent(11180): loss=2.4428625848918015\n",
      "Stochastic Gradient Descent(11181): loss=1.3912684137853215\n",
      "Stochastic Gradient Descent(11182): loss=22.794280659769456\n",
      "Stochastic Gradient Descent(11183): loss=0.005333080031531044\n",
      "Stochastic Gradient Descent(11184): loss=0.577778326408593\n",
      "Stochastic Gradient Descent(11185): loss=15.237690794632416\n",
      "Stochastic Gradient Descent(11186): loss=2.8847213781905294\n",
      "Stochastic Gradient Descent(11187): loss=3.7421395812847837\n",
      "Stochastic Gradient Descent(11188): loss=0.746049610079926\n",
      "Stochastic Gradient Descent(11189): loss=1.2053990067399785\n",
      "Stochastic Gradient Descent(11190): loss=0.12371050272622265\n",
      "Stochastic Gradient Descent(11191): loss=0.007795504922978621\n",
      "Stochastic Gradient Descent(11192): loss=2.283378821004494\n",
      "Stochastic Gradient Descent(11193): loss=0.3072982090440092\n",
      "Stochastic Gradient Descent(11194): loss=18.571365503122134\n",
      "Stochastic Gradient Descent(11195): loss=0.3104592572360395\n",
      "Stochastic Gradient Descent(11196): loss=0.8262248073788762\n",
      "Stochastic Gradient Descent(11197): loss=8.676790584381083\n",
      "Stochastic Gradient Descent(11198): loss=0.47643488908022674\n",
      "Stochastic Gradient Descent(11199): loss=0.7370108433973304\n",
      "Stochastic Gradient Descent(11200): loss=0.31713356540461707\n",
      "Stochastic Gradient Descent(11201): loss=6.839771064347475\n",
      "Stochastic Gradient Descent(11202): loss=0.00048821790322490106\n",
      "Stochastic Gradient Descent(11203): loss=0.26496489684917224\n",
      "Stochastic Gradient Descent(11204): loss=34.25207785572154\n",
      "Stochastic Gradient Descent(11205): loss=8.876385364702099\n",
      "Stochastic Gradient Descent(11206): loss=5.508628216709686\n",
      "Stochastic Gradient Descent(11207): loss=1.5526835080166332\n",
      "Stochastic Gradient Descent(11208): loss=9.647642581574132\n",
      "Stochastic Gradient Descent(11209): loss=6.499502749315311\n",
      "Stochastic Gradient Descent(11210): loss=22.922474329034277\n",
      "Stochastic Gradient Descent(11211): loss=3.319766418867229\n",
      "Stochastic Gradient Descent(11212): loss=0.7097379792874712\n",
      "Stochastic Gradient Descent(11213): loss=3.570858774516867\n",
      "Stochastic Gradient Descent(11214): loss=0.5597602055339259\n",
      "Stochastic Gradient Descent(11215): loss=3.069120445107114\n",
      "Stochastic Gradient Descent(11216): loss=71.52432315610312\n",
      "Stochastic Gradient Descent(11217): loss=21.739629460027082\n",
      "Stochastic Gradient Descent(11218): loss=54.56296163485134\n",
      "Stochastic Gradient Descent(11219): loss=14.98283348300691\n",
      "Stochastic Gradient Descent(11220): loss=34.644131374640196\n",
      "Stochastic Gradient Descent(11221): loss=0.00014740596291101023\n",
      "Stochastic Gradient Descent(11222): loss=3.1658251002392754\n",
      "Stochastic Gradient Descent(11223): loss=58.48486403162899\n",
      "Stochastic Gradient Descent(11224): loss=3.554486854650585\n",
      "Stochastic Gradient Descent(11225): loss=2.821423473256873\n",
      "Stochastic Gradient Descent(11226): loss=1.6528871484558716\n",
      "Stochastic Gradient Descent(11227): loss=5.177557744826333\n",
      "Stochastic Gradient Descent(11228): loss=7.860262259565554\n",
      "Stochastic Gradient Descent(11229): loss=0.29699901340025614\n",
      "Stochastic Gradient Descent(11230): loss=12.405295098283624\n",
      "Stochastic Gradient Descent(11231): loss=4.777224504511285\n",
      "Stochastic Gradient Descent(11232): loss=0.0073628628066072415\n",
      "Stochastic Gradient Descent(11233): loss=1.5327385852217028\n",
      "Stochastic Gradient Descent(11234): loss=0.5483260160693743\n",
      "Stochastic Gradient Descent(11235): loss=13.460274322106123\n",
      "Stochastic Gradient Descent(11236): loss=0.04366269003124261\n",
      "Stochastic Gradient Descent(11237): loss=1.2240613107361091\n",
      "Stochastic Gradient Descent(11238): loss=10.770370118642601\n",
      "Stochastic Gradient Descent(11239): loss=24.275333961456877\n",
      "Stochastic Gradient Descent(11240): loss=3.7865718706507114\n",
      "Stochastic Gradient Descent(11241): loss=0.9969078360906096\n",
      "Stochastic Gradient Descent(11242): loss=7.030759058026083\n",
      "Stochastic Gradient Descent(11243): loss=0.05590653547602572\n",
      "Stochastic Gradient Descent(11244): loss=0.660681023312938\n",
      "Stochastic Gradient Descent(11245): loss=6.997360362167713\n",
      "Stochastic Gradient Descent(11246): loss=0.3228214793679469\n",
      "Stochastic Gradient Descent(11247): loss=1.4715504673021358\n",
      "Stochastic Gradient Descent(11248): loss=1.7631040227972123\n",
      "Stochastic Gradient Descent(11249): loss=19.571389635622054\n",
      "Stochastic Gradient Descent(11250): loss=2.203204689789222\n",
      "Stochastic Gradient Descent(11251): loss=4.485630674347912\n",
      "Stochastic Gradient Descent(11252): loss=3.708980084727772\n",
      "Stochastic Gradient Descent(11253): loss=0.3700328018245417\n",
      "Stochastic Gradient Descent(11254): loss=0.0020624728358386957\n",
      "Stochastic Gradient Descent(11255): loss=0.15069859781595804\n",
      "Stochastic Gradient Descent(11256): loss=0.9897945401509987\n",
      "Stochastic Gradient Descent(11257): loss=4.405551612606231\n",
      "Stochastic Gradient Descent(11258): loss=0.4290562286753348\n",
      "Stochastic Gradient Descent(11259): loss=0.21298871117512136\n",
      "Stochastic Gradient Descent(11260): loss=14.205130027272581\n",
      "Stochastic Gradient Descent(11261): loss=0.03562848958494116\n",
      "Stochastic Gradient Descent(11262): loss=2.9972760822131983\n",
      "Stochastic Gradient Descent(11263): loss=0.16739395091169657\n",
      "Stochastic Gradient Descent(11264): loss=0.2753745663124585\n",
      "Stochastic Gradient Descent(11265): loss=4.407292032430214\n",
      "Stochastic Gradient Descent(11266): loss=2.979618042509723\n",
      "Stochastic Gradient Descent(11267): loss=5.768088528215923\n",
      "Stochastic Gradient Descent(11268): loss=0.9622938134330322\n",
      "Stochastic Gradient Descent(11269): loss=1.4625962786338111\n",
      "Stochastic Gradient Descent(11270): loss=3.151519106789575\n",
      "Stochastic Gradient Descent(11271): loss=13.367782480229746\n",
      "Stochastic Gradient Descent(11272): loss=15.002234156041329\n",
      "Stochastic Gradient Descent(11273): loss=0.07558940166365008\n",
      "Stochastic Gradient Descent(11274): loss=5.813233301545772\n",
      "Stochastic Gradient Descent(11275): loss=4.304656933245259\n",
      "Stochastic Gradient Descent(11276): loss=2.166661996172805\n",
      "Stochastic Gradient Descent(11277): loss=2.628190852439638\n",
      "Stochastic Gradient Descent(11278): loss=18.774365303831306\n",
      "Stochastic Gradient Descent(11279): loss=29.947115252955385\n",
      "Stochastic Gradient Descent(11280): loss=6.945872725574355\n",
      "Stochastic Gradient Descent(11281): loss=3.729640742166654\n",
      "Stochastic Gradient Descent(11282): loss=1.4811632693298191\n",
      "Stochastic Gradient Descent(11283): loss=0.6064279410832927\n",
      "Stochastic Gradient Descent(11284): loss=0.004915172093013428\n",
      "Stochastic Gradient Descent(11285): loss=39.316748455006646\n",
      "Stochastic Gradient Descent(11286): loss=16.41840409485131\n",
      "Stochastic Gradient Descent(11287): loss=4.008136649823368\n",
      "Stochastic Gradient Descent(11288): loss=1.0577626469454584\n",
      "Stochastic Gradient Descent(11289): loss=5.690571352122246\n",
      "Stochastic Gradient Descent(11290): loss=15.89247682155864\n",
      "Stochastic Gradient Descent(11291): loss=0.49204690732923756\n",
      "Stochastic Gradient Descent(11292): loss=25.388382721994834\n",
      "Stochastic Gradient Descent(11293): loss=58.792748675137496\n",
      "Stochastic Gradient Descent(11294): loss=0.02444403767646455\n",
      "Stochastic Gradient Descent(11295): loss=36.44256474204621\n",
      "Stochastic Gradient Descent(11296): loss=4.08141617231554\n",
      "Stochastic Gradient Descent(11297): loss=41.62236810519499\n",
      "Stochastic Gradient Descent(11298): loss=0.927912435041882\n",
      "Stochastic Gradient Descent(11299): loss=2.085626104515218\n",
      "Stochastic Gradient Descent(11300): loss=0.27875873044648575\n",
      "Stochastic Gradient Descent(11301): loss=1.0549697553497037\n",
      "Stochastic Gradient Descent(11302): loss=18.73254896738104\n",
      "Stochastic Gradient Descent(11303): loss=0.125549427744863\n",
      "Stochastic Gradient Descent(11304): loss=2.2285722988571206\n",
      "Stochastic Gradient Descent(11305): loss=0.02111172060693509\n",
      "Stochastic Gradient Descent(11306): loss=3.7182639847161028\n",
      "Stochastic Gradient Descent(11307): loss=8.132894227144462\n",
      "Stochastic Gradient Descent(11308): loss=19.773467623436986\n",
      "Stochastic Gradient Descent(11309): loss=2.419966796499473\n",
      "Stochastic Gradient Descent(11310): loss=0.1229543604388718\n",
      "Stochastic Gradient Descent(11311): loss=0.3463639581796219\n",
      "Stochastic Gradient Descent(11312): loss=4.95443846136738\n",
      "Stochastic Gradient Descent(11313): loss=1.7864645220797122\n",
      "Stochastic Gradient Descent(11314): loss=12.281141592154052\n",
      "Stochastic Gradient Descent(11315): loss=14.494206865081665\n",
      "Stochastic Gradient Descent(11316): loss=3.802716916791187\n",
      "Stochastic Gradient Descent(11317): loss=2.2220186001861055\n",
      "Stochastic Gradient Descent(11318): loss=4.741950971284447\n",
      "Stochastic Gradient Descent(11319): loss=0.1417523836038076\n",
      "Stochastic Gradient Descent(11320): loss=3.2552728768213592\n",
      "Stochastic Gradient Descent(11321): loss=0.900034397437433\n",
      "Stochastic Gradient Descent(11322): loss=0.6712926331260919\n",
      "Stochastic Gradient Descent(11323): loss=3.113401600961555\n",
      "Stochastic Gradient Descent(11324): loss=10.172636427066813\n",
      "Stochastic Gradient Descent(11325): loss=0.15199487910879855\n",
      "Stochastic Gradient Descent(11326): loss=1.1458680957473169\n",
      "Stochastic Gradient Descent(11327): loss=0.0380983869164492\n",
      "Stochastic Gradient Descent(11328): loss=28.001465702030753\n",
      "Stochastic Gradient Descent(11329): loss=12.36161582631457\n",
      "Stochastic Gradient Descent(11330): loss=3.758392896839427\n",
      "Stochastic Gradient Descent(11331): loss=19.411094763610745\n",
      "Stochastic Gradient Descent(11332): loss=0.7231646649324375\n",
      "Stochastic Gradient Descent(11333): loss=0.08974184345447275\n",
      "Stochastic Gradient Descent(11334): loss=1.8066606539800023\n",
      "Stochastic Gradient Descent(11335): loss=6.584298422685628\n",
      "Stochastic Gradient Descent(11336): loss=2.837905267083119\n",
      "Stochastic Gradient Descent(11337): loss=0.1801055607856835\n",
      "Stochastic Gradient Descent(11338): loss=0.1101758303141765\n",
      "Stochastic Gradient Descent(11339): loss=2.4943049389747536\n",
      "Stochastic Gradient Descent(11340): loss=0.3766895597769569\n",
      "Stochastic Gradient Descent(11341): loss=0.3050269830453799\n",
      "Stochastic Gradient Descent(11342): loss=0.9502641485710267\n",
      "Stochastic Gradient Descent(11343): loss=8.203042266579933\n",
      "Stochastic Gradient Descent(11344): loss=4.082034466163528\n",
      "Stochastic Gradient Descent(11345): loss=12.641689102981587\n",
      "Stochastic Gradient Descent(11346): loss=0.07618802727667631\n",
      "Stochastic Gradient Descent(11347): loss=3.7956272099946373\n",
      "Stochastic Gradient Descent(11348): loss=0.05863830851558748\n",
      "Stochastic Gradient Descent(11349): loss=0.12038630306194634\n",
      "Stochastic Gradient Descent(11350): loss=9.304977212240372\n",
      "Stochastic Gradient Descent(11351): loss=7.9145653586301865\n",
      "Stochastic Gradient Descent(11352): loss=0.022467686637002538\n",
      "Stochastic Gradient Descent(11353): loss=9.913310073234602\n",
      "Stochastic Gradient Descent(11354): loss=0.02137391553568767\n",
      "Stochastic Gradient Descent(11355): loss=0.5218859990764578\n",
      "Stochastic Gradient Descent(11356): loss=0.05466661590855588\n",
      "Stochastic Gradient Descent(11357): loss=2.21823609535783\n",
      "Stochastic Gradient Descent(11358): loss=0.1121557709661069\n",
      "Stochastic Gradient Descent(11359): loss=0.306889641362087\n",
      "Stochastic Gradient Descent(11360): loss=7.0045047943168175\n",
      "Stochastic Gradient Descent(11361): loss=1.7719567719900142\n",
      "Stochastic Gradient Descent(11362): loss=3.786013301053755\n",
      "Stochastic Gradient Descent(11363): loss=0.017064200167987747\n",
      "Stochastic Gradient Descent(11364): loss=0.09474422023253436\n",
      "Stochastic Gradient Descent(11365): loss=1.9110476791921218\n",
      "Stochastic Gradient Descent(11366): loss=2.515556559365927\n",
      "Stochastic Gradient Descent(11367): loss=1.052622897545222\n",
      "Stochastic Gradient Descent(11368): loss=1.634826781705221\n",
      "Stochastic Gradient Descent(11369): loss=1.6775792672769259\n",
      "Stochastic Gradient Descent(11370): loss=16.735852905845487\n",
      "Stochastic Gradient Descent(11371): loss=0.5830400260687661\n",
      "Stochastic Gradient Descent(11372): loss=1.991733189296498\n",
      "Stochastic Gradient Descent(11373): loss=16.390991801283324\n",
      "Stochastic Gradient Descent(11374): loss=16.545827182951463\n",
      "Stochastic Gradient Descent(11375): loss=21.9576971308109\n",
      "Stochastic Gradient Descent(11376): loss=3.342413246708768\n",
      "Stochastic Gradient Descent(11377): loss=21.224126921562068\n",
      "Stochastic Gradient Descent(11378): loss=9.560440620838522\n",
      "Stochastic Gradient Descent(11379): loss=1.4611388205195344\n",
      "Stochastic Gradient Descent(11380): loss=0.8672828423423508\n",
      "Stochastic Gradient Descent(11381): loss=0.3773107896679486\n",
      "Stochastic Gradient Descent(11382): loss=7.313192285912138\n",
      "Stochastic Gradient Descent(11383): loss=1.4145635290257104\n",
      "Stochastic Gradient Descent(11384): loss=7.818732938188171\n",
      "Stochastic Gradient Descent(11385): loss=2.6394443212647114\n",
      "Stochastic Gradient Descent(11386): loss=0.25043769686059325\n",
      "Stochastic Gradient Descent(11387): loss=0.3957588770405673\n",
      "Stochastic Gradient Descent(11388): loss=13.99949035483313\n",
      "Stochastic Gradient Descent(11389): loss=0.717461484358578\n",
      "Stochastic Gradient Descent(11390): loss=5.295195941903419\n",
      "Stochastic Gradient Descent(11391): loss=5.456642769216745\n",
      "Stochastic Gradient Descent(11392): loss=1.6994112765206133\n",
      "Stochastic Gradient Descent(11393): loss=0.6158537663094141\n",
      "Stochastic Gradient Descent(11394): loss=0.25577190886732976\n",
      "Stochastic Gradient Descent(11395): loss=3.739427361741365\n",
      "Stochastic Gradient Descent(11396): loss=3.4518192222696413\n",
      "Stochastic Gradient Descent(11397): loss=0.9517216869708224\n",
      "Stochastic Gradient Descent(11398): loss=0.019033042925494476\n",
      "Stochastic Gradient Descent(11399): loss=5.381785920727575\n",
      "Stochastic Gradient Descent(11400): loss=3.4680112304728636\n",
      "Stochastic Gradient Descent(11401): loss=0.7554896707620399\n",
      "Stochastic Gradient Descent(11402): loss=10.4087231850339\n",
      "Stochastic Gradient Descent(11403): loss=2.8612014289746472\n",
      "Stochastic Gradient Descent(11404): loss=5.872678858656483\n",
      "Stochastic Gradient Descent(11405): loss=0.26708012335300235\n",
      "Stochastic Gradient Descent(11406): loss=0.21040299943944504\n",
      "Stochastic Gradient Descent(11407): loss=1.9923520044713237\n",
      "Stochastic Gradient Descent(11408): loss=1.8009682370776465\n",
      "Stochastic Gradient Descent(11409): loss=7.549429513247817\n",
      "Stochastic Gradient Descent(11410): loss=0.4537812368610554\n",
      "Stochastic Gradient Descent(11411): loss=2.5868335189803853\n",
      "Stochastic Gradient Descent(11412): loss=0.9601603864607412\n",
      "Stochastic Gradient Descent(11413): loss=1.767308260955187\n",
      "Stochastic Gradient Descent(11414): loss=0.28109250269302544\n",
      "Stochastic Gradient Descent(11415): loss=0.08968854797445522\n",
      "Stochastic Gradient Descent(11416): loss=0.8071962507179762\n",
      "Stochastic Gradient Descent(11417): loss=2.9230912691472235\n",
      "Stochastic Gradient Descent(11418): loss=2.340820094044678\n",
      "Stochastic Gradient Descent(11419): loss=23.90770317213777\n",
      "Stochastic Gradient Descent(11420): loss=35.051303600483784\n",
      "Stochastic Gradient Descent(11421): loss=3.840966915592478\n",
      "Stochastic Gradient Descent(11422): loss=14.836929432008711\n",
      "Stochastic Gradient Descent(11423): loss=18.076537231798934\n",
      "Stochastic Gradient Descent(11424): loss=1.248120904714788\n",
      "Stochastic Gradient Descent(11425): loss=1.9257992048202035\n",
      "Stochastic Gradient Descent(11426): loss=0.607413301505746\n",
      "Stochastic Gradient Descent(11427): loss=0.016575363174326587\n",
      "Stochastic Gradient Descent(11428): loss=26.064091374047827\n",
      "Stochastic Gradient Descent(11429): loss=7.238161578884852\n",
      "Stochastic Gradient Descent(11430): loss=13.18036283696724\n",
      "Stochastic Gradient Descent(11431): loss=28.76945322062695\n",
      "Stochastic Gradient Descent(11432): loss=9.655774615096803\n",
      "Stochastic Gradient Descent(11433): loss=3.180796479178643\n",
      "Stochastic Gradient Descent(11434): loss=5.875148699601986\n",
      "Stochastic Gradient Descent(11435): loss=3.939999008287928\n",
      "Stochastic Gradient Descent(11436): loss=0.0771756505656265\n",
      "Stochastic Gradient Descent(11437): loss=0.6144119872963709\n",
      "Stochastic Gradient Descent(11438): loss=6.1810526809097315\n",
      "Stochastic Gradient Descent(11439): loss=2.900893813948094\n",
      "Stochastic Gradient Descent(11440): loss=0.2412238432373331\n",
      "Stochastic Gradient Descent(11441): loss=1.190645045028239\n",
      "Stochastic Gradient Descent(11442): loss=5.03554602204779\n",
      "Stochastic Gradient Descent(11443): loss=1.8208129645670015\n",
      "Stochastic Gradient Descent(11444): loss=2.087167465401922\n",
      "Stochastic Gradient Descent(11445): loss=3.992581757646348\n",
      "Stochastic Gradient Descent(11446): loss=8.024694142706954\n",
      "Stochastic Gradient Descent(11447): loss=5.442199490967435\n",
      "Stochastic Gradient Descent(11448): loss=14.69139951622352\n",
      "Stochastic Gradient Descent(11449): loss=1.49747607953917\n",
      "Stochastic Gradient Descent(11450): loss=8.846984976406002\n",
      "Stochastic Gradient Descent(11451): loss=5.7455071686258234\n",
      "Stochastic Gradient Descent(11452): loss=3.055048330453555\n",
      "Stochastic Gradient Descent(11453): loss=22.9437265710613\n",
      "Stochastic Gradient Descent(11454): loss=0.7529866768249743\n",
      "Stochastic Gradient Descent(11455): loss=10.479238991704324\n",
      "Stochastic Gradient Descent(11456): loss=35.03308006349018\n",
      "Stochastic Gradient Descent(11457): loss=42.270832467280044\n",
      "Stochastic Gradient Descent(11458): loss=10.13777763893763\n",
      "Stochastic Gradient Descent(11459): loss=0.2057127299082445\n",
      "Stochastic Gradient Descent(11460): loss=1.0266110612187813\n",
      "Stochastic Gradient Descent(11461): loss=1.2021524623423312\n",
      "Stochastic Gradient Descent(11462): loss=4.27715452923247\n",
      "Stochastic Gradient Descent(11463): loss=14.68278932307107\n",
      "Stochastic Gradient Descent(11464): loss=0.8874275391261235\n",
      "Stochastic Gradient Descent(11465): loss=8.824708435561698\n",
      "Stochastic Gradient Descent(11466): loss=13.372790230946462\n",
      "Stochastic Gradient Descent(11467): loss=0.24901580941805182\n",
      "Stochastic Gradient Descent(11468): loss=2.7891342598457047\n",
      "Stochastic Gradient Descent(11469): loss=0.9399350000575663\n",
      "Stochastic Gradient Descent(11470): loss=6.54889632472452\n",
      "Stochastic Gradient Descent(11471): loss=0.2682418464303907\n",
      "Stochastic Gradient Descent(11472): loss=0.47553128130909506\n",
      "Stochastic Gradient Descent(11473): loss=24.23831585557395\n",
      "Stochastic Gradient Descent(11474): loss=5.4196060799023895\n",
      "Stochastic Gradient Descent(11475): loss=12.20159571548386\n",
      "Stochastic Gradient Descent(11476): loss=12.001875253340687\n",
      "Stochastic Gradient Descent(11477): loss=0.6817612112417791\n",
      "Stochastic Gradient Descent(11478): loss=1.1776455897748654\n",
      "Stochastic Gradient Descent(11479): loss=2.690447678892793\n",
      "Stochastic Gradient Descent(11480): loss=0.48905711079312625\n",
      "Stochastic Gradient Descent(11481): loss=6.950546340653039\n",
      "Stochastic Gradient Descent(11482): loss=0.043311400839916685\n",
      "Stochastic Gradient Descent(11483): loss=20.578574093612325\n",
      "Stochastic Gradient Descent(11484): loss=7.999035384943204\n",
      "Stochastic Gradient Descent(11485): loss=0.29428233706973506\n",
      "Stochastic Gradient Descent(11486): loss=2.2753098623841037\n",
      "Stochastic Gradient Descent(11487): loss=0.08761802793537486\n",
      "Stochastic Gradient Descent(11488): loss=11.79610055454909\n",
      "Stochastic Gradient Descent(11489): loss=9.751487875246193\n",
      "Stochastic Gradient Descent(11490): loss=7.047084228930305\n",
      "Stochastic Gradient Descent(11491): loss=3.3287552874055932\n",
      "Stochastic Gradient Descent(11492): loss=69.8530327378462\n",
      "Stochastic Gradient Descent(11493): loss=57.7320473966491\n",
      "Stochastic Gradient Descent(11494): loss=0.33670586478549785\n",
      "Stochastic Gradient Descent(11495): loss=0.07831258635415356\n",
      "Stochastic Gradient Descent(11496): loss=31.578194933388936\n",
      "Stochastic Gradient Descent(11497): loss=11.605629248196298\n",
      "Stochastic Gradient Descent(11498): loss=8.321731324438876\n",
      "Stochastic Gradient Descent(11499): loss=0.057415579102380036\n",
      "Stochastic Gradient Descent(11500): loss=0.05077894150529171\n",
      "Stochastic Gradient Descent(11501): loss=6.484983920462043\n",
      "Stochastic Gradient Descent(11502): loss=4.763005873843931\n",
      "Stochastic Gradient Descent(11503): loss=5.694001710412477\n",
      "Stochastic Gradient Descent(11504): loss=0.4079084557190617\n",
      "Stochastic Gradient Descent(11505): loss=11.180072339547705\n",
      "Stochastic Gradient Descent(11506): loss=28.777286267138944\n",
      "Stochastic Gradient Descent(11507): loss=1.552354325509703\n",
      "Stochastic Gradient Descent(11508): loss=0.387081658378675\n",
      "Stochastic Gradient Descent(11509): loss=2.1490622315244865\n",
      "Stochastic Gradient Descent(11510): loss=0.9245641053779047\n",
      "Stochastic Gradient Descent(11511): loss=7.750043853230013\n",
      "Stochastic Gradient Descent(11512): loss=2.1910111039692537\n",
      "Stochastic Gradient Descent(11513): loss=2.3853361276380634\n",
      "Stochastic Gradient Descent(11514): loss=0.03850878383978357\n",
      "Stochastic Gradient Descent(11515): loss=0.10949733125124092\n",
      "Stochastic Gradient Descent(11516): loss=0.5644246330703776\n",
      "Stochastic Gradient Descent(11517): loss=12.133289585191255\n",
      "Stochastic Gradient Descent(11518): loss=6.458236253466605\n",
      "Stochastic Gradient Descent(11519): loss=26.460396116989937\n",
      "Stochastic Gradient Descent(11520): loss=0.07219386828839212\n",
      "Stochastic Gradient Descent(11521): loss=6.923139480261901\n",
      "Stochastic Gradient Descent(11522): loss=8.082179196068376\n",
      "Stochastic Gradient Descent(11523): loss=0.4033401651159137\n",
      "Stochastic Gradient Descent(11524): loss=0.10523401503066146\n",
      "Stochastic Gradient Descent(11525): loss=1.2351876286144134\n",
      "Stochastic Gradient Descent(11526): loss=0.022889430959000246\n",
      "Stochastic Gradient Descent(11527): loss=17.63111212266008\n",
      "Stochastic Gradient Descent(11528): loss=5.787691354625743\n",
      "Stochastic Gradient Descent(11529): loss=4.316627376881583\n",
      "Stochastic Gradient Descent(11530): loss=0.0002876274744734989\n",
      "Stochastic Gradient Descent(11531): loss=0.017990512463197727\n",
      "Stochastic Gradient Descent(11532): loss=3.631897628277216\n",
      "Stochastic Gradient Descent(11533): loss=0.5679052958108316\n",
      "Stochastic Gradient Descent(11534): loss=3.1096370608204635\n",
      "Stochastic Gradient Descent(11535): loss=0.4885239479254758\n",
      "Stochastic Gradient Descent(11536): loss=17.865089743850888\n",
      "Stochastic Gradient Descent(11537): loss=0.9604443523655332\n",
      "Stochastic Gradient Descent(11538): loss=6.902486282298193\n",
      "Stochastic Gradient Descent(11539): loss=5.918957882695595\n",
      "Stochastic Gradient Descent(11540): loss=20.076720435731357\n",
      "Stochastic Gradient Descent(11541): loss=0.12552775103869626\n",
      "Stochastic Gradient Descent(11542): loss=10.476164587134305\n",
      "Stochastic Gradient Descent(11543): loss=0.13909228565909412\n",
      "Stochastic Gradient Descent(11544): loss=11.291670168531883\n",
      "Stochastic Gradient Descent(11545): loss=7.280872696838472\n",
      "Stochastic Gradient Descent(11546): loss=11.717515744046052\n",
      "Stochastic Gradient Descent(11547): loss=0.33929890578492833\n",
      "Stochastic Gradient Descent(11548): loss=8.985369916023583\n",
      "Stochastic Gradient Descent(11549): loss=0.9665374846619296\n",
      "Stochastic Gradient Descent(11550): loss=0.8461218792668845\n",
      "Stochastic Gradient Descent(11551): loss=5.365578275836896\n",
      "Stochastic Gradient Descent(11552): loss=15.057959294905894\n",
      "Stochastic Gradient Descent(11553): loss=0.13255325409641894\n",
      "Stochastic Gradient Descent(11554): loss=2.351893919029959\n",
      "Stochastic Gradient Descent(11555): loss=1.106604192676969\n",
      "Stochastic Gradient Descent(11556): loss=3.4098922839084542\n",
      "Stochastic Gradient Descent(11557): loss=0.007966344677318488\n",
      "Stochastic Gradient Descent(11558): loss=2.805504392697215\n",
      "Stochastic Gradient Descent(11559): loss=22.970084880957486\n",
      "Stochastic Gradient Descent(11560): loss=2.808674706523475\n",
      "Stochastic Gradient Descent(11561): loss=0.1542102372190189\n",
      "Stochastic Gradient Descent(11562): loss=24.339502875817832\n",
      "Stochastic Gradient Descent(11563): loss=0.1211727952736258\n",
      "Stochastic Gradient Descent(11564): loss=21.025444296429576\n",
      "Stochastic Gradient Descent(11565): loss=3.7573904108461447\n",
      "Stochastic Gradient Descent(11566): loss=15.912826949662499\n",
      "Stochastic Gradient Descent(11567): loss=8.905649537779476\n",
      "Stochastic Gradient Descent(11568): loss=5.386546202189496\n",
      "Stochastic Gradient Descent(11569): loss=1.1808845611502294\n",
      "Stochastic Gradient Descent(11570): loss=0.7655075737407565\n",
      "Stochastic Gradient Descent(11571): loss=7.536557332763799\n",
      "Stochastic Gradient Descent(11572): loss=0.759915782776258\n",
      "Stochastic Gradient Descent(11573): loss=1.3992247313941337\n",
      "Stochastic Gradient Descent(11574): loss=56.94446321824179\n",
      "Stochastic Gradient Descent(11575): loss=10.783068517193906\n",
      "Stochastic Gradient Descent(11576): loss=3.327293473694085\n",
      "Stochastic Gradient Descent(11577): loss=27.644742068424385\n",
      "Stochastic Gradient Descent(11578): loss=3.6298557901776545\n",
      "Stochastic Gradient Descent(11579): loss=0.0028890455888304855\n",
      "Stochastic Gradient Descent(11580): loss=51.143739625815\n",
      "Stochastic Gradient Descent(11581): loss=0.0784931068817774\n",
      "Stochastic Gradient Descent(11582): loss=0.11523979056934484\n",
      "Stochastic Gradient Descent(11583): loss=0.01553120467256951\n",
      "Stochastic Gradient Descent(11584): loss=4.426482886500789\n",
      "Stochastic Gradient Descent(11585): loss=1.8213284232786773\n",
      "Stochastic Gradient Descent(11586): loss=28.690637034971733\n",
      "Stochastic Gradient Descent(11587): loss=6.170145718337171\n",
      "Stochastic Gradient Descent(11588): loss=0.5405648825502846\n",
      "Stochastic Gradient Descent(11589): loss=12.328713010566148\n",
      "Stochastic Gradient Descent(11590): loss=7.599476401431051\n",
      "Stochastic Gradient Descent(11591): loss=5.635619655369915\n",
      "Stochastic Gradient Descent(11592): loss=0.14834212723212284\n",
      "Stochastic Gradient Descent(11593): loss=3.305405956850557\n",
      "Stochastic Gradient Descent(11594): loss=1.0940112722325521\n",
      "Stochastic Gradient Descent(11595): loss=3.121068075094523\n",
      "Stochastic Gradient Descent(11596): loss=0.06523103443083579\n",
      "Stochastic Gradient Descent(11597): loss=0.747086073947556\n",
      "Stochastic Gradient Descent(11598): loss=2.556145242322621\n",
      "Stochastic Gradient Descent(11599): loss=1.6239510248966076\n",
      "Stochastic Gradient Descent(11600): loss=12.375003877200387\n",
      "Stochastic Gradient Descent(11601): loss=1.3953367244048531\n",
      "Stochastic Gradient Descent(11602): loss=0.0011185442091708794\n",
      "Stochastic Gradient Descent(11603): loss=0.00026149159034103423\n",
      "Stochastic Gradient Descent(11604): loss=0.11663048235687946\n",
      "Stochastic Gradient Descent(11605): loss=0.18167434577205657\n",
      "Stochastic Gradient Descent(11606): loss=3.173662411824831\n",
      "Stochastic Gradient Descent(11607): loss=2.844950310922766\n",
      "Stochastic Gradient Descent(11608): loss=0.7316625997197691\n",
      "Stochastic Gradient Descent(11609): loss=0.9038210804788935\n",
      "Stochastic Gradient Descent(11610): loss=25.877273677437834\n",
      "Stochastic Gradient Descent(11611): loss=3.774724765574739\n",
      "Stochastic Gradient Descent(11612): loss=12.699624807693905\n",
      "Stochastic Gradient Descent(11613): loss=0.1396908801350369\n",
      "Stochastic Gradient Descent(11614): loss=5.0200984330501095\n",
      "Stochastic Gradient Descent(11615): loss=0.4550407857543955\n",
      "Stochastic Gradient Descent(11616): loss=0.049645121256213925\n",
      "Stochastic Gradient Descent(11617): loss=1.775180013897048\n",
      "Stochastic Gradient Descent(11618): loss=41.02099171240001\n",
      "Stochastic Gradient Descent(11619): loss=41.40945941013715\n",
      "Stochastic Gradient Descent(11620): loss=145.99734652107716\n",
      "Stochastic Gradient Descent(11621): loss=2.939321048657723\n",
      "Stochastic Gradient Descent(11622): loss=1.6599299157768743\n",
      "Stochastic Gradient Descent(11623): loss=22.628023051193708\n",
      "Stochastic Gradient Descent(11624): loss=2.8746167400239546\n",
      "Stochastic Gradient Descent(11625): loss=0.04438104009342589\n",
      "Stochastic Gradient Descent(11626): loss=0.18132490154284062\n",
      "Stochastic Gradient Descent(11627): loss=11.950104759670161\n",
      "Stochastic Gradient Descent(11628): loss=0.22192736270222577\n",
      "Stochastic Gradient Descent(11629): loss=4.230668432587583\n",
      "Stochastic Gradient Descent(11630): loss=0.10932296161873653\n",
      "Stochastic Gradient Descent(11631): loss=1.8573760921358513\n",
      "Stochastic Gradient Descent(11632): loss=4.949835005712906\n",
      "Stochastic Gradient Descent(11633): loss=1.7139009524965252\n",
      "Stochastic Gradient Descent(11634): loss=1.6830163824339657\n",
      "Stochastic Gradient Descent(11635): loss=0.39200912073765354\n",
      "Stochastic Gradient Descent(11636): loss=17.097364939577293\n",
      "Stochastic Gradient Descent(11637): loss=0.1811628751487874\n",
      "Stochastic Gradient Descent(11638): loss=27.122857797666065\n",
      "Stochastic Gradient Descent(11639): loss=14.395415607797613\n",
      "Stochastic Gradient Descent(11640): loss=0.0005985673437224569\n",
      "Stochastic Gradient Descent(11641): loss=2.0352274304170326\n",
      "Stochastic Gradient Descent(11642): loss=5.920394373869034\n",
      "Stochastic Gradient Descent(11643): loss=1.72720093856699\n",
      "Stochastic Gradient Descent(11644): loss=5.875043873591727\n",
      "Stochastic Gradient Descent(11645): loss=39.17199216333201\n",
      "Stochastic Gradient Descent(11646): loss=3.208606368540124\n",
      "Stochastic Gradient Descent(11647): loss=4.496368018962394\n",
      "Stochastic Gradient Descent(11648): loss=2.2449585414251105\n",
      "Stochastic Gradient Descent(11649): loss=0.04697489591346237\n",
      "Stochastic Gradient Descent(11650): loss=1.352224602531626\n",
      "Stochastic Gradient Descent(11651): loss=1.4983547779434243\n",
      "Stochastic Gradient Descent(11652): loss=1.006716662199029\n",
      "Stochastic Gradient Descent(11653): loss=2.0307362644681803\n",
      "Stochastic Gradient Descent(11654): loss=0.4613529950085822\n",
      "Stochastic Gradient Descent(11655): loss=4.804911797499234\n",
      "Stochastic Gradient Descent(11656): loss=3.935459734215494\n",
      "Stochastic Gradient Descent(11657): loss=0.7199961929561944\n",
      "Stochastic Gradient Descent(11658): loss=1.4256391941887439\n",
      "Stochastic Gradient Descent(11659): loss=0.06591736090934062\n",
      "Stochastic Gradient Descent(11660): loss=1.6686801281848729\n",
      "Stochastic Gradient Descent(11661): loss=2.1758767981122884\n",
      "Stochastic Gradient Descent(11662): loss=6.448904014878007\n",
      "Stochastic Gradient Descent(11663): loss=0.6436566504700475\n",
      "Stochastic Gradient Descent(11664): loss=9.060905273086384\n",
      "Stochastic Gradient Descent(11665): loss=3.12151412036211\n",
      "Stochastic Gradient Descent(11666): loss=21.226292462396216\n",
      "Stochastic Gradient Descent(11667): loss=0.00128524332094461\n",
      "Stochastic Gradient Descent(11668): loss=0.16816775424102332\n",
      "Stochastic Gradient Descent(11669): loss=0.5124331997283907\n",
      "Stochastic Gradient Descent(11670): loss=3.916291502113979\n",
      "Stochastic Gradient Descent(11671): loss=2.1346243840121852\n",
      "Stochastic Gradient Descent(11672): loss=1.3845019993040923\n",
      "Stochastic Gradient Descent(11673): loss=12.4760348105071\n",
      "Stochastic Gradient Descent(11674): loss=6.3713327489362275\n",
      "Stochastic Gradient Descent(11675): loss=1.6733654130879405\n",
      "Stochastic Gradient Descent(11676): loss=1.05627092490291\n",
      "Stochastic Gradient Descent(11677): loss=2.6855963729389054\n",
      "Stochastic Gradient Descent(11678): loss=0.40508987296493615\n",
      "Stochastic Gradient Descent(11679): loss=5.803947225804237\n",
      "Stochastic Gradient Descent(11680): loss=0.18133233880226685\n",
      "Stochastic Gradient Descent(11681): loss=0.0800240779317799\n",
      "Stochastic Gradient Descent(11682): loss=1.3812398957236742\n",
      "Stochastic Gradient Descent(11683): loss=0.7232867513732181\n",
      "Stochastic Gradient Descent(11684): loss=0.039287112123849825\n",
      "Stochastic Gradient Descent(11685): loss=5.682454446668193\n",
      "Stochastic Gradient Descent(11686): loss=0.0025955463834938727\n",
      "Stochastic Gradient Descent(11687): loss=1.647802555810907\n",
      "Stochastic Gradient Descent(11688): loss=4.025275248794025\n",
      "Stochastic Gradient Descent(11689): loss=0.19213320431443512\n",
      "Stochastic Gradient Descent(11690): loss=0.08425040278781686\n",
      "Stochastic Gradient Descent(11691): loss=1.2576945550036536\n",
      "Stochastic Gradient Descent(11692): loss=8.028286955908465\n",
      "Stochastic Gradient Descent(11693): loss=0.8581969090684255\n",
      "Stochastic Gradient Descent(11694): loss=12.698328051006987\n",
      "Stochastic Gradient Descent(11695): loss=8.863256558473775\n",
      "Stochastic Gradient Descent(11696): loss=0.781631025505683\n",
      "Stochastic Gradient Descent(11697): loss=0.04502105341034088\n",
      "Stochastic Gradient Descent(11698): loss=1.1140133701228927\n",
      "Stochastic Gradient Descent(11699): loss=0.6952441257442448\n",
      "Stochastic Gradient Descent(11700): loss=0.4271235934870712\n",
      "Stochastic Gradient Descent(11701): loss=2.4064148102742866\n",
      "Stochastic Gradient Descent(11702): loss=0.23601368541907072\n",
      "Stochastic Gradient Descent(11703): loss=0.5573661426439247\n",
      "Stochastic Gradient Descent(11704): loss=0.03085213760851923\n",
      "Stochastic Gradient Descent(11705): loss=1.3861738496337814\n",
      "Stochastic Gradient Descent(11706): loss=0.553952123841486\n",
      "Stochastic Gradient Descent(11707): loss=12.094403222734579\n",
      "Stochastic Gradient Descent(11708): loss=1.4153907582201653e-05\n",
      "Stochastic Gradient Descent(11709): loss=1.0047408721718145\n",
      "Stochastic Gradient Descent(11710): loss=0.36921804339233\n",
      "Stochastic Gradient Descent(11711): loss=0.3515551710548814\n",
      "Stochastic Gradient Descent(11712): loss=0.18625680601743438\n",
      "Stochastic Gradient Descent(11713): loss=2.6081456955763898\n",
      "Stochastic Gradient Descent(11714): loss=0.041688170176227175\n",
      "Stochastic Gradient Descent(11715): loss=23.17477739087224\n",
      "Stochastic Gradient Descent(11716): loss=0.815068053309926\n",
      "Stochastic Gradient Descent(11717): loss=16.53350276058878\n",
      "Stochastic Gradient Descent(11718): loss=4.121143840706498\n",
      "Stochastic Gradient Descent(11719): loss=1.3644102238147438\n",
      "Stochastic Gradient Descent(11720): loss=2.627731304699171\n",
      "Stochastic Gradient Descent(11721): loss=3.783032760873097\n",
      "Stochastic Gradient Descent(11722): loss=32.64612362203679\n",
      "Stochastic Gradient Descent(11723): loss=0.5164920779464803\n",
      "Stochastic Gradient Descent(11724): loss=15.234784621411395\n",
      "Stochastic Gradient Descent(11725): loss=3.390863051391251\n",
      "Stochastic Gradient Descent(11726): loss=15.019320001663603\n",
      "Stochastic Gradient Descent(11727): loss=21.883647020827283\n",
      "Stochastic Gradient Descent(11728): loss=20.066037405991587\n",
      "Stochastic Gradient Descent(11729): loss=5.138716672757227\n",
      "Stochastic Gradient Descent(11730): loss=7.580624615727295\n",
      "Stochastic Gradient Descent(11731): loss=0.9579969691517093\n",
      "Stochastic Gradient Descent(11732): loss=47.73984778094476\n",
      "Stochastic Gradient Descent(11733): loss=2.314158044285646\n",
      "Stochastic Gradient Descent(11734): loss=5.7504945289558504\n",
      "Stochastic Gradient Descent(11735): loss=28.88320093713964\n",
      "Stochastic Gradient Descent(11736): loss=15.030974482755104\n",
      "Stochastic Gradient Descent(11737): loss=0.3175606276239553\n",
      "Stochastic Gradient Descent(11738): loss=8.703313132912017\n",
      "Stochastic Gradient Descent(11739): loss=9.974444479295896\n",
      "Stochastic Gradient Descent(11740): loss=6.766194560487212\n",
      "Stochastic Gradient Descent(11741): loss=15.398339995414098\n",
      "Stochastic Gradient Descent(11742): loss=1.2787078416175517\n",
      "Stochastic Gradient Descent(11743): loss=0.1233738030673487\n",
      "Stochastic Gradient Descent(11744): loss=0.8076915069793501\n",
      "Stochastic Gradient Descent(11745): loss=0.009236880861118743\n",
      "Stochastic Gradient Descent(11746): loss=0.1743925879438917\n",
      "Stochastic Gradient Descent(11747): loss=8.487957895817928\n",
      "Stochastic Gradient Descent(11748): loss=7.799402342376271\n",
      "Stochastic Gradient Descent(11749): loss=0.18728280069098246\n",
      "Stochastic Gradient Descent(11750): loss=1.258841408978105\n",
      "Stochastic Gradient Descent(11751): loss=16.079521985009983\n",
      "Stochastic Gradient Descent(11752): loss=10.657102285796823\n",
      "Stochastic Gradient Descent(11753): loss=1.3421110317024685\n",
      "Stochastic Gradient Descent(11754): loss=6.2491340749180875\n",
      "Stochastic Gradient Descent(11755): loss=7.202372498255859\n",
      "Stochastic Gradient Descent(11756): loss=0.062073439700775494\n",
      "Stochastic Gradient Descent(11757): loss=0.2236695564386545\n",
      "Stochastic Gradient Descent(11758): loss=5.1187501612895865\n",
      "Stochastic Gradient Descent(11759): loss=0.26449673710601845\n",
      "Stochastic Gradient Descent(11760): loss=0.6897728359777158\n",
      "Stochastic Gradient Descent(11761): loss=2.5915745283801486\n",
      "Stochastic Gradient Descent(11762): loss=1.0102394254556255\n",
      "Stochastic Gradient Descent(11763): loss=3.497650377512255\n",
      "Stochastic Gradient Descent(11764): loss=2.110601613529495\n",
      "Stochastic Gradient Descent(11765): loss=0.2701442248342401\n",
      "Stochastic Gradient Descent(11766): loss=16.553675212821727\n",
      "Stochastic Gradient Descent(11767): loss=9.397453022316299\n",
      "Stochastic Gradient Descent(11768): loss=0.036573521742339236\n",
      "Stochastic Gradient Descent(11769): loss=2.538426806662744\n",
      "Stochastic Gradient Descent(11770): loss=3.8140652690963632\n",
      "Stochastic Gradient Descent(11771): loss=0.7938930262710667\n",
      "Stochastic Gradient Descent(11772): loss=0.5623083507485543\n",
      "Stochastic Gradient Descent(11773): loss=3.338838094092122\n",
      "Stochastic Gradient Descent(11774): loss=0.004822417829018366\n",
      "Stochastic Gradient Descent(11775): loss=2.7214474122164827\n",
      "Stochastic Gradient Descent(11776): loss=0.08038328272602452\n",
      "Stochastic Gradient Descent(11777): loss=0.036686710809495024\n",
      "Stochastic Gradient Descent(11778): loss=9.332722450042638\n",
      "Stochastic Gradient Descent(11779): loss=0.4877950087360066\n",
      "Stochastic Gradient Descent(11780): loss=0.9766243370327589\n",
      "Stochastic Gradient Descent(11781): loss=5.5838484569104265\n",
      "Stochastic Gradient Descent(11782): loss=5.326635295747701\n",
      "Stochastic Gradient Descent(11783): loss=0.6480550759169426\n",
      "Stochastic Gradient Descent(11784): loss=1.4350046040865856\n",
      "Stochastic Gradient Descent(11785): loss=0.006419829106627427\n",
      "Stochastic Gradient Descent(11786): loss=3.8664252955998695\n",
      "Stochastic Gradient Descent(11787): loss=5.121245164381817\n",
      "Stochastic Gradient Descent(11788): loss=0.23622147772325452\n",
      "Stochastic Gradient Descent(11789): loss=2.363596279770488\n",
      "Stochastic Gradient Descent(11790): loss=1.2949649011303446\n",
      "Stochastic Gradient Descent(11791): loss=0.6545586490409945\n",
      "Stochastic Gradient Descent(11792): loss=0.5599055568596385\n",
      "Stochastic Gradient Descent(11793): loss=2.3776427417421604\n",
      "Stochastic Gradient Descent(11794): loss=1.2720231476276291\n",
      "Stochastic Gradient Descent(11795): loss=22.13416726175927\n",
      "Stochastic Gradient Descent(11796): loss=0.7863311050084909\n",
      "Stochastic Gradient Descent(11797): loss=0.471879231048944\n",
      "Stochastic Gradient Descent(11798): loss=16.130475969252053\n",
      "Stochastic Gradient Descent(11799): loss=1.8068894283141355\n",
      "Stochastic Gradient Descent(11800): loss=2.149713283454633\n",
      "Stochastic Gradient Descent(11801): loss=0.7044212868075265\n",
      "Stochastic Gradient Descent(11802): loss=9.181152451939768\n",
      "Stochastic Gradient Descent(11803): loss=11.313369333950856\n",
      "Stochastic Gradient Descent(11804): loss=1.9619085329698738\n",
      "Stochastic Gradient Descent(11805): loss=1.5840137068767175\n",
      "Stochastic Gradient Descent(11806): loss=0.027332998599434024\n",
      "Stochastic Gradient Descent(11807): loss=0.4494112998831745\n",
      "Stochastic Gradient Descent(11808): loss=2.4781974221893317\n",
      "Stochastic Gradient Descent(11809): loss=7.7381956099150875\n",
      "Stochastic Gradient Descent(11810): loss=1.8147864070371211\n",
      "Stochastic Gradient Descent(11811): loss=1.2540573161561022\n",
      "Stochastic Gradient Descent(11812): loss=1.6325902227260907\n",
      "Stochastic Gradient Descent(11813): loss=6.923297069293868\n",
      "Stochastic Gradient Descent(11814): loss=2.8641378801355195\n",
      "Stochastic Gradient Descent(11815): loss=24.88319848180702\n",
      "Stochastic Gradient Descent(11816): loss=26.92611403295534\n",
      "Stochastic Gradient Descent(11817): loss=0.7908413612138794\n",
      "Stochastic Gradient Descent(11818): loss=1.7955687683857164\n",
      "Stochastic Gradient Descent(11819): loss=4.202889857856251\n",
      "Stochastic Gradient Descent(11820): loss=0.7501500145990048\n",
      "Stochastic Gradient Descent(11821): loss=1.10721751456695\n",
      "Stochastic Gradient Descent(11822): loss=3.566158307335109\n",
      "Stochastic Gradient Descent(11823): loss=3.3162958260224604\n",
      "Stochastic Gradient Descent(11824): loss=4.704236273920505\n",
      "Stochastic Gradient Descent(11825): loss=0.09832599338035544\n",
      "Stochastic Gradient Descent(11826): loss=6.4019568154776065\n",
      "Stochastic Gradient Descent(11827): loss=2.010718166494927\n",
      "Stochastic Gradient Descent(11828): loss=10.585067226886014\n",
      "Stochastic Gradient Descent(11829): loss=0.3697884318887724\n",
      "Stochastic Gradient Descent(11830): loss=1.078305599068449\n",
      "Stochastic Gradient Descent(11831): loss=1.1141287969558251\n",
      "Stochastic Gradient Descent(11832): loss=2.7465607341823417\n",
      "Stochastic Gradient Descent(11833): loss=12.619412540833181\n",
      "Stochastic Gradient Descent(11834): loss=1.3514289764565792\n",
      "Stochastic Gradient Descent(11835): loss=1.928494199183015\n",
      "Stochastic Gradient Descent(11836): loss=3.4295044055622483\n",
      "Stochastic Gradient Descent(11837): loss=5.525823358227337e-07\n",
      "Stochastic Gradient Descent(11838): loss=3.5489695975719586\n",
      "Stochastic Gradient Descent(11839): loss=11.260999275892953\n",
      "Stochastic Gradient Descent(11840): loss=38.966777721071225\n",
      "Stochastic Gradient Descent(11841): loss=0.08345336491030844\n",
      "Stochastic Gradient Descent(11842): loss=12.76745144628438\n",
      "Stochastic Gradient Descent(11843): loss=6.340603234279488\n",
      "Stochastic Gradient Descent(11844): loss=0.14333907688300648\n",
      "Stochastic Gradient Descent(11845): loss=0.024204074202499547\n",
      "Stochastic Gradient Descent(11846): loss=12.446995414831468\n",
      "Stochastic Gradient Descent(11847): loss=1.3071431314376163\n",
      "Stochastic Gradient Descent(11848): loss=0.05143708131390323\n",
      "Stochastic Gradient Descent(11849): loss=1.697140208753792\n",
      "Stochastic Gradient Descent(11850): loss=7.812379313256003\n",
      "Stochastic Gradient Descent(11851): loss=9.255724807304897\n",
      "Stochastic Gradient Descent(11852): loss=1.65576333372394\n",
      "Stochastic Gradient Descent(11853): loss=6.727592906895515\n",
      "Stochastic Gradient Descent(11854): loss=0.9506538085730456\n",
      "Stochastic Gradient Descent(11855): loss=0.34148244943334344\n",
      "Stochastic Gradient Descent(11856): loss=0.5685564792412365\n",
      "Stochastic Gradient Descent(11857): loss=1.3377738648431066\n",
      "Stochastic Gradient Descent(11858): loss=0.024810713300302285\n",
      "Stochastic Gradient Descent(11859): loss=0.7442220005891024\n",
      "Stochastic Gradient Descent(11860): loss=0.010983971058927655\n",
      "Stochastic Gradient Descent(11861): loss=0.5052197641941425\n",
      "Stochastic Gradient Descent(11862): loss=6.241051794876925\n",
      "Stochastic Gradient Descent(11863): loss=35.22292158460496\n",
      "Stochastic Gradient Descent(11864): loss=2.517824631169619\n",
      "Stochastic Gradient Descent(11865): loss=1.0838599126498771\n",
      "Stochastic Gradient Descent(11866): loss=7.981164282265037\n",
      "Stochastic Gradient Descent(11867): loss=7.892684844232787\n",
      "Stochastic Gradient Descent(11868): loss=1.6332910936243619\n",
      "Stochastic Gradient Descent(11869): loss=13.814460276620473\n",
      "Stochastic Gradient Descent(11870): loss=1.5453640685747703\n",
      "Stochastic Gradient Descent(11871): loss=32.70686647112316\n",
      "Stochastic Gradient Descent(11872): loss=7.92858781357404\n",
      "Stochastic Gradient Descent(11873): loss=6.154796482970883\n",
      "Stochastic Gradient Descent(11874): loss=2.129352386554125\n",
      "Stochastic Gradient Descent(11875): loss=14.998184585101002\n",
      "Stochastic Gradient Descent(11876): loss=5.064086387458221\n",
      "Stochastic Gradient Descent(11877): loss=9.118123072343295\n",
      "Stochastic Gradient Descent(11878): loss=11.03186731760331\n",
      "Stochastic Gradient Descent(11879): loss=2.128643660157489\n",
      "Stochastic Gradient Descent(11880): loss=0.006022327954776477\n",
      "Stochastic Gradient Descent(11881): loss=0.2974267607946305\n",
      "Stochastic Gradient Descent(11882): loss=12.212805350098074\n",
      "Stochastic Gradient Descent(11883): loss=0.08442856737636356\n",
      "Stochastic Gradient Descent(11884): loss=2.2453923420354442\n",
      "Stochastic Gradient Descent(11885): loss=14.32065679552621\n",
      "Stochastic Gradient Descent(11886): loss=2.510429876457882\n",
      "Stochastic Gradient Descent(11887): loss=11.252175049365743\n",
      "Stochastic Gradient Descent(11888): loss=0.7949536676281553\n",
      "Stochastic Gradient Descent(11889): loss=4.873984771980151\n",
      "Stochastic Gradient Descent(11890): loss=9.486993252246892\n",
      "Stochastic Gradient Descent(11891): loss=3.142078332377637\n",
      "Stochastic Gradient Descent(11892): loss=4.69001088876841\n",
      "Stochastic Gradient Descent(11893): loss=0.5506183656917797\n",
      "Stochastic Gradient Descent(11894): loss=0.007859959625993129\n",
      "Stochastic Gradient Descent(11895): loss=10.936054136348432\n",
      "Stochastic Gradient Descent(11896): loss=4.065660088629604\n",
      "Stochastic Gradient Descent(11897): loss=13.695306865216345\n",
      "Stochastic Gradient Descent(11898): loss=14.24705753822662\n",
      "Stochastic Gradient Descent(11899): loss=7.367726258851919\n",
      "Stochastic Gradient Descent(11900): loss=18.609853295620262\n",
      "Stochastic Gradient Descent(11901): loss=0.010537128823242344\n",
      "Stochastic Gradient Descent(11902): loss=0.03983406702693759\n",
      "Stochastic Gradient Descent(11903): loss=11.222997061367339\n",
      "Stochastic Gradient Descent(11904): loss=0.5466718035310595\n",
      "Stochastic Gradient Descent(11905): loss=6.5726642701026075\n",
      "Stochastic Gradient Descent(11906): loss=0.024526048727832903\n",
      "Stochastic Gradient Descent(11907): loss=5.1448938965697515\n",
      "Stochastic Gradient Descent(11908): loss=26.35496945310268\n",
      "Stochastic Gradient Descent(11909): loss=2.2960912779117364\n",
      "Stochastic Gradient Descent(11910): loss=10.475604379853138\n",
      "Stochastic Gradient Descent(11911): loss=0.06386636645967426\n",
      "Stochastic Gradient Descent(11912): loss=0.1677515947484439\n",
      "Stochastic Gradient Descent(11913): loss=1.3971518523903024\n",
      "Stochastic Gradient Descent(11914): loss=3.120542332233282\n",
      "Stochastic Gradient Descent(11915): loss=1.1543043930883492\n",
      "Stochastic Gradient Descent(11916): loss=7.310911474258373\n",
      "Stochastic Gradient Descent(11917): loss=3.8583408318238273\n",
      "Stochastic Gradient Descent(11918): loss=4.010099398080231\n",
      "Stochastic Gradient Descent(11919): loss=4.17711624307175\n",
      "Stochastic Gradient Descent(11920): loss=0.02798195446514377\n",
      "Stochastic Gradient Descent(11921): loss=2.9343543117517648\n",
      "Stochastic Gradient Descent(11922): loss=4.000210964067564\n",
      "Stochastic Gradient Descent(11923): loss=0.007890004667154846\n",
      "Stochastic Gradient Descent(11924): loss=0.9002117664709243\n",
      "Stochastic Gradient Descent(11925): loss=6.1768331138416865\n",
      "Stochastic Gradient Descent(11926): loss=0.04070278408504095\n",
      "Stochastic Gradient Descent(11927): loss=25.936426309845334\n",
      "Stochastic Gradient Descent(11928): loss=2.218456286491598\n",
      "Stochastic Gradient Descent(11929): loss=3.935552453066386\n",
      "Stochastic Gradient Descent(11930): loss=11.919275827011713\n",
      "Stochastic Gradient Descent(11931): loss=0.17274386139908993\n",
      "Stochastic Gradient Descent(11932): loss=2.6147873926194083\n",
      "Stochastic Gradient Descent(11933): loss=1.6688954659641453\n",
      "Stochastic Gradient Descent(11934): loss=17.260090856423535\n",
      "Stochastic Gradient Descent(11935): loss=36.017500701069345\n",
      "Stochastic Gradient Descent(11936): loss=0.0026045860979679895\n",
      "Stochastic Gradient Descent(11937): loss=3.3213691131889025\n",
      "Stochastic Gradient Descent(11938): loss=2.651589162784174\n",
      "Stochastic Gradient Descent(11939): loss=0.5307651767717578\n",
      "Stochastic Gradient Descent(11940): loss=6.599320592192652\n",
      "Stochastic Gradient Descent(11941): loss=1.1405725476750936\n",
      "Stochastic Gradient Descent(11942): loss=1.9286322062284273\n",
      "Stochastic Gradient Descent(11943): loss=0.79179321240873\n",
      "Stochastic Gradient Descent(11944): loss=0.0011178719432483842\n",
      "Stochastic Gradient Descent(11945): loss=7.070855219852\n",
      "Stochastic Gradient Descent(11946): loss=34.19434678791203\n",
      "Stochastic Gradient Descent(11947): loss=8.878707886197398\n",
      "Stochastic Gradient Descent(11948): loss=0.24493418522813312\n",
      "Stochastic Gradient Descent(11949): loss=37.723607910559146\n",
      "Stochastic Gradient Descent(11950): loss=3.402373403781952\n",
      "Stochastic Gradient Descent(11951): loss=288.34674580251954\n",
      "Stochastic Gradient Descent(11952): loss=157.50583928541897\n",
      "Stochastic Gradient Descent(11953): loss=5.43521653768195\n",
      "Stochastic Gradient Descent(11954): loss=11.583485113456065\n",
      "Stochastic Gradient Descent(11955): loss=7.633411431495923\n",
      "Stochastic Gradient Descent(11956): loss=22.68741933605154\n",
      "Stochastic Gradient Descent(11957): loss=0.1509735432908198\n",
      "Stochastic Gradient Descent(11958): loss=13.263864109089816\n",
      "Stochastic Gradient Descent(11959): loss=3.0757369681011877\n",
      "Stochastic Gradient Descent(11960): loss=5.192478524485541\n",
      "Stochastic Gradient Descent(11961): loss=2.313481554026641\n",
      "Stochastic Gradient Descent(11962): loss=57.22983204513449\n",
      "Stochastic Gradient Descent(11963): loss=1.2158408173613715\n",
      "Stochastic Gradient Descent(11964): loss=1.2290970880922998\n",
      "Stochastic Gradient Descent(11965): loss=1.7446889106060626\n",
      "Stochastic Gradient Descent(11966): loss=0.9918629613611457\n",
      "Stochastic Gradient Descent(11967): loss=0.4077482681808153\n",
      "Stochastic Gradient Descent(11968): loss=22.67333256316682\n",
      "Stochastic Gradient Descent(11969): loss=5.95375611442491\n",
      "Stochastic Gradient Descent(11970): loss=13.101142792789718\n",
      "Stochastic Gradient Descent(11971): loss=18.144258017833337\n",
      "Stochastic Gradient Descent(11972): loss=5.70769242642404\n",
      "Stochastic Gradient Descent(11973): loss=19.21514151196229\n",
      "Stochastic Gradient Descent(11974): loss=1.1878616693716773\n",
      "Stochastic Gradient Descent(11975): loss=0.638383543220261\n",
      "Stochastic Gradient Descent(11976): loss=1.6939395231963323\n",
      "Stochastic Gradient Descent(11977): loss=5.826382499028818\n",
      "Stochastic Gradient Descent(11978): loss=5.271923187500261\n",
      "Stochastic Gradient Descent(11979): loss=0.5737605014018611\n",
      "Stochastic Gradient Descent(11980): loss=0.7843591382203906\n",
      "Stochastic Gradient Descent(11981): loss=0.056669105585468005\n",
      "Stochastic Gradient Descent(11982): loss=5.785881688468362\n",
      "Stochastic Gradient Descent(11983): loss=15.707731506945256\n",
      "Stochastic Gradient Descent(11984): loss=1.081281974525305\n",
      "Stochastic Gradient Descent(11985): loss=13.196032387308287\n",
      "Stochastic Gradient Descent(11986): loss=4.510047178731501\n",
      "Stochastic Gradient Descent(11987): loss=4.267923557065196\n",
      "Stochastic Gradient Descent(11988): loss=8.619897386449942\n",
      "Stochastic Gradient Descent(11989): loss=4.769775569841734\n",
      "Stochastic Gradient Descent(11990): loss=3.806134726733065\n",
      "Stochastic Gradient Descent(11991): loss=1.4570435856177335\n",
      "Stochastic Gradient Descent(11992): loss=0.5411315688169812\n",
      "Stochastic Gradient Descent(11993): loss=2.832759220706295\n",
      "Stochastic Gradient Descent(11994): loss=0.7806641148516928\n",
      "Stochastic Gradient Descent(11995): loss=4.407479766654305\n",
      "Stochastic Gradient Descent(11996): loss=0.6444131862264626\n",
      "Stochastic Gradient Descent(11997): loss=0.03985912281036689\n",
      "Stochastic Gradient Descent(11998): loss=2.1681631312851235\n",
      "Stochastic Gradient Descent(11999): loss=1.47710500889065\n",
      "Stochastic Gradient Descent(12000): loss=18.002122104728674\n",
      "Stochastic Gradient Descent(12001): loss=1.8684108204561363\n",
      "Stochastic Gradient Descent(12002): loss=0.0001785522085533612\n",
      "Stochastic Gradient Descent(12003): loss=0.08614661854690066\n",
      "Stochastic Gradient Descent(12004): loss=0.03369653459567231\n",
      "Stochastic Gradient Descent(12005): loss=25.50415823135604\n",
      "Stochastic Gradient Descent(12006): loss=10.37278564440346\n",
      "Stochastic Gradient Descent(12007): loss=3.251813083973519\n",
      "Stochastic Gradient Descent(12008): loss=16.911776384351516\n",
      "Stochastic Gradient Descent(12009): loss=11.593307573881182\n",
      "Stochastic Gradient Descent(12010): loss=1.711150085236769\n",
      "Stochastic Gradient Descent(12011): loss=3.7802045363445735\n",
      "Stochastic Gradient Descent(12012): loss=0.2686360853049967\n",
      "Stochastic Gradient Descent(12013): loss=2.252868842389667\n",
      "Stochastic Gradient Descent(12014): loss=18.275410351323828\n",
      "Stochastic Gradient Descent(12015): loss=0.44511022906439396\n",
      "Stochastic Gradient Descent(12016): loss=2.316330245271905\n",
      "Stochastic Gradient Descent(12017): loss=0.2411432609341311\n",
      "Stochastic Gradient Descent(12018): loss=0.32846228650237436\n",
      "Stochastic Gradient Descent(12019): loss=1.0764916595058243\n",
      "Stochastic Gradient Descent(12020): loss=6.1014785458460095\n",
      "Stochastic Gradient Descent(12021): loss=51.560774148694286\n",
      "Stochastic Gradient Descent(12022): loss=0.703412635732166\n",
      "Stochastic Gradient Descent(12023): loss=9.470482376308137\n",
      "Stochastic Gradient Descent(12024): loss=3.25297873000069\n",
      "Stochastic Gradient Descent(12025): loss=24.598755128686992\n",
      "Stochastic Gradient Descent(12026): loss=1.7745201616353852\n",
      "Stochastic Gradient Descent(12027): loss=0.11385760490841808\n",
      "Stochastic Gradient Descent(12028): loss=14.6458010794339\n",
      "Stochastic Gradient Descent(12029): loss=4.324978915255904\n",
      "Stochastic Gradient Descent(12030): loss=7.164981497782534\n",
      "Stochastic Gradient Descent(12031): loss=1.0072160699097523\n",
      "Stochastic Gradient Descent(12032): loss=0.3725157060525279\n",
      "Stochastic Gradient Descent(12033): loss=2.962504074010002\n",
      "Stochastic Gradient Descent(12034): loss=1.015893531482322\n",
      "Stochastic Gradient Descent(12035): loss=4.2811428819663915\n",
      "Stochastic Gradient Descent(12036): loss=5.610332649949905\n",
      "Stochastic Gradient Descent(12037): loss=5.05015614657743\n",
      "Stochastic Gradient Descent(12038): loss=10.712254494245862\n",
      "Stochastic Gradient Descent(12039): loss=8.55430804496801\n",
      "Stochastic Gradient Descent(12040): loss=1.9049680210385425\n",
      "Stochastic Gradient Descent(12041): loss=23.127994648569064\n",
      "Stochastic Gradient Descent(12042): loss=6.904099247438006\n",
      "Stochastic Gradient Descent(12043): loss=10.974154302745713\n",
      "Stochastic Gradient Descent(12044): loss=6.131080385662676\n",
      "Stochastic Gradient Descent(12045): loss=0.0005440038573333353\n",
      "Stochastic Gradient Descent(12046): loss=0.11061261975988379\n",
      "Stochastic Gradient Descent(12047): loss=1.7286782726334269\n",
      "Stochastic Gradient Descent(12048): loss=0.04036138617528635\n",
      "Stochastic Gradient Descent(12049): loss=0.011494956650258044\n",
      "Stochastic Gradient Descent(12050): loss=13.33012610086877\n",
      "Stochastic Gradient Descent(12051): loss=13.50901703028189\n",
      "Stochastic Gradient Descent(12052): loss=2.1765356438819796\n",
      "Stochastic Gradient Descent(12053): loss=8.678971839248343\n",
      "Stochastic Gradient Descent(12054): loss=18.243991423058244\n",
      "Stochastic Gradient Descent(12055): loss=2.369780279563446\n",
      "Stochastic Gradient Descent(12056): loss=7.954350660081455\n",
      "Stochastic Gradient Descent(12057): loss=0.07447229714189602\n",
      "Stochastic Gradient Descent(12058): loss=7.536722417943328\n",
      "Stochastic Gradient Descent(12059): loss=2.890744781815549\n",
      "Stochastic Gradient Descent(12060): loss=1.3660374734009946\n",
      "Stochastic Gradient Descent(12061): loss=0.3951350529102272\n",
      "Stochastic Gradient Descent(12062): loss=2.8336728807695963\n",
      "Stochastic Gradient Descent(12063): loss=4.23398264869567\n",
      "Stochastic Gradient Descent(12064): loss=1.4509659410611369\n",
      "Stochastic Gradient Descent(12065): loss=9.438235488945903\n",
      "Stochastic Gradient Descent(12066): loss=1.5794700538316466\n",
      "Stochastic Gradient Descent(12067): loss=0.014151229771476771\n",
      "Stochastic Gradient Descent(12068): loss=0.002055716662899981\n",
      "Stochastic Gradient Descent(12069): loss=8.014823916719562\n",
      "Stochastic Gradient Descent(12070): loss=7.585566115644574\n",
      "Stochastic Gradient Descent(12071): loss=0.5326077706265221\n",
      "Stochastic Gradient Descent(12072): loss=6.481893384941896\n",
      "Stochastic Gradient Descent(12073): loss=72.28460261543742\n",
      "Stochastic Gradient Descent(12074): loss=1.4191763776890531\n",
      "Stochastic Gradient Descent(12075): loss=4.75832159807783\n",
      "Stochastic Gradient Descent(12076): loss=0.41882835930644197\n",
      "Stochastic Gradient Descent(12077): loss=0.7492486052312299\n",
      "Stochastic Gradient Descent(12078): loss=0.03920251295708719\n",
      "Stochastic Gradient Descent(12079): loss=29.6922356126048\n",
      "Stochastic Gradient Descent(12080): loss=0.4471595755166806\n",
      "Stochastic Gradient Descent(12081): loss=16.96467036624544\n",
      "Stochastic Gradient Descent(12082): loss=0.10433735121654686\n",
      "Stochastic Gradient Descent(12083): loss=0.024550267957095907\n",
      "Stochastic Gradient Descent(12084): loss=0.08865327792088776\n",
      "Stochastic Gradient Descent(12085): loss=7.54578556894994\n",
      "Stochastic Gradient Descent(12086): loss=0.6290461013505206\n",
      "Stochastic Gradient Descent(12087): loss=0.045732597269059136\n",
      "Stochastic Gradient Descent(12088): loss=1.4702550033377308\n",
      "Stochastic Gradient Descent(12089): loss=2.4888808401703577\n",
      "Stochastic Gradient Descent(12090): loss=5.1082432582450386\n",
      "Stochastic Gradient Descent(12091): loss=3.341046509934384\n",
      "Stochastic Gradient Descent(12092): loss=0.14273326989029442\n",
      "Stochastic Gradient Descent(12093): loss=0.7992408837204124\n",
      "Stochastic Gradient Descent(12094): loss=0.6239801248992956\n",
      "Stochastic Gradient Descent(12095): loss=20.980976335438175\n",
      "Stochastic Gradient Descent(12096): loss=4.652608354482366\n",
      "Stochastic Gradient Descent(12097): loss=1.0377285598597918\n",
      "Stochastic Gradient Descent(12098): loss=3.257552591334113\n",
      "Stochastic Gradient Descent(12099): loss=13.647441323084308\n",
      "Stochastic Gradient Descent(12100): loss=1.6470108596827868\n",
      "Stochastic Gradient Descent(12101): loss=0.02102776643779786\n",
      "Stochastic Gradient Descent(12102): loss=0.056523260406029834\n",
      "Stochastic Gradient Descent(12103): loss=0.9999983033996573\n",
      "Stochastic Gradient Descent(12104): loss=18.507615477946413\n",
      "Stochastic Gradient Descent(12105): loss=13.860062516313809\n",
      "Stochastic Gradient Descent(12106): loss=32.21111052152618\n",
      "Stochastic Gradient Descent(12107): loss=11.386113670222278\n",
      "Stochastic Gradient Descent(12108): loss=0.015060036997595743\n",
      "Stochastic Gradient Descent(12109): loss=4.54288412614142\n",
      "Stochastic Gradient Descent(12110): loss=11.640465103250165\n",
      "Stochastic Gradient Descent(12111): loss=5.232303998345246\n",
      "Stochastic Gradient Descent(12112): loss=2.810038203705345\n",
      "Stochastic Gradient Descent(12113): loss=0.16281558624049056\n",
      "Stochastic Gradient Descent(12114): loss=4.104745156219682\n",
      "Stochastic Gradient Descent(12115): loss=4.650756124145296\n",
      "Stochastic Gradient Descent(12116): loss=0.6648269349427691\n",
      "Stochastic Gradient Descent(12117): loss=2.100442014070573\n",
      "Stochastic Gradient Descent(12118): loss=0.1767336128667407\n",
      "Stochastic Gradient Descent(12119): loss=0.37712015271234\n",
      "Stochastic Gradient Descent(12120): loss=3.768122884853689\n",
      "Stochastic Gradient Descent(12121): loss=0.7888327706362038\n",
      "Stochastic Gradient Descent(12122): loss=0.27860222407574775\n",
      "Stochastic Gradient Descent(12123): loss=0.9481313657801207\n",
      "Stochastic Gradient Descent(12124): loss=3.802797059542241\n",
      "Stochastic Gradient Descent(12125): loss=2.0571949578399065\n",
      "Stochastic Gradient Descent(12126): loss=2.034859137130367\n",
      "Stochastic Gradient Descent(12127): loss=0.002972461072893814\n",
      "Stochastic Gradient Descent(12128): loss=12.064146919522665\n",
      "Stochastic Gradient Descent(12129): loss=3.930319873415462\n",
      "Stochastic Gradient Descent(12130): loss=5.5910263590598905\n",
      "Stochastic Gradient Descent(12131): loss=5.832002424136293\n",
      "Stochastic Gradient Descent(12132): loss=2.2791931398088616\n",
      "Stochastic Gradient Descent(12133): loss=2.3903763207899176\n",
      "Stochastic Gradient Descent(12134): loss=0.413580455754715\n",
      "Stochastic Gradient Descent(12135): loss=8.926057537464823\n",
      "Stochastic Gradient Descent(12136): loss=6.589933718900593\n",
      "Stochastic Gradient Descent(12137): loss=8.658831533739148\n",
      "Stochastic Gradient Descent(12138): loss=2.8971838941086685\n",
      "Stochastic Gradient Descent(12139): loss=0.7176160824797668\n",
      "Stochastic Gradient Descent(12140): loss=1.909207698155815\n",
      "Stochastic Gradient Descent(12141): loss=1.3518813353158388\n",
      "Stochastic Gradient Descent(12142): loss=0.15325882555818662\n",
      "Stochastic Gradient Descent(12143): loss=0.033581334672599286\n",
      "Stochastic Gradient Descent(12144): loss=7.180358158547574\n",
      "Stochastic Gradient Descent(12145): loss=0.03132208158506311\n",
      "Stochastic Gradient Descent(12146): loss=0.797834658390575\n",
      "Stochastic Gradient Descent(12147): loss=0.010267771537470771\n",
      "Stochastic Gradient Descent(12148): loss=17.328683227745465\n",
      "Stochastic Gradient Descent(12149): loss=0.17403039762475941\n",
      "Stochastic Gradient Descent(12150): loss=47.25749190504581\n",
      "Stochastic Gradient Descent(12151): loss=1.3619541234139387\n",
      "Stochastic Gradient Descent(12152): loss=0.845722827531177\n",
      "Stochastic Gradient Descent(12153): loss=8.451369658437844\n",
      "Stochastic Gradient Descent(12154): loss=9.594941925542695\n",
      "Stochastic Gradient Descent(12155): loss=4.256415112603448\n",
      "Stochastic Gradient Descent(12156): loss=7.67331161104943\n",
      "Stochastic Gradient Descent(12157): loss=1.9236913445878319\n",
      "Stochastic Gradient Descent(12158): loss=0.0020723620433674692\n",
      "Stochastic Gradient Descent(12159): loss=3.0910080774373387\n",
      "Stochastic Gradient Descent(12160): loss=5.339054504311025\n",
      "Stochastic Gradient Descent(12161): loss=4.189664755877031\n",
      "Stochastic Gradient Descent(12162): loss=2.5867299047891454\n",
      "Stochastic Gradient Descent(12163): loss=1.1998209410939755\n",
      "Stochastic Gradient Descent(12164): loss=0.34267838329940176\n",
      "Stochastic Gradient Descent(12165): loss=1.7667031453596698\n",
      "Stochastic Gradient Descent(12166): loss=0.4378511424465995\n",
      "Stochastic Gradient Descent(12167): loss=8.868222857040747\n",
      "Stochastic Gradient Descent(12168): loss=3.614611050935791\n",
      "Stochastic Gradient Descent(12169): loss=14.167613656518299\n",
      "Stochastic Gradient Descent(12170): loss=1.1518705001530651\n",
      "Stochastic Gradient Descent(12171): loss=2.791276937444666\n",
      "Stochastic Gradient Descent(12172): loss=11.197668959289826\n",
      "Stochastic Gradient Descent(12173): loss=1.525853554757909\n",
      "Stochastic Gradient Descent(12174): loss=12.028613370773991\n",
      "Stochastic Gradient Descent(12175): loss=4.892981355092502\n",
      "Stochastic Gradient Descent(12176): loss=0.5105341255144166\n",
      "Stochastic Gradient Descent(12177): loss=0.9560262012426931\n",
      "Stochastic Gradient Descent(12178): loss=0.01562459153327597\n",
      "Stochastic Gradient Descent(12179): loss=24.175924828984883\n",
      "Stochastic Gradient Descent(12180): loss=1.4619209359955347\n",
      "Stochastic Gradient Descent(12181): loss=0.32867734758906036\n",
      "Stochastic Gradient Descent(12182): loss=7.222458721042653\n",
      "Stochastic Gradient Descent(12183): loss=7.416284351329724\n",
      "Stochastic Gradient Descent(12184): loss=6.2641201260994475\n",
      "Stochastic Gradient Descent(12185): loss=2.0175442258031278\n",
      "Stochastic Gradient Descent(12186): loss=10.279843142051055\n",
      "Stochastic Gradient Descent(12187): loss=2.9401480257723454\n",
      "Stochastic Gradient Descent(12188): loss=1.8058357370421585\n",
      "Stochastic Gradient Descent(12189): loss=0.2890466016345276\n",
      "Stochastic Gradient Descent(12190): loss=15.099329637233977\n",
      "Stochastic Gradient Descent(12191): loss=0.5188186247817604\n",
      "Stochastic Gradient Descent(12192): loss=5.761167314726681\n",
      "Stochastic Gradient Descent(12193): loss=1.0883537422264604\n",
      "Stochastic Gradient Descent(12194): loss=0.11158078735332486\n",
      "Stochastic Gradient Descent(12195): loss=11.96910650162206\n",
      "Stochastic Gradient Descent(12196): loss=10.546284297591002\n",
      "Stochastic Gradient Descent(12197): loss=12.687395849291612\n",
      "Stochastic Gradient Descent(12198): loss=4.780245411026768\n",
      "Stochastic Gradient Descent(12199): loss=2.731091436783754\n",
      "Stochastic Gradient Descent(12200): loss=0.46312440715210396\n",
      "Stochastic Gradient Descent(12201): loss=1.043207505114849\n",
      "Stochastic Gradient Descent(12202): loss=12.259642599287004\n",
      "Stochastic Gradient Descent(12203): loss=9.982841930451789\n",
      "Stochastic Gradient Descent(12204): loss=0.0015002392157657213\n",
      "Stochastic Gradient Descent(12205): loss=7.672713869672803\n",
      "Stochastic Gradient Descent(12206): loss=7.007979789938201\n",
      "Stochastic Gradient Descent(12207): loss=0.18914819640410557\n",
      "Stochastic Gradient Descent(12208): loss=14.847417784488146\n",
      "Stochastic Gradient Descent(12209): loss=0.23153880502564736\n",
      "Stochastic Gradient Descent(12210): loss=3.6702951088671547\n",
      "Stochastic Gradient Descent(12211): loss=3.180772758875928\n",
      "Stochastic Gradient Descent(12212): loss=0.06805097398426824\n",
      "Stochastic Gradient Descent(12213): loss=5.280225143302732\n",
      "Stochastic Gradient Descent(12214): loss=16.739052256298038\n",
      "Stochastic Gradient Descent(12215): loss=0.958645441773788\n",
      "Stochastic Gradient Descent(12216): loss=2.3380908296420584\n",
      "Stochastic Gradient Descent(12217): loss=3.5577524294060425\n",
      "Stochastic Gradient Descent(12218): loss=0.20633515929318078\n",
      "Stochastic Gradient Descent(12219): loss=1.5772728243184355\n",
      "Stochastic Gradient Descent(12220): loss=2.753215125976537\n",
      "Stochastic Gradient Descent(12221): loss=0.8156805100179907\n",
      "Stochastic Gradient Descent(12222): loss=8.827206221952599\n",
      "Stochastic Gradient Descent(12223): loss=0.2997587882820712\n",
      "Stochastic Gradient Descent(12224): loss=8.957637379050617\n",
      "Stochastic Gradient Descent(12225): loss=16.17970663488316\n",
      "Stochastic Gradient Descent(12226): loss=0.04714667022779517\n",
      "Stochastic Gradient Descent(12227): loss=0.5957419324470794\n",
      "Stochastic Gradient Descent(12228): loss=1.0001330105291062\n",
      "Stochastic Gradient Descent(12229): loss=17.510430032225983\n",
      "Stochastic Gradient Descent(12230): loss=3.8878176041822163\n",
      "Stochastic Gradient Descent(12231): loss=15.386314913933502\n",
      "Stochastic Gradient Descent(12232): loss=0.05987101747632588\n",
      "Stochastic Gradient Descent(12233): loss=0.02631520615677555\n",
      "Stochastic Gradient Descent(12234): loss=3.176123228508591\n",
      "Stochastic Gradient Descent(12235): loss=10.193200359167284\n",
      "Stochastic Gradient Descent(12236): loss=42.56342310902471\n",
      "Stochastic Gradient Descent(12237): loss=45.248319083647765\n",
      "Stochastic Gradient Descent(12238): loss=0.04654458692174579\n",
      "Stochastic Gradient Descent(12239): loss=3.7289844012236477\n",
      "Stochastic Gradient Descent(12240): loss=2.27537570325659\n",
      "Stochastic Gradient Descent(12241): loss=2.800100323331044\n",
      "Stochastic Gradient Descent(12242): loss=17.188121608482916\n",
      "Stochastic Gradient Descent(12243): loss=0.21471582795646843\n",
      "Stochastic Gradient Descent(12244): loss=33.64379012447602\n",
      "Stochastic Gradient Descent(12245): loss=16.62373908007303\n",
      "Stochastic Gradient Descent(12246): loss=0.6477195857301377\n",
      "Stochastic Gradient Descent(12247): loss=3.4499715484824356\n",
      "Stochastic Gradient Descent(12248): loss=2.852663733610447\n",
      "Stochastic Gradient Descent(12249): loss=1.3352682419139859\n",
      "Stochastic Gradient Descent(12250): loss=23.15549354411064\n",
      "Stochastic Gradient Descent(12251): loss=5.6524793738137316\n",
      "Stochastic Gradient Descent(12252): loss=0.48532550809456426\n",
      "Stochastic Gradient Descent(12253): loss=16.53631954352792\n",
      "Stochastic Gradient Descent(12254): loss=0.11222288651777142\n",
      "Stochastic Gradient Descent(12255): loss=4.263667686577463\n",
      "Stochastic Gradient Descent(12256): loss=0.30286178483225656\n",
      "Stochastic Gradient Descent(12257): loss=34.79293360291078\n",
      "Stochastic Gradient Descent(12258): loss=0.779059164154679\n",
      "Stochastic Gradient Descent(12259): loss=0.009579128698501034\n",
      "Stochastic Gradient Descent(12260): loss=1.4332268391551346\n",
      "Stochastic Gradient Descent(12261): loss=0.2939711979050632\n",
      "Stochastic Gradient Descent(12262): loss=8.932479494820948\n",
      "Stochastic Gradient Descent(12263): loss=3.7915453583123795\n",
      "Stochastic Gradient Descent(12264): loss=14.7947414422946\n",
      "Stochastic Gradient Descent(12265): loss=0.01633781846041479\n",
      "Stochastic Gradient Descent(12266): loss=0.07805226965134965\n",
      "Stochastic Gradient Descent(12267): loss=0.5876354396410235\n",
      "Stochastic Gradient Descent(12268): loss=0.018494255252323864\n",
      "Stochastic Gradient Descent(12269): loss=0.738968209865978\n",
      "Stochastic Gradient Descent(12270): loss=2.019619607864783\n",
      "Stochastic Gradient Descent(12271): loss=8.516200337058464\n",
      "Stochastic Gradient Descent(12272): loss=1.5440016087227528\n",
      "Stochastic Gradient Descent(12273): loss=0.6103780207610922\n",
      "Stochastic Gradient Descent(12274): loss=0.9619907121151288\n",
      "Stochastic Gradient Descent(12275): loss=1.1427054921053876\n",
      "Stochastic Gradient Descent(12276): loss=2.1504477381758416\n",
      "Stochastic Gradient Descent(12277): loss=7.989097742158551\n",
      "Stochastic Gradient Descent(12278): loss=0.010825557064109082\n",
      "Stochastic Gradient Descent(12279): loss=13.882595598065967\n",
      "Stochastic Gradient Descent(12280): loss=1.1152244148068264\n",
      "Stochastic Gradient Descent(12281): loss=0.5129603935424795\n",
      "Stochastic Gradient Descent(12282): loss=6.351332764616429\n",
      "Stochastic Gradient Descent(12283): loss=9.717437272167132\n",
      "Stochastic Gradient Descent(12284): loss=0.09673452659011117\n",
      "Stochastic Gradient Descent(12285): loss=0.3260130989435808\n",
      "Stochastic Gradient Descent(12286): loss=10.328526461975361\n",
      "Stochastic Gradient Descent(12287): loss=5.393324851473105\n",
      "Stochastic Gradient Descent(12288): loss=1.6427290938069619\n",
      "Stochastic Gradient Descent(12289): loss=6.019243857032777\n",
      "Stochastic Gradient Descent(12290): loss=0.7081504578373444\n",
      "Stochastic Gradient Descent(12291): loss=30.66624451549412\n",
      "Stochastic Gradient Descent(12292): loss=16.352045159910134\n",
      "Stochastic Gradient Descent(12293): loss=8.12313533654731\n",
      "Stochastic Gradient Descent(12294): loss=5.909899884571994\n",
      "Stochastic Gradient Descent(12295): loss=4.469285834029391\n",
      "Stochastic Gradient Descent(12296): loss=0.09416327297696533\n",
      "Stochastic Gradient Descent(12297): loss=1.411199770103663\n",
      "Stochastic Gradient Descent(12298): loss=4.48909121291556e-05\n",
      "Stochastic Gradient Descent(12299): loss=0.010423519661626952\n",
      "Stochastic Gradient Descent(12300): loss=2.04381599586156\n",
      "Stochastic Gradient Descent(12301): loss=0.5200227880921476\n",
      "Stochastic Gradient Descent(12302): loss=0.04864724213913135\n",
      "Stochastic Gradient Descent(12303): loss=0.8552849677221064\n",
      "Stochastic Gradient Descent(12304): loss=18.271745000514763\n",
      "Stochastic Gradient Descent(12305): loss=16.738193892653943\n",
      "Stochastic Gradient Descent(12306): loss=1.959808517521124\n",
      "Stochastic Gradient Descent(12307): loss=2.119944656721909\n",
      "Stochastic Gradient Descent(12308): loss=19.617023420817347\n",
      "Stochastic Gradient Descent(12309): loss=1.4464636721955562\n",
      "Stochastic Gradient Descent(12310): loss=2.1921615767149945\n",
      "Stochastic Gradient Descent(12311): loss=0.052858863187544146\n",
      "Stochastic Gradient Descent(12312): loss=3.38777024925336\n",
      "Stochastic Gradient Descent(12313): loss=3.6244632502176617\n",
      "Stochastic Gradient Descent(12314): loss=0.27747756502004306\n",
      "Stochastic Gradient Descent(12315): loss=8.817867569791451\n",
      "Stochastic Gradient Descent(12316): loss=0.22483990862206082\n",
      "Stochastic Gradient Descent(12317): loss=0.41220799569506766\n",
      "Stochastic Gradient Descent(12318): loss=0.886204542587947\n",
      "Stochastic Gradient Descent(12319): loss=3.394434016383477\n",
      "Stochastic Gradient Descent(12320): loss=0.5810972061490844\n",
      "Stochastic Gradient Descent(12321): loss=0.5206232532283734\n",
      "Stochastic Gradient Descent(12322): loss=0.0003306146164919733\n",
      "Stochastic Gradient Descent(12323): loss=0.40026545940085134\n",
      "Stochastic Gradient Descent(12324): loss=0.027478604023660196\n",
      "Stochastic Gradient Descent(12325): loss=2.362465355805244\n",
      "Stochastic Gradient Descent(12326): loss=0.4326271845663\n",
      "Stochastic Gradient Descent(12327): loss=8.11742422497387\n",
      "Stochastic Gradient Descent(12328): loss=0.5696876303312045\n",
      "Stochastic Gradient Descent(12329): loss=0.007734796292031455\n",
      "Stochastic Gradient Descent(12330): loss=31.20921738143346\n",
      "Stochastic Gradient Descent(12331): loss=0.06434138621175083\n",
      "Stochastic Gradient Descent(12332): loss=0.2713967947288818\n",
      "Stochastic Gradient Descent(12333): loss=6.743191660835431\n",
      "Stochastic Gradient Descent(12334): loss=1.6691531476205146\n",
      "Stochastic Gradient Descent(12335): loss=0.4368341906848831\n",
      "Stochastic Gradient Descent(12336): loss=0.004967482753883101\n",
      "Stochastic Gradient Descent(12337): loss=3.143849511177696\n",
      "Stochastic Gradient Descent(12338): loss=2.128838478659448\n",
      "Stochastic Gradient Descent(12339): loss=0.9596932095041986\n",
      "Stochastic Gradient Descent(12340): loss=21.335177067861594\n",
      "Stochastic Gradient Descent(12341): loss=3.4055662226327317\n",
      "Stochastic Gradient Descent(12342): loss=0.43595860701759415\n",
      "Stochastic Gradient Descent(12343): loss=0.8881882126871423\n",
      "Stochastic Gradient Descent(12344): loss=3.6890644864039603\n",
      "Stochastic Gradient Descent(12345): loss=8.971265537784829\n",
      "Stochastic Gradient Descent(12346): loss=0.21529118110756254\n",
      "Stochastic Gradient Descent(12347): loss=1.65820609373279\n",
      "Stochastic Gradient Descent(12348): loss=4.1083059688808055\n",
      "Stochastic Gradient Descent(12349): loss=2.621511185575145\n",
      "Stochastic Gradient Descent(12350): loss=21.876392759536255\n",
      "Stochastic Gradient Descent(12351): loss=3.160381153149037\n",
      "Stochastic Gradient Descent(12352): loss=0.005840616277078398\n",
      "Stochastic Gradient Descent(12353): loss=0.7938732337693181\n",
      "Stochastic Gradient Descent(12354): loss=16.121018592835963\n",
      "Stochastic Gradient Descent(12355): loss=1.554866092069233\n",
      "Stochastic Gradient Descent(12356): loss=8.458294217377402\n",
      "Stochastic Gradient Descent(12357): loss=0.15930995087158834\n",
      "Stochastic Gradient Descent(12358): loss=26.38207804236166\n",
      "Stochastic Gradient Descent(12359): loss=0.4658279652011043\n",
      "Stochastic Gradient Descent(12360): loss=0.9278710691976518\n",
      "Stochastic Gradient Descent(12361): loss=7.092875194015425\n",
      "Stochastic Gradient Descent(12362): loss=6.907321479697467\n",
      "Stochastic Gradient Descent(12363): loss=6.75310417974179\n",
      "Stochastic Gradient Descent(12364): loss=0.9012596179335163\n",
      "Stochastic Gradient Descent(12365): loss=1.6927591365404306\n",
      "Stochastic Gradient Descent(12366): loss=0.11253251680297244\n",
      "Stochastic Gradient Descent(12367): loss=0.11749199292615135\n",
      "Stochastic Gradient Descent(12368): loss=4.551335950608956\n",
      "Stochastic Gradient Descent(12369): loss=4.399027331756385\n",
      "Stochastic Gradient Descent(12370): loss=9.160484028173217\n",
      "Stochastic Gradient Descent(12371): loss=11.329591132464074\n",
      "Stochastic Gradient Descent(12372): loss=7.194304220311565\n",
      "Stochastic Gradient Descent(12373): loss=12.310574555586907\n",
      "Stochastic Gradient Descent(12374): loss=5.819185740413476\n",
      "Stochastic Gradient Descent(12375): loss=5.557231345128389\n",
      "Stochastic Gradient Descent(12376): loss=15.681236908143624\n",
      "Stochastic Gradient Descent(12377): loss=4.067712882552898\n",
      "Stochastic Gradient Descent(12378): loss=2.4876338070947996\n",
      "Stochastic Gradient Descent(12379): loss=0.7186946057231121\n",
      "Stochastic Gradient Descent(12380): loss=0.0007156406839737361\n",
      "Stochastic Gradient Descent(12381): loss=0.17170356892136496\n",
      "Stochastic Gradient Descent(12382): loss=5.750811855203647\n",
      "Stochastic Gradient Descent(12383): loss=79.44689718610704\n",
      "Stochastic Gradient Descent(12384): loss=0.05183828040452153\n",
      "Stochastic Gradient Descent(12385): loss=0.0019112458141238979\n",
      "Stochastic Gradient Descent(12386): loss=26.51409736975814\n",
      "Stochastic Gradient Descent(12387): loss=56.98057836762289\n",
      "Stochastic Gradient Descent(12388): loss=10.553088180360884\n",
      "Stochastic Gradient Descent(12389): loss=1.143013423392995\n",
      "Stochastic Gradient Descent(12390): loss=1.7204545947936773\n",
      "Stochastic Gradient Descent(12391): loss=3.1638934563614654\n",
      "Stochastic Gradient Descent(12392): loss=33.555114830039834\n",
      "Stochastic Gradient Descent(12393): loss=5.783744953439141\n",
      "Stochastic Gradient Descent(12394): loss=18.12428797695132\n",
      "Stochastic Gradient Descent(12395): loss=0.2238492723228901\n",
      "Stochastic Gradient Descent(12396): loss=5.305581713856556\n",
      "Stochastic Gradient Descent(12397): loss=0.025940874757708714\n",
      "Stochastic Gradient Descent(12398): loss=4.5720140022830185\n",
      "Stochastic Gradient Descent(12399): loss=3.444495913555539\n",
      "Stochastic Gradient Descent(12400): loss=0.2969660448059156\n",
      "Stochastic Gradient Descent(12401): loss=0.724250598283393\n",
      "Stochastic Gradient Descent(12402): loss=1.4253689232746485\n",
      "Stochastic Gradient Descent(12403): loss=5.265442612852903\n",
      "Stochastic Gradient Descent(12404): loss=0.575151825902906\n",
      "Stochastic Gradient Descent(12405): loss=0.003539840887640632\n",
      "Stochastic Gradient Descent(12406): loss=6.747189220853494\n",
      "Stochastic Gradient Descent(12407): loss=0.4799787489565311\n",
      "Stochastic Gradient Descent(12408): loss=1.4636809485316762\n",
      "Stochastic Gradient Descent(12409): loss=1.9336676197155045\n",
      "Stochastic Gradient Descent(12410): loss=0.49460498150605475\n",
      "Stochastic Gradient Descent(12411): loss=2.2019706112195214\n",
      "Stochastic Gradient Descent(12412): loss=0.8431121330976502\n",
      "Stochastic Gradient Descent(12413): loss=0.0002016952522368873\n",
      "Stochastic Gradient Descent(12414): loss=2.823008143164278\n",
      "Stochastic Gradient Descent(12415): loss=0.8404949853649453\n",
      "Stochastic Gradient Descent(12416): loss=24.005760619052698\n",
      "Stochastic Gradient Descent(12417): loss=17.3181827992722\n",
      "Stochastic Gradient Descent(12418): loss=10.091166643018362\n",
      "Stochastic Gradient Descent(12419): loss=0.8735677944833994\n",
      "Stochastic Gradient Descent(12420): loss=2.7968236410596945\n",
      "Stochastic Gradient Descent(12421): loss=38.59806170909969\n",
      "Stochastic Gradient Descent(12422): loss=0.010547502897017736\n",
      "Stochastic Gradient Descent(12423): loss=8.392099338153919\n",
      "Stochastic Gradient Descent(12424): loss=32.878087937816495\n",
      "Stochastic Gradient Descent(12425): loss=6.992969223174581\n",
      "Stochastic Gradient Descent(12426): loss=8.75431828257129\n",
      "Stochastic Gradient Descent(12427): loss=4.647252721100811\n",
      "Stochastic Gradient Descent(12428): loss=0.04511901357867089\n",
      "Stochastic Gradient Descent(12429): loss=6.712301032041238\n",
      "Stochastic Gradient Descent(12430): loss=2.6043227379813505\n",
      "Stochastic Gradient Descent(12431): loss=0.07484164829283152\n",
      "Stochastic Gradient Descent(12432): loss=1.1184309402228914\n",
      "Stochastic Gradient Descent(12433): loss=0.02749255747396802\n",
      "Stochastic Gradient Descent(12434): loss=23.533898308062195\n",
      "Stochastic Gradient Descent(12435): loss=0.29993646759232034\n",
      "Stochastic Gradient Descent(12436): loss=25.350077127069824\n",
      "Stochastic Gradient Descent(12437): loss=0.15925140352091063\n",
      "Stochastic Gradient Descent(12438): loss=0.4214053048737819\n",
      "Stochastic Gradient Descent(12439): loss=0.8040479217795182\n",
      "Stochastic Gradient Descent(12440): loss=0.15413063997214507\n",
      "Stochastic Gradient Descent(12441): loss=14.154067250570499\n",
      "Stochastic Gradient Descent(12442): loss=0.1967202353763116\n",
      "Stochastic Gradient Descent(12443): loss=2.3782879654236306\n",
      "Stochastic Gradient Descent(12444): loss=0.9434963803975555\n",
      "Stochastic Gradient Descent(12445): loss=1.6995051731913693\n",
      "Stochastic Gradient Descent(12446): loss=13.183417570141952\n",
      "Stochastic Gradient Descent(12447): loss=9.466256477542636\n",
      "Stochastic Gradient Descent(12448): loss=0.08821866106842445\n",
      "Stochastic Gradient Descent(12449): loss=1.4220381907132649\n",
      "Stochastic Gradient Descent(12450): loss=1.9564468318991193\n",
      "Stochastic Gradient Descent(12451): loss=1.3145808496972304\n",
      "Stochastic Gradient Descent(12452): loss=0.13665710121106733\n",
      "Stochastic Gradient Descent(12453): loss=7.712350419063424\n",
      "Stochastic Gradient Descent(12454): loss=0.7056400527869823\n",
      "Stochastic Gradient Descent(12455): loss=13.927742485361044\n",
      "Stochastic Gradient Descent(12456): loss=6.876593305792932\n",
      "Stochastic Gradient Descent(12457): loss=0.01588271565246181\n",
      "Stochastic Gradient Descent(12458): loss=3.941382741710791\n",
      "Stochastic Gradient Descent(12459): loss=9.7181405763422\n",
      "Stochastic Gradient Descent(12460): loss=17.65831454460992\n",
      "Stochastic Gradient Descent(12461): loss=3.6386681118367252\n",
      "Stochastic Gradient Descent(12462): loss=4.148406321007914\n",
      "Stochastic Gradient Descent(12463): loss=0.2655683205334964\n",
      "Stochastic Gradient Descent(12464): loss=0.682934980238402\n",
      "Stochastic Gradient Descent(12465): loss=0.11072080021589058\n",
      "Stochastic Gradient Descent(12466): loss=3.332317852430414\n",
      "Stochastic Gradient Descent(12467): loss=0.28632037629496554\n",
      "Stochastic Gradient Descent(12468): loss=2.0773002561279865\n",
      "Stochastic Gradient Descent(12469): loss=1.3353486880743524\n",
      "Stochastic Gradient Descent(12470): loss=0.09382551089056318\n",
      "Stochastic Gradient Descent(12471): loss=2.653150827637824\n",
      "Stochastic Gradient Descent(12472): loss=5.58445150231547\n",
      "Stochastic Gradient Descent(12473): loss=0.00018388792467471504\n",
      "Stochastic Gradient Descent(12474): loss=1.514623007254073\n",
      "Stochastic Gradient Descent(12475): loss=11.102116899186553\n",
      "Stochastic Gradient Descent(12476): loss=0.6861493695524351\n",
      "Stochastic Gradient Descent(12477): loss=3.0333245073369444\n",
      "Stochastic Gradient Descent(12478): loss=7.987924285413447\n",
      "Stochastic Gradient Descent(12479): loss=0.02540613747744097\n",
      "Stochastic Gradient Descent(12480): loss=0.02609724606473823\n",
      "Stochastic Gradient Descent(12481): loss=0.8593362127479474\n",
      "Stochastic Gradient Descent(12482): loss=23.10125322515602\n",
      "Stochastic Gradient Descent(12483): loss=3.528295475616558\n",
      "Stochastic Gradient Descent(12484): loss=0.0396446281702871\n",
      "Stochastic Gradient Descent(12485): loss=2.0323142307449182\n",
      "Stochastic Gradient Descent(12486): loss=4.668657008221358\n",
      "Stochastic Gradient Descent(12487): loss=18.47585253804143\n",
      "Stochastic Gradient Descent(12488): loss=0.1391607712957534\n",
      "Stochastic Gradient Descent(12489): loss=11.68292276437475\n",
      "Stochastic Gradient Descent(12490): loss=1.6365560096869844\n",
      "Stochastic Gradient Descent(12491): loss=16.655923601797358\n",
      "Stochastic Gradient Descent(12492): loss=0.2826378936554161\n",
      "Stochastic Gradient Descent(12493): loss=0.9913523064613087\n",
      "Stochastic Gradient Descent(12494): loss=0.026114028826979906\n",
      "Stochastic Gradient Descent(12495): loss=7.447676924883854\n",
      "Stochastic Gradient Descent(12496): loss=0.4302101222090006\n",
      "Stochastic Gradient Descent(12497): loss=4.108264853075864\n",
      "Stochastic Gradient Descent(12498): loss=9.836220628422465e-05\n",
      "Stochastic Gradient Descent(12499): loss=0.34235432606011645\n",
      "Stochastic Gradient Descent(12500): loss=0.9194603847742437\n",
      "Stochastic Gradient Descent(12501): loss=0.2557991659204454\n",
      "Stochastic Gradient Descent(12502): loss=3.399808632639015\n",
      "Stochastic Gradient Descent(12503): loss=15.7472406240466\n",
      "Stochastic Gradient Descent(12504): loss=8.305607709170925\n",
      "Stochastic Gradient Descent(12505): loss=10.74385390033797\n",
      "Stochastic Gradient Descent(12506): loss=0.3275784491492578\n",
      "Stochastic Gradient Descent(12507): loss=3.7279520620413527\n",
      "Stochastic Gradient Descent(12508): loss=4.561312478922511\n",
      "Stochastic Gradient Descent(12509): loss=0.4695317688637885\n",
      "Stochastic Gradient Descent(12510): loss=21.056701424082725\n",
      "Stochastic Gradient Descent(12511): loss=0.8584537893222938\n",
      "Stochastic Gradient Descent(12512): loss=3.7880281040955754\n",
      "Stochastic Gradient Descent(12513): loss=1.5952861398201144\n",
      "Stochastic Gradient Descent(12514): loss=6.146533623879627\n",
      "Stochastic Gradient Descent(12515): loss=6.850669315122074\n",
      "Stochastic Gradient Descent(12516): loss=0.17633877966210093\n",
      "Stochastic Gradient Descent(12517): loss=3.8911457705186705\n",
      "Stochastic Gradient Descent(12518): loss=7.977576687541389\n",
      "Stochastic Gradient Descent(12519): loss=24.100913958807723\n",
      "Stochastic Gradient Descent(12520): loss=0.3344743832241245\n",
      "Stochastic Gradient Descent(12521): loss=0.3306594953187185\n",
      "Stochastic Gradient Descent(12522): loss=5.4432194114725005\n",
      "Stochastic Gradient Descent(12523): loss=5.5327759963470555\n",
      "Stochastic Gradient Descent(12524): loss=2.1019514327494146\n",
      "Stochastic Gradient Descent(12525): loss=14.069446927081126\n",
      "Stochastic Gradient Descent(12526): loss=3.6966124937327627\n",
      "Stochastic Gradient Descent(12527): loss=3.907745961930708\n",
      "Stochastic Gradient Descent(12528): loss=1.4023859712982152\n",
      "Stochastic Gradient Descent(12529): loss=13.34042250413349\n",
      "Stochastic Gradient Descent(12530): loss=27.370686881046264\n",
      "Stochastic Gradient Descent(12531): loss=14.396366618597664\n",
      "Stochastic Gradient Descent(12532): loss=11.922979789079776\n",
      "Stochastic Gradient Descent(12533): loss=11.351638958638661\n",
      "Stochastic Gradient Descent(12534): loss=0.69579303977067\n",
      "Stochastic Gradient Descent(12535): loss=3.3018701658387384\n",
      "Stochastic Gradient Descent(12536): loss=6.242024435482839\n",
      "Stochastic Gradient Descent(12537): loss=0.7196912856066902\n",
      "Stochastic Gradient Descent(12538): loss=3.935219620801728\n",
      "Stochastic Gradient Descent(12539): loss=0.025799487300700374\n",
      "Stochastic Gradient Descent(12540): loss=2.141485849156101\n",
      "Stochastic Gradient Descent(12541): loss=0.9746462516148499\n",
      "Stochastic Gradient Descent(12542): loss=16.94625034307367\n",
      "Stochastic Gradient Descent(12543): loss=3.424796966457294\n",
      "Stochastic Gradient Descent(12544): loss=5.472102388436294\n",
      "Stochastic Gradient Descent(12545): loss=5.293728521616603\n",
      "Stochastic Gradient Descent(12546): loss=1.056375190609763\n",
      "Stochastic Gradient Descent(12547): loss=10.164625430188261\n",
      "Stochastic Gradient Descent(12548): loss=8.260882189306875\n",
      "Stochastic Gradient Descent(12549): loss=0.15599113067605905\n",
      "Stochastic Gradient Descent(12550): loss=0.0317103370556463\n",
      "Stochastic Gradient Descent(12551): loss=12.35615084382962\n",
      "Stochastic Gradient Descent(12552): loss=0.026002064403305446\n",
      "Stochastic Gradient Descent(12553): loss=1.595717506479452\n",
      "Stochastic Gradient Descent(12554): loss=5.763011155932462\n",
      "Stochastic Gradient Descent(12555): loss=9.863932147442931\n",
      "Stochastic Gradient Descent(12556): loss=0.22012652023688226\n",
      "Stochastic Gradient Descent(12557): loss=0.23859915352277758\n",
      "Stochastic Gradient Descent(12558): loss=0.0866648852185469\n",
      "Stochastic Gradient Descent(12559): loss=7.88614902592917\n",
      "Stochastic Gradient Descent(12560): loss=1.9340195283803006\n",
      "Stochastic Gradient Descent(12561): loss=10.796907835025515\n",
      "Stochastic Gradient Descent(12562): loss=0.9443649549657173\n",
      "Stochastic Gradient Descent(12563): loss=3.0145741112299413\n",
      "Stochastic Gradient Descent(12564): loss=2.4667279760238108\n",
      "Stochastic Gradient Descent(12565): loss=0.3199405494038644\n",
      "Stochastic Gradient Descent(12566): loss=20.866553093444182\n",
      "Stochastic Gradient Descent(12567): loss=10.935303452459467\n",
      "Stochastic Gradient Descent(12568): loss=0.6002645996582399\n",
      "Stochastic Gradient Descent(12569): loss=0.5390514742709777\n",
      "Stochastic Gradient Descent(12570): loss=0.19828801780233163\n",
      "Stochastic Gradient Descent(12571): loss=17.846070279325456\n",
      "Stochastic Gradient Descent(12572): loss=15.188130153565986\n",
      "Stochastic Gradient Descent(12573): loss=2.6709225454033767\n",
      "Stochastic Gradient Descent(12574): loss=5.898348983203725\n",
      "Stochastic Gradient Descent(12575): loss=0.6407990999544532\n",
      "Stochastic Gradient Descent(12576): loss=6.189953198418577\n",
      "Stochastic Gradient Descent(12577): loss=92.09552369976343\n",
      "Stochastic Gradient Descent(12578): loss=12.258503662210988\n",
      "Stochastic Gradient Descent(12579): loss=41.06777325650152\n",
      "Stochastic Gradient Descent(12580): loss=9.58128916231673\n",
      "Stochastic Gradient Descent(12581): loss=493.07334225653364\n",
      "Stochastic Gradient Descent(12582): loss=106.90449446381788\n",
      "Stochastic Gradient Descent(12583): loss=239.18143450686046\n",
      "Stochastic Gradient Descent(12584): loss=348.13136823471126\n",
      "Stochastic Gradient Descent(12585): loss=5.5902883252038436\n",
      "Stochastic Gradient Descent(12586): loss=7.752577261586498\n",
      "Stochastic Gradient Descent(12587): loss=6.767642096049778\n",
      "Stochastic Gradient Descent(12588): loss=0.1614097571073744\n",
      "Stochastic Gradient Descent(12589): loss=6.174205582267611\n",
      "Stochastic Gradient Descent(12590): loss=30.603191827049432\n",
      "Stochastic Gradient Descent(12591): loss=20.929083013056\n",
      "Stochastic Gradient Descent(12592): loss=5.256192572565115\n",
      "Stochastic Gradient Descent(12593): loss=3.599570855927848\n",
      "Stochastic Gradient Descent(12594): loss=2.1693235603214918\n",
      "Stochastic Gradient Descent(12595): loss=1.5666746430104885\n",
      "Stochastic Gradient Descent(12596): loss=17.423763726215316\n",
      "Stochastic Gradient Descent(12597): loss=19.97833729848275\n",
      "Stochastic Gradient Descent(12598): loss=2.888619718461965\n",
      "Stochastic Gradient Descent(12599): loss=3.0303492385352686\n",
      "Stochastic Gradient Descent(12600): loss=8.30144854518403\n",
      "Stochastic Gradient Descent(12601): loss=0.3368596952459094\n",
      "Stochastic Gradient Descent(12602): loss=2.3677090566701606\n",
      "Stochastic Gradient Descent(12603): loss=6.09816587939403\n",
      "Stochastic Gradient Descent(12604): loss=0.00017201490611902848\n",
      "Stochastic Gradient Descent(12605): loss=1.317522666577139\n",
      "Stochastic Gradient Descent(12606): loss=0.0020696175435815896\n",
      "Stochastic Gradient Descent(12607): loss=3.6423416799262314\n",
      "Stochastic Gradient Descent(12608): loss=0.5500531206714118\n",
      "Stochastic Gradient Descent(12609): loss=0.06295498499989682\n",
      "Stochastic Gradient Descent(12610): loss=0.6433184974291197\n",
      "Stochastic Gradient Descent(12611): loss=0.004462165045032896\n",
      "Stochastic Gradient Descent(12612): loss=0.4431766869981486\n",
      "Stochastic Gradient Descent(12613): loss=2.7270612654194033\n",
      "Stochastic Gradient Descent(12614): loss=0.8021189368229582\n",
      "Stochastic Gradient Descent(12615): loss=0.3314373998626267\n",
      "Stochastic Gradient Descent(12616): loss=0.12876493973580924\n",
      "Stochastic Gradient Descent(12617): loss=5.59098250314071\n",
      "Stochastic Gradient Descent(12618): loss=17.351304539928094\n",
      "Stochastic Gradient Descent(12619): loss=0.013287075580234687\n",
      "Stochastic Gradient Descent(12620): loss=4.087893608160524\n",
      "Stochastic Gradient Descent(12621): loss=0.717832259024106\n",
      "Stochastic Gradient Descent(12622): loss=25.68446327228774\n",
      "Stochastic Gradient Descent(12623): loss=5.3997449331104805\n",
      "Stochastic Gradient Descent(12624): loss=12.421783479140469\n",
      "Stochastic Gradient Descent(12625): loss=0.0807434590468223\n",
      "Stochastic Gradient Descent(12626): loss=0.08712801153831408\n",
      "Stochastic Gradient Descent(12627): loss=3.673363832318447\n",
      "Stochastic Gradient Descent(12628): loss=7.996132574860173\n",
      "Stochastic Gradient Descent(12629): loss=19.644541199055478\n",
      "Stochastic Gradient Descent(12630): loss=10.325684639027049\n",
      "Stochastic Gradient Descent(12631): loss=20.359669327587895\n",
      "Stochastic Gradient Descent(12632): loss=1.0538188412582126\n",
      "Stochastic Gradient Descent(12633): loss=9.185304088355942\n",
      "Stochastic Gradient Descent(12634): loss=0.4038304578008848\n",
      "Stochastic Gradient Descent(12635): loss=0.1729339973716228\n",
      "Stochastic Gradient Descent(12636): loss=0.24968293465873914\n",
      "Stochastic Gradient Descent(12637): loss=1.0994922997947436\n",
      "Stochastic Gradient Descent(12638): loss=2.6086280367562567\n",
      "Stochastic Gradient Descent(12639): loss=3.392761755748046\n",
      "Stochastic Gradient Descent(12640): loss=1.84148158981205\n",
      "Stochastic Gradient Descent(12641): loss=7.120129529058722\n",
      "Stochastic Gradient Descent(12642): loss=0.8353008266506883\n",
      "Stochastic Gradient Descent(12643): loss=0.007188473927731974\n",
      "Stochastic Gradient Descent(12644): loss=1.7221321179353282\n",
      "Stochastic Gradient Descent(12645): loss=0.3421382737677394\n",
      "Stochastic Gradient Descent(12646): loss=4.442383347468573\n",
      "Stochastic Gradient Descent(12647): loss=0.34137669966114537\n",
      "Stochastic Gradient Descent(12648): loss=5.079009818610882\n",
      "Stochastic Gradient Descent(12649): loss=1.2780132908654425\n",
      "Stochastic Gradient Descent(12650): loss=9.658871788717942\n",
      "Stochastic Gradient Descent(12651): loss=0.6820508624643719\n",
      "Stochastic Gradient Descent(12652): loss=1.153355600781948\n",
      "Stochastic Gradient Descent(12653): loss=4.63794928072218\n",
      "Stochastic Gradient Descent(12654): loss=1.4053438023654279\n",
      "Stochastic Gradient Descent(12655): loss=9.369082311700526\n",
      "Stochastic Gradient Descent(12656): loss=2.5404543167150355\n",
      "Stochastic Gradient Descent(12657): loss=20.836356214937517\n",
      "Stochastic Gradient Descent(12658): loss=0.12811324287383027\n",
      "Stochastic Gradient Descent(12659): loss=23.626880399097324\n",
      "Stochastic Gradient Descent(12660): loss=5.6848916831993055\n",
      "Stochastic Gradient Descent(12661): loss=0.11875649741403173\n",
      "Stochastic Gradient Descent(12662): loss=0.00023161274491827988\n",
      "Stochastic Gradient Descent(12663): loss=0.00812110946302253\n",
      "Stochastic Gradient Descent(12664): loss=4.353641515247354\n",
      "Stochastic Gradient Descent(12665): loss=1.5986449555480395\n",
      "Stochastic Gradient Descent(12666): loss=1.1943103058571587\n",
      "Stochastic Gradient Descent(12667): loss=3.028035722118467\n",
      "Stochastic Gradient Descent(12668): loss=1.7524235932102519\n",
      "Stochastic Gradient Descent(12669): loss=7.172842194573955\n",
      "Stochastic Gradient Descent(12670): loss=0.9348546593866296\n",
      "Stochastic Gradient Descent(12671): loss=1.066524825604125\n",
      "Stochastic Gradient Descent(12672): loss=2.247104032510197\n",
      "Stochastic Gradient Descent(12673): loss=32.84063038690087\n",
      "Stochastic Gradient Descent(12674): loss=0.03440205399098081\n",
      "Stochastic Gradient Descent(12675): loss=6.418929393883753\n",
      "Stochastic Gradient Descent(12676): loss=0.0016719688411172635\n",
      "Stochastic Gradient Descent(12677): loss=15.71342584993071\n",
      "Stochastic Gradient Descent(12678): loss=0.0868968128056148\n",
      "Stochastic Gradient Descent(12679): loss=0.3468828325091119\n",
      "Stochastic Gradient Descent(12680): loss=0.04424246193202297\n",
      "Stochastic Gradient Descent(12681): loss=4.9183435356015694\n",
      "Stochastic Gradient Descent(12682): loss=0.20847226204400507\n",
      "Stochastic Gradient Descent(12683): loss=4.2404635510925734\n",
      "Stochastic Gradient Descent(12684): loss=0.408741173246773\n",
      "Stochastic Gradient Descent(12685): loss=2.2838939967350016\n",
      "Stochastic Gradient Descent(12686): loss=0.3849386836111154\n",
      "Stochastic Gradient Descent(12687): loss=12.560465946903635\n",
      "Stochastic Gradient Descent(12688): loss=12.15944558424547\n",
      "Stochastic Gradient Descent(12689): loss=1.9843357893444649\n",
      "Stochastic Gradient Descent(12690): loss=14.061952253595312\n",
      "Stochastic Gradient Descent(12691): loss=4.683800977881698\n",
      "Stochastic Gradient Descent(12692): loss=0.06437414099281266\n",
      "Stochastic Gradient Descent(12693): loss=10.02151990240048\n",
      "Stochastic Gradient Descent(12694): loss=0.814004259866675\n",
      "Stochastic Gradient Descent(12695): loss=6.097394948961051\n",
      "Stochastic Gradient Descent(12696): loss=4.307799538361336\n",
      "Stochastic Gradient Descent(12697): loss=0.16060637597573812\n",
      "Stochastic Gradient Descent(12698): loss=0.013121023624561\n",
      "Stochastic Gradient Descent(12699): loss=1.384420909514413\n",
      "Stochastic Gradient Descent(12700): loss=2.8371577212853376\n",
      "Stochastic Gradient Descent(12701): loss=0.03173068688235881\n",
      "Stochastic Gradient Descent(12702): loss=0.0001629452971336321\n",
      "Stochastic Gradient Descent(12703): loss=1.7694162899846828\n",
      "Stochastic Gradient Descent(12704): loss=1.1479884102198348\n",
      "Stochastic Gradient Descent(12705): loss=5.0718557571290255\n",
      "Stochastic Gradient Descent(12706): loss=2.1360576115265157\n",
      "Stochastic Gradient Descent(12707): loss=2.3997883593361373\n",
      "Stochastic Gradient Descent(12708): loss=12.001922593427198\n",
      "Stochastic Gradient Descent(12709): loss=0.93704540733427\n",
      "Stochastic Gradient Descent(12710): loss=2.384825919365906\n",
      "Stochastic Gradient Descent(12711): loss=0.009453420336352757\n",
      "Stochastic Gradient Descent(12712): loss=9.303746027805994\n",
      "Stochastic Gradient Descent(12713): loss=0.3216807942999365\n",
      "Stochastic Gradient Descent(12714): loss=1.8845120348811455\n",
      "Stochastic Gradient Descent(12715): loss=6.784402729042281\n",
      "Stochastic Gradient Descent(12716): loss=0.005403280339718328\n",
      "Stochastic Gradient Descent(12717): loss=1.286108321928162\n",
      "Stochastic Gradient Descent(12718): loss=8.333604236673324\n",
      "Stochastic Gradient Descent(12719): loss=0.027461739526436078\n",
      "Stochastic Gradient Descent(12720): loss=1.0249151208992815\n",
      "Stochastic Gradient Descent(12721): loss=13.486197835623932\n",
      "Stochastic Gradient Descent(12722): loss=1.606200133296729\n",
      "Stochastic Gradient Descent(12723): loss=0.01712523817544597\n",
      "Stochastic Gradient Descent(12724): loss=3.299793186791471\n",
      "Stochastic Gradient Descent(12725): loss=1.0526001438629162\n",
      "Stochastic Gradient Descent(12726): loss=0.2782597747078803\n",
      "Stochastic Gradient Descent(12727): loss=0.3504951031667359\n",
      "Stochastic Gradient Descent(12728): loss=15.35954004934276\n",
      "Stochastic Gradient Descent(12729): loss=4.7330577324158005\n",
      "Stochastic Gradient Descent(12730): loss=0.030834061351709827\n",
      "Stochastic Gradient Descent(12731): loss=4.357483069447892\n",
      "Stochastic Gradient Descent(12732): loss=22.45082436473665\n",
      "Stochastic Gradient Descent(12733): loss=5.355398451871766\n",
      "Stochastic Gradient Descent(12734): loss=5.339498729216343\n",
      "Stochastic Gradient Descent(12735): loss=15.087166166735612\n",
      "Stochastic Gradient Descent(12736): loss=0.0030470684472641515\n",
      "Stochastic Gradient Descent(12737): loss=0.4569441204366515\n",
      "Stochastic Gradient Descent(12738): loss=1.6652072094405088\n",
      "Stochastic Gradient Descent(12739): loss=4.668012559179703\n",
      "Stochastic Gradient Descent(12740): loss=1.7143560551977677\n",
      "Stochastic Gradient Descent(12741): loss=3.573080888125962\n",
      "Stochastic Gradient Descent(12742): loss=2.573954908735601\n",
      "Stochastic Gradient Descent(12743): loss=2.9923613759398204\n",
      "Stochastic Gradient Descent(12744): loss=0.1912605848500241\n",
      "Stochastic Gradient Descent(12745): loss=0.13897891745405688\n",
      "Stochastic Gradient Descent(12746): loss=5.608777190356998\n",
      "Stochastic Gradient Descent(12747): loss=1.1608004658232887\n",
      "Stochastic Gradient Descent(12748): loss=0.8920080961070209\n",
      "Stochastic Gradient Descent(12749): loss=15.79338824240102\n",
      "Stochastic Gradient Descent(12750): loss=0.3857081395862678\n",
      "Stochastic Gradient Descent(12751): loss=0.0912345236971371\n",
      "Stochastic Gradient Descent(12752): loss=0.20303695960071821\n",
      "Stochastic Gradient Descent(12753): loss=1.7375935900196298\n",
      "Stochastic Gradient Descent(12754): loss=1.4611192539743594\n",
      "Stochastic Gradient Descent(12755): loss=1.5303493792213902\n",
      "Stochastic Gradient Descent(12756): loss=16.03977423400575\n",
      "Stochastic Gradient Descent(12757): loss=0.3506455925046554\n",
      "Stochastic Gradient Descent(12758): loss=14.99711539434425\n",
      "Stochastic Gradient Descent(12759): loss=0.5622353107754994\n",
      "Stochastic Gradient Descent(12760): loss=0.5677441601250179\n",
      "Stochastic Gradient Descent(12761): loss=1.5276690963830333\n",
      "Stochastic Gradient Descent(12762): loss=0.5738536388619301\n",
      "Stochastic Gradient Descent(12763): loss=1.5781626633117347\n",
      "Stochastic Gradient Descent(12764): loss=1.7617442096363438\n",
      "Stochastic Gradient Descent(12765): loss=7.611101325637221\n",
      "Stochastic Gradient Descent(12766): loss=5.107540376463191\n",
      "Stochastic Gradient Descent(12767): loss=6.659190958288284\n",
      "Stochastic Gradient Descent(12768): loss=2.2700276656375284\n",
      "Stochastic Gradient Descent(12769): loss=9.773601694458288\n",
      "Stochastic Gradient Descent(12770): loss=0.7344331383850169\n",
      "Stochastic Gradient Descent(12771): loss=0.9538895859833214\n",
      "Stochastic Gradient Descent(12772): loss=0.4456244289801037\n",
      "Stochastic Gradient Descent(12773): loss=0.9218688545833748\n",
      "Stochastic Gradient Descent(12774): loss=0.8126997664147568\n",
      "Stochastic Gradient Descent(12775): loss=2.3420043618969584\n",
      "Stochastic Gradient Descent(12776): loss=0.477320668511996\n",
      "Stochastic Gradient Descent(12777): loss=4.22024550276036\n",
      "Stochastic Gradient Descent(12778): loss=10.50405435250607\n",
      "Stochastic Gradient Descent(12779): loss=3.466318200065264\n",
      "Stochastic Gradient Descent(12780): loss=8.464098545447055\n",
      "Stochastic Gradient Descent(12781): loss=0.17919711213888828\n",
      "Stochastic Gradient Descent(12782): loss=1.6222282066743696\n",
      "Stochastic Gradient Descent(12783): loss=0.2058896525393846\n",
      "Stochastic Gradient Descent(12784): loss=1.4318749592745617\n",
      "Stochastic Gradient Descent(12785): loss=5.272579133502099\n",
      "Stochastic Gradient Descent(12786): loss=7.157468390887299\n",
      "Stochastic Gradient Descent(12787): loss=0.9694599356451319\n",
      "Stochastic Gradient Descent(12788): loss=18.06400240560226\n",
      "Stochastic Gradient Descent(12789): loss=1.2740091140271799\n",
      "Stochastic Gradient Descent(12790): loss=1.1008805563631952\n",
      "Stochastic Gradient Descent(12791): loss=3.368907866153208\n",
      "Stochastic Gradient Descent(12792): loss=4.119219688260873\n",
      "Stochastic Gradient Descent(12793): loss=0.2073831990179087\n",
      "Stochastic Gradient Descent(12794): loss=1.9592270422387048\n",
      "Stochastic Gradient Descent(12795): loss=2.3021174325916243\n",
      "Stochastic Gradient Descent(12796): loss=0.36832512003321827\n",
      "Stochastic Gradient Descent(12797): loss=2.506654941167672\n",
      "Stochastic Gradient Descent(12798): loss=5.911899862634665\n",
      "Stochastic Gradient Descent(12799): loss=4.6596659487447765\n",
      "Stochastic Gradient Descent(12800): loss=11.120358575065874\n",
      "Stochastic Gradient Descent(12801): loss=0.9466583640348863\n",
      "Stochastic Gradient Descent(12802): loss=9.065079240241287\n",
      "Stochastic Gradient Descent(12803): loss=0.023564434892274343\n",
      "Stochastic Gradient Descent(12804): loss=9.683960177608345\n",
      "Stochastic Gradient Descent(12805): loss=25.241961908170737\n",
      "Stochastic Gradient Descent(12806): loss=4.81743247266907\n",
      "Stochastic Gradient Descent(12807): loss=3.0211863414984466\n",
      "Stochastic Gradient Descent(12808): loss=0.15438193902259123\n",
      "Stochastic Gradient Descent(12809): loss=0.0010314921180837177\n",
      "Stochastic Gradient Descent(12810): loss=7.167465848462369\n",
      "Stochastic Gradient Descent(12811): loss=0.4599681419079808\n",
      "Stochastic Gradient Descent(12812): loss=9.049227175700848\n",
      "Stochastic Gradient Descent(12813): loss=17.218293585174028\n",
      "Stochastic Gradient Descent(12814): loss=0.021394359583674704\n",
      "Stochastic Gradient Descent(12815): loss=15.69182956977085\n",
      "Stochastic Gradient Descent(12816): loss=0.6888314262248475\n",
      "Stochastic Gradient Descent(12817): loss=10.1188895177805\n",
      "Stochastic Gradient Descent(12818): loss=3.7224804731209\n",
      "Stochastic Gradient Descent(12819): loss=0.8816118033219748\n",
      "Stochastic Gradient Descent(12820): loss=0.6303636926829539\n",
      "Stochastic Gradient Descent(12821): loss=0.2791030224005262\n",
      "Stochastic Gradient Descent(12822): loss=4.1543691479372\n",
      "Stochastic Gradient Descent(12823): loss=1.474624335865376\n",
      "Stochastic Gradient Descent(12824): loss=1.4619024114474946\n",
      "Stochastic Gradient Descent(12825): loss=1.9695093858094803\n",
      "Stochastic Gradient Descent(12826): loss=1.399872843544576\n",
      "Stochastic Gradient Descent(12827): loss=2.069355526593269\n",
      "Stochastic Gradient Descent(12828): loss=0.25940388220093413\n",
      "Stochastic Gradient Descent(12829): loss=2.171625047927453\n",
      "Stochastic Gradient Descent(12830): loss=13.623330356044244\n",
      "Stochastic Gradient Descent(12831): loss=38.22056521519257\n",
      "Stochastic Gradient Descent(12832): loss=7.942344391147372\n",
      "Stochastic Gradient Descent(12833): loss=4.423925677175524\n",
      "Stochastic Gradient Descent(12834): loss=4.585617921767778\n",
      "Stochastic Gradient Descent(12835): loss=2.388586812431055\n",
      "Stochastic Gradient Descent(12836): loss=4.98517198375654\n",
      "Stochastic Gradient Descent(12837): loss=12.499072728940707\n",
      "Stochastic Gradient Descent(12838): loss=6.341372430739214\n",
      "Stochastic Gradient Descent(12839): loss=1.6692993647151582\n",
      "Stochastic Gradient Descent(12840): loss=2.7738137386534514\n",
      "Stochastic Gradient Descent(12841): loss=2.693503526725967\n",
      "Stochastic Gradient Descent(12842): loss=1.0034767854192592\n",
      "Stochastic Gradient Descent(12843): loss=3.79385190022361\n",
      "Stochastic Gradient Descent(12844): loss=2.8290349026663497\n",
      "Stochastic Gradient Descent(12845): loss=2.455029505148272\n",
      "Stochastic Gradient Descent(12846): loss=4.106886578366147\n",
      "Stochastic Gradient Descent(12847): loss=3.640840036346225\n",
      "Stochastic Gradient Descent(12848): loss=0.04430288145665352\n",
      "Stochastic Gradient Descent(12849): loss=0.580681397098847\n",
      "Stochastic Gradient Descent(12850): loss=0.4468930998212659\n",
      "Stochastic Gradient Descent(12851): loss=7.769542264813884\n",
      "Stochastic Gradient Descent(12852): loss=0.9604901621508027\n",
      "Stochastic Gradient Descent(12853): loss=1.811175550201457\n",
      "Stochastic Gradient Descent(12854): loss=0.01037105746190069\n",
      "Stochastic Gradient Descent(12855): loss=9.339562612626734\n",
      "Stochastic Gradient Descent(12856): loss=0.008055141719883193\n",
      "Stochastic Gradient Descent(12857): loss=5.857985300619583\n",
      "Stochastic Gradient Descent(12858): loss=4.648223039341272\n",
      "Stochastic Gradient Descent(12859): loss=0.23383070510694992\n",
      "Stochastic Gradient Descent(12860): loss=0.46454687191744665\n",
      "Stochastic Gradient Descent(12861): loss=7.181768937815218\n",
      "Stochastic Gradient Descent(12862): loss=0.40112044570630456\n",
      "Stochastic Gradient Descent(12863): loss=2.5245823257041504\n",
      "Stochastic Gradient Descent(12864): loss=2.7239506699982132\n",
      "Stochastic Gradient Descent(12865): loss=1.9704048107833543\n",
      "Stochastic Gradient Descent(12866): loss=1.0399158919738984\n",
      "Stochastic Gradient Descent(12867): loss=0.05428469143768895\n",
      "Stochastic Gradient Descent(12868): loss=0.1405587429000507\n",
      "Stochastic Gradient Descent(12869): loss=5.904706739242105\n",
      "Stochastic Gradient Descent(12870): loss=9.795621454838235\n",
      "Stochastic Gradient Descent(12871): loss=0.36780988640139445\n",
      "Stochastic Gradient Descent(12872): loss=0.05777028926606249\n",
      "Stochastic Gradient Descent(12873): loss=15.57992100438499\n",
      "Stochastic Gradient Descent(12874): loss=0.24937138465400716\n",
      "Stochastic Gradient Descent(12875): loss=22.083177102050886\n",
      "Stochastic Gradient Descent(12876): loss=8.36988000275332\n",
      "Stochastic Gradient Descent(12877): loss=4.589976804076751\n",
      "Stochastic Gradient Descent(12878): loss=6.010440080796335\n",
      "Stochastic Gradient Descent(12879): loss=10.380498268193156\n",
      "Stochastic Gradient Descent(12880): loss=3.7024850807445135\n",
      "Stochastic Gradient Descent(12881): loss=7.6112138272653596\n",
      "Stochastic Gradient Descent(12882): loss=0.8446303400356243\n",
      "Stochastic Gradient Descent(12883): loss=3.779663807727729\n",
      "Stochastic Gradient Descent(12884): loss=9.668889002537917\n",
      "Stochastic Gradient Descent(12885): loss=6.599678582242358\n",
      "Stochastic Gradient Descent(12886): loss=0.2623552895968987\n",
      "Stochastic Gradient Descent(12887): loss=19.022749403500256\n",
      "Stochastic Gradient Descent(12888): loss=4.5891977107114785\n",
      "Stochastic Gradient Descent(12889): loss=3.960408572814272\n",
      "Stochastic Gradient Descent(12890): loss=0.4570785413675649\n",
      "Stochastic Gradient Descent(12891): loss=1.3337111453231734\n",
      "Stochastic Gradient Descent(12892): loss=0.47836121177999885\n",
      "Stochastic Gradient Descent(12893): loss=0.0241435718498331\n",
      "Stochastic Gradient Descent(12894): loss=4.570245195938389\n",
      "Stochastic Gradient Descent(12895): loss=13.370610375511978\n",
      "Stochastic Gradient Descent(12896): loss=3.5874839797080638\n",
      "Stochastic Gradient Descent(12897): loss=0.012091404776956714\n",
      "Stochastic Gradient Descent(12898): loss=0.005758775251815973\n",
      "Stochastic Gradient Descent(12899): loss=0.051103265210852566\n",
      "Stochastic Gradient Descent(12900): loss=0.3859930426036793\n",
      "Stochastic Gradient Descent(12901): loss=14.954573035710673\n",
      "Stochastic Gradient Descent(12902): loss=1.6047963379212544\n",
      "Stochastic Gradient Descent(12903): loss=0.002858229964390109\n",
      "Stochastic Gradient Descent(12904): loss=4.730821970560473\n",
      "Stochastic Gradient Descent(12905): loss=2.0090394425237097\n",
      "Stochastic Gradient Descent(12906): loss=3.8470925722509635\n",
      "Stochastic Gradient Descent(12907): loss=9.733929975417777\n",
      "Stochastic Gradient Descent(12908): loss=4.169978798616576\n",
      "Stochastic Gradient Descent(12909): loss=0.7231187627341903\n",
      "Stochastic Gradient Descent(12910): loss=3.532074751945979\n",
      "Stochastic Gradient Descent(12911): loss=0.8904369231515553\n",
      "Stochastic Gradient Descent(12912): loss=6.133978079148169\n",
      "Stochastic Gradient Descent(12913): loss=0.4774536625490925\n",
      "Stochastic Gradient Descent(12914): loss=7.181801536807187\n",
      "Stochastic Gradient Descent(12915): loss=0.09315368790990844\n",
      "Stochastic Gradient Descent(12916): loss=2.4103303923716575\n",
      "Stochastic Gradient Descent(12917): loss=1.5760470772061523\n",
      "Stochastic Gradient Descent(12918): loss=1.613312979436437\n",
      "Stochastic Gradient Descent(12919): loss=0.4329480600334325\n",
      "Stochastic Gradient Descent(12920): loss=5.496851175391235\n",
      "Stochastic Gradient Descent(12921): loss=1.3351240858339155\n",
      "Stochastic Gradient Descent(12922): loss=4.131261978980199\n",
      "Stochastic Gradient Descent(12923): loss=1.7958349193688203\n",
      "Stochastic Gradient Descent(12924): loss=0.0019244921615938176\n",
      "Stochastic Gradient Descent(12925): loss=0.24059489449132446\n",
      "Stochastic Gradient Descent(12926): loss=26.854735249109076\n",
      "Stochastic Gradient Descent(12927): loss=1.60574086706433\n",
      "Stochastic Gradient Descent(12928): loss=6.79641535154176\n",
      "Stochastic Gradient Descent(12929): loss=4.58522645758332\n",
      "Stochastic Gradient Descent(12930): loss=1.5470488828557467\n",
      "Stochastic Gradient Descent(12931): loss=41.47995778013755\n",
      "Stochastic Gradient Descent(12932): loss=0.19123230603798946\n",
      "Stochastic Gradient Descent(12933): loss=8.76714264139947\n",
      "Stochastic Gradient Descent(12934): loss=17.72529509994786\n",
      "Stochastic Gradient Descent(12935): loss=2.5329690906069926\n",
      "Stochastic Gradient Descent(12936): loss=6.901506961256741\n",
      "Stochastic Gradient Descent(12937): loss=0.8115393027362059\n",
      "Stochastic Gradient Descent(12938): loss=10.439675659386513\n",
      "Stochastic Gradient Descent(12939): loss=7.614263660642255\n",
      "Stochastic Gradient Descent(12940): loss=0.4649793754683991\n",
      "Stochastic Gradient Descent(12941): loss=0.9755127237475181\n",
      "Stochastic Gradient Descent(12942): loss=1.7427928278131168\n",
      "Stochastic Gradient Descent(12943): loss=0.03378939082081633\n",
      "Stochastic Gradient Descent(12944): loss=14.020412352006641\n",
      "Stochastic Gradient Descent(12945): loss=0.6923828485216875\n",
      "Stochastic Gradient Descent(12946): loss=1.329263081280746\n",
      "Stochastic Gradient Descent(12947): loss=0.01131495975457135\n",
      "Stochastic Gradient Descent(12948): loss=0.2376054537462023\n",
      "Stochastic Gradient Descent(12949): loss=1.0110694237058961\n",
      "Stochastic Gradient Descent(12950): loss=2.9878074181881233\n",
      "Stochastic Gradient Descent(12951): loss=12.666023778279083\n",
      "Stochastic Gradient Descent(12952): loss=13.222119193443492\n",
      "Stochastic Gradient Descent(12953): loss=1.118685044121412\n",
      "Stochastic Gradient Descent(12954): loss=26.068830054100403\n",
      "Stochastic Gradient Descent(12955): loss=0.4404297262218659\n",
      "Stochastic Gradient Descent(12956): loss=1.428943568074732\n",
      "Stochastic Gradient Descent(12957): loss=0.06295738643788126\n",
      "Stochastic Gradient Descent(12958): loss=31.3360560633605\n",
      "Stochastic Gradient Descent(12959): loss=0.07572828329404678\n",
      "Stochastic Gradient Descent(12960): loss=6.297216414638487\n",
      "Stochastic Gradient Descent(12961): loss=2.414623220313281\n",
      "Stochastic Gradient Descent(12962): loss=0.3807129120246213\n",
      "Stochastic Gradient Descent(12963): loss=4.3542601974538035\n",
      "Stochastic Gradient Descent(12964): loss=0.5452096039480698\n",
      "Stochastic Gradient Descent(12965): loss=0.1221689631901506\n",
      "Stochastic Gradient Descent(12966): loss=2.3625006113531324\n",
      "Stochastic Gradient Descent(12967): loss=2.9010162707904503\n",
      "Stochastic Gradient Descent(12968): loss=3.8350428267264918\n",
      "Stochastic Gradient Descent(12969): loss=10.88636015830775\n",
      "Stochastic Gradient Descent(12970): loss=4.547276470972531\n",
      "Stochastic Gradient Descent(12971): loss=1.986316493976558\n",
      "Stochastic Gradient Descent(12972): loss=6.642123013802091\n",
      "Stochastic Gradient Descent(12973): loss=10.923505975001447\n",
      "Stochastic Gradient Descent(12974): loss=18.259112980466462\n",
      "Stochastic Gradient Descent(12975): loss=3.8603332981206933\n",
      "Stochastic Gradient Descent(12976): loss=13.497386239102685\n",
      "Stochastic Gradient Descent(12977): loss=1.772945366099145\n",
      "Stochastic Gradient Descent(12978): loss=1.1821486249627904\n",
      "Stochastic Gradient Descent(12979): loss=30.89474459172393\n",
      "Stochastic Gradient Descent(12980): loss=2.1573416301298174\n",
      "Stochastic Gradient Descent(12981): loss=4.943499220662274\n",
      "Stochastic Gradient Descent(12982): loss=0.21233443809573863\n",
      "Stochastic Gradient Descent(12983): loss=34.71187611321471\n",
      "Stochastic Gradient Descent(12984): loss=14.038413776747829\n",
      "Stochastic Gradient Descent(12985): loss=28.71828974011933\n",
      "Stochastic Gradient Descent(12986): loss=0.18759969598780893\n",
      "Stochastic Gradient Descent(12987): loss=4.2924469163326\n",
      "Stochastic Gradient Descent(12988): loss=12.168662577356129\n",
      "Stochastic Gradient Descent(12989): loss=4.337854923648998\n",
      "Stochastic Gradient Descent(12990): loss=0.02895654717525199\n",
      "Stochastic Gradient Descent(12991): loss=7.7079350432808145\n",
      "Stochastic Gradient Descent(12992): loss=0.07120242327687287\n",
      "Stochastic Gradient Descent(12993): loss=3.360498140832508\n",
      "Stochastic Gradient Descent(12994): loss=1.8676693323139644\n",
      "Stochastic Gradient Descent(12995): loss=3.180476538639975\n",
      "Stochastic Gradient Descent(12996): loss=17.43620706580075\n",
      "Stochastic Gradient Descent(12997): loss=7.222051062086444\n",
      "Stochastic Gradient Descent(12998): loss=4.008085579579183\n",
      "Stochastic Gradient Descent(12999): loss=0.44427233967483604\n",
      "Stochastic Gradient Descent(13000): loss=6.461521037112679\n",
      "Stochastic Gradient Descent(13001): loss=0.0050295342948289985\n",
      "Stochastic Gradient Descent(13002): loss=0.03749803919516123\n",
      "Stochastic Gradient Descent(13003): loss=1.8783420168163099\n",
      "Stochastic Gradient Descent(13004): loss=4.602043812869024\n",
      "Stochastic Gradient Descent(13005): loss=3.696000913487443\n",
      "Stochastic Gradient Descent(13006): loss=14.051030649417129\n",
      "Stochastic Gradient Descent(13007): loss=0.5607004667110344\n",
      "Stochastic Gradient Descent(13008): loss=0.4954557339881163\n",
      "Stochastic Gradient Descent(13009): loss=5.997525363076534\n",
      "Stochastic Gradient Descent(13010): loss=6.983770593036773\n",
      "Stochastic Gradient Descent(13011): loss=7.511887891402764\n",
      "Stochastic Gradient Descent(13012): loss=0.23031787162693898\n",
      "Stochastic Gradient Descent(13013): loss=1.294059560775861\n",
      "Stochastic Gradient Descent(13014): loss=0.016347586725151773\n",
      "Stochastic Gradient Descent(13015): loss=3.2457285148832113\n",
      "Stochastic Gradient Descent(13016): loss=3.6455946302171767\n",
      "Stochastic Gradient Descent(13017): loss=29.96638589783301\n",
      "Stochastic Gradient Descent(13018): loss=0.050286458177443344\n",
      "Stochastic Gradient Descent(13019): loss=0.1265365178763307\n",
      "Stochastic Gradient Descent(13020): loss=6.649749971063269\n",
      "Stochastic Gradient Descent(13021): loss=2.9362735539297544\n",
      "Stochastic Gradient Descent(13022): loss=3.0245330371928168e-05\n",
      "Stochastic Gradient Descent(13023): loss=14.659382846423322\n",
      "Stochastic Gradient Descent(13024): loss=12.603883660766545\n",
      "Stochastic Gradient Descent(13025): loss=0.016529577923317808\n",
      "Stochastic Gradient Descent(13026): loss=0.9862379161886966\n",
      "Stochastic Gradient Descent(13027): loss=1.0740677359698227\n",
      "Stochastic Gradient Descent(13028): loss=12.218412737084238\n",
      "Stochastic Gradient Descent(13029): loss=38.54940095525489\n",
      "Stochastic Gradient Descent(13030): loss=3.132591015536642\n",
      "Stochastic Gradient Descent(13031): loss=0.1896646241015658\n",
      "Stochastic Gradient Descent(13032): loss=1.0530710177659925\n",
      "Stochastic Gradient Descent(13033): loss=27.607773892047575\n",
      "Stochastic Gradient Descent(13034): loss=39.4808248416827\n",
      "Stochastic Gradient Descent(13035): loss=0.056728505322327226\n",
      "Stochastic Gradient Descent(13036): loss=9.653534398180314\n",
      "Stochastic Gradient Descent(13037): loss=0.9792555612979376\n",
      "Stochastic Gradient Descent(13038): loss=0.6851470624558472\n",
      "Stochastic Gradient Descent(13039): loss=0.27362829597014904\n",
      "Stochastic Gradient Descent(13040): loss=0.9551852225796427\n",
      "Stochastic Gradient Descent(13041): loss=0.054845931480581304\n",
      "Stochastic Gradient Descent(13042): loss=1.6836551238318491\n",
      "Stochastic Gradient Descent(13043): loss=7.811980391041215\n",
      "Stochastic Gradient Descent(13044): loss=0.8464567908036211\n",
      "Stochastic Gradient Descent(13045): loss=5.884951979459472\n",
      "Stochastic Gradient Descent(13046): loss=3.444421755872442\n",
      "Stochastic Gradient Descent(13047): loss=1.545043008539381\n",
      "Stochastic Gradient Descent(13048): loss=6.351148141314624\n",
      "Stochastic Gradient Descent(13049): loss=20.132032808468534\n",
      "Stochastic Gradient Descent(13050): loss=0.012972912549855215\n",
      "Stochastic Gradient Descent(13051): loss=6.7229685406945965\n",
      "Stochastic Gradient Descent(13052): loss=10.60940499160402\n",
      "Stochastic Gradient Descent(13053): loss=0.8881952003279405\n",
      "Stochastic Gradient Descent(13054): loss=9.992424715340777\n",
      "Stochastic Gradient Descent(13055): loss=2.42206134595547\n",
      "Stochastic Gradient Descent(13056): loss=0.9690780027636517\n",
      "Stochastic Gradient Descent(13057): loss=0.8924989511861382\n",
      "Stochastic Gradient Descent(13058): loss=0.3822872221211734\n",
      "Stochastic Gradient Descent(13059): loss=0.029744745206308876\n",
      "Stochastic Gradient Descent(13060): loss=8.311988957936984\n",
      "Stochastic Gradient Descent(13061): loss=0.0008135456965773058\n",
      "Stochastic Gradient Descent(13062): loss=3.379532374515229\n",
      "Stochastic Gradient Descent(13063): loss=7.065492571849042\n",
      "Stochastic Gradient Descent(13064): loss=0.17026452097408995\n",
      "Stochastic Gradient Descent(13065): loss=9.738640920446926\n",
      "Stochastic Gradient Descent(13066): loss=21.624933811094245\n",
      "Stochastic Gradient Descent(13067): loss=2.125203552463292\n",
      "Stochastic Gradient Descent(13068): loss=4.056061425027929\n",
      "Stochastic Gradient Descent(13069): loss=0.011548616354922162\n",
      "Stochastic Gradient Descent(13070): loss=0.04196359586012047\n",
      "Stochastic Gradient Descent(13071): loss=3.0092630990340186\n",
      "Stochastic Gradient Descent(13072): loss=0.37183637924549695\n",
      "Stochastic Gradient Descent(13073): loss=0.01954639086737721\n",
      "Stochastic Gradient Descent(13074): loss=0.1596521939908236\n",
      "Stochastic Gradient Descent(13075): loss=4.125277756036202\n",
      "Stochastic Gradient Descent(13076): loss=8.919033819003527\n",
      "Stochastic Gradient Descent(13077): loss=0.16832089258785757\n",
      "Stochastic Gradient Descent(13078): loss=8.560784656473989e-06\n",
      "Stochastic Gradient Descent(13079): loss=3.099975588466949\n",
      "Stochastic Gradient Descent(13080): loss=0.035150705082049885\n",
      "Stochastic Gradient Descent(13081): loss=0.12808305750849014\n",
      "Stochastic Gradient Descent(13082): loss=10.250596434906111\n",
      "Stochastic Gradient Descent(13083): loss=0.561954218135572\n",
      "Stochastic Gradient Descent(13084): loss=2.584666367429952\n",
      "Stochastic Gradient Descent(13085): loss=9.168430305020589\n",
      "Stochastic Gradient Descent(13086): loss=0.5120683056605845\n",
      "Stochastic Gradient Descent(13087): loss=3.106048029509367\n",
      "Stochastic Gradient Descent(13088): loss=9.9790309646149\n",
      "Stochastic Gradient Descent(13089): loss=18.161146712429467\n",
      "Stochastic Gradient Descent(13090): loss=1.391415744025442\n",
      "Stochastic Gradient Descent(13091): loss=5.844671038746753\n",
      "Stochastic Gradient Descent(13092): loss=0.02200465296626626\n",
      "Stochastic Gradient Descent(13093): loss=2.616623963641637\n",
      "Stochastic Gradient Descent(13094): loss=5.589132618638825\n",
      "Stochastic Gradient Descent(13095): loss=6.99073141288311\n",
      "Stochastic Gradient Descent(13096): loss=20.09048078914621\n",
      "Stochastic Gradient Descent(13097): loss=2.5404976883104866\n",
      "Stochastic Gradient Descent(13098): loss=59.93424504335111\n",
      "Stochastic Gradient Descent(13099): loss=22.877746523298963\n",
      "Stochastic Gradient Descent(13100): loss=2.010109335050713\n",
      "Stochastic Gradient Descent(13101): loss=2.5683579561824414\n",
      "Stochastic Gradient Descent(13102): loss=0.10854611960518518\n",
      "Stochastic Gradient Descent(13103): loss=2.8038949513623357\n",
      "Stochastic Gradient Descent(13104): loss=0.06962956040832523\n",
      "Stochastic Gradient Descent(13105): loss=0.8692634497170322\n",
      "Stochastic Gradient Descent(13106): loss=2.93652156225627\n",
      "Stochastic Gradient Descent(13107): loss=12.514346606087315\n",
      "Stochastic Gradient Descent(13108): loss=4.220217494086127\n",
      "Stochastic Gradient Descent(13109): loss=0.34466748619654713\n",
      "Stochastic Gradient Descent(13110): loss=7.73554683607455\n",
      "Stochastic Gradient Descent(13111): loss=4.177591036782513\n",
      "Stochastic Gradient Descent(13112): loss=0.0003753036944543343\n",
      "Stochastic Gradient Descent(13113): loss=1.3475190193040039\n",
      "Stochastic Gradient Descent(13114): loss=3.0604380972807395\n",
      "Stochastic Gradient Descent(13115): loss=0.03255239512682865\n",
      "Stochastic Gradient Descent(13116): loss=0.08784866487928432\n",
      "Stochastic Gradient Descent(13117): loss=43.8610878436872\n",
      "Stochastic Gradient Descent(13118): loss=1.870996201798143\n",
      "Stochastic Gradient Descent(13119): loss=6.743211888553127\n",
      "Stochastic Gradient Descent(13120): loss=0.13109412902799153\n",
      "Stochastic Gradient Descent(13121): loss=30.95543558917911\n",
      "Stochastic Gradient Descent(13122): loss=39.590571970871885\n",
      "Stochastic Gradient Descent(13123): loss=3.9355958997099862\n",
      "Stochastic Gradient Descent(13124): loss=0.052908633081574424\n",
      "Stochastic Gradient Descent(13125): loss=5.771781725644388\n",
      "Stochastic Gradient Descent(13126): loss=0.7184924352369896\n",
      "Stochastic Gradient Descent(13127): loss=6.466246219135737\n",
      "Stochastic Gradient Descent(13128): loss=0.27116178637602756\n",
      "Stochastic Gradient Descent(13129): loss=3.2177900461159794\n",
      "Stochastic Gradient Descent(13130): loss=0.4375868806617541\n",
      "Stochastic Gradient Descent(13131): loss=0.051628105827077615\n",
      "Stochastic Gradient Descent(13132): loss=2.158512879734462\n",
      "Stochastic Gradient Descent(13133): loss=0.6575240961936337\n",
      "Stochastic Gradient Descent(13134): loss=15.19305581385014\n",
      "Stochastic Gradient Descent(13135): loss=0.004578697600251014\n",
      "Stochastic Gradient Descent(13136): loss=3.5678940341292353\n",
      "Stochastic Gradient Descent(13137): loss=3.3713127329494514\n",
      "Stochastic Gradient Descent(13138): loss=5.796589685115534\n",
      "Stochastic Gradient Descent(13139): loss=0.016556464645128076\n",
      "Stochastic Gradient Descent(13140): loss=0.4520315077347195\n",
      "Stochastic Gradient Descent(13141): loss=0.03529822197730966\n",
      "Stochastic Gradient Descent(13142): loss=0.08002357121261625\n",
      "Stochastic Gradient Descent(13143): loss=0.4692748078618854\n",
      "Stochastic Gradient Descent(13144): loss=2.1710158786539897\n",
      "Stochastic Gradient Descent(13145): loss=4.014305741634023\n",
      "Stochastic Gradient Descent(13146): loss=15.392302812223951\n",
      "Stochastic Gradient Descent(13147): loss=2.6149500359468556\n",
      "Stochastic Gradient Descent(13148): loss=0.0201892763355986\n",
      "Stochastic Gradient Descent(13149): loss=4.3065473841382484\n",
      "Stochastic Gradient Descent(13150): loss=0.667479025113107\n",
      "Stochastic Gradient Descent(13151): loss=0.0006844643787533453\n",
      "Stochastic Gradient Descent(13152): loss=1.778831714898999\n",
      "Stochastic Gradient Descent(13153): loss=2.3719017450503412\n",
      "Stochastic Gradient Descent(13154): loss=0.016284028947450135\n",
      "Stochastic Gradient Descent(13155): loss=1.1393806166405775\n",
      "Stochastic Gradient Descent(13156): loss=0.12051995245356668\n",
      "Stochastic Gradient Descent(13157): loss=6.130577075164149\n",
      "Stochastic Gradient Descent(13158): loss=3.0101359298952435\n",
      "Stochastic Gradient Descent(13159): loss=41.46098059547435\n",
      "Stochastic Gradient Descent(13160): loss=0.5721348670100116\n",
      "Stochastic Gradient Descent(13161): loss=30.5872435031195\n",
      "Stochastic Gradient Descent(13162): loss=4.869330406642586\n",
      "Stochastic Gradient Descent(13163): loss=0.07194047911132599\n",
      "Stochastic Gradient Descent(13164): loss=6.225469958095511\n",
      "Stochastic Gradient Descent(13165): loss=0.6688803969488655\n",
      "Stochastic Gradient Descent(13166): loss=0.526813780454017\n",
      "Stochastic Gradient Descent(13167): loss=1.4989040807108036\n",
      "Stochastic Gradient Descent(13168): loss=2.592023222420573\n",
      "Stochastic Gradient Descent(13169): loss=7.519708468171394\n",
      "Stochastic Gradient Descent(13170): loss=0.1987012780345513\n",
      "Stochastic Gradient Descent(13171): loss=8.990664249010916\n",
      "Stochastic Gradient Descent(13172): loss=0.2748083357570932\n",
      "Stochastic Gradient Descent(13173): loss=5.842030222885147\n",
      "Stochastic Gradient Descent(13174): loss=0.5923525530786827\n",
      "Stochastic Gradient Descent(13175): loss=1.799868477503186\n",
      "Stochastic Gradient Descent(13176): loss=0.4398129215802786\n",
      "Stochastic Gradient Descent(13177): loss=0.0006278057184467473\n",
      "Stochastic Gradient Descent(13178): loss=3.0040233942137213\n",
      "Stochastic Gradient Descent(13179): loss=0.11367487375230367\n",
      "Stochastic Gradient Descent(13180): loss=7.052808224849316\n",
      "Stochastic Gradient Descent(13181): loss=0.24833453268354116\n",
      "Stochastic Gradient Descent(13182): loss=0.6763696095369444\n",
      "Stochastic Gradient Descent(13183): loss=0.31669654699354854\n",
      "Stochastic Gradient Descent(13184): loss=2.0074853507379466\n",
      "Stochastic Gradient Descent(13185): loss=1.123282729303\n",
      "Stochastic Gradient Descent(13186): loss=11.12691302241334\n",
      "Stochastic Gradient Descent(13187): loss=3.338886812338212\n",
      "Stochastic Gradient Descent(13188): loss=16.647441741857286\n",
      "Stochastic Gradient Descent(13189): loss=2.772712434180095\n",
      "Stochastic Gradient Descent(13190): loss=0.0010204567557437073\n",
      "Stochastic Gradient Descent(13191): loss=0.9931924649064945\n",
      "Stochastic Gradient Descent(13192): loss=1.2718957517509666\n",
      "Stochastic Gradient Descent(13193): loss=0.16824112084838855\n",
      "Stochastic Gradient Descent(13194): loss=0.5938522783691598\n",
      "Stochastic Gradient Descent(13195): loss=7.217108346904764\n",
      "Stochastic Gradient Descent(13196): loss=0.08784435948051277\n",
      "Stochastic Gradient Descent(13197): loss=0.08262548605108043\n",
      "Stochastic Gradient Descent(13198): loss=0.3350644514758512\n",
      "Stochastic Gradient Descent(13199): loss=1.278094553406605\n",
      "Stochastic Gradient Descent(13200): loss=1.045860475137735\n",
      "Stochastic Gradient Descent(13201): loss=3.5217827833701856\n",
      "Stochastic Gradient Descent(13202): loss=5.935285071203815\n",
      "Stochastic Gradient Descent(13203): loss=5.0431942839972\n",
      "Stochastic Gradient Descent(13204): loss=1.1344738480684768\n",
      "Stochastic Gradient Descent(13205): loss=0.012839912594933858\n",
      "Stochastic Gradient Descent(13206): loss=1.453689079606876\n",
      "Stochastic Gradient Descent(13207): loss=0.5124879468582719\n",
      "Stochastic Gradient Descent(13208): loss=4.171785565269595\n",
      "Stochastic Gradient Descent(13209): loss=0.020950289144561\n",
      "Stochastic Gradient Descent(13210): loss=0.8697735685839084\n",
      "Stochastic Gradient Descent(13211): loss=0.02285216414482603\n",
      "Stochastic Gradient Descent(13212): loss=7.161617977928188\n",
      "Stochastic Gradient Descent(13213): loss=0.9745488693933367\n",
      "Stochastic Gradient Descent(13214): loss=15.804201169051417\n",
      "Stochastic Gradient Descent(13215): loss=0.0075272488015068065\n",
      "Stochastic Gradient Descent(13216): loss=2.829431169098745\n",
      "Stochastic Gradient Descent(13217): loss=14.533097534686084\n",
      "Stochastic Gradient Descent(13218): loss=1.8824236202469709\n",
      "Stochastic Gradient Descent(13219): loss=3.271158557093408\n",
      "Stochastic Gradient Descent(13220): loss=1.1890077133921269\n",
      "Stochastic Gradient Descent(13221): loss=4.7797659148085465\n",
      "Stochastic Gradient Descent(13222): loss=1.7277230288275935\n",
      "Stochastic Gradient Descent(13223): loss=0.16832354222464388\n",
      "Stochastic Gradient Descent(13224): loss=3.7212794325488945\n",
      "Stochastic Gradient Descent(13225): loss=5.007268526069457\n",
      "Stochastic Gradient Descent(13226): loss=1.270684904632608\n",
      "Stochastic Gradient Descent(13227): loss=0.031677127032666334\n",
      "Stochastic Gradient Descent(13228): loss=0.18457604728990748\n",
      "Stochastic Gradient Descent(13229): loss=1.1827758469065073\n",
      "Stochastic Gradient Descent(13230): loss=7.938296885460124\n",
      "Stochastic Gradient Descent(13231): loss=7.104183181989598\n",
      "Stochastic Gradient Descent(13232): loss=1.5863254068202677\n",
      "Stochastic Gradient Descent(13233): loss=10.197952545152823\n",
      "Stochastic Gradient Descent(13234): loss=6.939400626988324\n",
      "Stochastic Gradient Descent(13235): loss=0.025383373095582962\n",
      "Stochastic Gradient Descent(13236): loss=1.1578437467233351\n",
      "Stochastic Gradient Descent(13237): loss=0.4932398838226924\n",
      "Stochastic Gradient Descent(13238): loss=0.02625282706610417\n",
      "Stochastic Gradient Descent(13239): loss=12.364320216373521\n",
      "Stochastic Gradient Descent(13240): loss=2.2900647572284067\n",
      "Stochastic Gradient Descent(13241): loss=14.424631835930816\n",
      "Stochastic Gradient Descent(13242): loss=0.4625639407470715\n",
      "Stochastic Gradient Descent(13243): loss=4.124835107495497\n",
      "Stochastic Gradient Descent(13244): loss=0.09725085879063738\n",
      "Stochastic Gradient Descent(13245): loss=0.06859947986498784\n",
      "Stochastic Gradient Descent(13246): loss=0.03582477039209969\n",
      "Stochastic Gradient Descent(13247): loss=6.815610000431347\n",
      "Stochastic Gradient Descent(13248): loss=1.2356529356941435\n",
      "Stochastic Gradient Descent(13249): loss=2.6950346143299027\n",
      "Stochastic Gradient Descent(13250): loss=10.711048745980351\n",
      "Stochastic Gradient Descent(13251): loss=0.3655903486771916\n",
      "Stochastic Gradient Descent(13252): loss=0.26099804379677866\n",
      "Stochastic Gradient Descent(13253): loss=4.456084868542508\n",
      "Stochastic Gradient Descent(13254): loss=0.12768112264577164\n",
      "Stochastic Gradient Descent(13255): loss=3.068528381638376\n",
      "Stochastic Gradient Descent(13256): loss=1.7625919893660387\n",
      "Stochastic Gradient Descent(13257): loss=2.4099752411978534\n",
      "Stochastic Gradient Descent(13258): loss=0.40856447006531976\n",
      "Stochastic Gradient Descent(13259): loss=7.766127508907376\n",
      "Stochastic Gradient Descent(13260): loss=9.51555119700121\n",
      "Stochastic Gradient Descent(13261): loss=10.420365341323262\n",
      "Stochastic Gradient Descent(13262): loss=14.880281224906097\n",
      "Stochastic Gradient Descent(13263): loss=0.24215104100807938\n",
      "Stochastic Gradient Descent(13264): loss=0.11870870327576125\n",
      "Stochastic Gradient Descent(13265): loss=12.311751093004704\n",
      "Stochastic Gradient Descent(13266): loss=2.132779026334283\n",
      "Stochastic Gradient Descent(13267): loss=14.064931469185836\n",
      "Stochastic Gradient Descent(13268): loss=0.13271108757078118\n",
      "Stochastic Gradient Descent(13269): loss=0.6828627839816515\n",
      "Stochastic Gradient Descent(13270): loss=10.338195008994555\n",
      "Stochastic Gradient Descent(13271): loss=1.2383193749487467\n",
      "Stochastic Gradient Descent(13272): loss=0.21801047868292064\n",
      "Stochastic Gradient Descent(13273): loss=6.272600360977468\n",
      "Stochastic Gradient Descent(13274): loss=14.20550418894359\n",
      "Stochastic Gradient Descent(13275): loss=2.376302202303685\n",
      "Stochastic Gradient Descent(13276): loss=7.353838469274463\n",
      "Stochastic Gradient Descent(13277): loss=1.100669176075708\n",
      "Stochastic Gradient Descent(13278): loss=0.22344578162079454\n",
      "Stochastic Gradient Descent(13279): loss=2.080772835146446\n",
      "Stochastic Gradient Descent(13280): loss=3.772191145388981\n",
      "Stochastic Gradient Descent(13281): loss=2.0107780425564115\n",
      "Stochastic Gradient Descent(13282): loss=1.3284610434668689\n",
      "Stochastic Gradient Descent(13283): loss=0.0783241864724876\n",
      "Stochastic Gradient Descent(13284): loss=3.188188730919364\n",
      "Stochastic Gradient Descent(13285): loss=12.941512521258126\n",
      "Stochastic Gradient Descent(13286): loss=0.08989948555289981\n",
      "Stochastic Gradient Descent(13287): loss=6.750563049012707\n",
      "Stochastic Gradient Descent(13288): loss=3.2356916346850952\n",
      "Stochastic Gradient Descent(13289): loss=2.6453507165998658\n",
      "Stochastic Gradient Descent(13290): loss=2.7138014574659786\n",
      "Stochastic Gradient Descent(13291): loss=0.0024725744546425592\n",
      "Stochastic Gradient Descent(13292): loss=0.09563486396015326\n",
      "Stochastic Gradient Descent(13293): loss=0.13393770405176106\n",
      "Stochastic Gradient Descent(13294): loss=5.744700453287082\n",
      "Stochastic Gradient Descent(13295): loss=25.58449857127441\n",
      "Stochastic Gradient Descent(13296): loss=15.270177062600796\n",
      "Stochastic Gradient Descent(13297): loss=4.937328281986198\n",
      "Stochastic Gradient Descent(13298): loss=0.5755220005800828\n",
      "Stochastic Gradient Descent(13299): loss=16.989430369691277\n",
      "Stochastic Gradient Descent(13300): loss=3.71759652538425\n",
      "Stochastic Gradient Descent(13301): loss=2.0286117771397185\n",
      "Stochastic Gradient Descent(13302): loss=42.329474426810975\n",
      "Stochastic Gradient Descent(13303): loss=0.8321948450810666\n",
      "Stochastic Gradient Descent(13304): loss=3.2980320046033786\n",
      "Stochastic Gradient Descent(13305): loss=5.187168543651472\n",
      "Stochastic Gradient Descent(13306): loss=0.0574001945936165\n",
      "Stochastic Gradient Descent(13307): loss=7.647076866394882\n",
      "Stochastic Gradient Descent(13308): loss=8.91689106884575\n",
      "Stochastic Gradient Descent(13309): loss=0.8117267541185391\n",
      "Stochastic Gradient Descent(13310): loss=0.16104834066615237\n",
      "Stochastic Gradient Descent(13311): loss=7.363427272099708\n",
      "Stochastic Gradient Descent(13312): loss=1.6407939616609097\n",
      "Stochastic Gradient Descent(13313): loss=10.147536802236392\n",
      "Stochastic Gradient Descent(13314): loss=0.07146990991837004\n",
      "Stochastic Gradient Descent(13315): loss=0.4711267101571039\n",
      "Stochastic Gradient Descent(13316): loss=4.1212381057731\n",
      "Stochastic Gradient Descent(13317): loss=0.3033815502797336\n",
      "Stochastic Gradient Descent(13318): loss=2.590712606251129\n",
      "Stochastic Gradient Descent(13319): loss=0.13373776664310075\n",
      "Stochastic Gradient Descent(13320): loss=8.667528769369401\n",
      "Stochastic Gradient Descent(13321): loss=0.13250441133223498\n",
      "Stochastic Gradient Descent(13322): loss=0.2753855985266615\n",
      "Stochastic Gradient Descent(13323): loss=0.19236047443430154\n",
      "Stochastic Gradient Descent(13324): loss=5.41809660437602\n",
      "Stochastic Gradient Descent(13325): loss=11.519864445258223\n",
      "Stochastic Gradient Descent(13326): loss=2.6599931294986665\n",
      "Stochastic Gradient Descent(13327): loss=0.3685756308074802\n",
      "Stochastic Gradient Descent(13328): loss=0.049472564556889115\n",
      "Stochastic Gradient Descent(13329): loss=3.148186630086641\n",
      "Stochastic Gradient Descent(13330): loss=13.859000307537274\n",
      "Stochastic Gradient Descent(13331): loss=13.712845670464535\n",
      "Stochastic Gradient Descent(13332): loss=15.917074049647157\n",
      "Stochastic Gradient Descent(13333): loss=4.938849820178687\n",
      "Stochastic Gradient Descent(13334): loss=4.184036425729675\n",
      "Stochastic Gradient Descent(13335): loss=0.5045078517059307\n",
      "Stochastic Gradient Descent(13336): loss=0.3839202236331642\n",
      "Stochastic Gradient Descent(13337): loss=0.8926655030167457\n",
      "Stochastic Gradient Descent(13338): loss=2.998571130426632\n",
      "Stochastic Gradient Descent(13339): loss=4.886800367098347\n",
      "Stochastic Gradient Descent(13340): loss=3.5436242270382685\n",
      "Stochastic Gradient Descent(13341): loss=1.8770901262886655\n",
      "Stochastic Gradient Descent(13342): loss=19.75681667337456\n",
      "Stochastic Gradient Descent(13343): loss=0.45021622884256524\n",
      "Stochastic Gradient Descent(13344): loss=0.053368615175780995\n",
      "Stochastic Gradient Descent(13345): loss=3.2342972141197666\n",
      "Stochastic Gradient Descent(13346): loss=1.3822551477570022\n",
      "Stochastic Gradient Descent(13347): loss=5.505408263245807\n",
      "Stochastic Gradient Descent(13348): loss=1.4755067088890588\n",
      "Stochastic Gradient Descent(13349): loss=4.394390777670444\n",
      "Stochastic Gradient Descent(13350): loss=6.425575157115251\n",
      "Stochastic Gradient Descent(13351): loss=0.30967016622432647\n",
      "Stochastic Gradient Descent(13352): loss=0.0005632036802329346\n",
      "Stochastic Gradient Descent(13353): loss=1.3632693264712468\n",
      "Stochastic Gradient Descent(13354): loss=8.423634703160157\n",
      "Stochastic Gradient Descent(13355): loss=2.4270204507670843\n",
      "Stochastic Gradient Descent(13356): loss=1.6719666216635511\n",
      "Stochastic Gradient Descent(13357): loss=5.079559896377446\n",
      "Stochastic Gradient Descent(13358): loss=6.0731155859981145\n",
      "Stochastic Gradient Descent(13359): loss=0.8898761217878947\n",
      "Stochastic Gradient Descent(13360): loss=0.3377866727825541\n",
      "Stochastic Gradient Descent(13361): loss=0.6830826728052286\n",
      "Stochastic Gradient Descent(13362): loss=27.465257667454463\n",
      "Stochastic Gradient Descent(13363): loss=19.078198292185323\n",
      "Stochastic Gradient Descent(13364): loss=9.439533154243906\n",
      "Stochastic Gradient Descent(13365): loss=0.41954197220454653\n",
      "Stochastic Gradient Descent(13366): loss=7.9796911484066015\n",
      "Stochastic Gradient Descent(13367): loss=3.8439359250539966\n",
      "Stochastic Gradient Descent(13368): loss=0.8663163188959844\n",
      "Stochastic Gradient Descent(13369): loss=39.60673733675426\n",
      "Stochastic Gradient Descent(13370): loss=0.912352190266142\n",
      "Stochastic Gradient Descent(13371): loss=14.981218316502227\n",
      "Stochastic Gradient Descent(13372): loss=44.32546483751155\n",
      "Stochastic Gradient Descent(13373): loss=0.36157815564461426\n",
      "Stochastic Gradient Descent(13374): loss=0.671625389258073\n",
      "Stochastic Gradient Descent(13375): loss=0.07075075422180883\n",
      "Stochastic Gradient Descent(13376): loss=0.20144830013505766\n",
      "Stochastic Gradient Descent(13377): loss=0.28896397705209453\n",
      "Stochastic Gradient Descent(13378): loss=0.017083332063092444\n",
      "Stochastic Gradient Descent(13379): loss=0.0980243431131306\n",
      "Stochastic Gradient Descent(13380): loss=0.09533613085001912\n",
      "Stochastic Gradient Descent(13381): loss=0.15641384460831098\n",
      "Stochastic Gradient Descent(13382): loss=1.1503152541060344\n",
      "Stochastic Gradient Descent(13383): loss=3.4232354457489635\n",
      "Stochastic Gradient Descent(13384): loss=9.417617982057802\n",
      "Stochastic Gradient Descent(13385): loss=1.042930944396526\n",
      "Stochastic Gradient Descent(13386): loss=0.9441486037237371\n",
      "Stochastic Gradient Descent(13387): loss=2.342601330905959\n",
      "Stochastic Gradient Descent(13388): loss=9.688581721501228\n",
      "Stochastic Gradient Descent(13389): loss=7.102922436833938\n",
      "Stochastic Gradient Descent(13390): loss=0.0934009803036284\n",
      "Stochastic Gradient Descent(13391): loss=0.5483932056788444\n",
      "Stochastic Gradient Descent(13392): loss=64.8014973647892\n",
      "Stochastic Gradient Descent(13393): loss=0.7537587083270376\n",
      "Stochastic Gradient Descent(13394): loss=1.019207436756617\n",
      "Stochastic Gradient Descent(13395): loss=2.316950138125835\n",
      "Stochastic Gradient Descent(13396): loss=1.3019447106129445\n",
      "Stochastic Gradient Descent(13397): loss=8.808120320116137\n",
      "Stochastic Gradient Descent(13398): loss=37.59634289702557\n",
      "Stochastic Gradient Descent(13399): loss=2.609976487675043\n",
      "Stochastic Gradient Descent(13400): loss=0.4941066980645398\n",
      "Stochastic Gradient Descent(13401): loss=0.16547921571472224\n",
      "Stochastic Gradient Descent(13402): loss=18.814871422087265\n",
      "Stochastic Gradient Descent(13403): loss=57.42562340373988\n",
      "Stochastic Gradient Descent(13404): loss=11.96983245215741\n",
      "Stochastic Gradient Descent(13405): loss=1.1792185584735464\n",
      "Stochastic Gradient Descent(13406): loss=10.128851494040118\n",
      "Stochastic Gradient Descent(13407): loss=0.8048816627141397\n",
      "Stochastic Gradient Descent(13408): loss=21.586909955013756\n",
      "Stochastic Gradient Descent(13409): loss=6.826832206756167\n",
      "Stochastic Gradient Descent(13410): loss=6.785152486957378\n",
      "Stochastic Gradient Descent(13411): loss=32.76828096909302\n",
      "Stochastic Gradient Descent(13412): loss=8.863568589683336\n",
      "Stochastic Gradient Descent(13413): loss=24.289882265444547\n",
      "Stochastic Gradient Descent(13414): loss=0.1693342767585853\n",
      "Stochastic Gradient Descent(13415): loss=0.0022601448064425455\n",
      "Stochastic Gradient Descent(13416): loss=3.8887671868911062\n",
      "Stochastic Gradient Descent(13417): loss=6.755854842793195\n",
      "Stochastic Gradient Descent(13418): loss=11.71078821059689\n",
      "Stochastic Gradient Descent(13419): loss=0.8026134454487442\n",
      "Stochastic Gradient Descent(13420): loss=11.583372215410487\n",
      "Stochastic Gradient Descent(13421): loss=4.503943935699939\n",
      "Stochastic Gradient Descent(13422): loss=5.770188334058059\n",
      "Stochastic Gradient Descent(13423): loss=8.115985821199533\n",
      "Stochastic Gradient Descent(13424): loss=0.4852961024958941\n",
      "Stochastic Gradient Descent(13425): loss=0.39395793891210956\n",
      "Stochastic Gradient Descent(13426): loss=0.14453592845770108\n",
      "Stochastic Gradient Descent(13427): loss=1.0436301873423235\n",
      "Stochastic Gradient Descent(13428): loss=1.0756466465528207\n",
      "Stochastic Gradient Descent(13429): loss=5.833981881242199\n",
      "Stochastic Gradient Descent(13430): loss=0.21335196714183416\n",
      "Stochastic Gradient Descent(13431): loss=0.12884265539289333\n",
      "Stochastic Gradient Descent(13432): loss=8.551767568419073\n",
      "Stochastic Gradient Descent(13433): loss=0.0011209851937603323\n",
      "Stochastic Gradient Descent(13434): loss=4.079293726989428\n",
      "Stochastic Gradient Descent(13435): loss=18.840631639544686\n",
      "Stochastic Gradient Descent(13436): loss=0.2552835656355312\n",
      "Stochastic Gradient Descent(13437): loss=0.9060487644151142\n",
      "Stochastic Gradient Descent(13438): loss=0.4751784803720335\n",
      "Stochastic Gradient Descent(13439): loss=0.9371428206979693\n",
      "Stochastic Gradient Descent(13440): loss=1.9787214110880857\n",
      "Stochastic Gradient Descent(13441): loss=0.0387510688319694\n",
      "Stochastic Gradient Descent(13442): loss=0.5678796757985778\n",
      "Stochastic Gradient Descent(13443): loss=0.399559384267829\n",
      "Stochastic Gradient Descent(13444): loss=8.376022619154877\n",
      "Stochastic Gradient Descent(13445): loss=2.2706722770966064\n",
      "Stochastic Gradient Descent(13446): loss=0.0006416416383799362\n",
      "Stochastic Gradient Descent(13447): loss=1.3271638551652514\n",
      "Stochastic Gradient Descent(13448): loss=0.03120652523235279\n",
      "Stochastic Gradient Descent(13449): loss=0.8416234347266117\n",
      "Stochastic Gradient Descent(13450): loss=0.5269127118346221\n",
      "Stochastic Gradient Descent(13451): loss=0.0757261060756272\n",
      "Stochastic Gradient Descent(13452): loss=17.624825688729562\n",
      "Stochastic Gradient Descent(13453): loss=8.81884154453375\n",
      "Stochastic Gradient Descent(13454): loss=0.6440271494477887\n",
      "Stochastic Gradient Descent(13455): loss=1.0530725565488035\n",
      "Stochastic Gradient Descent(13456): loss=2.504762823386717\n",
      "Stochastic Gradient Descent(13457): loss=1.2312568184594046\n",
      "Stochastic Gradient Descent(13458): loss=5.619966375243832\n",
      "Stochastic Gradient Descent(13459): loss=0.35661977441169945\n",
      "Stochastic Gradient Descent(13460): loss=8.36940146465987\n",
      "Stochastic Gradient Descent(13461): loss=1.0103078161853791\n",
      "Stochastic Gradient Descent(13462): loss=1.264517977900755\n",
      "Stochastic Gradient Descent(13463): loss=0.6244718499419915\n",
      "Stochastic Gradient Descent(13464): loss=0.3166931817703344\n",
      "Stochastic Gradient Descent(13465): loss=4.215400677947812\n",
      "Stochastic Gradient Descent(13466): loss=6.063015124586692\n",
      "Stochastic Gradient Descent(13467): loss=12.989990093637838\n",
      "Stochastic Gradient Descent(13468): loss=7.593720794911523\n",
      "Stochastic Gradient Descent(13469): loss=11.606112045813427\n",
      "Stochastic Gradient Descent(13470): loss=1.182869430578988\n",
      "Stochastic Gradient Descent(13471): loss=0.3349880128125889\n",
      "Stochastic Gradient Descent(13472): loss=0.35227362505199933\n",
      "Stochastic Gradient Descent(13473): loss=0.0016335550657003393\n",
      "Stochastic Gradient Descent(13474): loss=0.06317546780960426\n",
      "Stochastic Gradient Descent(13475): loss=0.3199387664176388\n",
      "Stochastic Gradient Descent(13476): loss=0.08605409421118777\n",
      "Stochastic Gradient Descent(13477): loss=4.704556273287769\n",
      "Stochastic Gradient Descent(13478): loss=0.36980787332360554\n",
      "Stochastic Gradient Descent(13479): loss=18.27687527450195\n",
      "Stochastic Gradient Descent(13480): loss=6.1427490999623\n",
      "Stochastic Gradient Descent(13481): loss=1.6294484101882643\n",
      "Stochastic Gradient Descent(13482): loss=1.7806044636949754\n",
      "Stochastic Gradient Descent(13483): loss=1.6369656280866882\n",
      "Stochastic Gradient Descent(13484): loss=1.0016301673146581\n",
      "Stochastic Gradient Descent(13485): loss=0.3236375286185634\n",
      "Stochastic Gradient Descent(13486): loss=0.005352174430731653\n",
      "Stochastic Gradient Descent(13487): loss=1.5488607055905752\n",
      "Stochastic Gradient Descent(13488): loss=0.7984447469336199\n",
      "Stochastic Gradient Descent(13489): loss=1.6864263509108999\n",
      "Stochastic Gradient Descent(13490): loss=1.6964568176957553\n",
      "Stochastic Gradient Descent(13491): loss=2.5917460783821777\n",
      "Stochastic Gradient Descent(13492): loss=2.6335074283459425\n",
      "Stochastic Gradient Descent(13493): loss=0.8754402003470597\n",
      "Stochastic Gradient Descent(13494): loss=4.817474048231787\n",
      "Stochastic Gradient Descent(13495): loss=32.079695560107254\n",
      "Stochastic Gradient Descent(13496): loss=11.336352967298083\n",
      "Stochastic Gradient Descent(13497): loss=5.129356711735043\n",
      "Stochastic Gradient Descent(13498): loss=0.06677488214133093\n",
      "Stochastic Gradient Descent(13499): loss=2.194654622735886\n",
      "Stochastic Gradient Descent(13500): loss=0.06441131329457393\n",
      "Stochastic Gradient Descent(13501): loss=0.07001796684428298\n",
      "Stochastic Gradient Descent(13502): loss=18.764612360427094\n",
      "Stochastic Gradient Descent(13503): loss=0.14191041212437758\n",
      "Stochastic Gradient Descent(13504): loss=4.430996295437593\n",
      "Stochastic Gradient Descent(13505): loss=6.48078400142646\n",
      "Stochastic Gradient Descent(13506): loss=0.08959667458064481\n",
      "Stochastic Gradient Descent(13507): loss=4.835176830451223\n",
      "Stochastic Gradient Descent(13508): loss=3.449892239505742\n",
      "Stochastic Gradient Descent(13509): loss=0.3496589696188002\n",
      "Stochastic Gradient Descent(13510): loss=2.2864846931934237\n",
      "Stochastic Gradient Descent(13511): loss=15.817665272544085\n",
      "Stochastic Gradient Descent(13512): loss=0.003583562714159506\n",
      "Stochastic Gradient Descent(13513): loss=6.2776246183200195\n",
      "Stochastic Gradient Descent(13514): loss=0.4710445198191473\n",
      "Stochastic Gradient Descent(13515): loss=2.614711132003819\n",
      "Stochastic Gradient Descent(13516): loss=0.25111715298432896\n",
      "Stochastic Gradient Descent(13517): loss=0.6122656600258057\n",
      "Stochastic Gradient Descent(13518): loss=4.453124057851543\n",
      "Stochastic Gradient Descent(13519): loss=0.42714796903594804\n",
      "Stochastic Gradient Descent(13520): loss=0.3454259746806428\n",
      "Stochastic Gradient Descent(13521): loss=5.587545862406137\n",
      "Stochastic Gradient Descent(13522): loss=2.5897178320319547\n",
      "Stochastic Gradient Descent(13523): loss=11.950739964615012\n",
      "Stochastic Gradient Descent(13524): loss=0.37210767155383007\n",
      "Stochastic Gradient Descent(13525): loss=4.215864599216555\n",
      "Stochastic Gradient Descent(13526): loss=2.059793823396196\n",
      "Stochastic Gradient Descent(13527): loss=0.3937805550263662\n",
      "Stochastic Gradient Descent(13528): loss=8.068978637143214\n",
      "Stochastic Gradient Descent(13529): loss=5.643432521263894\n",
      "Stochastic Gradient Descent(13530): loss=3.711554299793328\n",
      "Stochastic Gradient Descent(13531): loss=2.2409205075788554\n",
      "Stochastic Gradient Descent(13532): loss=0.06485607932956386\n",
      "Stochastic Gradient Descent(13533): loss=2.1016485317560183\n",
      "Stochastic Gradient Descent(13534): loss=1.2904947991153448\n",
      "Stochastic Gradient Descent(13535): loss=0.45721586593838603\n",
      "Stochastic Gradient Descent(13536): loss=11.33649221239023\n",
      "Stochastic Gradient Descent(13537): loss=0.004228390151639814\n",
      "Stochastic Gradient Descent(13538): loss=3.330310214324087\n",
      "Stochastic Gradient Descent(13539): loss=0.01853421909692322\n",
      "Stochastic Gradient Descent(13540): loss=0.20732946923317694\n",
      "Stochastic Gradient Descent(13541): loss=0.6858794094707651\n",
      "Stochastic Gradient Descent(13542): loss=5.50693594259363\n",
      "Stochastic Gradient Descent(13543): loss=8.131693608674627\n",
      "Stochastic Gradient Descent(13544): loss=7.11783677996927\n",
      "Stochastic Gradient Descent(13545): loss=0.08569372890530468\n",
      "Stochastic Gradient Descent(13546): loss=0.817144519932639\n",
      "Stochastic Gradient Descent(13547): loss=15.832383822281265\n",
      "Stochastic Gradient Descent(13548): loss=2.64444160987236\n",
      "Stochastic Gradient Descent(13549): loss=11.357395112991778\n",
      "Stochastic Gradient Descent(13550): loss=0.0820835119931651\n",
      "Stochastic Gradient Descent(13551): loss=6.363639511446287\n",
      "Stochastic Gradient Descent(13552): loss=25.131586189624898\n",
      "Stochastic Gradient Descent(13553): loss=37.05109517432783\n",
      "Stochastic Gradient Descent(13554): loss=0.7451140440351022\n",
      "Stochastic Gradient Descent(13555): loss=1.426532912553256\n",
      "Stochastic Gradient Descent(13556): loss=5.273358101641261\n",
      "Stochastic Gradient Descent(13557): loss=1.9677229180342921\n",
      "Stochastic Gradient Descent(13558): loss=0.0009101470954966\n",
      "Stochastic Gradient Descent(13559): loss=2.496684664486659\n",
      "Stochastic Gradient Descent(13560): loss=74.33542532889706\n",
      "Stochastic Gradient Descent(13561): loss=3.8790520730987548\n",
      "Stochastic Gradient Descent(13562): loss=1.7433419068851568\n",
      "Stochastic Gradient Descent(13563): loss=0.004688067330081792\n",
      "Stochastic Gradient Descent(13564): loss=1.939676492261384\n",
      "Stochastic Gradient Descent(13565): loss=3.0384241526563387\n",
      "Stochastic Gradient Descent(13566): loss=3.590352417204532\n",
      "Stochastic Gradient Descent(13567): loss=54.478901942242544\n",
      "Stochastic Gradient Descent(13568): loss=7.2381440300680175\n",
      "Stochastic Gradient Descent(13569): loss=17.55858834113864\n",
      "Stochastic Gradient Descent(13570): loss=2.8007042700369618\n",
      "Stochastic Gradient Descent(13571): loss=6.150312386832415\n",
      "Stochastic Gradient Descent(13572): loss=0.03425538848539023\n",
      "Stochastic Gradient Descent(13573): loss=0.47578777799528615\n",
      "Stochastic Gradient Descent(13574): loss=10.63978805756155\n",
      "Stochastic Gradient Descent(13575): loss=0.4278529008949738\n",
      "Stochastic Gradient Descent(13576): loss=0.7044775078007572\n",
      "Stochastic Gradient Descent(13577): loss=0.27070699546072924\n",
      "Stochastic Gradient Descent(13578): loss=2.3012012448940227\n",
      "Stochastic Gradient Descent(13579): loss=0.07368043541477834\n",
      "Stochastic Gradient Descent(13580): loss=6.260403726434037\n",
      "Stochastic Gradient Descent(13581): loss=0.24711058340336922\n",
      "Stochastic Gradient Descent(13582): loss=5.084752851448802\n",
      "Stochastic Gradient Descent(13583): loss=0.004566299774100203\n",
      "Stochastic Gradient Descent(13584): loss=0.014841634219803562\n",
      "Stochastic Gradient Descent(13585): loss=0.7724099215672473\n",
      "Stochastic Gradient Descent(13586): loss=2.963418675625423\n",
      "Stochastic Gradient Descent(13587): loss=1.0261821483289246\n",
      "Stochastic Gradient Descent(13588): loss=3.0962193232233086\n",
      "Stochastic Gradient Descent(13589): loss=4.723488648253986\n",
      "Stochastic Gradient Descent(13590): loss=0.006834972651895932\n",
      "Stochastic Gradient Descent(13591): loss=1.5125576909679468\n",
      "Stochastic Gradient Descent(13592): loss=32.79463519886811\n",
      "Stochastic Gradient Descent(13593): loss=0.02281402410118006\n",
      "Stochastic Gradient Descent(13594): loss=5.691053293632449\n",
      "Stochastic Gradient Descent(13595): loss=4.964577779680781\n",
      "Stochastic Gradient Descent(13596): loss=0.029986777480508663\n",
      "Stochastic Gradient Descent(13597): loss=4.204191995188647\n",
      "Stochastic Gradient Descent(13598): loss=2.2650978226094316\n",
      "Stochastic Gradient Descent(13599): loss=2.217261429798887\n",
      "Stochastic Gradient Descent(13600): loss=9.881198147262447\n",
      "Stochastic Gradient Descent(13601): loss=0.27140191412781217\n",
      "Stochastic Gradient Descent(13602): loss=15.53945503200034\n",
      "Stochastic Gradient Descent(13603): loss=0.25922701354068567\n",
      "Stochastic Gradient Descent(13604): loss=6.676860316297595\n",
      "Stochastic Gradient Descent(13605): loss=1.5921882376450993\n",
      "Stochastic Gradient Descent(13606): loss=36.8769411996524\n",
      "Stochastic Gradient Descent(13607): loss=0.9146615211678438\n",
      "Stochastic Gradient Descent(13608): loss=0.000681300931357527\n",
      "Stochastic Gradient Descent(13609): loss=4.2724960862036445\n",
      "Stochastic Gradient Descent(13610): loss=7.450421553257128\n",
      "Stochastic Gradient Descent(13611): loss=13.019729780133302\n",
      "Stochastic Gradient Descent(13612): loss=2.5568583182790485\n",
      "Stochastic Gradient Descent(13613): loss=1.5128507672447755\n",
      "Stochastic Gradient Descent(13614): loss=0.5156017869522354\n",
      "Stochastic Gradient Descent(13615): loss=23.323385163066337\n",
      "Stochastic Gradient Descent(13616): loss=0.13627425409368382\n",
      "Stochastic Gradient Descent(13617): loss=5.45759931189936\n",
      "Stochastic Gradient Descent(13618): loss=0.1117067388349714\n",
      "Stochastic Gradient Descent(13619): loss=0.45504985520477553\n",
      "Stochastic Gradient Descent(13620): loss=1.0487702600758917\n",
      "Stochastic Gradient Descent(13621): loss=5.831545263659245\n",
      "Stochastic Gradient Descent(13622): loss=8.249439477297914\n",
      "Stochastic Gradient Descent(13623): loss=11.89125043880938\n",
      "Stochastic Gradient Descent(13624): loss=0.30668762269040406\n",
      "Stochastic Gradient Descent(13625): loss=1.7589552290195414\n",
      "Stochastic Gradient Descent(13626): loss=2.892033826132042\n",
      "Stochastic Gradient Descent(13627): loss=5.754601898643813\n",
      "Stochastic Gradient Descent(13628): loss=8.546753397915126\n",
      "Stochastic Gradient Descent(13629): loss=0.0715268066093455\n",
      "Stochastic Gradient Descent(13630): loss=7.479468494491146\n",
      "Stochastic Gradient Descent(13631): loss=2.327582180159408\n",
      "Stochastic Gradient Descent(13632): loss=0.07663774402477147\n",
      "Stochastic Gradient Descent(13633): loss=0.19014013912237435\n",
      "Stochastic Gradient Descent(13634): loss=18.542866679205492\n",
      "Stochastic Gradient Descent(13635): loss=5.306299922994525\n",
      "Stochastic Gradient Descent(13636): loss=5.149487203247301\n",
      "Stochastic Gradient Descent(13637): loss=14.336469536524962\n",
      "Stochastic Gradient Descent(13638): loss=3.6244722422110685\n",
      "Stochastic Gradient Descent(13639): loss=18.667244247425362\n",
      "Stochastic Gradient Descent(13640): loss=1.5320791585371072\n",
      "Stochastic Gradient Descent(13641): loss=0.28619806124482455\n",
      "Stochastic Gradient Descent(13642): loss=7.010996615755095\n",
      "Stochastic Gradient Descent(13643): loss=9.455787410068776\n",
      "Stochastic Gradient Descent(13644): loss=6.7952571072802455\n",
      "Stochastic Gradient Descent(13645): loss=4.8223587014479135\n",
      "Stochastic Gradient Descent(13646): loss=0.0588705204978337\n",
      "Stochastic Gradient Descent(13647): loss=2.5759602000874424\n",
      "Stochastic Gradient Descent(13648): loss=21.234203894078245\n",
      "Stochastic Gradient Descent(13649): loss=2.6877951073431077\n",
      "Stochastic Gradient Descent(13650): loss=0.1967913317512829\n",
      "Stochastic Gradient Descent(13651): loss=0.13818714219274725\n",
      "Stochastic Gradient Descent(13652): loss=0.40906645737473724\n",
      "Stochastic Gradient Descent(13653): loss=2.910042686902151\n",
      "Stochastic Gradient Descent(13654): loss=9.455873351103545\n",
      "Stochastic Gradient Descent(13655): loss=4.693999410608286\n",
      "Stochastic Gradient Descent(13656): loss=26.925372143351407\n",
      "Stochastic Gradient Descent(13657): loss=7.96706884550027\n",
      "Stochastic Gradient Descent(13658): loss=0.014353476500437565\n",
      "Stochastic Gradient Descent(13659): loss=0.2812882632995631\n",
      "Stochastic Gradient Descent(13660): loss=1.243533558116093\n",
      "Stochastic Gradient Descent(13661): loss=35.17525476390504\n",
      "Stochastic Gradient Descent(13662): loss=1.3578033873482258\n",
      "Stochastic Gradient Descent(13663): loss=8.847985374414627\n",
      "Stochastic Gradient Descent(13664): loss=7.93589469106375\n",
      "Stochastic Gradient Descent(13665): loss=1.771253046530949\n",
      "Stochastic Gradient Descent(13666): loss=19.763804101689292\n",
      "Stochastic Gradient Descent(13667): loss=0.4648197750571171\n",
      "Stochastic Gradient Descent(13668): loss=19.531130509490218\n",
      "Stochastic Gradient Descent(13669): loss=0.15167700663563902\n",
      "Stochastic Gradient Descent(13670): loss=16.574228598314303\n",
      "Stochastic Gradient Descent(13671): loss=3.606928103711246\n",
      "Stochastic Gradient Descent(13672): loss=0.02437996532771425\n",
      "Stochastic Gradient Descent(13673): loss=3.220742366107956\n",
      "Stochastic Gradient Descent(13674): loss=4.522982476254671\n",
      "Stochastic Gradient Descent(13675): loss=0.19814457896127247\n",
      "Stochastic Gradient Descent(13676): loss=7.8216883207160395\n",
      "Stochastic Gradient Descent(13677): loss=0.24117274833738717\n",
      "Stochastic Gradient Descent(13678): loss=16.67446429784908\n",
      "Stochastic Gradient Descent(13679): loss=231.4952911100861\n",
      "Stochastic Gradient Descent(13680): loss=46.98405520067256\n",
      "Stochastic Gradient Descent(13681): loss=41.87327131653045\n",
      "Stochastic Gradient Descent(13682): loss=65.90281908908483\n",
      "Stochastic Gradient Descent(13683): loss=0.9759756836854776\n",
      "Stochastic Gradient Descent(13684): loss=0.540011110247197\n",
      "Stochastic Gradient Descent(13685): loss=9.957513870288638\n",
      "Stochastic Gradient Descent(13686): loss=19.69363978785085\n",
      "Stochastic Gradient Descent(13687): loss=1.6737981344231048\n",
      "Stochastic Gradient Descent(13688): loss=1.5218030280464891\n",
      "Stochastic Gradient Descent(13689): loss=1.4490507863451074\n",
      "Stochastic Gradient Descent(13690): loss=0.1218276466581363\n",
      "Stochastic Gradient Descent(13691): loss=0.3172198584264947\n",
      "Stochastic Gradient Descent(13692): loss=0.844843862352993\n",
      "Stochastic Gradient Descent(13693): loss=2.071486115372429\n",
      "Stochastic Gradient Descent(13694): loss=2.661029304657697\n",
      "Stochastic Gradient Descent(13695): loss=22.893578182890714\n",
      "Stochastic Gradient Descent(13696): loss=18.660028758720994\n",
      "Stochastic Gradient Descent(13697): loss=1.7998241627212928\n",
      "Stochastic Gradient Descent(13698): loss=6.192401667220584\n",
      "Stochastic Gradient Descent(13699): loss=2.1086282402328558\n",
      "Stochastic Gradient Descent(13700): loss=4.395561986446894\n",
      "Stochastic Gradient Descent(13701): loss=0.5373458873560653\n",
      "Stochastic Gradient Descent(13702): loss=3.0973566112582582\n",
      "Stochastic Gradient Descent(13703): loss=1.5786641974322524\n",
      "Stochastic Gradient Descent(13704): loss=0.028626753060598555\n",
      "Stochastic Gradient Descent(13705): loss=0.29819334325864266\n",
      "Stochastic Gradient Descent(13706): loss=3.188330270675936\n",
      "Stochastic Gradient Descent(13707): loss=1.498718916372801\n",
      "Stochastic Gradient Descent(13708): loss=0.008625196566902579\n",
      "Stochastic Gradient Descent(13709): loss=0.7972623757695193\n",
      "Stochastic Gradient Descent(13710): loss=0.10994505990303587\n",
      "Stochastic Gradient Descent(13711): loss=4.387112247657325\n",
      "Stochastic Gradient Descent(13712): loss=2.4330745529316897\n",
      "Stochastic Gradient Descent(13713): loss=1.218580502369823\n",
      "Stochastic Gradient Descent(13714): loss=11.224445447089428\n",
      "Stochastic Gradient Descent(13715): loss=5.149802382152153\n",
      "Stochastic Gradient Descent(13716): loss=6.997569261671711\n",
      "Stochastic Gradient Descent(13717): loss=2.224342648876572\n",
      "Stochastic Gradient Descent(13718): loss=1.230901029773094\n",
      "Stochastic Gradient Descent(13719): loss=2.267341279825353\n",
      "Stochastic Gradient Descent(13720): loss=12.506594919226465\n",
      "Stochastic Gradient Descent(13721): loss=4.002902336075505\n",
      "Stochastic Gradient Descent(13722): loss=10.119061061144572\n",
      "Stochastic Gradient Descent(13723): loss=17.23271855210176\n",
      "Stochastic Gradient Descent(13724): loss=4.444801353150706\n",
      "Stochastic Gradient Descent(13725): loss=7.408389665171628\n",
      "Stochastic Gradient Descent(13726): loss=6.655341943865569\n",
      "Stochastic Gradient Descent(13727): loss=0.0037935222474534025\n",
      "Stochastic Gradient Descent(13728): loss=4.082331818659232\n",
      "Stochastic Gradient Descent(13729): loss=0.07070427183153587\n",
      "Stochastic Gradient Descent(13730): loss=14.783920368664756\n",
      "Stochastic Gradient Descent(13731): loss=0.023375063996099614\n",
      "Stochastic Gradient Descent(13732): loss=20.570013287589287\n",
      "Stochastic Gradient Descent(13733): loss=2.5943309264773786\n",
      "Stochastic Gradient Descent(13734): loss=0.0512611546743808\n",
      "Stochastic Gradient Descent(13735): loss=2.0214006437967944\n",
      "Stochastic Gradient Descent(13736): loss=0.00993901788637174\n",
      "Stochastic Gradient Descent(13737): loss=8.927533938120083\n",
      "Stochastic Gradient Descent(13738): loss=4.655304388416161\n",
      "Stochastic Gradient Descent(13739): loss=7.9232579549798805\n",
      "Stochastic Gradient Descent(13740): loss=4.6947963632670175\n",
      "Stochastic Gradient Descent(13741): loss=3.4205042657740603\n",
      "Stochastic Gradient Descent(13742): loss=16.35252065258282\n",
      "Stochastic Gradient Descent(13743): loss=0.609733538802111\n",
      "Stochastic Gradient Descent(13744): loss=0.25171987346711777\n",
      "Stochastic Gradient Descent(13745): loss=1.5552731650802485\n",
      "Stochastic Gradient Descent(13746): loss=12.271334389953983\n",
      "Stochastic Gradient Descent(13747): loss=9.19656091043149\n",
      "Stochastic Gradient Descent(13748): loss=4.410766730106892\n",
      "Stochastic Gradient Descent(13749): loss=7.748346641080397\n",
      "Stochastic Gradient Descent(13750): loss=1.047713634851184\n",
      "Stochastic Gradient Descent(13751): loss=1.996101646641262\n",
      "Stochastic Gradient Descent(13752): loss=1.6539679774035987\n",
      "Stochastic Gradient Descent(13753): loss=0.18179584575268914\n",
      "Stochastic Gradient Descent(13754): loss=0.41690645037447044\n",
      "Stochastic Gradient Descent(13755): loss=0.339264200715241\n",
      "Stochastic Gradient Descent(13756): loss=30.614899814336052\n",
      "Stochastic Gradient Descent(13757): loss=0.01407328903894095\n",
      "Stochastic Gradient Descent(13758): loss=0.5031862000618933\n",
      "Stochastic Gradient Descent(13759): loss=2.267173484621563\n",
      "Stochastic Gradient Descent(13760): loss=11.924736847987619\n",
      "Stochastic Gradient Descent(13761): loss=2.391462093926315\n",
      "Stochastic Gradient Descent(13762): loss=6.170787348843217\n",
      "Stochastic Gradient Descent(13763): loss=5.194899702928753\n",
      "Stochastic Gradient Descent(13764): loss=1.9502231889434531\n",
      "Stochastic Gradient Descent(13765): loss=8.043163557108482\n",
      "Stochastic Gradient Descent(13766): loss=5.79397473866062\n",
      "Stochastic Gradient Descent(13767): loss=8.26793683927134\n",
      "Stochastic Gradient Descent(13768): loss=0.010994365845220531\n",
      "Stochastic Gradient Descent(13769): loss=17.790293720515663\n",
      "Stochastic Gradient Descent(13770): loss=0.7026122842045925\n",
      "Stochastic Gradient Descent(13771): loss=1.8578616595817101\n",
      "Stochastic Gradient Descent(13772): loss=2.7549321558419826\n",
      "Stochastic Gradient Descent(13773): loss=2.647394531465997\n",
      "Stochastic Gradient Descent(13774): loss=23.641957967781654\n",
      "Stochastic Gradient Descent(13775): loss=0.00020796545823166377\n",
      "Stochastic Gradient Descent(13776): loss=8.941364842049827\n",
      "Stochastic Gradient Descent(13777): loss=1.3002319141784326\n",
      "Stochastic Gradient Descent(13778): loss=0.25520352810614577\n",
      "Stochastic Gradient Descent(13779): loss=1.3097628079874633\n",
      "Stochastic Gradient Descent(13780): loss=0.005368668772237147\n",
      "Stochastic Gradient Descent(13781): loss=4.998805587939813\n",
      "Stochastic Gradient Descent(13782): loss=9.207402225792599\n",
      "Stochastic Gradient Descent(13783): loss=0.16567680392040507\n",
      "Stochastic Gradient Descent(13784): loss=3.542403149135469\n",
      "Stochastic Gradient Descent(13785): loss=1.473728890543267\n",
      "Stochastic Gradient Descent(13786): loss=11.032253721396868\n",
      "Stochastic Gradient Descent(13787): loss=8.569454775553616\n",
      "Stochastic Gradient Descent(13788): loss=0.9601323354938398\n",
      "Stochastic Gradient Descent(13789): loss=8.76519027123358e-05\n",
      "Stochastic Gradient Descent(13790): loss=1.1217676202180695\n",
      "Stochastic Gradient Descent(13791): loss=0.11308036029133102\n",
      "Stochastic Gradient Descent(13792): loss=6.616779387460365\n",
      "Stochastic Gradient Descent(13793): loss=40.04735571811694\n",
      "Stochastic Gradient Descent(13794): loss=9.551061277199315\n",
      "Stochastic Gradient Descent(13795): loss=0.055265902910844905\n",
      "Stochastic Gradient Descent(13796): loss=2.0084841043868766\n",
      "Stochastic Gradient Descent(13797): loss=14.696995937898278\n",
      "Stochastic Gradient Descent(13798): loss=26.796987440002297\n",
      "Stochastic Gradient Descent(13799): loss=0.19932104562611458\n",
      "Stochastic Gradient Descent(13800): loss=10.269153311881569\n",
      "Stochastic Gradient Descent(13801): loss=19.394388137933287\n",
      "Stochastic Gradient Descent(13802): loss=16.657226137768607\n",
      "Stochastic Gradient Descent(13803): loss=2.958770220588005\n",
      "Stochastic Gradient Descent(13804): loss=20.37131422755908\n",
      "Stochastic Gradient Descent(13805): loss=0.00027406037627389395\n",
      "Stochastic Gradient Descent(13806): loss=0.023776715621938758\n",
      "Stochastic Gradient Descent(13807): loss=32.43960348017806\n",
      "Stochastic Gradient Descent(13808): loss=9.451308810443331\n",
      "Stochastic Gradient Descent(13809): loss=0.36779102073984604\n",
      "Stochastic Gradient Descent(13810): loss=0.07523710682552848\n",
      "Stochastic Gradient Descent(13811): loss=0.8460692387905895\n",
      "Stochastic Gradient Descent(13812): loss=4.6968403068603415\n",
      "Stochastic Gradient Descent(13813): loss=0.5059504589720814\n",
      "Stochastic Gradient Descent(13814): loss=4.737597501626709\n",
      "Stochastic Gradient Descent(13815): loss=5.521045676798947\n",
      "Stochastic Gradient Descent(13816): loss=0.800267013675829\n",
      "Stochastic Gradient Descent(13817): loss=2.4751567273690247\n",
      "Stochastic Gradient Descent(13818): loss=0.2884902876926075\n",
      "Stochastic Gradient Descent(13819): loss=5.199145127685461\n",
      "Stochastic Gradient Descent(13820): loss=2.701097571521695\n",
      "Stochastic Gradient Descent(13821): loss=13.126386494241062\n",
      "Stochastic Gradient Descent(13822): loss=7.1096973344140535\n",
      "Stochastic Gradient Descent(13823): loss=0.7191384829983201\n",
      "Stochastic Gradient Descent(13824): loss=0.05352504530302843\n",
      "Stochastic Gradient Descent(13825): loss=16.57542211948934\n",
      "Stochastic Gradient Descent(13826): loss=1.461303773452146\n",
      "Stochastic Gradient Descent(13827): loss=10.006731144229331\n",
      "Stochastic Gradient Descent(13828): loss=7.835818648609129\n",
      "Stochastic Gradient Descent(13829): loss=12.106540551425882\n",
      "Stochastic Gradient Descent(13830): loss=1.6544270762885123\n",
      "Stochastic Gradient Descent(13831): loss=9.038502360645854\n",
      "Stochastic Gradient Descent(13832): loss=8.411083749425545\n",
      "Stochastic Gradient Descent(13833): loss=0.8554833524051774\n",
      "Stochastic Gradient Descent(13834): loss=12.44440631544282\n",
      "Stochastic Gradient Descent(13835): loss=2.0163364795808545\n",
      "Stochastic Gradient Descent(13836): loss=6.789504387589571\n",
      "Stochastic Gradient Descent(13837): loss=0.702899450615893\n",
      "Stochastic Gradient Descent(13838): loss=1.0710302784060126\n",
      "Stochastic Gradient Descent(13839): loss=26.965441592067755\n",
      "Stochastic Gradient Descent(13840): loss=51.29910345820999\n",
      "Stochastic Gradient Descent(13841): loss=1.2890693567407985\n",
      "Stochastic Gradient Descent(13842): loss=6.151677488286646\n",
      "Stochastic Gradient Descent(13843): loss=0.43790389423818193\n",
      "Stochastic Gradient Descent(13844): loss=7.327341361523405\n",
      "Stochastic Gradient Descent(13845): loss=7.524685282734207\n",
      "Stochastic Gradient Descent(13846): loss=1.5072745203649212\n",
      "Stochastic Gradient Descent(13847): loss=0.05834256256796683\n",
      "Stochastic Gradient Descent(13848): loss=0.39240223111127215\n",
      "Stochastic Gradient Descent(13849): loss=4.848175575633094\n",
      "Stochastic Gradient Descent(13850): loss=5.875142459254935\n",
      "Stochastic Gradient Descent(13851): loss=0.0002039807564114257\n",
      "Stochastic Gradient Descent(13852): loss=2.46334247754597\n",
      "Stochastic Gradient Descent(13853): loss=1.1665871754193646\n",
      "Stochastic Gradient Descent(13854): loss=1.900024064016275\n",
      "Stochastic Gradient Descent(13855): loss=18.58039199386043\n",
      "Stochastic Gradient Descent(13856): loss=0.010331761280512903\n",
      "Stochastic Gradient Descent(13857): loss=7.184815872025252\n",
      "Stochastic Gradient Descent(13858): loss=0.5581669547919195\n",
      "Stochastic Gradient Descent(13859): loss=0.03948934127664968\n",
      "Stochastic Gradient Descent(13860): loss=22.709079847451207\n",
      "Stochastic Gradient Descent(13861): loss=1.5495470915794889\n",
      "Stochastic Gradient Descent(13862): loss=3.8930933843334534\n",
      "Stochastic Gradient Descent(13863): loss=9.226424623536605\n",
      "Stochastic Gradient Descent(13864): loss=11.627226727819972\n",
      "Stochastic Gradient Descent(13865): loss=25.10252378037602\n",
      "Stochastic Gradient Descent(13866): loss=0.2887535173855128\n",
      "Stochastic Gradient Descent(13867): loss=3.706506504669155\n",
      "Stochastic Gradient Descent(13868): loss=8.722852341628265\n",
      "Stochastic Gradient Descent(13869): loss=0.15295840219271822\n",
      "Stochastic Gradient Descent(13870): loss=7.529154746632322\n",
      "Stochastic Gradient Descent(13871): loss=8.2951271762511\n",
      "Stochastic Gradient Descent(13872): loss=3.941799420081232\n",
      "Stochastic Gradient Descent(13873): loss=4.202954618369863\n",
      "Stochastic Gradient Descent(13874): loss=1.3179102949332815\n",
      "Stochastic Gradient Descent(13875): loss=10.423410877203814\n",
      "Stochastic Gradient Descent(13876): loss=0.20476369765951052\n",
      "Stochastic Gradient Descent(13877): loss=2.873269656809813\n",
      "Stochastic Gradient Descent(13878): loss=0.25945495697720344\n",
      "Stochastic Gradient Descent(13879): loss=2.6074242354222283\n",
      "Stochastic Gradient Descent(13880): loss=0.6222444346777173\n",
      "Stochastic Gradient Descent(13881): loss=0.7859132025557497\n",
      "Stochastic Gradient Descent(13882): loss=0.2585712013751173\n",
      "Stochastic Gradient Descent(13883): loss=21.03937029315521\n",
      "Stochastic Gradient Descent(13884): loss=2.648094207729068\n",
      "Stochastic Gradient Descent(13885): loss=0.04057231417823154\n",
      "Stochastic Gradient Descent(13886): loss=0.43180018250668223\n",
      "Stochastic Gradient Descent(13887): loss=0.005093761802661884\n",
      "Stochastic Gradient Descent(13888): loss=4.577394293541052\n",
      "Stochastic Gradient Descent(13889): loss=1.4470663616741282\n",
      "Stochastic Gradient Descent(13890): loss=3.2355650247247127\n",
      "Stochastic Gradient Descent(13891): loss=6.2658234445772765\n",
      "Stochastic Gradient Descent(13892): loss=14.284929326587832\n",
      "Stochastic Gradient Descent(13893): loss=0.1040435832879647\n",
      "Stochastic Gradient Descent(13894): loss=0.48318081883613406\n",
      "Stochastic Gradient Descent(13895): loss=3.34410161557281\n",
      "Stochastic Gradient Descent(13896): loss=3.4533785543456315\n",
      "Stochastic Gradient Descent(13897): loss=0.0011851809535980379\n",
      "Stochastic Gradient Descent(13898): loss=4.042037212829525e-05\n",
      "Stochastic Gradient Descent(13899): loss=4.350455262307556\n",
      "Stochastic Gradient Descent(13900): loss=0.17615790532984593\n",
      "Stochastic Gradient Descent(13901): loss=5.423148555484094\n",
      "Stochastic Gradient Descent(13902): loss=1.7505801808362416\n",
      "Stochastic Gradient Descent(13903): loss=0.18148931545176852\n",
      "Stochastic Gradient Descent(13904): loss=0.5277851388167341\n",
      "Stochastic Gradient Descent(13905): loss=1.3957911248729908\n",
      "Stochastic Gradient Descent(13906): loss=21.70026046657742\n",
      "Stochastic Gradient Descent(13907): loss=8.133930255822554\n",
      "Stochastic Gradient Descent(13908): loss=0.2322193062339771\n",
      "Stochastic Gradient Descent(13909): loss=15.423958191053313\n",
      "Stochastic Gradient Descent(13910): loss=4.035859910606869\n",
      "Stochastic Gradient Descent(13911): loss=6.56710531329738\n",
      "Stochastic Gradient Descent(13912): loss=6.437025358029585\n",
      "Stochastic Gradient Descent(13913): loss=7.528472316341425\n",
      "Stochastic Gradient Descent(13914): loss=0.26756635128360584\n",
      "Stochastic Gradient Descent(13915): loss=3.9453271071999794\n",
      "Stochastic Gradient Descent(13916): loss=0.7657749734934522\n",
      "Stochastic Gradient Descent(13917): loss=1.4116500330920305\n",
      "Stochastic Gradient Descent(13918): loss=13.241657663581293\n",
      "Stochastic Gradient Descent(13919): loss=1.6938887525166455\n",
      "Stochastic Gradient Descent(13920): loss=1.523600343327091\n",
      "Stochastic Gradient Descent(13921): loss=10.943285414495422\n",
      "Stochastic Gradient Descent(13922): loss=2.288366953187274\n",
      "Stochastic Gradient Descent(13923): loss=8.27859904498\n",
      "Stochastic Gradient Descent(13924): loss=1.6104919873972683\n",
      "Stochastic Gradient Descent(13925): loss=17.68105543941165\n",
      "Stochastic Gradient Descent(13926): loss=8.648002149051221\n",
      "Stochastic Gradient Descent(13927): loss=0.0006329079150555398\n",
      "Stochastic Gradient Descent(13928): loss=3.7109394343821553\n",
      "Stochastic Gradient Descent(13929): loss=3.9766283046973596\n",
      "Stochastic Gradient Descent(13930): loss=4.503117180443267\n",
      "Stochastic Gradient Descent(13931): loss=23.75179828935597\n",
      "Stochastic Gradient Descent(13932): loss=0.7341102777036179\n",
      "Stochastic Gradient Descent(13933): loss=2.674800053919102\n",
      "Stochastic Gradient Descent(13934): loss=3.8662391208524114\n",
      "Stochastic Gradient Descent(13935): loss=0.44964795333230906\n",
      "Stochastic Gradient Descent(13936): loss=4.008510716424152\n",
      "Stochastic Gradient Descent(13937): loss=11.425272374312119\n",
      "Stochastic Gradient Descent(13938): loss=1.4569643837611392\n",
      "Stochastic Gradient Descent(13939): loss=0.5209360464350119\n",
      "Stochastic Gradient Descent(13940): loss=2.7096614184737735\n",
      "Stochastic Gradient Descent(13941): loss=12.753336670818545\n",
      "Stochastic Gradient Descent(13942): loss=0.26068153010194917\n",
      "Stochastic Gradient Descent(13943): loss=1.448004600547651\n",
      "Stochastic Gradient Descent(13944): loss=26.167378941060264\n",
      "Stochastic Gradient Descent(13945): loss=0.5900782271282893\n",
      "Stochastic Gradient Descent(13946): loss=0.20350668604984248\n",
      "Stochastic Gradient Descent(13947): loss=8.083129311086049\n",
      "Stochastic Gradient Descent(13948): loss=21.178627737079363\n",
      "Stochastic Gradient Descent(13949): loss=8.685010801771625\n",
      "Stochastic Gradient Descent(13950): loss=0.04168635770061715\n",
      "Stochastic Gradient Descent(13951): loss=0.43314657795962036\n",
      "Stochastic Gradient Descent(13952): loss=0.17985263137514026\n",
      "Stochastic Gradient Descent(13953): loss=6.14693830446859\n",
      "Stochastic Gradient Descent(13954): loss=0.027572527378969\n",
      "Stochastic Gradient Descent(13955): loss=4.4853419502618594\n",
      "Stochastic Gradient Descent(13956): loss=0.023712020138194352\n",
      "Stochastic Gradient Descent(13957): loss=9.024109060384527\n",
      "Stochastic Gradient Descent(13958): loss=0.01546548208075303\n",
      "Stochastic Gradient Descent(13959): loss=2.532703424858276\n",
      "Stochastic Gradient Descent(13960): loss=8.66364127086631\n",
      "Stochastic Gradient Descent(13961): loss=3.4380259683679353\n",
      "Stochastic Gradient Descent(13962): loss=0.18240525570116553\n",
      "Stochastic Gradient Descent(13963): loss=7.683882270190944\n",
      "Stochastic Gradient Descent(13964): loss=0.006797642393612901\n",
      "Stochastic Gradient Descent(13965): loss=0.13553493577527004\n",
      "Stochastic Gradient Descent(13966): loss=1.7632948009040317\n",
      "Stochastic Gradient Descent(13967): loss=1.1369152667042615\n",
      "Stochastic Gradient Descent(13968): loss=1.0412645658585278\n",
      "Stochastic Gradient Descent(13969): loss=6.832793210775114\n",
      "Stochastic Gradient Descent(13970): loss=6.2626684764006395\n",
      "Stochastic Gradient Descent(13971): loss=1.5439528310998725\n",
      "Stochastic Gradient Descent(13972): loss=2.575085551828992\n",
      "Stochastic Gradient Descent(13973): loss=0.7154549380842954\n",
      "Stochastic Gradient Descent(13974): loss=1.6719939267314805\n",
      "Stochastic Gradient Descent(13975): loss=6.191814969327244\n",
      "Stochastic Gradient Descent(13976): loss=5.314159723343715\n",
      "Stochastic Gradient Descent(13977): loss=0.5797151963033005\n",
      "Stochastic Gradient Descent(13978): loss=1.7919254289816346\n",
      "Stochastic Gradient Descent(13979): loss=0.3121083259421063\n",
      "Stochastic Gradient Descent(13980): loss=0.2725994942047972\n",
      "Stochastic Gradient Descent(13981): loss=0.8476427482805458\n",
      "Stochastic Gradient Descent(13982): loss=1.7737352527045642\n",
      "Stochastic Gradient Descent(13983): loss=0.6790450743773824\n",
      "Stochastic Gradient Descent(13984): loss=7.652759928207715\n",
      "Stochastic Gradient Descent(13985): loss=0.46388611392374196\n",
      "Stochastic Gradient Descent(13986): loss=0.42022363507076443\n",
      "Stochastic Gradient Descent(13987): loss=9.413813247711587\n",
      "Stochastic Gradient Descent(13988): loss=0.17043256645472896\n",
      "Stochastic Gradient Descent(13989): loss=6.040474510842238\n",
      "Stochastic Gradient Descent(13990): loss=7.968242690950834\n",
      "Stochastic Gradient Descent(13991): loss=3.091390293726666\n",
      "Stochastic Gradient Descent(13992): loss=0.05947270290473795\n",
      "Stochastic Gradient Descent(13993): loss=4.843367143953647\n",
      "Stochastic Gradient Descent(13994): loss=0.9999724030958034\n",
      "Stochastic Gradient Descent(13995): loss=0.2770900211925752\n",
      "Stochastic Gradient Descent(13996): loss=0.5915303499388936\n",
      "Stochastic Gradient Descent(13997): loss=5.78626429582996\n",
      "Stochastic Gradient Descent(13998): loss=0.5166499634193005\n",
      "Stochastic Gradient Descent(13999): loss=0.16292892148535834\n",
      "Stochastic Gradient Descent(14000): loss=1.2527221819551775\n",
      "Stochastic Gradient Descent(14001): loss=0.0002818183634715484\n",
      "Stochastic Gradient Descent(14002): loss=48.31059553196002\n",
      "Stochastic Gradient Descent(14003): loss=0.03228475431438909\n",
      "Stochastic Gradient Descent(14004): loss=0.20468074731869806\n",
      "Stochastic Gradient Descent(14005): loss=1.6572274539734348\n",
      "Stochastic Gradient Descent(14006): loss=3.9536806737354797\n",
      "Stochastic Gradient Descent(14007): loss=0.06807557510292916\n",
      "Stochastic Gradient Descent(14008): loss=3.4920133168022818\n",
      "Stochastic Gradient Descent(14009): loss=1.0693142966044031\n",
      "Stochastic Gradient Descent(14010): loss=1.8626401683916156\n",
      "Stochastic Gradient Descent(14011): loss=2.068059842014328\n",
      "Stochastic Gradient Descent(14012): loss=0.3265777270839891\n",
      "Stochastic Gradient Descent(14013): loss=17.640810761933807\n",
      "Stochastic Gradient Descent(14014): loss=7.981813614407536\n",
      "Stochastic Gradient Descent(14015): loss=0.11746303102435626\n",
      "Stochastic Gradient Descent(14016): loss=0.8225480597903364\n",
      "Stochastic Gradient Descent(14017): loss=1.3890376758432696\n",
      "Stochastic Gradient Descent(14018): loss=0.007973509203963608\n",
      "Stochastic Gradient Descent(14019): loss=0.6318421324848199\n",
      "Stochastic Gradient Descent(14020): loss=12.314049410236667\n",
      "Stochastic Gradient Descent(14021): loss=8.54871111356555\n",
      "Stochastic Gradient Descent(14022): loss=0.5374095367463726\n",
      "Stochastic Gradient Descent(14023): loss=18.165084715815084\n",
      "Stochastic Gradient Descent(14024): loss=0.0021599379221872166\n",
      "Stochastic Gradient Descent(14025): loss=0.65991833478926\n",
      "Stochastic Gradient Descent(14026): loss=1.601534074899123\n",
      "Stochastic Gradient Descent(14027): loss=13.633560946067321\n",
      "Stochastic Gradient Descent(14028): loss=3.568933656299609\n",
      "Stochastic Gradient Descent(14029): loss=2.888081891582806\n",
      "Stochastic Gradient Descent(14030): loss=3.2962626186200663\n",
      "Stochastic Gradient Descent(14031): loss=0.043856501837866955\n",
      "Stochastic Gradient Descent(14032): loss=0.08222121405753519\n",
      "Stochastic Gradient Descent(14033): loss=8.90536101475212\n",
      "Stochastic Gradient Descent(14034): loss=17.51718697707841\n",
      "Stochastic Gradient Descent(14035): loss=1.574126125842282\n",
      "Stochastic Gradient Descent(14036): loss=0.6066571776632994\n",
      "Stochastic Gradient Descent(14037): loss=2.3629757022557865\n",
      "Stochastic Gradient Descent(14038): loss=22.85804867163027\n",
      "Stochastic Gradient Descent(14039): loss=1.0108453859594726\n",
      "Stochastic Gradient Descent(14040): loss=22.0726582693801\n",
      "Stochastic Gradient Descent(14041): loss=9.117619120575025\n",
      "Stochastic Gradient Descent(14042): loss=0.3918447477414595\n",
      "Stochastic Gradient Descent(14043): loss=0.29420498057266803\n",
      "Stochastic Gradient Descent(14044): loss=6.564219903362348\n",
      "Stochastic Gradient Descent(14045): loss=1.718344349402315\n",
      "Stochastic Gradient Descent(14046): loss=1.2030571891816488\n",
      "Stochastic Gradient Descent(14047): loss=0.221593307660954\n",
      "Stochastic Gradient Descent(14048): loss=4.510225127443088\n",
      "Stochastic Gradient Descent(14049): loss=0.32318436507426596\n",
      "Stochastic Gradient Descent(14050): loss=2.2055559550448836\n",
      "Stochastic Gradient Descent(14051): loss=1.235985214773784\n",
      "Stochastic Gradient Descent(14052): loss=7.461074395129773\n",
      "Stochastic Gradient Descent(14053): loss=15.422730199669386\n",
      "Stochastic Gradient Descent(14054): loss=6.435248071057741\n",
      "Stochastic Gradient Descent(14055): loss=15.882177738269109\n",
      "Stochastic Gradient Descent(14056): loss=3.0745550505645074\n",
      "Stochastic Gradient Descent(14057): loss=0.001176377150056512\n",
      "Stochastic Gradient Descent(14058): loss=0.15847051542457474\n",
      "Stochastic Gradient Descent(14059): loss=0.17629042702689737\n",
      "Stochastic Gradient Descent(14060): loss=14.672744244035574\n",
      "Stochastic Gradient Descent(14061): loss=0.9215523515955716\n",
      "Stochastic Gradient Descent(14062): loss=6.817802274927962\n",
      "Stochastic Gradient Descent(14063): loss=5.964813661739356\n",
      "Stochastic Gradient Descent(14064): loss=4.382432490642677\n",
      "Stochastic Gradient Descent(14065): loss=6.748603967478478\n",
      "Stochastic Gradient Descent(14066): loss=3.267033277821438\n",
      "Stochastic Gradient Descent(14067): loss=14.775815056108973\n",
      "Stochastic Gradient Descent(14068): loss=1.4602489011783268\n",
      "Stochastic Gradient Descent(14069): loss=3.529971305094925\n",
      "Stochastic Gradient Descent(14070): loss=0.031253099447346805\n",
      "Stochastic Gradient Descent(14071): loss=2.6857017474766596\n",
      "Stochastic Gradient Descent(14072): loss=0.00045081905558033014\n",
      "Stochastic Gradient Descent(14073): loss=0.6824692904542968\n",
      "Stochastic Gradient Descent(14074): loss=7.899077073803164\n",
      "Stochastic Gradient Descent(14075): loss=16.501861143605005\n",
      "Stochastic Gradient Descent(14076): loss=2.8558217845183127\n",
      "Stochastic Gradient Descent(14077): loss=0.8768913416931982\n",
      "Stochastic Gradient Descent(14078): loss=7.303880326539737\n",
      "Stochastic Gradient Descent(14079): loss=0.011710090708096433\n",
      "Stochastic Gradient Descent(14080): loss=2.1169220761850918\n",
      "Stochastic Gradient Descent(14081): loss=1.1569273663255872\n",
      "Stochastic Gradient Descent(14082): loss=0.09153551931089955\n",
      "Stochastic Gradient Descent(14083): loss=9.217195365185235\n",
      "Stochastic Gradient Descent(14084): loss=9.405473470205104\n",
      "Stochastic Gradient Descent(14085): loss=4.769621956951716\n",
      "Stochastic Gradient Descent(14086): loss=0.674741665826503\n",
      "Stochastic Gradient Descent(14087): loss=2.8422228423699027\n",
      "Stochastic Gradient Descent(14088): loss=13.925760046768657\n",
      "Stochastic Gradient Descent(14089): loss=21.971186233991737\n",
      "Stochastic Gradient Descent(14090): loss=0.19186247031046294\n",
      "Stochastic Gradient Descent(14091): loss=5.4520793851724525\n",
      "Stochastic Gradient Descent(14092): loss=0.43395716058233136\n",
      "Stochastic Gradient Descent(14093): loss=2.104722134037616\n",
      "Stochastic Gradient Descent(14094): loss=2.189285444654638\n",
      "Stochastic Gradient Descent(14095): loss=5.250140449137513\n",
      "Stochastic Gradient Descent(14096): loss=1.1597471708045923\n",
      "Stochastic Gradient Descent(14097): loss=0.0658231571453674\n",
      "Stochastic Gradient Descent(14098): loss=0.25984423603026063\n",
      "Stochastic Gradient Descent(14099): loss=0.010500083572340285\n",
      "Stochastic Gradient Descent(14100): loss=3.2541851252980165\n",
      "Stochastic Gradient Descent(14101): loss=0.40863804827038014\n",
      "Stochastic Gradient Descent(14102): loss=37.24483941088854\n",
      "Stochastic Gradient Descent(14103): loss=31.402552286817635\n",
      "Stochastic Gradient Descent(14104): loss=1.1817842997944217\n",
      "Stochastic Gradient Descent(14105): loss=7.099277667895642\n",
      "Stochastic Gradient Descent(14106): loss=1.1901684865539026\n",
      "Stochastic Gradient Descent(14107): loss=12.870401188449799\n",
      "Stochastic Gradient Descent(14108): loss=11.189024606782647\n",
      "Stochastic Gradient Descent(14109): loss=0.1402309048292676\n",
      "Stochastic Gradient Descent(14110): loss=11.748281581429746\n",
      "Stochastic Gradient Descent(14111): loss=3.358104571180215\n",
      "Stochastic Gradient Descent(14112): loss=1.4207957872578016\n",
      "Stochastic Gradient Descent(14113): loss=0.27488417212254557\n",
      "Stochastic Gradient Descent(14114): loss=7.035618831021981\n",
      "Stochastic Gradient Descent(14115): loss=2.496035524529289\n",
      "Stochastic Gradient Descent(14116): loss=0.24803209566880383\n",
      "Stochastic Gradient Descent(14117): loss=1.7239146839936472\n",
      "Stochastic Gradient Descent(14118): loss=13.18304237408416\n",
      "Stochastic Gradient Descent(14119): loss=1.3576148437559228\n",
      "Stochastic Gradient Descent(14120): loss=3.9916895182280037\n",
      "Stochastic Gradient Descent(14121): loss=5.394007905474169\n",
      "Stochastic Gradient Descent(14122): loss=0.0061821052960138205\n",
      "Stochastic Gradient Descent(14123): loss=20.40078694412007\n",
      "Stochastic Gradient Descent(14124): loss=0.17590966007987874\n",
      "Stochastic Gradient Descent(14125): loss=0.2632498668772984\n",
      "Stochastic Gradient Descent(14126): loss=0.32080982998344676\n",
      "Stochastic Gradient Descent(14127): loss=1.4068048622212326\n",
      "Stochastic Gradient Descent(14128): loss=0.01240246427829924\n",
      "Stochastic Gradient Descent(14129): loss=11.998469288865396\n",
      "Stochastic Gradient Descent(14130): loss=0.3218721845209177\n",
      "Stochastic Gradient Descent(14131): loss=12.083817161398377\n",
      "Stochastic Gradient Descent(14132): loss=31.088441918654677\n",
      "Stochastic Gradient Descent(14133): loss=16.494213206519508\n",
      "Stochastic Gradient Descent(14134): loss=4.889191854651074\n",
      "Stochastic Gradient Descent(14135): loss=8.043867638316732\n",
      "Stochastic Gradient Descent(14136): loss=2.662310153269195\n",
      "Stochastic Gradient Descent(14137): loss=5.930726629111972\n",
      "Stochastic Gradient Descent(14138): loss=17.194023099347614\n",
      "Stochastic Gradient Descent(14139): loss=1.5737480398614603\n",
      "Stochastic Gradient Descent(14140): loss=0.10354839841131992\n",
      "Stochastic Gradient Descent(14141): loss=0.00016518026848781153\n",
      "Stochastic Gradient Descent(14142): loss=9.419146402330755\n",
      "Stochastic Gradient Descent(14143): loss=0.10269547615044668\n",
      "Stochastic Gradient Descent(14144): loss=0.04411200490580164\n",
      "Stochastic Gradient Descent(14145): loss=1.3793180058300154\n",
      "Stochastic Gradient Descent(14146): loss=13.056871677917748\n",
      "Stochastic Gradient Descent(14147): loss=0.8578352135184973\n",
      "Stochastic Gradient Descent(14148): loss=0.8750888270613645\n",
      "Stochastic Gradient Descent(14149): loss=0.022838078523465682\n",
      "Stochastic Gradient Descent(14150): loss=5.9272486020973645\n",
      "Stochastic Gradient Descent(14151): loss=1.5446250588177353\n",
      "Stochastic Gradient Descent(14152): loss=13.877185947575331\n",
      "Stochastic Gradient Descent(14153): loss=1.1436914924004185\n",
      "Stochastic Gradient Descent(14154): loss=10.379875303197668\n",
      "Stochastic Gradient Descent(14155): loss=3.65345682450415\n",
      "Stochastic Gradient Descent(14156): loss=0.8734800305553259\n",
      "Stochastic Gradient Descent(14157): loss=10.511420850862807\n",
      "Stochastic Gradient Descent(14158): loss=9.580801897593844\n",
      "Stochastic Gradient Descent(14159): loss=0.3168683093781601\n",
      "Stochastic Gradient Descent(14160): loss=14.551107956552055\n",
      "Stochastic Gradient Descent(14161): loss=31.396718297584417\n",
      "Stochastic Gradient Descent(14162): loss=3.4750313837023494\n",
      "Stochastic Gradient Descent(14163): loss=8.748832550280763\n",
      "Stochastic Gradient Descent(14164): loss=6.54325386264744\n",
      "Stochastic Gradient Descent(14165): loss=0.7557062440332017\n",
      "Stochastic Gradient Descent(14166): loss=2.761284718624521\n",
      "Stochastic Gradient Descent(14167): loss=0.6993564163099164\n",
      "Stochastic Gradient Descent(14168): loss=0.7535824262793336\n",
      "Stochastic Gradient Descent(14169): loss=0.04608483332799427\n",
      "Stochastic Gradient Descent(14170): loss=0.422182339376418\n",
      "Stochastic Gradient Descent(14171): loss=0.19115388341094644\n",
      "Stochastic Gradient Descent(14172): loss=0.10043934198135565\n",
      "Stochastic Gradient Descent(14173): loss=0.22579708561594963\n",
      "Stochastic Gradient Descent(14174): loss=2.1367247085957155\n",
      "Stochastic Gradient Descent(14175): loss=2.4113197261892116\n",
      "Stochastic Gradient Descent(14176): loss=2.8244894495395805\n",
      "Stochastic Gradient Descent(14177): loss=0.17629885311324275\n",
      "Stochastic Gradient Descent(14178): loss=2.4973648869801734\n",
      "Stochastic Gradient Descent(14179): loss=1.4586619512184449\n",
      "Stochastic Gradient Descent(14180): loss=1.2736984653247643\n",
      "Stochastic Gradient Descent(14181): loss=0.31668518088148173\n",
      "Stochastic Gradient Descent(14182): loss=1.5346033952470723\n",
      "Stochastic Gradient Descent(14183): loss=1.4319192662842604\n",
      "Stochastic Gradient Descent(14184): loss=2.4085874913549996\n",
      "Stochastic Gradient Descent(14185): loss=0.6920458439659406\n",
      "Stochastic Gradient Descent(14186): loss=20.666842627547375\n",
      "Stochastic Gradient Descent(14187): loss=0.08931943241411847\n",
      "Stochastic Gradient Descent(14188): loss=0.013850686715221407\n",
      "Stochastic Gradient Descent(14189): loss=0.1599530592601004\n",
      "Stochastic Gradient Descent(14190): loss=5.136295008724154\n",
      "Stochastic Gradient Descent(14191): loss=4.087994416854155\n",
      "Stochastic Gradient Descent(14192): loss=0.2749156675217664\n",
      "Stochastic Gradient Descent(14193): loss=3.9931255795084457\n",
      "Stochastic Gradient Descent(14194): loss=10.675838615154037\n",
      "Stochastic Gradient Descent(14195): loss=14.278504256484004\n",
      "Stochastic Gradient Descent(14196): loss=3.6149225772132114\n",
      "Stochastic Gradient Descent(14197): loss=0.4003443440180604\n",
      "Stochastic Gradient Descent(14198): loss=4.779185573757754\n",
      "Stochastic Gradient Descent(14199): loss=0.4470315949011201\n",
      "Stochastic Gradient Descent(14200): loss=0.1998118974837351\n",
      "Stochastic Gradient Descent(14201): loss=1.4011754590864807\n",
      "Stochastic Gradient Descent(14202): loss=0.038497180083898436\n",
      "Stochastic Gradient Descent(14203): loss=9.353540225968131e-05\n",
      "Stochastic Gradient Descent(14204): loss=0.34742592988091164\n",
      "Stochastic Gradient Descent(14205): loss=4.178168134784148\n",
      "Stochastic Gradient Descent(14206): loss=0.7779246264838795\n",
      "Stochastic Gradient Descent(14207): loss=2.4016545496870703\n",
      "Stochastic Gradient Descent(14208): loss=7.885740555545242\n",
      "Stochastic Gradient Descent(14209): loss=0.33064065601763454\n",
      "Stochastic Gradient Descent(14210): loss=6.1704928938129\n",
      "Stochastic Gradient Descent(14211): loss=0.7485098578171059\n",
      "Stochastic Gradient Descent(14212): loss=0.011838530405634934\n",
      "Stochastic Gradient Descent(14213): loss=0.658940964939732\n",
      "Stochastic Gradient Descent(14214): loss=0.9386267099514487\n",
      "Stochastic Gradient Descent(14215): loss=0.3685082776431914\n",
      "Stochastic Gradient Descent(14216): loss=9.865947311048997\n",
      "Stochastic Gradient Descent(14217): loss=0.2509135945541066\n",
      "Stochastic Gradient Descent(14218): loss=1.2475693994816879\n",
      "Stochastic Gradient Descent(14219): loss=0.1479767040029556\n",
      "Stochastic Gradient Descent(14220): loss=0.5263324339505345\n",
      "Stochastic Gradient Descent(14221): loss=1.8400160851749658\n",
      "Stochastic Gradient Descent(14222): loss=9.296484422642147\n",
      "Stochastic Gradient Descent(14223): loss=0.08126697386969615\n",
      "Stochastic Gradient Descent(14224): loss=7.0217909546552955\n",
      "Stochastic Gradient Descent(14225): loss=40.06459218107329\n",
      "Stochastic Gradient Descent(14226): loss=1.0639279626361653\n",
      "Stochastic Gradient Descent(14227): loss=3.0511199824182174\n",
      "Stochastic Gradient Descent(14228): loss=0.3804860620954685\n",
      "Stochastic Gradient Descent(14229): loss=2.077251849438336\n",
      "Stochastic Gradient Descent(14230): loss=0.1500884896872701\n",
      "Stochastic Gradient Descent(14231): loss=1.5111157656712189\n",
      "Stochastic Gradient Descent(14232): loss=0.00017721929268452175\n",
      "Stochastic Gradient Descent(14233): loss=0.4457657527898519\n",
      "Stochastic Gradient Descent(14234): loss=0.0011987616831093733\n",
      "Stochastic Gradient Descent(14235): loss=1.74277033410738\n",
      "Stochastic Gradient Descent(14236): loss=0.15186643972492542\n",
      "Stochastic Gradient Descent(14237): loss=22.374430944150124\n",
      "Stochastic Gradient Descent(14238): loss=0.25467178971959314\n",
      "Stochastic Gradient Descent(14239): loss=17.668921849407717\n",
      "Stochastic Gradient Descent(14240): loss=1.9700843967151864\n",
      "Stochastic Gradient Descent(14241): loss=2.4264842880663644\n",
      "Stochastic Gradient Descent(14242): loss=0.8678116310851185\n",
      "Stochastic Gradient Descent(14243): loss=5.448439905187789\n",
      "Stochastic Gradient Descent(14244): loss=11.019859861465052\n",
      "Stochastic Gradient Descent(14245): loss=6.412360741186794\n",
      "Stochastic Gradient Descent(14246): loss=0.2758524486135417\n",
      "Stochastic Gradient Descent(14247): loss=1.8535416473494686\n",
      "Stochastic Gradient Descent(14248): loss=0.8832350636997349\n",
      "Stochastic Gradient Descent(14249): loss=2.1804674567231577\n",
      "Stochastic Gradient Descent(14250): loss=2.0515238267248597\n",
      "Stochastic Gradient Descent(14251): loss=0.0644036428309777\n",
      "Stochastic Gradient Descent(14252): loss=0.4300950037123739\n",
      "Stochastic Gradient Descent(14253): loss=12.849184423016073\n",
      "Stochastic Gradient Descent(14254): loss=15.940892903305624\n",
      "Stochastic Gradient Descent(14255): loss=0.315114579230875\n",
      "Stochastic Gradient Descent(14256): loss=1.8485940667632725\n",
      "Stochastic Gradient Descent(14257): loss=0.5241246045890354\n",
      "Stochastic Gradient Descent(14258): loss=0.3554378130550744\n",
      "Stochastic Gradient Descent(14259): loss=0.25674569897770605\n",
      "Stochastic Gradient Descent(14260): loss=1.5425626362092721\n",
      "Stochastic Gradient Descent(14261): loss=1.6654276723715427\n",
      "Stochastic Gradient Descent(14262): loss=1.70763648346795\n",
      "Stochastic Gradient Descent(14263): loss=0.5185127798130204\n",
      "Stochastic Gradient Descent(14264): loss=0.5774361300333227\n",
      "Stochastic Gradient Descent(14265): loss=3.513571062931515\n",
      "Stochastic Gradient Descent(14266): loss=2.541700243176106\n",
      "Stochastic Gradient Descent(14267): loss=4.08906922228848\n",
      "Stochastic Gradient Descent(14268): loss=3.319828659646438\n",
      "Stochastic Gradient Descent(14269): loss=3.8495527985222644\n",
      "Stochastic Gradient Descent(14270): loss=0.06655774695924364\n",
      "Stochastic Gradient Descent(14271): loss=0.11900211214753031\n",
      "Stochastic Gradient Descent(14272): loss=0.032825649555112\n",
      "Stochastic Gradient Descent(14273): loss=0.46005210465275415\n",
      "Stochastic Gradient Descent(14274): loss=12.562133765031918\n",
      "Stochastic Gradient Descent(14275): loss=27.513109604749364\n",
      "Stochastic Gradient Descent(14276): loss=0.004014115084187009\n",
      "Stochastic Gradient Descent(14277): loss=0.3196058004983786\n",
      "Stochastic Gradient Descent(14278): loss=1.9114502579280284\n",
      "Stochastic Gradient Descent(14279): loss=2.576725841283724\n",
      "Stochastic Gradient Descent(14280): loss=1.0807178558230972\n",
      "Stochastic Gradient Descent(14281): loss=0.7696517584550034\n",
      "Stochastic Gradient Descent(14282): loss=4.8372370234201805\n",
      "Stochastic Gradient Descent(14283): loss=4.996810736472009\n",
      "Stochastic Gradient Descent(14284): loss=24.327644929302796\n",
      "Stochastic Gradient Descent(14285): loss=19.838968905211665\n",
      "Stochastic Gradient Descent(14286): loss=0.9068170347513241\n",
      "Stochastic Gradient Descent(14287): loss=2.2828272532924707\n",
      "Stochastic Gradient Descent(14288): loss=17.04995210891434\n",
      "Stochastic Gradient Descent(14289): loss=3.1840676742472493\n",
      "Stochastic Gradient Descent(14290): loss=0.0007142777784852272\n",
      "Stochastic Gradient Descent(14291): loss=2.5043437322616238\n",
      "Stochastic Gradient Descent(14292): loss=0.7840300255713608\n",
      "Stochastic Gradient Descent(14293): loss=0.13866354745614823\n",
      "Stochastic Gradient Descent(14294): loss=1.5314777254486218\n",
      "Stochastic Gradient Descent(14295): loss=0.21101720658952847\n",
      "Stochastic Gradient Descent(14296): loss=0.5421740090418818\n",
      "Stochastic Gradient Descent(14297): loss=3.092181479788317\n",
      "Stochastic Gradient Descent(14298): loss=0.30068943077884586\n",
      "Stochastic Gradient Descent(14299): loss=6.655588307376037\n",
      "Stochastic Gradient Descent(14300): loss=0.49996731464004324\n",
      "Stochastic Gradient Descent(14301): loss=0.6142402799870121\n",
      "Stochastic Gradient Descent(14302): loss=8.586319114120412\n",
      "Stochastic Gradient Descent(14303): loss=1.0686201030782025\n",
      "Stochastic Gradient Descent(14304): loss=1.4874893515151684\n",
      "Stochastic Gradient Descent(14305): loss=6.383445912027374\n",
      "Stochastic Gradient Descent(14306): loss=0.0036034860045226678\n",
      "Stochastic Gradient Descent(14307): loss=6.9334069305055985\n",
      "Stochastic Gradient Descent(14308): loss=0.020631803447203334\n",
      "Stochastic Gradient Descent(14309): loss=0.6457944753250048\n",
      "Stochastic Gradient Descent(14310): loss=0.5487245822994568\n",
      "Stochastic Gradient Descent(14311): loss=2.0478544997857866\n",
      "Stochastic Gradient Descent(14312): loss=3.382520231263105\n",
      "Stochastic Gradient Descent(14313): loss=0.27897587365071264\n",
      "Stochastic Gradient Descent(14314): loss=10.400927909075019\n",
      "Stochastic Gradient Descent(14315): loss=0.002548358816254424\n",
      "Stochastic Gradient Descent(14316): loss=8.615221446468732\n",
      "Stochastic Gradient Descent(14317): loss=1.5557678426376942\n",
      "Stochastic Gradient Descent(14318): loss=3.2058275172292734\n",
      "Stochastic Gradient Descent(14319): loss=0.14689140181641278\n",
      "Stochastic Gradient Descent(14320): loss=15.512703337756594\n",
      "Stochastic Gradient Descent(14321): loss=0.1811727755096216\n",
      "Stochastic Gradient Descent(14322): loss=4.1056377610464185\n",
      "Stochastic Gradient Descent(14323): loss=1.4857337147238985\n",
      "Stochastic Gradient Descent(14324): loss=0.16547946013956663\n",
      "Stochastic Gradient Descent(14325): loss=7.5863094821328545\n",
      "Stochastic Gradient Descent(14326): loss=11.690857551293412\n",
      "Stochastic Gradient Descent(14327): loss=1.9297217318494766\n",
      "Stochastic Gradient Descent(14328): loss=2.903614478320404\n",
      "Stochastic Gradient Descent(14329): loss=0.5746647183787358\n",
      "Stochastic Gradient Descent(14330): loss=0.7048721081847225\n",
      "Stochastic Gradient Descent(14331): loss=0.7342334308037517\n",
      "Stochastic Gradient Descent(14332): loss=0.5424881374514596\n",
      "Stochastic Gradient Descent(14333): loss=3.2088255632028306\n",
      "Stochastic Gradient Descent(14334): loss=4.860241437635437\n",
      "Stochastic Gradient Descent(14335): loss=5.558703391489101\n",
      "Stochastic Gradient Descent(14336): loss=1.9538690412566584\n",
      "Stochastic Gradient Descent(14337): loss=4.188050736555775\n",
      "Stochastic Gradient Descent(14338): loss=0.09964443847412886\n",
      "Stochastic Gradient Descent(14339): loss=1.5927936380750163\n",
      "Stochastic Gradient Descent(14340): loss=20.57820315580479\n",
      "Stochastic Gradient Descent(14341): loss=5.346910904629976\n",
      "Stochastic Gradient Descent(14342): loss=17.060314648512936\n",
      "Stochastic Gradient Descent(14343): loss=0.06673809443745582\n",
      "Stochastic Gradient Descent(14344): loss=0.8563097611505961\n",
      "Stochastic Gradient Descent(14345): loss=7.646634964770401\n",
      "Stochastic Gradient Descent(14346): loss=0.29735320185587555\n",
      "Stochastic Gradient Descent(14347): loss=0.4666787883862735\n",
      "Stochastic Gradient Descent(14348): loss=3.975588358059449\n",
      "Stochastic Gradient Descent(14349): loss=1.4414237045802614\n",
      "Stochastic Gradient Descent(14350): loss=0.10201978109632737\n",
      "Stochastic Gradient Descent(14351): loss=12.252659395839931\n",
      "Stochastic Gradient Descent(14352): loss=5.243760419390784\n",
      "Stochastic Gradient Descent(14353): loss=0.7436527056447263\n",
      "Stochastic Gradient Descent(14354): loss=4.012823714422526\n",
      "Stochastic Gradient Descent(14355): loss=3.060759971601975\n",
      "Stochastic Gradient Descent(14356): loss=6.174745567748966\n",
      "Stochastic Gradient Descent(14357): loss=0.42835860015596894\n",
      "Stochastic Gradient Descent(14358): loss=0.4804542339395401\n",
      "Stochastic Gradient Descent(14359): loss=21.76529978251102\n",
      "Stochastic Gradient Descent(14360): loss=39.19342406942702\n",
      "Stochastic Gradient Descent(14361): loss=127.2644808404651\n",
      "Stochastic Gradient Descent(14362): loss=64.45539831214602\n",
      "Stochastic Gradient Descent(14363): loss=1.3225969071413703\n",
      "Stochastic Gradient Descent(14364): loss=1.0100176603405817\n",
      "Stochastic Gradient Descent(14365): loss=5.5082430542882745\n",
      "Stochastic Gradient Descent(14366): loss=3.303743177206074\n",
      "Stochastic Gradient Descent(14367): loss=3.9471456127533986\n",
      "Stochastic Gradient Descent(14368): loss=0.16964595939916183\n",
      "Stochastic Gradient Descent(14369): loss=5.440999947209464\n",
      "Stochastic Gradient Descent(14370): loss=0.7031536411242791\n",
      "Stochastic Gradient Descent(14371): loss=0.6264340257929527\n",
      "Stochastic Gradient Descent(14372): loss=11.992456740640987\n",
      "Stochastic Gradient Descent(14373): loss=6.82117954880093\n",
      "Stochastic Gradient Descent(14374): loss=6.816734539799066\n",
      "Stochastic Gradient Descent(14375): loss=7.6239061115549145\n",
      "Stochastic Gradient Descent(14376): loss=1.4480734360395997\n",
      "Stochastic Gradient Descent(14377): loss=0.17118370807507718\n",
      "Stochastic Gradient Descent(14378): loss=0.2674299896510694\n",
      "Stochastic Gradient Descent(14379): loss=0.7338044435883662\n",
      "Stochastic Gradient Descent(14380): loss=5.255364528654259\n",
      "Stochastic Gradient Descent(14381): loss=2.3930570398106887\n",
      "Stochastic Gradient Descent(14382): loss=6.265968856157511\n",
      "Stochastic Gradient Descent(14383): loss=8.739919685179565\n",
      "Stochastic Gradient Descent(14384): loss=0.2664703543439664\n",
      "Stochastic Gradient Descent(14385): loss=2.1132443305489854\n",
      "Stochastic Gradient Descent(14386): loss=0.07167489731162298\n",
      "Stochastic Gradient Descent(14387): loss=0.3239966130366992\n",
      "Stochastic Gradient Descent(14388): loss=10.302871746178836\n",
      "Stochastic Gradient Descent(14389): loss=2.364345594388486\n",
      "Stochastic Gradient Descent(14390): loss=0.4406419641345453\n",
      "Stochastic Gradient Descent(14391): loss=1.113931415362694\n",
      "Stochastic Gradient Descent(14392): loss=0.9791062500721011\n",
      "Stochastic Gradient Descent(14393): loss=2.9361262299906015\n",
      "Stochastic Gradient Descent(14394): loss=15.390466572321353\n",
      "Stochastic Gradient Descent(14395): loss=14.385297677047461\n",
      "Stochastic Gradient Descent(14396): loss=0.02594284111285936\n",
      "Stochastic Gradient Descent(14397): loss=0.044144141991819624\n",
      "Stochastic Gradient Descent(14398): loss=9.78019194102617\n",
      "Stochastic Gradient Descent(14399): loss=1.630407708379129\n",
      "Stochastic Gradient Descent(14400): loss=0.03285517962769122\n",
      "Stochastic Gradient Descent(14401): loss=0.03439062457633476\n",
      "Stochastic Gradient Descent(14402): loss=0.34699349038708505\n",
      "Stochastic Gradient Descent(14403): loss=10.157190649784104\n",
      "Stochastic Gradient Descent(14404): loss=2.883091234332777\n",
      "Stochastic Gradient Descent(14405): loss=2.3585131548103697\n",
      "Stochastic Gradient Descent(14406): loss=17.355447903169562\n",
      "Stochastic Gradient Descent(14407): loss=3.8831708659285415\n",
      "Stochastic Gradient Descent(14408): loss=4.973657562174194\n",
      "Stochastic Gradient Descent(14409): loss=0.29511496458692044\n",
      "Stochastic Gradient Descent(14410): loss=7.742684250744502\n",
      "Stochastic Gradient Descent(14411): loss=2.5050212971584616\n",
      "Stochastic Gradient Descent(14412): loss=20.38586054339239\n",
      "Stochastic Gradient Descent(14413): loss=6.337337109723103\n",
      "Stochastic Gradient Descent(14414): loss=8.93825479917\n",
      "Stochastic Gradient Descent(14415): loss=0.4868425497543054\n",
      "Stochastic Gradient Descent(14416): loss=21.147152663153243\n",
      "Stochastic Gradient Descent(14417): loss=15.329116505585642\n",
      "Stochastic Gradient Descent(14418): loss=13.061790746523327\n",
      "Stochastic Gradient Descent(14419): loss=11.564272065416137\n",
      "Stochastic Gradient Descent(14420): loss=2.0432952663064348\n",
      "Stochastic Gradient Descent(14421): loss=6.254949103246066\n",
      "Stochastic Gradient Descent(14422): loss=4.793263246569823\n",
      "Stochastic Gradient Descent(14423): loss=7.270444363721711\n",
      "Stochastic Gradient Descent(14424): loss=16.13293545931162\n",
      "Stochastic Gradient Descent(14425): loss=1.3720126793713652\n",
      "Stochastic Gradient Descent(14426): loss=6.4530745358814485\n",
      "Stochastic Gradient Descent(14427): loss=8.798776812359367\n",
      "Stochastic Gradient Descent(14428): loss=8.791591116043449\n",
      "Stochastic Gradient Descent(14429): loss=1.343677514181792\n",
      "Stochastic Gradient Descent(14430): loss=1.5027442603806236\n",
      "Stochastic Gradient Descent(14431): loss=5.801752696256682\n",
      "Stochastic Gradient Descent(14432): loss=0.885070375459692\n",
      "Stochastic Gradient Descent(14433): loss=1.0322416644756398\n",
      "Stochastic Gradient Descent(14434): loss=9.249703325523324\n",
      "Stochastic Gradient Descent(14435): loss=1.8342611790595837\n",
      "Stochastic Gradient Descent(14436): loss=0.4030478348580422\n",
      "Stochastic Gradient Descent(14437): loss=7.013002578024809\n",
      "Stochastic Gradient Descent(14438): loss=2.2633415886215165\n",
      "Stochastic Gradient Descent(14439): loss=27.61645172979039\n",
      "Stochastic Gradient Descent(14440): loss=5.058037647673529\n",
      "Stochastic Gradient Descent(14441): loss=0.00015360205106768626\n",
      "Stochastic Gradient Descent(14442): loss=0.025488627974056977\n",
      "Stochastic Gradient Descent(14443): loss=0.2219442972401273\n",
      "Stochastic Gradient Descent(14444): loss=2.25556727834208\n",
      "Stochastic Gradient Descent(14445): loss=0.18498990880392374\n",
      "Stochastic Gradient Descent(14446): loss=15.180001825587569\n",
      "Stochastic Gradient Descent(14447): loss=1.1990658757068344\n",
      "Stochastic Gradient Descent(14448): loss=0.629737616424594\n",
      "Stochastic Gradient Descent(14449): loss=0.0004323671944960612\n",
      "Stochastic Gradient Descent(14450): loss=9.698905009433037\n",
      "Stochastic Gradient Descent(14451): loss=0.03179791193215862\n",
      "Stochastic Gradient Descent(14452): loss=18.590805799368432\n",
      "Stochastic Gradient Descent(14453): loss=1.6997960729435795\n",
      "Stochastic Gradient Descent(14454): loss=3.0808060692433314\n",
      "Stochastic Gradient Descent(14455): loss=0.07519527184903671\n",
      "Stochastic Gradient Descent(14456): loss=0.0006232249198198443\n",
      "Stochastic Gradient Descent(14457): loss=2.5415132305094263\n",
      "Stochastic Gradient Descent(14458): loss=2.0478970630305082\n",
      "Stochastic Gradient Descent(14459): loss=6.534609155131203\n",
      "Stochastic Gradient Descent(14460): loss=0.012750975053801664\n",
      "Stochastic Gradient Descent(14461): loss=4.256162909810399\n",
      "Stochastic Gradient Descent(14462): loss=4.831048320429937\n",
      "Stochastic Gradient Descent(14463): loss=0.7046877216615856\n",
      "Stochastic Gradient Descent(14464): loss=3.0050266214019348\n",
      "Stochastic Gradient Descent(14465): loss=0.9326950983398515\n",
      "Stochastic Gradient Descent(14466): loss=0.06503137016406223\n",
      "Stochastic Gradient Descent(14467): loss=0.004594638639362031\n",
      "Stochastic Gradient Descent(14468): loss=0.07756122343917479\n",
      "Stochastic Gradient Descent(14469): loss=0.1296159202569838\n",
      "Stochastic Gradient Descent(14470): loss=6.410466700570878\n",
      "Stochastic Gradient Descent(14471): loss=0.7600577306066043\n",
      "Stochastic Gradient Descent(14472): loss=1.3811088798924105\n",
      "Stochastic Gradient Descent(14473): loss=3.3077840487690278\n",
      "Stochastic Gradient Descent(14474): loss=8.894137110726849\n",
      "Stochastic Gradient Descent(14475): loss=2.5003522095150132\n",
      "Stochastic Gradient Descent(14476): loss=3.3811822616132763\n",
      "Stochastic Gradient Descent(14477): loss=0.3255740891501052\n",
      "Stochastic Gradient Descent(14478): loss=0.38726136618319595\n",
      "Stochastic Gradient Descent(14479): loss=0.023494320202902638\n",
      "Stochastic Gradient Descent(14480): loss=21.839294264691247\n",
      "Stochastic Gradient Descent(14481): loss=13.046596022177015\n",
      "Stochastic Gradient Descent(14482): loss=8.8009053001888\n",
      "Stochastic Gradient Descent(14483): loss=6.703307706685745\n",
      "Stochastic Gradient Descent(14484): loss=25.07910533826912\n",
      "Stochastic Gradient Descent(14485): loss=4.804452604447898\n",
      "Stochastic Gradient Descent(14486): loss=2.8614825413686478\n",
      "Stochastic Gradient Descent(14487): loss=1.4134621201788555\n",
      "Stochastic Gradient Descent(14488): loss=0.1327191790350031\n",
      "Stochastic Gradient Descent(14489): loss=1.0950581939483657\n",
      "Stochastic Gradient Descent(14490): loss=1.3678371369004712\n",
      "Stochastic Gradient Descent(14491): loss=2.368733925168392\n",
      "Stochastic Gradient Descent(14492): loss=10.021765929511472\n",
      "Stochastic Gradient Descent(14493): loss=0.8108922280008592\n",
      "Stochastic Gradient Descent(14494): loss=1.4912141689284943\n",
      "Stochastic Gradient Descent(14495): loss=27.555616870972173\n",
      "Stochastic Gradient Descent(14496): loss=29.459029352541858\n",
      "Stochastic Gradient Descent(14497): loss=0.10829013184023124\n",
      "Stochastic Gradient Descent(14498): loss=35.59946929589339\n",
      "Stochastic Gradient Descent(14499): loss=9.576801605568113\n",
      "Stochastic Gradient Descent(14500): loss=1.429440348639602\n",
      "Stochastic Gradient Descent(14501): loss=0.7740469220087961\n",
      "Stochastic Gradient Descent(14502): loss=0.9533943182041577\n",
      "Stochastic Gradient Descent(14503): loss=14.660559071133905\n",
      "Stochastic Gradient Descent(14504): loss=0.06565097311988646\n",
      "Stochastic Gradient Descent(14505): loss=0.9211954111493673\n",
      "Stochastic Gradient Descent(14506): loss=0.36936357790628976\n",
      "Stochastic Gradient Descent(14507): loss=0.027322459925215516\n",
      "Stochastic Gradient Descent(14508): loss=5.08979777400256\n",
      "Stochastic Gradient Descent(14509): loss=3.276445976941549\n",
      "Stochastic Gradient Descent(14510): loss=0.06171417134047114\n",
      "Stochastic Gradient Descent(14511): loss=0.07194184629406893\n",
      "Stochastic Gradient Descent(14512): loss=6.482482671744328\n",
      "Stochastic Gradient Descent(14513): loss=2.751287816178622\n",
      "Stochastic Gradient Descent(14514): loss=0.5416428056231514\n",
      "Stochastic Gradient Descent(14515): loss=0.0505925744847933\n",
      "Stochastic Gradient Descent(14516): loss=11.969422953143843\n",
      "Stochastic Gradient Descent(14517): loss=6.4109249099920405\n",
      "Stochastic Gradient Descent(14518): loss=8.21907374796906\n",
      "Stochastic Gradient Descent(14519): loss=12.648553156580165\n",
      "Stochastic Gradient Descent(14520): loss=5.76771075051227\n",
      "Stochastic Gradient Descent(14521): loss=3.594981571834484\n",
      "Stochastic Gradient Descent(14522): loss=0.7388209412520718\n",
      "Stochastic Gradient Descent(14523): loss=6.192579916911017\n",
      "Stochastic Gradient Descent(14524): loss=0.3431735228155199\n",
      "Stochastic Gradient Descent(14525): loss=0.05747965091968882\n",
      "Stochastic Gradient Descent(14526): loss=1.3185085826066494\n",
      "Stochastic Gradient Descent(14527): loss=10.794888418399422\n",
      "Stochastic Gradient Descent(14528): loss=28.949069459079183\n",
      "Stochastic Gradient Descent(14529): loss=139.83580046237716\n",
      "Stochastic Gradient Descent(14530): loss=0.011200792988089873\n",
      "Stochastic Gradient Descent(14531): loss=0.17729777557955942\n",
      "Stochastic Gradient Descent(14532): loss=1.5954920171319464\n",
      "Stochastic Gradient Descent(14533): loss=0.8815751583572625\n",
      "Stochastic Gradient Descent(14534): loss=3.7567409465853943\n",
      "Stochastic Gradient Descent(14535): loss=34.53265453031216\n",
      "Stochastic Gradient Descent(14536): loss=9.046860765233925\n",
      "Stochastic Gradient Descent(14537): loss=8.828231563917425\n",
      "Stochastic Gradient Descent(14538): loss=0.4738925091066106\n",
      "Stochastic Gradient Descent(14539): loss=0.14690835102204974\n",
      "Stochastic Gradient Descent(14540): loss=0.004811779686732966\n",
      "Stochastic Gradient Descent(14541): loss=2.277052461647186\n",
      "Stochastic Gradient Descent(14542): loss=52.857672132890485\n",
      "Stochastic Gradient Descent(14543): loss=0.09664742079523043\n",
      "Stochastic Gradient Descent(14544): loss=19.502738186643743\n",
      "Stochastic Gradient Descent(14545): loss=12.849003520884782\n",
      "Stochastic Gradient Descent(14546): loss=7.840010748163416\n",
      "Stochastic Gradient Descent(14547): loss=0.24222154729694398\n",
      "Stochastic Gradient Descent(14548): loss=14.759145704143183\n",
      "Stochastic Gradient Descent(14549): loss=33.93187525835531\n",
      "Stochastic Gradient Descent(14550): loss=6.832126600764039\n",
      "Stochastic Gradient Descent(14551): loss=1.8960308322714667\n",
      "Stochastic Gradient Descent(14552): loss=1.0987106186379063\n",
      "Stochastic Gradient Descent(14553): loss=0.7828933751110462\n",
      "Stochastic Gradient Descent(14554): loss=0.10251284441468532\n",
      "Stochastic Gradient Descent(14555): loss=14.653471228332641\n",
      "Stochastic Gradient Descent(14556): loss=7.755815838530703e-05\n",
      "Stochastic Gradient Descent(14557): loss=13.119933237328464\n",
      "Stochastic Gradient Descent(14558): loss=13.980301790692074\n",
      "Stochastic Gradient Descent(14559): loss=16.74484017030037\n",
      "Stochastic Gradient Descent(14560): loss=0.1817287961145929\n",
      "Stochastic Gradient Descent(14561): loss=3.202816072792499\n",
      "Stochastic Gradient Descent(14562): loss=19.5339106518565\n",
      "Stochastic Gradient Descent(14563): loss=9.096698035261026\n",
      "Stochastic Gradient Descent(14564): loss=0.031743953988682856\n",
      "Stochastic Gradient Descent(14565): loss=0.944865788329286\n",
      "Stochastic Gradient Descent(14566): loss=0.0012359014476022606\n",
      "Stochastic Gradient Descent(14567): loss=0.17542291903835996\n",
      "Stochastic Gradient Descent(14568): loss=1.5338157439096942\n",
      "Stochastic Gradient Descent(14569): loss=12.60855746221939\n",
      "Stochastic Gradient Descent(14570): loss=5.995248302893054\n",
      "Stochastic Gradient Descent(14571): loss=2.0771024588425187\n",
      "Stochastic Gradient Descent(14572): loss=3.119883199024977\n",
      "Stochastic Gradient Descent(14573): loss=5.886932520224417\n",
      "Stochastic Gradient Descent(14574): loss=0.14555041762139614\n",
      "Stochastic Gradient Descent(14575): loss=6.388960138809902\n",
      "Stochastic Gradient Descent(14576): loss=4.1239478698916106\n",
      "Stochastic Gradient Descent(14577): loss=0.3274028082857737\n",
      "Stochastic Gradient Descent(14578): loss=13.332638528978853\n",
      "Stochastic Gradient Descent(14579): loss=0.3236265626269943\n",
      "Stochastic Gradient Descent(14580): loss=1.741462445880188\n",
      "Stochastic Gradient Descent(14581): loss=7.0229432522178765\n",
      "Stochastic Gradient Descent(14582): loss=20.621001706264412\n",
      "Stochastic Gradient Descent(14583): loss=1.6116373938345443\n",
      "Stochastic Gradient Descent(14584): loss=4.7505077224131185\n",
      "Stochastic Gradient Descent(14585): loss=1.2014172068737614\n",
      "Stochastic Gradient Descent(14586): loss=5.784841554655629\n",
      "Stochastic Gradient Descent(14587): loss=14.460090089756184\n",
      "Stochastic Gradient Descent(14588): loss=2.74297178201853\n",
      "Stochastic Gradient Descent(14589): loss=0.01349945284850023\n",
      "Stochastic Gradient Descent(14590): loss=24.711616397748614\n",
      "Stochastic Gradient Descent(14591): loss=1.076600153113882\n",
      "Stochastic Gradient Descent(14592): loss=0.6665001848201795\n",
      "Stochastic Gradient Descent(14593): loss=8.358739619692674\n",
      "Stochastic Gradient Descent(14594): loss=0.05869338095366485\n",
      "Stochastic Gradient Descent(14595): loss=1.4378725083317245\n",
      "Stochastic Gradient Descent(14596): loss=0.03369563298429537\n",
      "Stochastic Gradient Descent(14597): loss=14.93424422012964\n",
      "Stochastic Gradient Descent(14598): loss=0.03740921187423128\n",
      "Stochastic Gradient Descent(14599): loss=2.1124981918929797\n",
      "Stochastic Gradient Descent(14600): loss=3.620145271556517\n",
      "Stochastic Gradient Descent(14601): loss=2.578447453598678\n",
      "Stochastic Gradient Descent(14602): loss=4.3385889979030345\n",
      "Stochastic Gradient Descent(14603): loss=1.8013020460452747\n",
      "Stochastic Gradient Descent(14604): loss=16.214213338084093\n",
      "Stochastic Gradient Descent(14605): loss=0.15415421529122156\n",
      "Stochastic Gradient Descent(14606): loss=1.8541516941360552\n",
      "Stochastic Gradient Descent(14607): loss=0.5043241397031416\n",
      "Stochastic Gradient Descent(14608): loss=0.20366116647476726\n",
      "Stochastic Gradient Descent(14609): loss=6.707181261246369\n",
      "Stochastic Gradient Descent(14610): loss=1.880528054493085\n",
      "Stochastic Gradient Descent(14611): loss=0.009291710160712677\n",
      "Stochastic Gradient Descent(14612): loss=5.5058488963310035\n",
      "Stochastic Gradient Descent(14613): loss=0.018969675229406638\n",
      "Stochastic Gradient Descent(14614): loss=1.2747883831071791\n",
      "Stochastic Gradient Descent(14615): loss=4.1548493810973435\n",
      "Stochastic Gradient Descent(14616): loss=9.400773874929847\n",
      "Stochastic Gradient Descent(14617): loss=2.8889576893305975\n",
      "Stochastic Gradient Descent(14618): loss=0.0049350523271146285\n",
      "Stochastic Gradient Descent(14619): loss=3.8466982251897086\n",
      "Stochastic Gradient Descent(14620): loss=13.89504396906644\n",
      "Stochastic Gradient Descent(14621): loss=4.224712110296278\n",
      "Stochastic Gradient Descent(14622): loss=0.014451829590258324\n",
      "Stochastic Gradient Descent(14623): loss=16.760261212702073\n",
      "Stochastic Gradient Descent(14624): loss=5.721675550555113\n",
      "Stochastic Gradient Descent(14625): loss=0.5581871694253857\n",
      "Stochastic Gradient Descent(14626): loss=13.909943779466266\n",
      "Stochastic Gradient Descent(14627): loss=0.46861871013720674\n",
      "Stochastic Gradient Descent(14628): loss=0.0067089374276102665\n",
      "Stochastic Gradient Descent(14629): loss=35.23938953557658\n",
      "Stochastic Gradient Descent(14630): loss=5.566742303476285\n",
      "Stochastic Gradient Descent(14631): loss=2.357045034807535\n",
      "Stochastic Gradient Descent(14632): loss=23.495980135343178\n",
      "Stochastic Gradient Descent(14633): loss=4.403841524998907\n",
      "Stochastic Gradient Descent(14634): loss=8.022106015844916\n",
      "Stochastic Gradient Descent(14635): loss=3.5001063312015663\n",
      "Stochastic Gradient Descent(14636): loss=15.717560275843946\n",
      "Stochastic Gradient Descent(14637): loss=3.7235871324666947\n",
      "Stochastic Gradient Descent(14638): loss=12.206012275300925\n",
      "Stochastic Gradient Descent(14639): loss=5.664169122224601\n",
      "Stochastic Gradient Descent(14640): loss=2.6214685474476394\n",
      "Stochastic Gradient Descent(14641): loss=5.31401719233603\n",
      "Stochastic Gradient Descent(14642): loss=7.282473155571419\n",
      "Stochastic Gradient Descent(14643): loss=6.035251341949855\n",
      "Stochastic Gradient Descent(14644): loss=4.3077107454932\n",
      "Stochastic Gradient Descent(14645): loss=9.128548003484457\n",
      "Stochastic Gradient Descent(14646): loss=0.031799656346968214\n",
      "Stochastic Gradient Descent(14647): loss=0.642298551893353\n",
      "Stochastic Gradient Descent(14648): loss=11.433908566039937\n",
      "Stochastic Gradient Descent(14649): loss=0.15745229035110472\n",
      "Stochastic Gradient Descent(14650): loss=4.360486411057534\n",
      "Stochastic Gradient Descent(14651): loss=1.8787554577505723\n",
      "Stochastic Gradient Descent(14652): loss=3.1311298009804496\n",
      "Stochastic Gradient Descent(14653): loss=0.6476818461503059\n",
      "Stochastic Gradient Descent(14654): loss=13.091412914331254\n",
      "Stochastic Gradient Descent(14655): loss=0.04727227245859717\n",
      "Stochastic Gradient Descent(14656): loss=8.36182296396996\n",
      "Stochastic Gradient Descent(14657): loss=8.631264730037314\n",
      "Stochastic Gradient Descent(14658): loss=12.548160789397977\n",
      "Stochastic Gradient Descent(14659): loss=0.00802431955141799\n",
      "Stochastic Gradient Descent(14660): loss=1.6553358281189794\n",
      "Stochastic Gradient Descent(14661): loss=0.06960363944819109\n",
      "Stochastic Gradient Descent(14662): loss=8.843484241139365\n",
      "Stochastic Gradient Descent(14663): loss=0.04488686064100403\n",
      "Stochastic Gradient Descent(14664): loss=0.2661670200511209\n",
      "Stochastic Gradient Descent(14665): loss=0.026071318978935193\n",
      "Stochastic Gradient Descent(14666): loss=1.1939153636912376\n",
      "Stochastic Gradient Descent(14667): loss=2.0272896120329587\n",
      "Stochastic Gradient Descent(14668): loss=4.441700597360686\n",
      "Stochastic Gradient Descent(14669): loss=6.369876376579095\n",
      "Stochastic Gradient Descent(14670): loss=1.3493423883137592\n",
      "Stochastic Gradient Descent(14671): loss=0.41270698292717584\n",
      "Stochastic Gradient Descent(14672): loss=18.728489749584035\n",
      "Stochastic Gradient Descent(14673): loss=0.018657563650651434\n",
      "Stochastic Gradient Descent(14674): loss=0.17897034908672801\n",
      "Stochastic Gradient Descent(14675): loss=10.587706843859504\n",
      "Stochastic Gradient Descent(14676): loss=2.026641167603101\n",
      "Stochastic Gradient Descent(14677): loss=1.2793206352072908\n",
      "Stochastic Gradient Descent(14678): loss=0.2545322904771191\n",
      "Stochastic Gradient Descent(14679): loss=0.9960805629818585\n",
      "Stochastic Gradient Descent(14680): loss=2.9387607368672897\n",
      "Stochastic Gradient Descent(14681): loss=2.6948853048305286\n",
      "Stochastic Gradient Descent(14682): loss=42.019660312519555\n",
      "Stochastic Gradient Descent(14683): loss=17.59697335366723\n",
      "Stochastic Gradient Descent(14684): loss=9.841987864144807\n",
      "Stochastic Gradient Descent(14685): loss=0.4448242915941217\n",
      "Stochastic Gradient Descent(14686): loss=0.9147551463279768\n",
      "Stochastic Gradient Descent(14687): loss=4.174150657295721\n",
      "Stochastic Gradient Descent(14688): loss=6.713866490379412\n",
      "Stochastic Gradient Descent(14689): loss=11.024235923572938\n",
      "Stochastic Gradient Descent(14690): loss=0.22649579842571174\n",
      "Stochastic Gradient Descent(14691): loss=3.8553689450583284\n",
      "Stochastic Gradient Descent(14692): loss=0.4401740705687107\n",
      "Stochastic Gradient Descent(14693): loss=2.5858880200439436\n",
      "Stochastic Gradient Descent(14694): loss=2.7035760649857994\n",
      "Stochastic Gradient Descent(14695): loss=1.1986891667511206\n",
      "Stochastic Gradient Descent(14696): loss=1.0063781648636987\n",
      "Stochastic Gradient Descent(14697): loss=3.1371141070560595\n",
      "Stochastic Gradient Descent(14698): loss=0.6982270200366372\n",
      "Stochastic Gradient Descent(14699): loss=0.06582321425626082\n",
      "Stochastic Gradient Descent(14700): loss=18.55520383812466\n",
      "Stochastic Gradient Descent(14701): loss=2.1521117120532463\n",
      "Stochastic Gradient Descent(14702): loss=2.0125624408123666\n",
      "Stochastic Gradient Descent(14703): loss=13.12350208757084\n",
      "Stochastic Gradient Descent(14704): loss=0.6309139450158214\n",
      "Stochastic Gradient Descent(14705): loss=0.04300925334526999\n",
      "Stochastic Gradient Descent(14706): loss=6.139293178140877\n",
      "Stochastic Gradient Descent(14707): loss=1.413599667939855\n",
      "Stochastic Gradient Descent(14708): loss=0.07874761390087986\n",
      "Stochastic Gradient Descent(14709): loss=4.2447260953000585\n",
      "Stochastic Gradient Descent(14710): loss=0.0050685263745165994\n",
      "Stochastic Gradient Descent(14711): loss=23.331450872049018\n",
      "Stochastic Gradient Descent(14712): loss=0.21487135948448713\n",
      "Stochastic Gradient Descent(14713): loss=0.1043791074642933\n",
      "Stochastic Gradient Descent(14714): loss=2.9816580699700426\n",
      "Stochastic Gradient Descent(14715): loss=7.843082449418727\n",
      "Stochastic Gradient Descent(14716): loss=3.4357868050546383\n",
      "Stochastic Gradient Descent(14717): loss=22.135927117072193\n",
      "Stochastic Gradient Descent(14718): loss=7.174380782249503\n",
      "Stochastic Gradient Descent(14719): loss=0.008999337887412725\n",
      "Stochastic Gradient Descent(14720): loss=1.1377564151699897\n",
      "Stochastic Gradient Descent(14721): loss=8.446621687629609\n",
      "Stochastic Gradient Descent(14722): loss=0.33352538074175725\n",
      "Stochastic Gradient Descent(14723): loss=1.6427468136401857\n",
      "Stochastic Gradient Descent(14724): loss=0.8549590570364881\n",
      "Stochastic Gradient Descent(14725): loss=1.1659944374021758\n",
      "Stochastic Gradient Descent(14726): loss=2.788948953234844\n",
      "Stochastic Gradient Descent(14727): loss=8.30498832043628\n",
      "Stochastic Gradient Descent(14728): loss=7.36075201816603\n",
      "Stochastic Gradient Descent(14729): loss=0.013396931882689757\n",
      "Stochastic Gradient Descent(14730): loss=0.04692117509103357\n",
      "Stochastic Gradient Descent(14731): loss=0.8468302552432172\n",
      "Stochastic Gradient Descent(14732): loss=2.902497261417379\n",
      "Stochastic Gradient Descent(14733): loss=6.774929119880003\n",
      "Stochastic Gradient Descent(14734): loss=5.031921175612464\n",
      "Stochastic Gradient Descent(14735): loss=2.5959810331515056\n",
      "Stochastic Gradient Descent(14736): loss=4.293691278559113\n",
      "Stochastic Gradient Descent(14737): loss=2.8367427122148388\n",
      "Stochastic Gradient Descent(14738): loss=22.713468717975232\n",
      "Stochastic Gradient Descent(14739): loss=9.24847160770416\n",
      "Stochastic Gradient Descent(14740): loss=0.9077122296780513\n",
      "Stochastic Gradient Descent(14741): loss=13.332499235156961\n",
      "Stochastic Gradient Descent(14742): loss=0.49616016610817937\n",
      "Stochastic Gradient Descent(14743): loss=7.648639828373983\n",
      "Stochastic Gradient Descent(14744): loss=4.193884376066685\n",
      "Stochastic Gradient Descent(14745): loss=0.8885641797866929\n",
      "Stochastic Gradient Descent(14746): loss=152.9446198517017\n",
      "Stochastic Gradient Descent(14747): loss=1.8077211517873208\n",
      "Stochastic Gradient Descent(14748): loss=48.546616749032665\n",
      "Stochastic Gradient Descent(14749): loss=11.80027792704344\n",
      "Stochastic Gradient Descent(14750): loss=19.970919225240326\n",
      "Stochastic Gradient Descent(14751): loss=54.10596641728063\n",
      "Stochastic Gradient Descent(14752): loss=298.0253411329395\n",
      "Stochastic Gradient Descent(14753): loss=2.907646153567382\n",
      "Stochastic Gradient Descent(14754): loss=0.026157765214987305\n",
      "Stochastic Gradient Descent(14755): loss=0.015197133497467472\n",
      "Stochastic Gradient Descent(14756): loss=1.0462662928065098\n",
      "Stochastic Gradient Descent(14757): loss=6.333435068330144\n",
      "Stochastic Gradient Descent(14758): loss=0.3642008930721626\n",
      "Stochastic Gradient Descent(14759): loss=0.7159669125668752\n",
      "Stochastic Gradient Descent(14760): loss=0.20003569313189073\n",
      "Stochastic Gradient Descent(14761): loss=1.9335894624309518\n",
      "Stochastic Gradient Descent(14762): loss=0.6868006685977893\n",
      "Stochastic Gradient Descent(14763): loss=15.886547414642056\n",
      "Stochastic Gradient Descent(14764): loss=0.02092492448082722\n",
      "Stochastic Gradient Descent(14765): loss=4.175648258301331\n",
      "Stochastic Gradient Descent(14766): loss=1.8799379514523522\n",
      "Stochastic Gradient Descent(14767): loss=0.5980265039451096\n",
      "Stochastic Gradient Descent(14768): loss=3.002043797340559\n",
      "Stochastic Gradient Descent(14769): loss=2.7184088532230497\n",
      "Stochastic Gradient Descent(14770): loss=0.10341998023688992\n",
      "Stochastic Gradient Descent(14771): loss=2.803334696979883\n",
      "Stochastic Gradient Descent(14772): loss=2.075069222620722\n",
      "Stochastic Gradient Descent(14773): loss=0.2361836227548233\n",
      "Stochastic Gradient Descent(14774): loss=0.7221308244590962\n",
      "Stochastic Gradient Descent(14775): loss=1.5898509974942199\n",
      "Stochastic Gradient Descent(14776): loss=9.024692838144926\n",
      "Stochastic Gradient Descent(14777): loss=1.640033611208894\n",
      "Stochastic Gradient Descent(14778): loss=1.8027133186109512\n",
      "Stochastic Gradient Descent(14779): loss=1.1356950142830864\n",
      "Stochastic Gradient Descent(14780): loss=0.5597466785423622\n",
      "Stochastic Gradient Descent(14781): loss=5.602615448092654\n",
      "Stochastic Gradient Descent(14782): loss=5.952458606610907\n",
      "Stochastic Gradient Descent(14783): loss=0.7718654536055868\n",
      "Stochastic Gradient Descent(14784): loss=1.0938310021561368\n",
      "Stochastic Gradient Descent(14785): loss=5.1308073851503115\n",
      "Stochastic Gradient Descent(14786): loss=2.212291930594753\n",
      "Stochastic Gradient Descent(14787): loss=10.635184267142693\n",
      "Stochastic Gradient Descent(14788): loss=0.0487406033877895\n",
      "Stochastic Gradient Descent(14789): loss=0.47187683664119745\n",
      "Stochastic Gradient Descent(14790): loss=53.711034011288774\n",
      "Stochastic Gradient Descent(14791): loss=16.043555992885448\n",
      "Stochastic Gradient Descent(14792): loss=2.350440194600923\n",
      "Stochastic Gradient Descent(14793): loss=31.7483763935023\n",
      "Stochastic Gradient Descent(14794): loss=34.18687756784311\n",
      "Stochastic Gradient Descent(14795): loss=2.5637169030075646\n",
      "Stochastic Gradient Descent(14796): loss=45.30014125073243\n",
      "Stochastic Gradient Descent(14797): loss=15.20367263801778\n",
      "Stochastic Gradient Descent(14798): loss=11.055942546254661\n",
      "Stochastic Gradient Descent(14799): loss=14.375967556896605\n",
      "Stochastic Gradient Descent(14800): loss=0.005993807000641841\n",
      "Stochastic Gradient Descent(14801): loss=0.5984058859562026\n",
      "Stochastic Gradient Descent(14802): loss=1.725024145762886\n",
      "Stochastic Gradient Descent(14803): loss=0.18703808239421246\n",
      "Stochastic Gradient Descent(14804): loss=8.036089345649295\n",
      "Stochastic Gradient Descent(14805): loss=15.18676954303622\n",
      "Stochastic Gradient Descent(14806): loss=0.9324824385988936\n",
      "Stochastic Gradient Descent(14807): loss=10.03018728196761\n",
      "Stochastic Gradient Descent(14808): loss=1.062703096281393\n",
      "Stochastic Gradient Descent(14809): loss=1.0798467620965204\n",
      "Stochastic Gradient Descent(14810): loss=0.021929016902437815\n",
      "Stochastic Gradient Descent(14811): loss=8.649107213690387\n",
      "Stochastic Gradient Descent(14812): loss=0.1876407332282498\n",
      "Stochastic Gradient Descent(14813): loss=38.83052046889414\n",
      "Stochastic Gradient Descent(14814): loss=5.134311812162175\n",
      "Stochastic Gradient Descent(14815): loss=2.2232977872574984\n",
      "Stochastic Gradient Descent(14816): loss=0.05470515108440958\n",
      "Stochastic Gradient Descent(14817): loss=26.863814761022862\n",
      "Stochastic Gradient Descent(14818): loss=23.013821594741877\n",
      "Stochastic Gradient Descent(14819): loss=4.424743213860178\n",
      "Stochastic Gradient Descent(14820): loss=9.678245584636455\n",
      "Stochastic Gradient Descent(14821): loss=52.87415896338576\n",
      "Stochastic Gradient Descent(14822): loss=1.9257946057156952\n",
      "Stochastic Gradient Descent(14823): loss=0.23762780213911636\n",
      "Stochastic Gradient Descent(14824): loss=21.468879261986523\n",
      "Stochastic Gradient Descent(14825): loss=0.37995185781054036\n",
      "Stochastic Gradient Descent(14826): loss=9.158686465551455\n",
      "Stochastic Gradient Descent(14827): loss=0.001072319084164476\n",
      "Stochastic Gradient Descent(14828): loss=0.0006268110476306803\n",
      "Stochastic Gradient Descent(14829): loss=0.09027377228627896\n",
      "Stochastic Gradient Descent(14830): loss=1.0864131545830935\n",
      "Stochastic Gradient Descent(14831): loss=0.12438417102557632\n",
      "Stochastic Gradient Descent(14832): loss=1.4410973759737593\n",
      "Stochastic Gradient Descent(14833): loss=0.10751292099228407\n",
      "Stochastic Gradient Descent(14834): loss=0.43472221621141943\n",
      "Stochastic Gradient Descent(14835): loss=0.41813487326196563\n",
      "Stochastic Gradient Descent(14836): loss=6.079234221746892\n",
      "Stochastic Gradient Descent(14837): loss=7.475715815370441\n",
      "Stochastic Gradient Descent(14838): loss=4.935753191330946\n",
      "Stochastic Gradient Descent(14839): loss=0.7723540962674679\n",
      "Stochastic Gradient Descent(14840): loss=0.4384154331588591\n",
      "Stochastic Gradient Descent(14841): loss=10.693323058631487\n",
      "Stochastic Gradient Descent(14842): loss=1.433233710984685\n",
      "Stochastic Gradient Descent(14843): loss=9.73193016882705\n",
      "Stochastic Gradient Descent(14844): loss=0.40438336998714847\n",
      "Stochastic Gradient Descent(14845): loss=4.094906004053261\n",
      "Stochastic Gradient Descent(14846): loss=0.13832413396999677\n",
      "Stochastic Gradient Descent(14847): loss=0.11283628816285494\n",
      "Stochastic Gradient Descent(14848): loss=3.947693342248074\n",
      "Stochastic Gradient Descent(14849): loss=2.3191760547960225\n",
      "Stochastic Gradient Descent(14850): loss=4.626869345867791\n",
      "Stochastic Gradient Descent(14851): loss=2.342426793529567\n",
      "Stochastic Gradient Descent(14852): loss=0.9897525287891658\n",
      "Stochastic Gradient Descent(14853): loss=49.108652899402976\n",
      "Stochastic Gradient Descent(14854): loss=2.238196942850451\n",
      "Stochastic Gradient Descent(14855): loss=16.701748098475562\n",
      "Stochastic Gradient Descent(14856): loss=0.6586661497972918\n",
      "Stochastic Gradient Descent(14857): loss=18.80426277545113\n",
      "Stochastic Gradient Descent(14858): loss=0.5154696102183904\n",
      "Stochastic Gradient Descent(14859): loss=6.703550167943263\n",
      "Stochastic Gradient Descent(14860): loss=0.6426401093999394\n",
      "Stochastic Gradient Descent(14861): loss=7.889003047263983\n",
      "Stochastic Gradient Descent(14862): loss=14.573655710863399\n",
      "Stochastic Gradient Descent(14863): loss=0.017162820708069494\n",
      "Stochastic Gradient Descent(14864): loss=0.006592374002412432\n",
      "Stochastic Gradient Descent(14865): loss=0.18196842322887738\n",
      "Stochastic Gradient Descent(14866): loss=0.5561026238371195\n",
      "Stochastic Gradient Descent(14867): loss=0.026312967480379403\n",
      "Stochastic Gradient Descent(14868): loss=0.8113985966730899\n",
      "Stochastic Gradient Descent(14869): loss=12.519332913609514\n",
      "Stochastic Gradient Descent(14870): loss=3.528681903398355\n",
      "Stochastic Gradient Descent(14871): loss=10.727297110521077\n",
      "Stochastic Gradient Descent(14872): loss=15.012721976818256\n",
      "Stochastic Gradient Descent(14873): loss=5.573523975895406\n",
      "Stochastic Gradient Descent(14874): loss=33.68801718377807\n",
      "Stochastic Gradient Descent(14875): loss=14.696594265250859\n",
      "Stochastic Gradient Descent(14876): loss=10.97739356442891\n",
      "Stochastic Gradient Descent(14877): loss=2.0366219559622354\n",
      "Stochastic Gradient Descent(14878): loss=0.5395885632655778\n",
      "Stochastic Gradient Descent(14879): loss=0.00664593832801773\n",
      "Stochastic Gradient Descent(14880): loss=2.3192027152976027\n",
      "Stochastic Gradient Descent(14881): loss=5.150730912994781\n",
      "Stochastic Gradient Descent(14882): loss=4.224526298671434\n",
      "Stochastic Gradient Descent(14883): loss=24.365644340086426\n",
      "Stochastic Gradient Descent(14884): loss=3.9229172224865914\n",
      "Stochastic Gradient Descent(14885): loss=0.21906225392538678\n",
      "Stochastic Gradient Descent(14886): loss=0.000932361327161181\n",
      "Stochastic Gradient Descent(14887): loss=1.2350484358263145\n",
      "Stochastic Gradient Descent(14888): loss=0.40781505510474414\n",
      "Stochastic Gradient Descent(14889): loss=0.27605911729872357\n",
      "Stochastic Gradient Descent(14890): loss=0.6472050019431209\n",
      "Stochastic Gradient Descent(14891): loss=4.335205579374701\n",
      "Stochastic Gradient Descent(14892): loss=2.686102262527891\n",
      "Stochastic Gradient Descent(14893): loss=3.4230537689427614\n",
      "Stochastic Gradient Descent(14894): loss=2.073434870046531\n",
      "Stochastic Gradient Descent(14895): loss=2.066807420857582\n",
      "Stochastic Gradient Descent(14896): loss=0.004111224247214098\n",
      "Stochastic Gradient Descent(14897): loss=16.943652323653186\n",
      "Stochastic Gradient Descent(14898): loss=3.3048118009272347\n",
      "Stochastic Gradient Descent(14899): loss=0.8318306079809862\n",
      "Stochastic Gradient Descent(14900): loss=0.06890633341401008\n",
      "Stochastic Gradient Descent(14901): loss=7.501388540196538\n",
      "Stochastic Gradient Descent(14902): loss=1.3943094996756316\n",
      "Stochastic Gradient Descent(14903): loss=0.2767026394103906\n",
      "Stochastic Gradient Descent(14904): loss=3.031715224708091\n",
      "Stochastic Gradient Descent(14905): loss=4.57889430166381\n",
      "Stochastic Gradient Descent(14906): loss=0.06064923994021035\n",
      "Stochastic Gradient Descent(14907): loss=4.894661077905592\n",
      "Stochastic Gradient Descent(14908): loss=0.13238079545342946\n",
      "Stochastic Gradient Descent(14909): loss=1.988583286838455\n",
      "Stochastic Gradient Descent(14910): loss=0.05831624045611946\n",
      "Stochastic Gradient Descent(14911): loss=3.0152866923436186\n",
      "Stochastic Gradient Descent(14912): loss=0.15420010502653597\n",
      "Stochastic Gradient Descent(14913): loss=9.40828131501062\n",
      "Stochastic Gradient Descent(14914): loss=0.4003368948823877\n",
      "Stochastic Gradient Descent(14915): loss=0.011743845367046023\n",
      "Stochastic Gradient Descent(14916): loss=24.20614217152007\n",
      "Stochastic Gradient Descent(14917): loss=4.858769529309126\n",
      "Stochastic Gradient Descent(14918): loss=5.146646700469806\n",
      "Stochastic Gradient Descent(14919): loss=3.0274951958585063\n",
      "Stochastic Gradient Descent(14920): loss=0.1814496624015878\n",
      "Stochastic Gradient Descent(14921): loss=0.4810972094744387\n",
      "Stochastic Gradient Descent(14922): loss=12.95588583458842\n",
      "Stochastic Gradient Descent(14923): loss=7.133171957073275\n",
      "Stochastic Gradient Descent(14924): loss=0.4407995583554259\n",
      "Stochastic Gradient Descent(14925): loss=0.782491332774439\n",
      "Stochastic Gradient Descent(14926): loss=0.39784312206270334\n",
      "Stochastic Gradient Descent(14927): loss=0.0002773531492488559\n",
      "Stochastic Gradient Descent(14928): loss=0.8152164286475798\n",
      "Stochastic Gradient Descent(14929): loss=0.17977132114640093\n",
      "Stochastic Gradient Descent(14930): loss=0.7385375966554971\n",
      "Stochastic Gradient Descent(14931): loss=0.028610216682923812\n",
      "Stochastic Gradient Descent(14932): loss=0.272139441875863\n",
      "Stochastic Gradient Descent(14933): loss=5.384362178643421\n",
      "Stochastic Gradient Descent(14934): loss=0.05638823131428899\n",
      "Stochastic Gradient Descent(14935): loss=0.02596969819087424\n",
      "Stochastic Gradient Descent(14936): loss=2.3373027734976377\n",
      "Stochastic Gradient Descent(14937): loss=8.948275179484314\n",
      "Stochastic Gradient Descent(14938): loss=6.2747977185876165\n",
      "Stochastic Gradient Descent(14939): loss=1.3691619067663068\n",
      "Stochastic Gradient Descent(14940): loss=0.7286908633276343\n",
      "Stochastic Gradient Descent(14941): loss=4.773246357736289\n",
      "Stochastic Gradient Descent(14942): loss=16.069286185869654\n",
      "Stochastic Gradient Descent(14943): loss=6.8037840651549635\n",
      "Stochastic Gradient Descent(14944): loss=2.6275782415723197\n",
      "Stochastic Gradient Descent(14945): loss=28.355442133421626\n",
      "Stochastic Gradient Descent(14946): loss=0.0014117724685978653\n",
      "Stochastic Gradient Descent(14947): loss=2.481114916411585\n",
      "Stochastic Gradient Descent(14948): loss=0.014308801286126561\n",
      "Stochastic Gradient Descent(14949): loss=3.5666351882363365\n",
      "Stochastic Gradient Descent(14950): loss=0.024087284733336652\n",
      "Stochastic Gradient Descent(14951): loss=0.11274801235927909\n",
      "Stochastic Gradient Descent(14952): loss=0.23738398013155818\n",
      "Stochastic Gradient Descent(14953): loss=0.03362724393163459\n",
      "Stochastic Gradient Descent(14954): loss=13.452216334124229\n",
      "Stochastic Gradient Descent(14955): loss=0.003242200640951392\n",
      "Stochastic Gradient Descent(14956): loss=5.74170720571353\n",
      "Stochastic Gradient Descent(14957): loss=4.638948282610551\n",
      "Stochastic Gradient Descent(14958): loss=0.014062662884236734\n",
      "Stochastic Gradient Descent(14959): loss=0.18461781801339897\n",
      "Stochastic Gradient Descent(14960): loss=0.9946613485960496\n",
      "Stochastic Gradient Descent(14961): loss=9.36094645753393\n",
      "Stochastic Gradient Descent(14962): loss=3.750365102492233\n",
      "Stochastic Gradient Descent(14963): loss=1.7232274154690554\n",
      "Stochastic Gradient Descent(14964): loss=3.5984124297258355\n",
      "Stochastic Gradient Descent(14965): loss=2.396783757276317\n",
      "Stochastic Gradient Descent(14966): loss=0.29319999631355503\n",
      "Stochastic Gradient Descent(14967): loss=3.35799862830887\n",
      "Stochastic Gradient Descent(14968): loss=0.3588973966658852\n",
      "Stochastic Gradient Descent(14969): loss=0.5282045751292772\n",
      "Stochastic Gradient Descent(14970): loss=1.7942634232860044\n",
      "Stochastic Gradient Descent(14971): loss=1.1416504503833096\n",
      "Stochastic Gradient Descent(14972): loss=0.41145535533573896\n",
      "Stochastic Gradient Descent(14973): loss=0.273835350376305\n",
      "Stochastic Gradient Descent(14974): loss=3.952436246323229\n",
      "Stochastic Gradient Descent(14975): loss=11.607375746568103\n",
      "Stochastic Gradient Descent(14976): loss=0.9677279708644112\n",
      "Stochastic Gradient Descent(14977): loss=6.007290704481944\n",
      "Stochastic Gradient Descent(14978): loss=9.652774781026828\n",
      "Stochastic Gradient Descent(14979): loss=5.336036122015031\n",
      "Stochastic Gradient Descent(14980): loss=0.3265908666907034\n",
      "Stochastic Gradient Descent(14981): loss=7.991127591394279\n",
      "Stochastic Gradient Descent(14982): loss=0.46730106031259194\n",
      "Stochastic Gradient Descent(14983): loss=9.74637370993663\n",
      "Stochastic Gradient Descent(14984): loss=10.362521059642988\n",
      "Stochastic Gradient Descent(14985): loss=30.00283829593883\n",
      "Stochastic Gradient Descent(14986): loss=0.9811642981487334\n",
      "Stochastic Gradient Descent(14987): loss=4.781614334365289\n",
      "Stochastic Gradient Descent(14988): loss=43.32586927287122\n",
      "Stochastic Gradient Descent(14989): loss=13.683838507533011\n",
      "Stochastic Gradient Descent(14990): loss=7.480307199321377\n",
      "Stochastic Gradient Descent(14991): loss=0.07035502536482999\n",
      "Stochastic Gradient Descent(14992): loss=12.922281753442256\n",
      "Stochastic Gradient Descent(14993): loss=9.077488337054755\n",
      "Stochastic Gradient Descent(14994): loss=0.7490304913943706\n",
      "Stochastic Gradient Descent(14995): loss=1.1887791157280632\n",
      "Stochastic Gradient Descent(14996): loss=1.1263458180117545\n",
      "Stochastic Gradient Descent(14997): loss=0.21286246447621993\n",
      "Stochastic Gradient Descent(14998): loss=50.380471432011596\n",
      "Stochastic Gradient Descent(14999): loss=0.5644149955556351\n",
      "Stochastic Gradient Descent(15000): loss=1.0932621898036163\n",
      "Stochastic Gradient Descent(15001): loss=0.9728117699372768\n",
      "Stochastic Gradient Descent(15002): loss=13.154473463732916\n",
      "Stochastic Gradient Descent(15003): loss=1.5771809072827332\n",
      "Stochastic Gradient Descent(15004): loss=0.003508936783423432\n",
      "Stochastic Gradient Descent(15005): loss=0.3041370870165081\n",
      "Stochastic Gradient Descent(15006): loss=1.718485624650634\n",
      "Stochastic Gradient Descent(15007): loss=53.7933525807447\n",
      "Stochastic Gradient Descent(15008): loss=0.4576057306928639\n",
      "Stochastic Gradient Descent(15009): loss=1.425585805740353\n",
      "Stochastic Gradient Descent(15010): loss=2.41436926247824\n",
      "Stochastic Gradient Descent(15011): loss=3.9634265644134294\n",
      "Stochastic Gradient Descent(15012): loss=9.666754700706848\n",
      "Stochastic Gradient Descent(15013): loss=3.4068559898292334\n",
      "Stochastic Gradient Descent(15014): loss=3.80373921179325\n",
      "Stochastic Gradient Descent(15015): loss=0.8941768939204017\n",
      "Stochastic Gradient Descent(15016): loss=0.18761909881671246\n",
      "Stochastic Gradient Descent(15017): loss=0.11220053666650295\n",
      "Stochastic Gradient Descent(15018): loss=3.0264360436843\n",
      "Stochastic Gradient Descent(15019): loss=3.8301090663951065\n",
      "Stochastic Gradient Descent(15020): loss=4.325188101835938\n",
      "Stochastic Gradient Descent(15021): loss=0.10149452734806359\n",
      "Stochastic Gradient Descent(15022): loss=0.8770042670895775\n",
      "Stochastic Gradient Descent(15023): loss=6.169510972307829\n",
      "Stochastic Gradient Descent(15024): loss=0.7592825800380862\n",
      "Stochastic Gradient Descent(15025): loss=3.5929578508746536\n",
      "Stochastic Gradient Descent(15026): loss=6.132831827419324\n",
      "Stochastic Gradient Descent(15027): loss=11.295438827959115\n",
      "Stochastic Gradient Descent(15028): loss=39.3894155517619\n",
      "Stochastic Gradient Descent(15029): loss=0.045141219785044616\n",
      "Stochastic Gradient Descent(15030): loss=10.07039569222573\n",
      "Stochastic Gradient Descent(15031): loss=11.084613621660138\n",
      "Stochastic Gradient Descent(15032): loss=16.63550839561114\n",
      "Stochastic Gradient Descent(15033): loss=10.759332642433677\n",
      "Stochastic Gradient Descent(15034): loss=4.483697068866594\n",
      "Stochastic Gradient Descent(15035): loss=0.09357199949709856\n",
      "Stochastic Gradient Descent(15036): loss=2.419433501690293\n",
      "Stochastic Gradient Descent(15037): loss=5.026529067579086\n",
      "Stochastic Gradient Descent(15038): loss=4.003460541334845\n",
      "Stochastic Gradient Descent(15039): loss=8.791731814055794\n",
      "Stochastic Gradient Descent(15040): loss=6.335602282199918e-05\n",
      "Stochastic Gradient Descent(15041): loss=2.5165646293307464\n",
      "Stochastic Gradient Descent(15042): loss=5.956789439760901\n",
      "Stochastic Gradient Descent(15043): loss=2.6692095072028055\n",
      "Stochastic Gradient Descent(15044): loss=4.978460669781837\n",
      "Stochastic Gradient Descent(15045): loss=11.10292072809154\n",
      "Stochastic Gradient Descent(15046): loss=8.023605037698067\n",
      "Stochastic Gradient Descent(15047): loss=1.817376358909811\n",
      "Stochastic Gradient Descent(15048): loss=0.6010672649622986\n",
      "Stochastic Gradient Descent(15049): loss=10.620690060940143\n",
      "Stochastic Gradient Descent(15050): loss=1.5811508952994129\n",
      "Stochastic Gradient Descent(15051): loss=9.313352418698827\n",
      "Stochastic Gradient Descent(15052): loss=1.250069198083648\n",
      "Stochastic Gradient Descent(15053): loss=1.0162544435754894\n",
      "Stochastic Gradient Descent(15054): loss=4.546589878863856\n",
      "Stochastic Gradient Descent(15055): loss=6.635079209282757\n",
      "Stochastic Gradient Descent(15056): loss=0.10086018128855288\n",
      "Stochastic Gradient Descent(15057): loss=6.581609222270314\n",
      "Stochastic Gradient Descent(15058): loss=5.148951970872066\n",
      "Stochastic Gradient Descent(15059): loss=0.00082646222341456\n",
      "Stochastic Gradient Descent(15060): loss=3.18363041982352\n",
      "Stochastic Gradient Descent(15061): loss=4.935005590020564\n",
      "Stochastic Gradient Descent(15062): loss=2.519174680363786\n",
      "Stochastic Gradient Descent(15063): loss=5.109474122753398\n",
      "Stochastic Gradient Descent(15064): loss=0.8732380610174121\n",
      "Stochastic Gradient Descent(15065): loss=0.009438396985382512\n",
      "Stochastic Gradient Descent(15066): loss=0.16129721193401578\n",
      "Stochastic Gradient Descent(15067): loss=1.5856130269879585\n",
      "Stochastic Gradient Descent(15068): loss=1.222696566907608\n",
      "Stochastic Gradient Descent(15069): loss=10.974439565792041\n",
      "Stochastic Gradient Descent(15070): loss=4.304943176737455\n",
      "Stochastic Gradient Descent(15071): loss=0.030053688299472286\n",
      "Stochastic Gradient Descent(15072): loss=4.388021322298594\n",
      "Stochastic Gradient Descent(15073): loss=0.40762137525717007\n",
      "Stochastic Gradient Descent(15074): loss=1.2429210264857986\n",
      "Stochastic Gradient Descent(15075): loss=0.7328762155296905\n",
      "Stochastic Gradient Descent(15076): loss=2.553667450878249\n",
      "Stochastic Gradient Descent(15077): loss=5.073608233086562\n",
      "Stochastic Gradient Descent(15078): loss=12.166447962584659\n",
      "Stochastic Gradient Descent(15079): loss=13.640726439492973\n",
      "Stochastic Gradient Descent(15080): loss=0.1912277887144843\n",
      "Stochastic Gradient Descent(15081): loss=17.226914941835712\n",
      "Stochastic Gradient Descent(15082): loss=4.725205466521412\n",
      "Stochastic Gradient Descent(15083): loss=1.272644897308307\n",
      "Stochastic Gradient Descent(15084): loss=9.419298363714208\n",
      "Stochastic Gradient Descent(15085): loss=7.488542848502863\n",
      "Stochastic Gradient Descent(15086): loss=11.044724756393885\n",
      "Stochastic Gradient Descent(15087): loss=27.127826694810523\n",
      "Stochastic Gradient Descent(15088): loss=14.995602205550528\n",
      "Stochastic Gradient Descent(15089): loss=0.28481929458632116\n",
      "Stochastic Gradient Descent(15090): loss=12.800422412752537\n",
      "Stochastic Gradient Descent(15091): loss=0.27339710017460933\n",
      "Stochastic Gradient Descent(15092): loss=5.188046899199302\n",
      "Stochastic Gradient Descent(15093): loss=0.9283267379895438\n",
      "Stochastic Gradient Descent(15094): loss=1.5250632935934854\n",
      "Stochastic Gradient Descent(15095): loss=0.10778963835302864\n",
      "Stochastic Gradient Descent(15096): loss=0.6024115444944055\n",
      "Stochastic Gradient Descent(15097): loss=4.489764548815049\n",
      "Stochastic Gradient Descent(15098): loss=4.036205969322879\n",
      "Stochastic Gradient Descent(15099): loss=0.1149391379862069\n",
      "Stochastic Gradient Descent(15100): loss=1.6759893967392072\n",
      "Stochastic Gradient Descent(15101): loss=1.1441387981155056\n",
      "Stochastic Gradient Descent(15102): loss=0.0004956581618046518\n",
      "Stochastic Gradient Descent(15103): loss=0.22149403664063477\n",
      "Stochastic Gradient Descent(15104): loss=3.768551207377362\n",
      "Stochastic Gradient Descent(15105): loss=0.04253998836098055\n",
      "Stochastic Gradient Descent(15106): loss=1.0314317671906943\n",
      "Stochastic Gradient Descent(15107): loss=0.7115413827693444\n",
      "Stochastic Gradient Descent(15108): loss=1.0505658389328572\n",
      "Stochastic Gradient Descent(15109): loss=0.07191919545708221\n",
      "Stochastic Gradient Descent(15110): loss=1.3669281295297695\n",
      "Stochastic Gradient Descent(15111): loss=1.4737468374348817\n",
      "Stochastic Gradient Descent(15112): loss=3.223873448498787\n",
      "Stochastic Gradient Descent(15113): loss=0.2057181140949192\n",
      "Stochastic Gradient Descent(15114): loss=6.24924704993655\n",
      "Stochastic Gradient Descent(15115): loss=3.9298057772900425\n",
      "Stochastic Gradient Descent(15116): loss=5.751656664573347\n",
      "Stochastic Gradient Descent(15117): loss=6.374552626127235\n",
      "Stochastic Gradient Descent(15118): loss=0.2638900097174004\n",
      "Stochastic Gradient Descent(15119): loss=0.03970377706684636\n",
      "Stochastic Gradient Descent(15120): loss=3.3833694501174345\n",
      "Stochastic Gradient Descent(15121): loss=1.9293674185473662\n",
      "Stochastic Gradient Descent(15122): loss=0.2155084425013331\n",
      "Stochastic Gradient Descent(15123): loss=0.96280992418105\n",
      "Stochastic Gradient Descent(15124): loss=4.811180336425116\n",
      "Stochastic Gradient Descent(15125): loss=3.8838645090467585\n",
      "Stochastic Gradient Descent(15126): loss=6.802199315313044\n",
      "Stochastic Gradient Descent(15127): loss=0.5405401331496188\n",
      "Stochastic Gradient Descent(15128): loss=0.0028349233417832133\n",
      "Stochastic Gradient Descent(15129): loss=9.91101535134806\n",
      "Stochastic Gradient Descent(15130): loss=2.6702889916242447\n",
      "Stochastic Gradient Descent(15131): loss=0.32742768207976825\n",
      "Stochastic Gradient Descent(15132): loss=0.7041752022411637\n",
      "Stochastic Gradient Descent(15133): loss=3.442967297095031\n",
      "Stochastic Gradient Descent(15134): loss=0.9332756860184537\n",
      "Stochastic Gradient Descent(15135): loss=3.17388938809803\n",
      "Stochastic Gradient Descent(15136): loss=0.13710675334869885\n",
      "Stochastic Gradient Descent(15137): loss=0.7233319798026855\n",
      "Stochastic Gradient Descent(15138): loss=1.852577882929958\n",
      "Stochastic Gradient Descent(15139): loss=1.2044742622608988\n",
      "Stochastic Gradient Descent(15140): loss=0.3352981087715846\n",
      "Stochastic Gradient Descent(15141): loss=2.746410604164855\n",
      "Stochastic Gradient Descent(15142): loss=0.031281648734273165\n",
      "Stochastic Gradient Descent(15143): loss=2.7946826305889587\n",
      "Stochastic Gradient Descent(15144): loss=14.145106466842405\n",
      "Stochastic Gradient Descent(15145): loss=2.1703801177419098\n",
      "Stochastic Gradient Descent(15146): loss=1.0834245482564067\n",
      "Stochastic Gradient Descent(15147): loss=0.8753257458909595\n",
      "Stochastic Gradient Descent(15148): loss=0.7348615090166873\n",
      "Stochastic Gradient Descent(15149): loss=1.8608043270219907\n",
      "Stochastic Gradient Descent(15150): loss=4.344024972323378\n",
      "Stochastic Gradient Descent(15151): loss=0.6318062488317006\n",
      "Stochastic Gradient Descent(15152): loss=0.44258860485419205\n",
      "Stochastic Gradient Descent(15153): loss=0.10634853091809297\n",
      "Stochastic Gradient Descent(15154): loss=0.1596123124612478\n",
      "Stochastic Gradient Descent(15155): loss=0.4558004536525703\n",
      "Stochastic Gradient Descent(15156): loss=2.802470831809817\n",
      "Stochastic Gradient Descent(15157): loss=3.325267419274094\n",
      "Stochastic Gradient Descent(15158): loss=2.5682425597457157\n",
      "Stochastic Gradient Descent(15159): loss=0.012547974034504193\n",
      "Stochastic Gradient Descent(15160): loss=8.594774759808958\n",
      "Stochastic Gradient Descent(15161): loss=2.095625958794181\n",
      "Stochastic Gradient Descent(15162): loss=22.319613232082922\n",
      "Stochastic Gradient Descent(15163): loss=2.2856676176679644\n",
      "Stochastic Gradient Descent(15164): loss=0.28938411755112425\n",
      "Stochastic Gradient Descent(15165): loss=9.783001570309857\n",
      "Stochastic Gradient Descent(15166): loss=0.2781532605205196\n",
      "Stochastic Gradient Descent(15167): loss=9.655475817209599\n",
      "Stochastic Gradient Descent(15168): loss=4.850022736740048\n",
      "Stochastic Gradient Descent(15169): loss=0.010561672747558856\n",
      "Stochastic Gradient Descent(15170): loss=2.660211541230636\n",
      "Stochastic Gradient Descent(15171): loss=5.233036525556359\n",
      "Stochastic Gradient Descent(15172): loss=0.16614637754380016\n",
      "Stochastic Gradient Descent(15173): loss=3.3628396991008125\n",
      "Stochastic Gradient Descent(15174): loss=3.094702751101988\n",
      "Stochastic Gradient Descent(15175): loss=6.215821229134373\n",
      "Stochastic Gradient Descent(15176): loss=5.078882548325155\n",
      "Stochastic Gradient Descent(15177): loss=0.8007009190138593\n",
      "Stochastic Gradient Descent(15178): loss=8.103918987481956\n",
      "Stochastic Gradient Descent(15179): loss=1.0126528117689062\n",
      "Stochastic Gradient Descent(15180): loss=3.370042426311845\n",
      "Stochastic Gradient Descent(15181): loss=7.079428039981605\n",
      "Stochastic Gradient Descent(15182): loss=4.198664374283586\n",
      "Stochastic Gradient Descent(15183): loss=0.34068455722318264\n",
      "Stochastic Gradient Descent(15184): loss=14.084283931462256\n",
      "Stochastic Gradient Descent(15185): loss=0.7588596065524141\n",
      "Stochastic Gradient Descent(15186): loss=1.708317246923213\n",
      "Stochastic Gradient Descent(15187): loss=0.1285109088402529\n",
      "Stochastic Gradient Descent(15188): loss=14.03201382383137\n",
      "Stochastic Gradient Descent(15189): loss=8.196019219668985\n",
      "Stochastic Gradient Descent(15190): loss=0.06684064935256512\n",
      "Stochastic Gradient Descent(15191): loss=17.30234798909303\n",
      "Stochastic Gradient Descent(15192): loss=64.26542115861068\n",
      "Stochastic Gradient Descent(15193): loss=51.30792101177142\n",
      "Stochastic Gradient Descent(15194): loss=7.001883449906393\n",
      "Stochastic Gradient Descent(15195): loss=44.74796316914974\n",
      "Stochastic Gradient Descent(15196): loss=0.056301472664837544\n",
      "Stochastic Gradient Descent(15197): loss=9.6258912280453\n",
      "Stochastic Gradient Descent(15198): loss=0.20403008019850827\n",
      "Stochastic Gradient Descent(15199): loss=0.015546240223018551\n",
      "Stochastic Gradient Descent(15200): loss=4.117228228317786\n",
      "Stochastic Gradient Descent(15201): loss=0.9183646880554001\n",
      "Stochastic Gradient Descent(15202): loss=0.09747280276099025\n",
      "Stochastic Gradient Descent(15203): loss=2.336519615181914\n",
      "Stochastic Gradient Descent(15204): loss=0.504011750680699\n",
      "Stochastic Gradient Descent(15205): loss=9.07047507431373\n",
      "Stochastic Gradient Descent(15206): loss=0.6247552043322021\n",
      "Stochastic Gradient Descent(15207): loss=15.312625046395372\n",
      "Stochastic Gradient Descent(15208): loss=1.5721483779586596\n",
      "Stochastic Gradient Descent(15209): loss=8.07659399925523\n",
      "Stochastic Gradient Descent(15210): loss=0.7046018915559976\n",
      "Stochastic Gradient Descent(15211): loss=0.12528875089152228\n",
      "Stochastic Gradient Descent(15212): loss=0.9755559798625841\n",
      "Stochastic Gradient Descent(15213): loss=1.525767482777603\n",
      "Stochastic Gradient Descent(15214): loss=8.0410600815157\n",
      "Stochastic Gradient Descent(15215): loss=3.638303020048062\n",
      "Stochastic Gradient Descent(15216): loss=11.484376778787436\n",
      "Stochastic Gradient Descent(15217): loss=0.14217509889758181\n",
      "Stochastic Gradient Descent(15218): loss=0.992412101012263\n",
      "Stochastic Gradient Descent(15219): loss=1.3270596790738016\n",
      "Stochastic Gradient Descent(15220): loss=0.43833629123313084\n",
      "Stochastic Gradient Descent(15221): loss=26.4250307524981\n",
      "Stochastic Gradient Descent(15222): loss=0.8270999040212464\n",
      "Stochastic Gradient Descent(15223): loss=15.199497896500072\n",
      "Stochastic Gradient Descent(15224): loss=1.5409424412607013\n",
      "Stochastic Gradient Descent(15225): loss=1.4294488850677172\n",
      "Stochastic Gradient Descent(15226): loss=0.004081656428559628\n",
      "Stochastic Gradient Descent(15227): loss=0.7158651647376609\n",
      "Stochastic Gradient Descent(15228): loss=1.2743764027794526\n",
      "Stochastic Gradient Descent(15229): loss=4.550690799498105\n",
      "Stochastic Gradient Descent(15230): loss=0.0689173359145873\n",
      "Stochastic Gradient Descent(15231): loss=6.353642843912967\n",
      "Stochastic Gradient Descent(15232): loss=9.513200412668873\n",
      "Stochastic Gradient Descent(15233): loss=0.08685607996156891\n",
      "Stochastic Gradient Descent(15234): loss=1.112938614840331\n",
      "Stochastic Gradient Descent(15235): loss=9.033312955656585\n",
      "Stochastic Gradient Descent(15236): loss=5.842295376110702\n",
      "Stochastic Gradient Descent(15237): loss=4.716067830743131\n",
      "Stochastic Gradient Descent(15238): loss=8.335378700628594\n",
      "Stochastic Gradient Descent(15239): loss=9.405719703545794\n",
      "Stochastic Gradient Descent(15240): loss=1.6167771744290087\n",
      "Stochastic Gradient Descent(15241): loss=0.07844160765747758\n",
      "Stochastic Gradient Descent(15242): loss=5.405985300248882\n",
      "Stochastic Gradient Descent(15243): loss=13.552679678819107\n",
      "Stochastic Gradient Descent(15244): loss=3.848130554599298\n",
      "Stochastic Gradient Descent(15245): loss=0.002989714561424194\n",
      "Stochastic Gradient Descent(15246): loss=16.163433059686657\n",
      "Stochastic Gradient Descent(15247): loss=0.005357632353809853\n",
      "Stochastic Gradient Descent(15248): loss=0.30509287209181735\n",
      "Stochastic Gradient Descent(15249): loss=0.13871110781775522\n",
      "Stochastic Gradient Descent(15250): loss=23.1906866601647\n",
      "Stochastic Gradient Descent(15251): loss=2.4546727678499645\n",
      "Stochastic Gradient Descent(15252): loss=79.10048391022386\n",
      "Stochastic Gradient Descent(15253): loss=19.889234166800584\n",
      "Stochastic Gradient Descent(15254): loss=1.5600579358221625\n",
      "Stochastic Gradient Descent(15255): loss=0.9298294250837725\n",
      "Stochastic Gradient Descent(15256): loss=0.1153738186835046\n",
      "Stochastic Gradient Descent(15257): loss=0.09416433241942664\n",
      "Stochastic Gradient Descent(15258): loss=4.928963909024875\n",
      "Stochastic Gradient Descent(15259): loss=2.2677926883745267\n",
      "Stochastic Gradient Descent(15260): loss=4.981483088292652\n",
      "Stochastic Gradient Descent(15261): loss=10.210102925330792\n",
      "Stochastic Gradient Descent(15262): loss=4.542099800193861\n",
      "Stochastic Gradient Descent(15263): loss=2.1857567997923115\n",
      "Stochastic Gradient Descent(15264): loss=2.3214238288484177\n",
      "Stochastic Gradient Descent(15265): loss=0.7494716750253138\n",
      "Stochastic Gradient Descent(15266): loss=1.424064881146874\n",
      "Stochastic Gradient Descent(15267): loss=17.974747224099804\n",
      "Stochastic Gradient Descent(15268): loss=0.09304024255392947\n",
      "Stochastic Gradient Descent(15269): loss=0.24022863213076026\n",
      "Stochastic Gradient Descent(15270): loss=9.019183326024221\n",
      "Stochastic Gradient Descent(15271): loss=22.666524659296755\n",
      "Stochastic Gradient Descent(15272): loss=3.038531859881576\n",
      "Stochastic Gradient Descent(15273): loss=0.8037833847299477\n",
      "Stochastic Gradient Descent(15274): loss=6.330171201412954\n",
      "Stochastic Gradient Descent(15275): loss=12.866412450125228\n",
      "Stochastic Gradient Descent(15276): loss=3.240612823207437\n",
      "Stochastic Gradient Descent(15277): loss=3.9986538045324274\n",
      "Stochastic Gradient Descent(15278): loss=12.390294821890341\n",
      "Stochastic Gradient Descent(15279): loss=4.136135536584044\n",
      "Stochastic Gradient Descent(15280): loss=7.987727472935666\n",
      "Stochastic Gradient Descent(15281): loss=20.98581968357828\n",
      "Stochastic Gradient Descent(15282): loss=0.022417958160764768\n",
      "Stochastic Gradient Descent(15283): loss=0.038949577148841084\n",
      "Stochastic Gradient Descent(15284): loss=8.676273737678052\n",
      "Stochastic Gradient Descent(15285): loss=43.97325458900503\n",
      "Stochastic Gradient Descent(15286): loss=0.1753226225277396\n",
      "Stochastic Gradient Descent(15287): loss=9.389916032769943\n",
      "Stochastic Gradient Descent(15288): loss=7.085983652367641\n",
      "Stochastic Gradient Descent(15289): loss=0.6202106642703229\n",
      "Stochastic Gradient Descent(15290): loss=4.225446033541908\n",
      "Stochastic Gradient Descent(15291): loss=13.362156853884803\n",
      "Stochastic Gradient Descent(15292): loss=2.005820235433414\n",
      "Stochastic Gradient Descent(15293): loss=0.8136158408473508\n",
      "Stochastic Gradient Descent(15294): loss=18.05455938695305\n",
      "Stochastic Gradient Descent(15295): loss=4.583966974612197\n",
      "Stochastic Gradient Descent(15296): loss=6.381978972344185\n",
      "Stochastic Gradient Descent(15297): loss=5.729527287218342\n",
      "Stochastic Gradient Descent(15298): loss=0.2914413521989126\n",
      "Stochastic Gradient Descent(15299): loss=2.893492936423139\n",
      "Stochastic Gradient Descent(15300): loss=0.7534574841251496\n",
      "Stochastic Gradient Descent(15301): loss=0.6977060329811875\n",
      "Stochastic Gradient Descent(15302): loss=0.5549773486388074\n",
      "Stochastic Gradient Descent(15303): loss=0.023249196391587616\n",
      "Stochastic Gradient Descent(15304): loss=1.461404050894937\n",
      "Stochastic Gradient Descent(15305): loss=0.08159700887591602\n",
      "Stochastic Gradient Descent(15306): loss=8.325861273367943\n",
      "Stochastic Gradient Descent(15307): loss=0.02747507503089912\n",
      "Stochastic Gradient Descent(15308): loss=5.302312312218655\n",
      "Stochastic Gradient Descent(15309): loss=3.6691391311477726\n",
      "Stochastic Gradient Descent(15310): loss=3.6520612112178723\n",
      "Stochastic Gradient Descent(15311): loss=13.018885750244742\n",
      "Stochastic Gradient Descent(15312): loss=12.05510066362633\n",
      "Stochastic Gradient Descent(15313): loss=6.093748003162624\n",
      "Stochastic Gradient Descent(15314): loss=1.9773041595351057\n",
      "Stochastic Gradient Descent(15315): loss=0.21339571682114283\n",
      "Stochastic Gradient Descent(15316): loss=1.275530587132443\n",
      "Stochastic Gradient Descent(15317): loss=2.2783029770822236\n",
      "Stochastic Gradient Descent(15318): loss=4.66302608949991\n",
      "Stochastic Gradient Descent(15319): loss=1.0490148716831684\n",
      "Stochastic Gradient Descent(15320): loss=0.8163979525684754\n",
      "Stochastic Gradient Descent(15321): loss=0.004155917700381094\n",
      "Stochastic Gradient Descent(15322): loss=1.1087671034273308\n",
      "Stochastic Gradient Descent(15323): loss=0.1441690821140031\n",
      "Stochastic Gradient Descent(15324): loss=1.0264482986705095\n",
      "Stochastic Gradient Descent(15325): loss=0.762350442323046\n",
      "Stochastic Gradient Descent(15326): loss=5.326102738819138\n",
      "Stochastic Gradient Descent(15327): loss=1.5365902516268124\n",
      "Stochastic Gradient Descent(15328): loss=0.06582966303798427\n",
      "Stochastic Gradient Descent(15329): loss=6.679204515542992\n",
      "Stochastic Gradient Descent(15330): loss=0.4045835315500286\n",
      "Stochastic Gradient Descent(15331): loss=5.266742475868749\n",
      "Stochastic Gradient Descent(15332): loss=0.0024139715380683473\n",
      "Stochastic Gradient Descent(15333): loss=0.9268325029482124\n",
      "Stochastic Gradient Descent(15334): loss=39.97761206424217\n",
      "Stochastic Gradient Descent(15335): loss=7.831501190230418\n",
      "Stochastic Gradient Descent(15336): loss=4.051032297126535\n",
      "Stochastic Gradient Descent(15337): loss=8.990302292304682\n",
      "Stochastic Gradient Descent(15338): loss=14.99264140749981\n",
      "Stochastic Gradient Descent(15339): loss=2.9495429945636555\n",
      "Stochastic Gradient Descent(15340): loss=2.618275406014309\n",
      "Stochastic Gradient Descent(15341): loss=0.17913505400695157\n",
      "Stochastic Gradient Descent(15342): loss=8.553523145018026\n",
      "Stochastic Gradient Descent(15343): loss=0.10235382633254983\n",
      "Stochastic Gradient Descent(15344): loss=13.246248557481524\n",
      "Stochastic Gradient Descent(15345): loss=5.701555207534777\n",
      "Stochastic Gradient Descent(15346): loss=2.592500783848382\n",
      "Stochastic Gradient Descent(15347): loss=0.00997841826674641\n",
      "Stochastic Gradient Descent(15348): loss=1.0785889863569578\n",
      "Stochastic Gradient Descent(15349): loss=0.0064432831195976896\n",
      "Stochastic Gradient Descent(15350): loss=0.22306122039062953\n",
      "Stochastic Gradient Descent(15351): loss=0.07974743067111328\n",
      "Stochastic Gradient Descent(15352): loss=10.149036213970064\n",
      "Stochastic Gradient Descent(15353): loss=1.7171576783475082\n",
      "Stochastic Gradient Descent(15354): loss=3.4151665619626077\n",
      "Stochastic Gradient Descent(15355): loss=1.1479851670082781\n",
      "Stochastic Gradient Descent(15356): loss=1.8075391982826565\n",
      "Stochastic Gradient Descent(15357): loss=4.168082465033856\n",
      "Stochastic Gradient Descent(15358): loss=1.1686085371823591\n",
      "Stochastic Gradient Descent(15359): loss=1.402137142442372\n",
      "Stochastic Gradient Descent(15360): loss=0.7971679383665964\n",
      "Stochastic Gradient Descent(15361): loss=2.308748385784888\n",
      "Stochastic Gradient Descent(15362): loss=28.91302844622716\n",
      "Stochastic Gradient Descent(15363): loss=0.39777473709448524\n",
      "Stochastic Gradient Descent(15364): loss=6.505252216910882\n",
      "Stochastic Gradient Descent(15365): loss=23.52061137229971\n",
      "Stochastic Gradient Descent(15366): loss=15.312805302326256\n",
      "Stochastic Gradient Descent(15367): loss=1.0292031749632566\n",
      "Stochastic Gradient Descent(15368): loss=0.8466240930719208\n",
      "Stochastic Gradient Descent(15369): loss=4.6907390932828505\n",
      "Stochastic Gradient Descent(15370): loss=5.494228840324632\n",
      "Stochastic Gradient Descent(15371): loss=1.4742078917314407\n",
      "Stochastic Gradient Descent(15372): loss=20.898370522476203\n",
      "Stochastic Gradient Descent(15373): loss=8.728083854932784\n",
      "Stochastic Gradient Descent(15374): loss=0.010401693762051205\n",
      "Stochastic Gradient Descent(15375): loss=9.342678410673461e-06\n",
      "Stochastic Gradient Descent(15376): loss=1.9874027382355859\n",
      "Stochastic Gradient Descent(15377): loss=48.33237233055377\n",
      "Stochastic Gradient Descent(15378): loss=1.5124300821719103\n",
      "Stochastic Gradient Descent(15379): loss=10.539084799765607\n",
      "Stochastic Gradient Descent(15380): loss=3.2259967930105704\n",
      "Stochastic Gradient Descent(15381): loss=12.126347732728098\n",
      "Stochastic Gradient Descent(15382): loss=13.949454519963604\n",
      "Stochastic Gradient Descent(15383): loss=8.075440504089729\n",
      "Stochastic Gradient Descent(15384): loss=2.3376481334026202\n",
      "Stochastic Gradient Descent(15385): loss=1.7730557277285688\n",
      "Stochastic Gradient Descent(15386): loss=3.676483517966272\n",
      "Stochastic Gradient Descent(15387): loss=0.30076281374648045\n",
      "Stochastic Gradient Descent(15388): loss=0.009008630837557997\n",
      "Stochastic Gradient Descent(15389): loss=3.5265823706961887\n",
      "Stochastic Gradient Descent(15390): loss=0.5406591951381036\n",
      "Stochastic Gradient Descent(15391): loss=2.7679842564498127\n",
      "Stochastic Gradient Descent(15392): loss=30.25579782874691\n",
      "Stochastic Gradient Descent(15393): loss=2.376235694166206\n",
      "Stochastic Gradient Descent(15394): loss=0.08748274567648631\n",
      "Stochastic Gradient Descent(15395): loss=5.638272972789861\n",
      "Stochastic Gradient Descent(15396): loss=10.724053647280543\n",
      "Stochastic Gradient Descent(15397): loss=2.2227558772268243\n",
      "Stochastic Gradient Descent(15398): loss=2.6226646885427094\n",
      "Stochastic Gradient Descent(15399): loss=0.714425847133537\n",
      "Stochastic Gradient Descent(15400): loss=2.712497698342897\n",
      "Stochastic Gradient Descent(15401): loss=23.14552289470413\n",
      "Stochastic Gradient Descent(15402): loss=0.3793927375533932\n",
      "Stochastic Gradient Descent(15403): loss=2.875554492653417\n",
      "Stochastic Gradient Descent(15404): loss=2.213329019631454\n",
      "Stochastic Gradient Descent(15405): loss=0.0005066489161582523\n",
      "Stochastic Gradient Descent(15406): loss=0.055898239067233325\n",
      "Stochastic Gradient Descent(15407): loss=0.09479703001326989\n",
      "Stochastic Gradient Descent(15408): loss=8.955479895903608\n",
      "Stochastic Gradient Descent(15409): loss=17.723265449031647\n",
      "Stochastic Gradient Descent(15410): loss=1.301978447312013\n",
      "Stochastic Gradient Descent(15411): loss=0.49784254509446274\n",
      "Stochastic Gradient Descent(15412): loss=1.7315774600349978\n",
      "Stochastic Gradient Descent(15413): loss=0.0831791609975893\n",
      "Stochastic Gradient Descent(15414): loss=0.09748717544707614\n",
      "Stochastic Gradient Descent(15415): loss=2.7303599482408374\n",
      "Stochastic Gradient Descent(15416): loss=14.007296815414866\n",
      "Stochastic Gradient Descent(15417): loss=1.0461480735199824\n",
      "Stochastic Gradient Descent(15418): loss=0.24777424105781254\n",
      "Stochastic Gradient Descent(15419): loss=3.597201766796879\n",
      "Stochastic Gradient Descent(15420): loss=22.09805534354143\n",
      "Stochastic Gradient Descent(15421): loss=5.802330856608665\n",
      "Stochastic Gradient Descent(15422): loss=7.40049151818478\n",
      "Stochastic Gradient Descent(15423): loss=1.8540943714581364\n",
      "Stochastic Gradient Descent(15424): loss=1.0466626798474987\n",
      "Stochastic Gradient Descent(15425): loss=1.8253467299701425\n",
      "Stochastic Gradient Descent(15426): loss=6.57484564797013\n",
      "Stochastic Gradient Descent(15427): loss=1.3150332723557265\n",
      "Stochastic Gradient Descent(15428): loss=13.049350090008698\n",
      "Stochastic Gradient Descent(15429): loss=0.19893895394281788\n",
      "Stochastic Gradient Descent(15430): loss=19.090196685597505\n",
      "Stochastic Gradient Descent(15431): loss=9.420690221812329\n",
      "Stochastic Gradient Descent(15432): loss=2.1173960653862025\n",
      "Stochastic Gradient Descent(15433): loss=6.569305581738469\n",
      "Stochastic Gradient Descent(15434): loss=5.490955717517943\n",
      "Stochastic Gradient Descent(15435): loss=1.124418074409685\n",
      "Stochastic Gradient Descent(15436): loss=14.890565894387958\n",
      "Stochastic Gradient Descent(15437): loss=0.02333789494185447\n",
      "Stochastic Gradient Descent(15438): loss=14.368082765858256\n",
      "Stochastic Gradient Descent(15439): loss=11.778163488261859\n",
      "Stochastic Gradient Descent(15440): loss=0.005438436885047929\n",
      "Stochastic Gradient Descent(15441): loss=23.003093256375255\n",
      "Stochastic Gradient Descent(15442): loss=0.47761719783125917\n",
      "Stochastic Gradient Descent(15443): loss=36.301211697998625\n",
      "Stochastic Gradient Descent(15444): loss=1.1372122264630289\n",
      "Stochastic Gradient Descent(15445): loss=0.39200554227643675\n",
      "Stochastic Gradient Descent(15446): loss=4.791743902136361\n",
      "Stochastic Gradient Descent(15447): loss=7.121545507833004\n",
      "Stochastic Gradient Descent(15448): loss=0.0029644475648547895\n",
      "Stochastic Gradient Descent(15449): loss=38.82700820672631\n",
      "Stochastic Gradient Descent(15450): loss=4.407966990873441\n",
      "Stochastic Gradient Descent(15451): loss=7.912671758821326\n",
      "Stochastic Gradient Descent(15452): loss=1.6304974713297542\n",
      "Stochastic Gradient Descent(15453): loss=0.0007974407210916722\n",
      "Stochastic Gradient Descent(15454): loss=0.1724514506099228\n",
      "Stochastic Gradient Descent(15455): loss=3.2053025355243396\n",
      "Stochastic Gradient Descent(15456): loss=3.260790776014288\n",
      "Stochastic Gradient Descent(15457): loss=0.020132604549998885\n",
      "Stochastic Gradient Descent(15458): loss=3.8876624633482826\n",
      "Stochastic Gradient Descent(15459): loss=0.25449602460094384\n",
      "Stochastic Gradient Descent(15460): loss=28.626560211440164\n",
      "Stochastic Gradient Descent(15461): loss=4.606643025848894\n",
      "Stochastic Gradient Descent(15462): loss=3.2559908829356736\n",
      "Stochastic Gradient Descent(15463): loss=4.133528203950071\n",
      "Stochastic Gradient Descent(15464): loss=23.883672460768526\n",
      "Stochastic Gradient Descent(15465): loss=19.148471103588324\n",
      "Stochastic Gradient Descent(15466): loss=2.1355656249911985\n",
      "Stochastic Gradient Descent(15467): loss=0.09866543809680606\n",
      "Stochastic Gradient Descent(15468): loss=1.0376345084111884\n",
      "Stochastic Gradient Descent(15469): loss=5.985686056055016\n",
      "Stochastic Gradient Descent(15470): loss=0.4264558823230694\n",
      "Stochastic Gradient Descent(15471): loss=0.0014178868895267415\n",
      "Stochastic Gradient Descent(15472): loss=0.4560532351303186\n",
      "Stochastic Gradient Descent(15473): loss=0.7187070139592191\n",
      "Stochastic Gradient Descent(15474): loss=2.855093205801893\n",
      "Stochastic Gradient Descent(15475): loss=28.94335474976889\n",
      "Stochastic Gradient Descent(15476): loss=1.229485336255661\n",
      "Stochastic Gradient Descent(15477): loss=0.09716855749694076\n",
      "Stochastic Gradient Descent(15478): loss=5.027632138553145\n",
      "Stochastic Gradient Descent(15479): loss=4.562839456075826\n",
      "Stochastic Gradient Descent(15480): loss=0.6301358590963201\n",
      "Stochastic Gradient Descent(15481): loss=5.54146849798869\n",
      "Stochastic Gradient Descent(15482): loss=2.32025817966349\n",
      "Stochastic Gradient Descent(15483): loss=18.923376079158164\n",
      "Stochastic Gradient Descent(15484): loss=6.777753548737231\n",
      "Stochastic Gradient Descent(15485): loss=0.3012705895196919\n",
      "Stochastic Gradient Descent(15486): loss=0.0013314990720402018\n",
      "Stochastic Gradient Descent(15487): loss=10.759839886365441\n",
      "Stochastic Gradient Descent(15488): loss=0.019266754367419515\n",
      "Stochastic Gradient Descent(15489): loss=0.17597423417970112\n",
      "Stochastic Gradient Descent(15490): loss=3.002770100589707\n",
      "Stochastic Gradient Descent(15491): loss=0.027929823502074694\n",
      "Stochastic Gradient Descent(15492): loss=0.004791034324955564\n",
      "Stochastic Gradient Descent(15493): loss=4.667107019218732\n",
      "Stochastic Gradient Descent(15494): loss=1.2276047991033119\n",
      "Stochastic Gradient Descent(15495): loss=4.769626359122483\n",
      "Stochastic Gradient Descent(15496): loss=0.07025405031934483\n",
      "Stochastic Gradient Descent(15497): loss=22.953986784042165\n",
      "Stochastic Gradient Descent(15498): loss=2.368691337254881\n",
      "Stochastic Gradient Descent(15499): loss=34.55366054089199\n",
      "Stochastic Gradient Descent(15500): loss=5.38755458527459\n",
      "Stochastic Gradient Descent(15501): loss=0.3568102914965153\n",
      "Stochastic Gradient Descent(15502): loss=0.4371549021227626\n",
      "Stochastic Gradient Descent(15503): loss=0.4154259194284922\n",
      "Stochastic Gradient Descent(15504): loss=12.445632151280119\n",
      "Stochastic Gradient Descent(15505): loss=3.2158251476505098\n",
      "Stochastic Gradient Descent(15506): loss=6.932474264679094\n",
      "Stochastic Gradient Descent(15507): loss=8.977816018532895\n",
      "Stochastic Gradient Descent(15508): loss=4.853226338307948\n",
      "Stochastic Gradient Descent(15509): loss=4.024232219224394\n",
      "Stochastic Gradient Descent(15510): loss=16.321091797904437\n",
      "Stochastic Gradient Descent(15511): loss=19.22065989284189\n",
      "Stochastic Gradient Descent(15512): loss=14.523902356475554\n",
      "Stochastic Gradient Descent(15513): loss=3.2314937693367756\n",
      "Stochastic Gradient Descent(15514): loss=1.74766046780919\n",
      "Stochastic Gradient Descent(15515): loss=18.52822174974955\n",
      "Stochastic Gradient Descent(15516): loss=32.419633433665034\n",
      "Stochastic Gradient Descent(15517): loss=5.678473355959191\n",
      "Stochastic Gradient Descent(15518): loss=7.3382062458389825\n",
      "Stochastic Gradient Descent(15519): loss=0.16196534568982732\n",
      "Stochastic Gradient Descent(15520): loss=8.713976331090374\n",
      "Stochastic Gradient Descent(15521): loss=0.056337179819350464\n",
      "Stochastic Gradient Descent(15522): loss=1.8946116036730107\n",
      "Stochastic Gradient Descent(15523): loss=0.580505318450327\n",
      "Stochastic Gradient Descent(15524): loss=2.123830948661962\n",
      "Stochastic Gradient Descent(15525): loss=22.7467390731861\n",
      "Stochastic Gradient Descent(15526): loss=12.81412117366728\n",
      "Stochastic Gradient Descent(15527): loss=21.64149796287528\n",
      "Stochastic Gradient Descent(15528): loss=2.478421380896221\n",
      "Stochastic Gradient Descent(15529): loss=8.082009549607072\n",
      "Stochastic Gradient Descent(15530): loss=0.27948941740744143\n",
      "Stochastic Gradient Descent(15531): loss=1.2459919914966435\n",
      "Stochastic Gradient Descent(15532): loss=3.976648567980816\n",
      "Stochastic Gradient Descent(15533): loss=1.7676500741522552\n",
      "Stochastic Gradient Descent(15534): loss=2.313974865137099\n",
      "Stochastic Gradient Descent(15535): loss=2.5893801150759055\n",
      "Stochastic Gradient Descent(15536): loss=0.049627825649941185\n",
      "Stochastic Gradient Descent(15537): loss=1.6407486239613809\n",
      "Stochastic Gradient Descent(15538): loss=1.4018853084485716\n",
      "Stochastic Gradient Descent(15539): loss=4.4163933429687665\n",
      "Stochastic Gradient Descent(15540): loss=4.475692578749476\n",
      "Stochastic Gradient Descent(15541): loss=2.4721316516999274\n",
      "Stochastic Gradient Descent(15542): loss=0.29979186110359346\n",
      "Stochastic Gradient Descent(15543): loss=0.12424368684672242\n",
      "Stochastic Gradient Descent(15544): loss=10.671236790800016\n",
      "Stochastic Gradient Descent(15545): loss=0.001601569221374805\n",
      "Stochastic Gradient Descent(15546): loss=0.8615499324152737\n",
      "Stochastic Gradient Descent(15547): loss=4.393728233455055\n",
      "Stochastic Gradient Descent(15548): loss=1.8801474961135085\n",
      "Stochastic Gradient Descent(15549): loss=16.96869336983235\n",
      "Stochastic Gradient Descent(15550): loss=1.3392453085043965\n",
      "Stochastic Gradient Descent(15551): loss=0.5270919198209846\n",
      "Stochastic Gradient Descent(15552): loss=8.434771230208026\n",
      "Stochastic Gradient Descent(15553): loss=1.217716022549468\n",
      "Stochastic Gradient Descent(15554): loss=1.9254773970014267\n",
      "Stochastic Gradient Descent(15555): loss=21.354120834522323\n",
      "Stochastic Gradient Descent(15556): loss=38.981121591372556\n",
      "Stochastic Gradient Descent(15557): loss=2.758485288032332\n",
      "Stochastic Gradient Descent(15558): loss=0.7673105092171544\n",
      "Stochastic Gradient Descent(15559): loss=14.892266539902861\n",
      "Stochastic Gradient Descent(15560): loss=0.9083756687007873\n",
      "Stochastic Gradient Descent(15561): loss=14.994958179990762\n",
      "Stochastic Gradient Descent(15562): loss=2.258516583650255\n",
      "Stochastic Gradient Descent(15563): loss=1.078813815777958\n",
      "Stochastic Gradient Descent(15564): loss=0.5248561165125226\n",
      "Stochastic Gradient Descent(15565): loss=0.47813164505170036\n",
      "Stochastic Gradient Descent(15566): loss=6.222091276306565\n",
      "Stochastic Gradient Descent(15567): loss=3.249898225023689\n",
      "Stochastic Gradient Descent(15568): loss=13.99959499543769\n",
      "Stochastic Gradient Descent(15569): loss=0.42671900605387747\n",
      "Stochastic Gradient Descent(15570): loss=0.03755994970379302\n",
      "Stochastic Gradient Descent(15571): loss=1.5233243298514292\n",
      "Stochastic Gradient Descent(15572): loss=0.2766964164373617\n",
      "Stochastic Gradient Descent(15573): loss=29.8219721870762\n",
      "Stochastic Gradient Descent(15574): loss=1.9383045414034445\n",
      "Stochastic Gradient Descent(15575): loss=25.8516791107557\n",
      "Stochastic Gradient Descent(15576): loss=1.7351722594605943\n",
      "Stochastic Gradient Descent(15577): loss=0.22459320629966378\n",
      "Stochastic Gradient Descent(15578): loss=22.201694728220772\n",
      "Stochastic Gradient Descent(15579): loss=0.4327621541129665\n",
      "Stochastic Gradient Descent(15580): loss=3.015719617950181\n",
      "Stochastic Gradient Descent(15581): loss=2.374261411034482\n",
      "Stochastic Gradient Descent(15582): loss=0.8173686553599426\n",
      "Stochastic Gradient Descent(15583): loss=8.16934221409427\n",
      "Stochastic Gradient Descent(15584): loss=42.08548631600175\n",
      "Stochastic Gradient Descent(15585): loss=3.2736463523441652\n",
      "Stochastic Gradient Descent(15586): loss=2.8648787790294326\n",
      "Stochastic Gradient Descent(15587): loss=5.138644868050941\n",
      "Stochastic Gradient Descent(15588): loss=0.04715382463422782\n",
      "Stochastic Gradient Descent(15589): loss=4.855153415818749\n",
      "Stochastic Gradient Descent(15590): loss=4.315087822030762\n",
      "Stochastic Gradient Descent(15591): loss=24.80240581227328\n",
      "Stochastic Gradient Descent(15592): loss=9.577507460349644\n",
      "Stochastic Gradient Descent(15593): loss=0.0833577256700386\n",
      "Stochastic Gradient Descent(15594): loss=5.852592580679859\n",
      "Stochastic Gradient Descent(15595): loss=6.496853118127955\n",
      "Stochastic Gradient Descent(15596): loss=3.095266742585571\n",
      "Stochastic Gradient Descent(15597): loss=1.2152303166097675\n",
      "Stochastic Gradient Descent(15598): loss=16.195971674588204\n",
      "Stochastic Gradient Descent(15599): loss=0.07251769490236018\n",
      "Stochastic Gradient Descent(15600): loss=7.020064710292595\n",
      "Stochastic Gradient Descent(15601): loss=4.005922017809704\n",
      "Stochastic Gradient Descent(15602): loss=50.72209344974637\n",
      "Stochastic Gradient Descent(15603): loss=11.239116178966123\n",
      "Stochastic Gradient Descent(15604): loss=3.296632948986078\n",
      "Stochastic Gradient Descent(15605): loss=22.430427884208143\n",
      "Stochastic Gradient Descent(15606): loss=12.316493252719276\n",
      "Stochastic Gradient Descent(15607): loss=16.49693158936906\n",
      "Stochastic Gradient Descent(15608): loss=1.065721514108001\n",
      "Stochastic Gradient Descent(15609): loss=10.137414917739102\n",
      "Stochastic Gradient Descent(15610): loss=10.205308446475128\n",
      "Stochastic Gradient Descent(15611): loss=5.542588891255233\n",
      "Stochastic Gradient Descent(15612): loss=0.9138754247311047\n",
      "Stochastic Gradient Descent(15613): loss=0.3929336595916571\n",
      "Stochastic Gradient Descent(15614): loss=5.092113106785764\n",
      "Stochastic Gradient Descent(15615): loss=3.894872042092038\n",
      "Stochastic Gradient Descent(15616): loss=4.818186663355949\n",
      "Stochastic Gradient Descent(15617): loss=12.354997086568012\n",
      "Stochastic Gradient Descent(15618): loss=1.742905248686373\n",
      "Stochastic Gradient Descent(15619): loss=12.764713261909627\n",
      "Stochastic Gradient Descent(15620): loss=0.9442579440340055\n",
      "Stochastic Gradient Descent(15621): loss=0.5990538456780008\n",
      "Stochastic Gradient Descent(15622): loss=7.177847297979862\n",
      "Stochastic Gradient Descent(15623): loss=3.070671127545295\n",
      "Stochastic Gradient Descent(15624): loss=3.0920042981568763\n",
      "Stochastic Gradient Descent(15625): loss=0.41155658880788765\n",
      "Stochastic Gradient Descent(15626): loss=14.172842024408942\n",
      "Stochastic Gradient Descent(15627): loss=0.17973383975129337\n",
      "Stochastic Gradient Descent(15628): loss=6.634590185049568\n",
      "Stochastic Gradient Descent(15629): loss=11.53506603820867\n",
      "Stochastic Gradient Descent(15630): loss=0.5345901570750345\n",
      "Stochastic Gradient Descent(15631): loss=0.3311200521935447\n",
      "Stochastic Gradient Descent(15632): loss=1.006005462601905\n",
      "Stochastic Gradient Descent(15633): loss=6.525583896597682\n",
      "Stochastic Gradient Descent(15634): loss=1.793578034889351\n",
      "Stochastic Gradient Descent(15635): loss=0.9181178585692552\n",
      "Stochastic Gradient Descent(15636): loss=0.7274071899427276\n",
      "Stochastic Gradient Descent(15637): loss=0.18035535129024063\n",
      "Stochastic Gradient Descent(15638): loss=14.176864643919968\n",
      "Stochastic Gradient Descent(15639): loss=12.16045798588282\n",
      "Stochastic Gradient Descent(15640): loss=8.78082628867359\n",
      "Stochastic Gradient Descent(15641): loss=0.1926510091255854\n",
      "Stochastic Gradient Descent(15642): loss=0.25470445629250454\n",
      "Stochastic Gradient Descent(15643): loss=0.8986405129798966\n",
      "Stochastic Gradient Descent(15644): loss=3.0201105469385525\n",
      "Stochastic Gradient Descent(15645): loss=0.7253069769043058\n",
      "Stochastic Gradient Descent(15646): loss=0.13671146524405395\n",
      "Stochastic Gradient Descent(15647): loss=0.02820144856662515\n",
      "Stochastic Gradient Descent(15648): loss=1.251069388443503\n",
      "Stochastic Gradient Descent(15649): loss=31.994395059176817\n",
      "Stochastic Gradient Descent(15650): loss=3.1511759167012583\n",
      "Stochastic Gradient Descent(15651): loss=0.8010901874741335\n",
      "Stochastic Gradient Descent(15652): loss=3.601969561673739\n",
      "Stochastic Gradient Descent(15653): loss=6.592461204810832\n",
      "Stochastic Gradient Descent(15654): loss=3.167809179853113\n",
      "Stochastic Gradient Descent(15655): loss=0.15585545154759894\n",
      "Stochastic Gradient Descent(15656): loss=0.9788739047296936\n",
      "Stochastic Gradient Descent(15657): loss=0.017477332898600074\n",
      "Stochastic Gradient Descent(15658): loss=0.3093683604117464\n",
      "Stochastic Gradient Descent(15659): loss=36.46876850200148\n",
      "Stochastic Gradient Descent(15660): loss=0.5437251718308567\n",
      "Stochastic Gradient Descent(15661): loss=0.0017508345318764374\n",
      "Stochastic Gradient Descent(15662): loss=6.4957516020474175\n",
      "Stochastic Gradient Descent(15663): loss=2.241208805044292\n",
      "Stochastic Gradient Descent(15664): loss=0.0006296912095801416\n",
      "Stochastic Gradient Descent(15665): loss=1.726770396523341\n",
      "Stochastic Gradient Descent(15666): loss=3.9797991641931127\n",
      "Stochastic Gradient Descent(15667): loss=0.018001752581416938\n",
      "Stochastic Gradient Descent(15668): loss=0.7652625472121187\n",
      "Stochastic Gradient Descent(15669): loss=2.468358873177353\n",
      "Stochastic Gradient Descent(15670): loss=4.122666066316419\n",
      "Stochastic Gradient Descent(15671): loss=0.5106304934088326\n",
      "Stochastic Gradient Descent(15672): loss=2.9911035694056096\n",
      "Stochastic Gradient Descent(15673): loss=0.09925907520159284\n",
      "Stochastic Gradient Descent(15674): loss=34.62356654712695\n",
      "Stochastic Gradient Descent(15675): loss=0.7365589109062213\n",
      "Stochastic Gradient Descent(15676): loss=2.50527733286613\n",
      "Stochastic Gradient Descent(15677): loss=0.32623796546217476\n",
      "Stochastic Gradient Descent(15678): loss=4.501998685622815\n",
      "Stochastic Gradient Descent(15679): loss=8.892491085184654\n",
      "Stochastic Gradient Descent(15680): loss=2.706134211192452\n",
      "Stochastic Gradient Descent(15681): loss=20.482606397088215\n",
      "Stochastic Gradient Descent(15682): loss=8.880957709259699\n",
      "Stochastic Gradient Descent(15683): loss=9.88648787822053\n",
      "Stochastic Gradient Descent(15684): loss=0.19399790180839818\n",
      "Stochastic Gradient Descent(15685): loss=1.0320058049544334\n",
      "Stochastic Gradient Descent(15686): loss=1.983208344582866\n",
      "Stochastic Gradient Descent(15687): loss=2.4343053949552638\n",
      "Stochastic Gradient Descent(15688): loss=2.6019967749660315\n",
      "Stochastic Gradient Descent(15689): loss=9.575290906915315\n",
      "Stochastic Gradient Descent(15690): loss=2.16297724323201\n",
      "Stochastic Gradient Descent(15691): loss=3.8314775810681185\n",
      "Stochastic Gradient Descent(15692): loss=4.568533708114137\n",
      "Stochastic Gradient Descent(15693): loss=16.82759923155815\n",
      "Stochastic Gradient Descent(15694): loss=7.764143401554046\n",
      "Stochastic Gradient Descent(15695): loss=2.1564966178089544\n",
      "Stochastic Gradient Descent(15696): loss=11.485030478528433\n",
      "Stochastic Gradient Descent(15697): loss=27.076450154314802\n",
      "Stochastic Gradient Descent(15698): loss=199.90169871628527\n",
      "Stochastic Gradient Descent(15699): loss=4.676128507928734\n",
      "Stochastic Gradient Descent(15700): loss=31.112783709426868\n",
      "Stochastic Gradient Descent(15701): loss=0.023553298985259187\n",
      "Stochastic Gradient Descent(15702): loss=2.3604138155417997\n",
      "Stochastic Gradient Descent(15703): loss=11.59081966215955\n",
      "Stochastic Gradient Descent(15704): loss=14.406226670458564\n",
      "Stochastic Gradient Descent(15705): loss=0.03763627340858438\n",
      "Stochastic Gradient Descent(15706): loss=5.250620284249263\n",
      "Stochastic Gradient Descent(15707): loss=0.9793146354419059\n",
      "Stochastic Gradient Descent(15708): loss=5.2102352261142775\n",
      "Stochastic Gradient Descent(15709): loss=6.144542186620612\n",
      "Stochastic Gradient Descent(15710): loss=0.0050117630637634275\n",
      "Stochastic Gradient Descent(15711): loss=1.4054503332147341\n",
      "Stochastic Gradient Descent(15712): loss=0.005108337096600239\n",
      "Stochastic Gradient Descent(15713): loss=1.9131684690334203\n",
      "Stochastic Gradient Descent(15714): loss=0.0016947234271969744\n",
      "Stochastic Gradient Descent(15715): loss=0.05236913442041521\n",
      "Stochastic Gradient Descent(15716): loss=4.891346684116323\n",
      "Stochastic Gradient Descent(15717): loss=5.757873089768957\n",
      "Stochastic Gradient Descent(15718): loss=5.076421322619599\n",
      "Stochastic Gradient Descent(15719): loss=4.387787297713103\n",
      "Stochastic Gradient Descent(15720): loss=5.578157047041827\n",
      "Stochastic Gradient Descent(15721): loss=4.861407222454523\n",
      "Stochastic Gradient Descent(15722): loss=7.527278546028941\n",
      "Stochastic Gradient Descent(15723): loss=0.7475117566183904\n",
      "Stochastic Gradient Descent(15724): loss=3.3439805157172646\n",
      "Stochastic Gradient Descent(15725): loss=0.1078804108713146\n",
      "Stochastic Gradient Descent(15726): loss=2.174112530005595\n",
      "Stochastic Gradient Descent(15727): loss=8.974044872135615\n",
      "Stochastic Gradient Descent(15728): loss=10.264650548567403\n",
      "Stochastic Gradient Descent(15729): loss=0.09748625151921386\n",
      "Stochastic Gradient Descent(15730): loss=0.42860368028235435\n",
      "Stochastic Gradient Descent(15731): loss=1.2258896371279093\n",
      "Stochastic Gradient Descent(15732): loss=3.04555472804466\n",
      "Stochastic Gradient Descent(15733): loss=1.1133697700892309\n",
      "Stochastic Gradient Descent(15734): loss=23.897519656937888\n",
      "Stochastic Gradient Descent(15735): loss=3.0291733716670413\n",
      "Stochastic Gradient Descent(15736): loss=0.6165329602901422\n",
      "Stochastic Gradient Descent(15737): loss=0.11413681207820237\n",
      "Stochastic Gradient Descent(15738): loss=0.7775176393144783\n",
      "Stochastic Gradient Descent(15739): loss=0.9189898253828231\n",
      "Stochastic Gradient Descent(15740): loss=6.743641012069924\n",
      "Stochastic Gradient Descent(15741): loss=3.953425786733024\n",
      "Stochastic Gradient Descent(15742): loss=17.180796874632396\n",
      "Stochastic Gradient Descent(15743): loss=14.265396433076473\n",
      "Stochastic Gradient Descent(15744): loss=0.38422449183816787\n",
      "Stochastic Gradient Descent(15745): loss=6.642585876697769\n",
      "Stochastic Gradient Descent(15746): loss=6.350426632084897\n",
      "Stochastic Gradient Descent(15747): loss=0.1569554663669621\n",
      "Stochastic Gradient Descent(15748): loss=10.20298117337671\n",
      "Stochastic Gradient Descent(15749): loss=1.1964637852613393\n",
      "Stochastic Gradient Descent(15750): loss=0.37251350321340404\n",
      "Stochastic Gradient Descent(15751): loss=1.625582246142704\n",
      "Stochastic Gradient Descent(15752): loss=0.8042221033752333\n",
      "Stochastic Gradient Descent(15753): loss=0.518789286926549\n",
      "Stochastic Gradient Descent(15754): loss=1.7219664029545052\n",
      "Stochastic Gradient Descent(15755): loss=0.9029414748146859\n",
      "Stochastic Gradient Descent(15756): loss=1.2094043112387955\n",
      "Stochastic Gradient Descent(15757): loss=9.015411254378732\n",
      "Stochastic Gradient Descent(15758): loss=0.04461516269164826\n",
      "Stochastic Gradient Descent(15759): loss=1.2658091054639613\n",
      "Stochastic Gradient Descent(15760): loss=4.252200936601775\n",
      "Stochastic Gradient Descent(15761): loss=7.8829612877537585\n",
      "Stochastic Gradient Descent(15762): loss=0.2033724655654477\n",
      "Stochastic Gradient Descent(15763): loss=0.1976864831704052\n",
      "Stochastic Gradient Descent(15764): loss=6.087017445784576\n",
      "Stochastic Gradient Descent(15765): loss=2.901227188434797\n",
      "Stochastic Gradient Descent(15766): loss=3.860641569732532\n",
      "Stochastic Gradient Descent(15767): loss=3.1767344511215287\n",
      "Stochastic Gradient Descent(15768): loss=17.024349764560508\n",
      "Stochastic Gradient Descent(15769): loss=6.9800097468787135\n",
      "Stochastic Gradient Descent(15770): loss=3.641284175567676\n",
      "Stochastic Gradient Descent(15771): loss=3.692427987745885\n",
      "Stochastic Gradient Descent(15772): loss=10.29186198899021\n",
      "Stochastic Gradient Descent(15773): loss=2.008868797898343\n",
      "Stochastic Gradient Descent(15774): loss=0.5651026651571374\n",
      "Stochastic Gradient Descent(15775): loss=3.583312848366913\n",
      "Stochastic Gradient Descent(15776): loss=15.590869704860687\n",
      "Stochastic Gradient Descent(15777): loss=3.0125811093723565\n",
      "Stochastic Gradient Descent(15778): loss=2.1022059437277996\n",
      "Stochastic Gradient Descent(15779): loss=0.13847949757832356\n",
      "Stochastic Gradient Descent(15780): loss=0.023650520237224498\n",
      "Stochastic Gradient Descent(15781): loss=37.607154835751345\n",
      "Stochastic Gradient Descent(15782): loss=1.8419099394099994\n",
      "Stochastic Gradient Descent(15783): loss=12.58439267574079\n",
      "Stochastic Gradient Descent(15784): loss=2.4842081400444918\n",
      "Stochastic Gradient Descent(15785): loss=0.6556080127351189\n",
      "Stochastic Gradient Descent(15786): loss=1.034973699294331\n",
      "Stochastic Gradient Descent(15787): loss=4.783050747465575\n",
      "Stochastic Gradient Descent(15788): loss=3.884140791052637\n",
      "Stochastic Gradient Descent(15789): loss=6.945905264604977\n",
      "Stochastic Gradient Descent(15790): loss=0.39105762730766486\n",
      "Stochastic Gradient Descent(15791): loss=6.901972927286056\n",
      "Stochastic Gradient Descent(15792): loss=33.8427481277488\n",
      "Stochastic Gradient Descent(15793): loss=3.0889246442247584\n",
      "Stochastic Gradient Descent(15794): loss=1.4334225403793825\n",
      "Stochastic Gradient Descent(15795): loss=3.345224109598358\n",
      "Stochastic Gradient Descent(15796): loss=7.594040908186892\n",
      "Stochastic Gradient Descent(15797): loss=3.5099647918829997\n",
      "Stochastic Gradient Descent(15798): loss=2.914988808342232\n",
      "Stochastic Gradient Descent(15799): loss=0.03601322240489511\n",
      "Stochastic Gradient Descent(15800): loss=0.559341694202728\n",
      "Stochastic Gradient Descent(15801): loss=22.77671644007152\n",
      "Stochastic Gradient Descent(15802): loss=2.768898494028508\n",
      "Stochastic Gradient Descent(15803): loss=0.04367671698128983\n",
      "Stochastic Gradient Descent(15804): loss=0.9229361880084248\n",
      "Stochastic Gradient Descent(15805): loss=0.13959999586770305\n",
      "Stochastic Gradient Descent(15806): loss=1.4824204854903642\n",
      "Stochastic Gradient Descent(15807): loss=7.9050381725020715\n",
      "Stochastic Gradient Descent(15808): loss=0.03134916096851429\n",
      "Stochastic Gradient Descent(15809): loss=0.7689385198043364\n",
      "Stochastic Gradient Descent(15810): loss=15.007012951468273\n",
      "Stochastic Gradient Descent(15811): loss=5.5710927294345165\n",
      "Stochastic Gradient Descent(15812): loss=2.254505605543154\n",
      "Stochastic Gradient Descent(15813): loss=2.432776118890034\n",
      "Stochastic Gradient Descent(15814): loss=3.0227006500239595\n",
      "Stochastic Gradient Descent(15815): loss=5.620968775650732\n",
      "Stochastic Gradient Descent(15816): loss=9.386382431424444\n",
      "Stochastic Gradient Descent(15817): loss=1.3413235961782413\n",
      "Stochastic Gradient Descent(15818): loss=12.342613747493148\n",
      "Stochastic Gradient Descent(15819): loss=0.6753036960539243\n",
      "Stochastic Gradient Descent(15820): loss=4.029029628677546\n",
      "Stochastic Gradient Descent(15821): loss=0.47274020850629717\n",
      "Stochastic Gradient Descent(15822): loss=9.929514007183068\n",
      "Stochastic Gradient Descent(15823): loss=1.693928642779383\n",
      "Stochastic Gradient Descent(15824): loss=10.349471537154756\n",
      "Stochastic Gradient Descent(15825): loss=7.687443524621853\n",
      "Stochastic Gradient Descent(15826): loss=22.274248498670676\n",
      "Stochastic Gradient Descent(15827): loss=1.9694937235256078\n",
      "Stochastic Gradient Descent(15828): loss=8.884077996225654\n",
      "Stochastic Gradient Descent(15829): loss=0.23592197133238435\n",
      "Stochastic Gradient Descent(15830): loss=9.153076099282178\n",
      "Stochastic Gradient Descent(15831): loss=2.2326105374522762\n",
      "Stochastic Gradient Descent(15832): loss=0.18833604416262406\n",
      "Stochastic Gradient Descent(15833): loss=2.453213018235124\n",
      "Stochastic Gradient Descent(15834): loss=11.510206355177072\n",
      "Stochastic Gradient Descent(15835): loss=4.429934996017052\n",
      "Stochastic Gradient Descent(15836): loss=0.12149442279862031\n",
      "Stochastic Gradient Descent(15837): loss=2.8406843614443527\n",
      "Stochastic Gradient Descent(15838): loss=0.3563854526861078\n",
      "Stochastic Gradient Descent(15839): loss=0.42172849801475504\n",
      "Stochastic Gradient Descent(15840): loss=6.908149925103691\n",
      "Stochastic Gradient Descent(15841): loss=69.80798576744647\n",
      "Stochastic Gradient Descent(15842): loss=14.001372056860651\n",
      "Stochastic Gradient Descent(15843): loss=0.24020335404034493\n",
      "Stochastic Gradient Descent(15844): loss=3.76740746634424\n",
      "Stochastic Gradient Descent(15845): loss=4.20727477655482\n",
      "Stochastic Gradient Descent(15846): loss=0.35496357825973246\n",
      "Stochastic Gradient Descent(15847): loss=1.0323565745743148\n",
      "Stochastic Gradient Descent(15848): loss=8.162699992544976\n",
      "Stochastic Gradient Descent(15849): loss=0.7113455293608898\n",
      "Stochastic Gradient Descent(15850): loss=3.560371310233829\n",
      "Stochastic Gradient Descent(15851): loss=0.4073735850540921\n",
      "Stochastic Gradient Descent(15852): loss=22.20678288833666\n",
      "Stochastic Gradient Descent(15853): loss=0.23412337764906718\n",
      "Stochastic Gradient Descent(15854): loss=7.343515529304643\n",
      "Stochastic Gradient Descent(15855): loss=1.1331358850354875e-05\n",
      "Stochastic Gradient Descent(15856): loss=0.025985698854376484\n",
      "Stochastic Gradient Descent(15857): loss=8.833758510075237\n",
      "Stochastic Gradient Descent(15858): loss=6.31751423036155\n",
      "Stochastic Gradient Descent(15859): loss=11.062914114249185\n",
      "Stochastic Gradient Descent(15860): loss=2.789924021080121\n",
      "Stochastic Gradient Descent(15861): loss=0.5974541276273463\n",
      "Stochastic Gradient Descent(15862): loss=7.891443176916275\n",
      "Stochastic Gradient Descent(15863): loss=0.5928027930036271\n",
      "Stochastic Gradient Descent(15864): loss=0.1348154845889921\n",
      "Stochastic Gradient Descent(15865): loss=2.953120261059499\n",
      "Stochastic Gradient Descent(15866): loss=5.243980991140012\n",
      "Stochastic Gradient Descent(15867): loss=6.615002377331977\n",
      "Stochastic Gradient Descent(15868): loss=2.6981103075149653\n",
      "Stochastic Gradient Descent(15869): loss=0.1133324181669766\n",
      "Stochastic Gradient Descent(15870): loss=1.6331781356162505\n",
      "Stochastic Gradient Descent(15871): loss=38.01572826623079\n",
      "Stochastic Gradient Descent(15872): loss=0.8106233438846369\n",
      "Stochastic Gradient Descent(15873): loss=30.785013760140014\n",
      "Stochastic Gradient Descent(15874): loss=0.03248844387792532\n",
      "Stochastic Gradient Descent(15875): loss=13.205484332811775\n",
      "Stochastic Gradient Descent(15876): loss=0.045260150134748066\n",
      "Stochastic Gradient Descent(15877): loss=0.4255970007799615\n",
      "Stochastic Gradient Descent(15878): loss=4.8274643856280735\n",
      "Stochastic Gradient Descent(15879): loss=16.584477350449905\n",
      "Stochastic Gradient Descent(15880): loss=0.5130172485633391\n",
      "Stochastic Gradient Descent(15881): loss=3.986204647773017\n",
      "Stochastic Gradient Descent(15882): loss=10.471401552313122\n",
      "Stochastic Gradient Descent(15883): loss=8.914339677152523\n",
      "Stochastic Gradient Descent(15884): loss=6.243066548204972\n",
      "Stochastic Gradient Descent(15885): loss=0.5845160178092577\n",
      "Stochastic Gradient Descent(15886): loss=18.51631445914779\n",
      "Stochastic Gradient Descent(15887): loss=0.755804654896589\n",
      "Stochastic Gradient Descent(15888): loss=0.552506814977851\n",
      "Stochastic Gradient Descent(15889): loss=0.32467411736036794\n",
      "Stochastic Gradient Descent(15890): loss=9.618023935115328\n",
      "Stochastic Gradient Descent(15891): loss=2.5756982342983785\n",
      "Stochastic Gradient Descent(15892): loss=0.4467130719239672\n",
      "Stochastic Gradient Descent(15893): loss=3.8097034884267185\n",
      "Stochastic Gradient Descent(15894): loss=4.37472464142262\n",
      "Stochastic Gradient Descent(15895): loss=2.9400992180036503\n",
      "Stochastic Gradient Descent(15896): loss=0.005669642876470902\n",
      "Stochastic Gradient Descent(15897): loss=3.3063043673837904\n",
      "Stochastic Gradient Descent(15898): loss=4.580715430253667\n",
      "Stochastic Gradient Descent(15899): loss=0.5419591753282822\n",
      "Stochastic Gradient Descent(15900): loss=0.04417120280452636\n",
      "Stochastic Gradient Descent(15901): loss=0.17531553308649186\n",
      "Stochastic Gradient Descent(15902): loss=13.464945377316589\n",
      "Stochastic Gradient Descent(15903): loss=5.747864058044177\n",
      "Stochastic Gradient Descent(15904): loss=2.668716451789771\n",
      "Stochastic Gradient Descent(15905): loss=11.015401334343494\n",
      "Stochastic Gradient Descent(15906): loss=0.06659299690742483\n",
      "Stochastic Gradient Descent(15907): loss=3.0848132775179677\n",
      "Stochastic Gradient Descent(15908): loss=15.244834575987376\n",
      "Stochastic Gradient Descent(15909): loss=0.010112238605000115\n",
      "Stochastic Gradient Descent(15910): loss=1.5165378238695888\n",
      "Stochastic Gradient Descent(15911): loss=1.5308845021423685\n",
      "Stochastic Gradient Descent(15912): loss=0.7372906654807139\n",
      "Stochastic Gradient Descent(15913): loss=0.05469476620870904\n",
      "Stochastic Gradient Descent(15914): loss=2.066491522983071\n",
      "Stochastic Gradient Descent(15915): loss=2.1300488095690877\n",
      "Stochastic Gradient Descent(15916): loss=7.470263729460574\n",
      "Stochastic Gradient Descent(15917): loss=19.888131969067285\n",
      "Stochastic Gradient Descent(15918): loss=1.3737952414701102\n",
      "Stochastic Gradient Descent(15919): loss=1.3515751865799583\n",
      "Stochastic Gradient Descent(15920): loss=17.31935122753517\n",
      "Stochastic Gradient Descent(15921): loss=2.50632105777396\n",
      "Stochastic Gradient Descent(15922): loss=0.0925307373382085\n",
      "Stochastic Gradient Descent(15923): loss=4.06911203950442\n",
      "Stochastic Gradient Descent(15924): loss=5.326426243401289\n",
      "Stochastic Gradient Descent(15925): loss=0.28513006978863936\n",
      "Stochastic Gradient Descent(15926): loss=0.7482140797362945\n",
      "Stochastic Gradient Descent(15927): loss=0.6016096960367594\n",
      "Stochastic Gradient Descent(15928): loss=0.5630850408160316\n",
      "Stochastic Gradient Descent(15929): loss=0.6579432889561285\n",
      "Stochastic Gradient Descent(15930): loss=5.123283181756894\n",
      "Stochastic Gradient Descent(15931): loss=0.06411299918201313\n",
      "Stochastic Gradient Descent(15932): loss=2.9702282108324365\n",
      "Stochastic Gradient Descent(15933): loss=0.11234116688165773\n",
      "Stochastic Gradient Descent(15934): loss=8.907029672815183\n",
      "Stochastic Gradient Descent(15935): loss=1.6882046049182766\n",
      "Stochastic Gradient Descent(15936): loss=22.74764491097908\n",
      "Stochastic Gradient Descent(15937): loss=2.370414391963427\n",
      "Stochastic Gradient Descent(15938): loss=0.39553583766342915\n",
      "Stochastic Gradient Descent(15939): loss=0.5575937278420106\n",
      "Stochastic Gradient Descent(15940): loss=18.176023104014533\n",
      "Stochastic Gradient Descent(15941): loss=0.6179936456202343\n",
      "Stochastic Gradient Descent(15942): loss=8.558091319497864\n",
      "Stochastic Gradient Descent(15943): loss=6.373432929416948\n",
      "Stochastic Gradient Descent(15944): loss=0.7261882720516322\n",
      "Stochastic Gradient Descent(15945): loss=16.339288415746296\n",
      "Stochastic Gradient Descent(15946): loss=0.33198968105458126\n",
      "Stochastic Gradient Descent(15947): loss=0.49836178930392316\n",
      "Stochastic Gradient Descent(15948): loss=0.19050630427957174\n",
      "Stochastic Gradient Descent(15949): loss=0.2469481418318318\n",
      "Stochastic Gradient Descent(15950): loss=1.3117912862321828\n",
      "Stochastic Gradient Descent(15951): loss=0.9193677353272504\n",
      "Stochastic Gradient Descent(15952): loss=0.01527644953310579\n",
      "Stochastic Gradient Descent(15953): loss=6.13448272382551\n",
      "Stochastic Gradient Descent(15954): loss=0.07712053300578343\n",
      "Stochastic Gradient Descent(15955): loss=3.111380348089947\n",
      "Stochastic Gradient Descent(15956): loss=2.869327344614359\n",
      "Stochastic Gradient Descent(15957): loss=7.4242061914418676\n",
      "Stochastic Gradient Descent(15958): loss=4.829214766804449\n",
      "Stochastic Gradient Descent(15959): loss=0.21036152607789588\n",
      "Stochastic Gradient Descent(15960): loss=3.7518185812626204\n",
      "Stochastic Gradient Descent(15961): loss=0.2711815404442033\n",
      "Stochastic Gradient Descent(15962): loss=1.3715489309593478\n",
      "Stochastic Gradient Descent(15963): loss=15.45457020023642\n",
      "Stochastic Gradient Descent(15964): loss=0.039962084512616444\n",
      "Stochastic Gradient Descent(15965): loss=0.43477099733374736\n",
      "Stochastic Gradient Descent(15966): loss=22.211658703664323\n",
      "Stochastic Gradient Descent(15967): loss=53.5511832266243\n",
      "Stochastic Gradient Descent(15968): loss=3.078456251959611\n",
      "Stochastic Gradient Descent(15969): loss=0.4650441731412396\n",
      "Stochastic Gradient Descent(15970): loss=2.130828182023713\n",
      "Stochastic Gradient Descent(15971): loss=0.004262142115289326\n",
      "Stochastic Gradient Descent(15972): loss=13.38560260206902\n",
      "Stochastic Gradient Descent(15973): loss=0.7593920430865703\n",
      "Stochastic Gradient Descent(15974): loss=6.3115773227827265\n",
      "Stochastic Gradient Descent(15975): loss=2.2817009128608143\n",
      "Stochastic Gradient Descent(15976): loss=0.9073675929609646\n",
      "Stochastic Gradient Descent(15977): loss=4.467188080065392\n",
      "Stochastic Gradient Descent(15978): loss=12.562925296595466\n",
      "Stochastic Gradient Descent(15979): loss=52.400238643327405\n",
      "Stochastic Gradient Descent(15980): loss=0.07743379578773975\n",
      "Stochastic Gradient Descent(15981): loss=0.12943964177973505\n",
      "Stochastic Gradient Descent(15982): loss=15.333612441138358\n",
      "Stochastic Gradient Descent(15983): loss=0.736890412461867\n",
      "Stochastic Gradient Descent(15984): loss=0.6814983792944896\n",
      "Stochastic Gradient Descent(15985): loss=14.16686564375135\n",
      "Stochastic Gradient Descent(15986): loss=6.3645973499245105\n",
      "Stochastic Gradient Descent(15987): loss=0.8392992005783725\n",
      "Stochastic Gradient Descent(15988): loss=5.822288033356742\n",
      "Stochastic Gradient Descent(15989): loss=0.7056241295440656\n",
      "Stochastic Gradient Descent(15990): loss=0.015280490610214597\n",
      "Stochastic Gradient Descent(15991): loss=1.403650948248513\n",
      "Stochastic Gradient Descent(15992): loss=0.6095160636059048\n",
      "Stochastic Gradient Descent(15993): loss=14.253561558346416\n",
      "Stochastic Gradient Descent(15994): loss=0.2048950982953988\n",
      "Stochastic Gradient Descent(15995): loss=0.06418522896256318\n",
      "Stochastic Gradient Descent(15996): loss=9.561215817306788\n",
      "Stochastic Gradient Descent(15997): loss=6.0020278260165405\n",
      "Stochastic Gradient Descent(15998): loss=15.591774225216108\n",
      "Stochastic Gradient Descent(15999): loss=0.781300233736603\n",
      "Stochastic Gradient Descent(16000): loss=7.938840942949951\n",
      "Stochastic Gradient Descent(16001): loss=14.295293369534193\n",
      "Stochastic Gradient Descent(16002): loss=0.0019211211693340045\n",
      "Stochastic Gradient Descent(16003): loss=10.250466703175416\n",
      "Stochastic Gradient Descent(16004): loss=8.730379955980288\n",
      "Stochastic Gradient Descent(16005): loss=1.6994180098426623\n",
      "Stochastic Gradient Descent(16006): loss=8.024509569821218\n",
      "Stochastic Gradient Descent(16007): loss=0.5969383115868873\n",
      "Stochastic Gradient Descent(16008): loss=1.4274055985162337\n",
      "Stochastic Gradient Descent(16009): loss=8.393786571337818\n",
      "Stochastic Gradient Descent(16010): loss=21.638830944087577\n",
      "Stochastic Gradient Descent(16011): loss=0.8407992434411415\n",
      "Stochastic Gradient Descent(16012): loss=1.2904964285157623\n",
      "Stochastic Gradient Descent(16013): loss=0.04023699491424075\n",
      "Stochastic Gradient Descent(16014): loss=3.17727510745259\n",
      "Stochastic Gradient Descent(16015): loss=1.2154491697975947\n",
      "Stochastic Gradient Descent(16016): loss=16.43833177934624\n",
      "Stochastic Gradient Descent(16017): loss=0.03818128539363022\n",
      "Stochastic Gradient Descent(16018): loss=7.799200694478319\n",
      "Stochastic Gradient Descent(16019): loss=0.08168157917068873\n",
      "Stochastic Gradient Descent(16020): loss=0.8959189605437969\n",
      "Stochastic Gradient Descent(16021): loss=2.633411319782044\n",
      "Stochastic Gradient Descent(16022): loss=0.07098488870477722\n",
      "Stochastic Gradient Descent(16023): loss=1.8701741061903974\n",
      "Stochastic Gradient Descent(16024): loss=0.018194799055707207\n",
      "Stochastic Gradient Descent(16025): loss=17.837740509523528\n",
      "Stochastic Gradient Descent(16026): loss=3.2207470220665257\n",
      "Stochastic Gradient Descent(16027): loss=0.08260001409138205\n",
      "Stochastic Gradient Descent(16028): loss=2.731094571442705\n",
      "Stochastic Gradient Descent(16029): loss=1.8545282145497617\n",
      "Stochastic Gradient Descent(16030): loss=0.17967443423475996\n",
      "Stochastic Gradient Descent(16031): loss=0.060599789115559674\n",
      "Stochastic Gradient Descent(16032): loss=4.8944371082760485\n",
      "Stochastic Gradient Descent(16033): loss=2.556912310100381\n",
      "Stochastic Gradient Descent(16034): loss=3.476368844628078\n",
      "Stochastic Gradient Descent(16035): loss=0.9723391863697057\n",
      "Stochastic Gradient Descent(16036): loss=1.0991544679410283\n",
      "Stochastic Gradient Descent(16037): loss=15.090816292422625\n",
      "Stochastic Gradient Descent(16038): loss=0.2398219440783781\n",
      "Stochastic Gradient Descent(16039): loss=3.092883590581315\n",
      "Stochastic Gradient Descent(16040): loss=33.35105584609281\n",
      "Stochastic Gradient Descent(16041): loss=11.021795925106181\n",
      "Stochastic Gradient Descent(16042): loss=15.976555925140058\n",
      "Stochastic Gradient Descent(16043): loss=2.5293780302374254\n",
      "Stochastic Gradient Descent(16044): loss=0.4058008781822655\n",
      "Stochastic Gradient Descent(16045): loss=1.5564793210841552\n",
      "Stochastic Gradient Descent(16046): loss=1.778666837870961\n",
      "Stochastic Gradient Descent(16047): loss=0.07010724003919054\n",
      "Stochastic Gradient Descent(16048): loss=0.25933722969424877\n",
      "Stochastic Gradient Descent(16049): loss=1.769520590219401\n",
      "Stochastic Gradient Descent(16050): loss=1.4804254841273718\n",
      "Stochastic Gradient Descent(16051): loss=0.00020532366694800392\n",
      "Stochastic Gradient Descent(16052): loss=6.365344576117683\n",
      "Stochastic Gradient Descent(16053): loss=3.3444151405883162\n",
      "Stochastic Gradient Descent(16054): loss=5.515191321693539\n",
      "Stochastic Gradient Descent(16055): loss=0.579285213209465\n",
      "Stochastic Gradient Descent(16056): loss=0.269723981216517\n",
      "Stochastic Gradient Descent(16057): loss=28.33499031968359\n",
      "Stochastic Gradient Descent(16058): loss=0.4758884857793723\n",
      "Stochastic Gradient Descent(16059): loss=4.376404139554873\n",
      "Stochastic Gradient Descent(16060): loss=2.25104323143222\n",
      "Stochastic Gradient Descent(16061): loss=20.530073325220528\n",
      "Stochastic Gradient Descent(16062): loss=0.09197608155027026\n",
      "Stochastic Gradient Descent(16063): loss=13.914060459040682\n",
      "Stochastic Gradient Descent(16064): loss=5.730166157400705\n",
      "Stochastic Gradient Descent(16065): loss=3.6849746189261117\n",
      "Stochastic Gradient Descent(16066): loss=2.085595617275972\n",
      "Stochastic Gradient Descent(16067): loss=0.2415508421087261\n",
      "Stochastic Gradient Descent(16068): loss=0.1964627561786057\n",
      "Stochastic Gradient Descent(16069): loss=0.05133990074308722\n",
      "Stochastic Gradient Descent(16070): loss=0.03288121486263724\n",
      "Stochastic Gradient Descent(16071): loss=1.776070276700128\n",
      "Stochastic Gradient Descent(16072): loss=1.199264542141147\n",
      "Stochastic Gradient Descent(16073): loss=1.3225913884363487\n",
      "Stochastic Gradient Descent(16074): loss=1.0543001642941334\n",
      "Stochastic Gradient Descent(16075): loss=5.628392429646787\n",
      "Stochastic Gradient Descent(16076): loss=13.779598519276908\n",
      "Stochastic Gradient Descent(16077): loss=1.9118840200104827\n",
      "Stochastic Gradient Descent(16078): loss=0.11369148722595249\n",
      "Stochastic Gradient Descent(16079): loss=1.210832157198921\n",
      "Stochastic Gradient Descent(16080): loss=2.8815124955358637\n",
      "Stochastic Gradient Descent(16081): loss=1.7899199040330795\n",
      "Stochastic Gradient Descent(16082): loss=0.28598385734063586\n",
      "Stochastic Gradient Descent(16083): loss=0.008946736070664374\n",
      "Stochastic Gradient Descent(16084): loss=5.637225722279069\n",
      "Stochastic Gradient Descent(16085): loss=12.713790272009497\n",
      "Stochastic Gradient Descent(16086): loss=7.200295822963185\n",
      "Stochastic Gradient Descent(16087): loss=3.42695132243096\n",
      "Stochastic Gradient Descent(16088): loss=9.901190224142756\n",
      "Stochastic Gradient Descent(16089): loss=1.597923655051973\n",
      "Stochastic Gradient Descent(16090): loss=1.228361440071278\n",
      "Stochastic Gradient Descent(16091): loss=0.2498424000291334\n",
      "Stochastic Gradient Descent(16092): loss=0.08226377591560724\n",
      "Stochastic Gradient Descent(16093): loss=1.1739253965216716\n",
      "Stochastic Gradient Descent(16094): loss=8.117257528041462\n",
      "Stochastic Gradient Descent(16095): loss=3.510530006423459\n",
      "Stochastic Gradient Descent(16096): loss=0.3652947308476456\n",
      "Stochastic Gradient Descent(16097): loss=4.410743159129449\n",
      "Stochastic Gradient Descent(16098): loss=3.798412447824789\n",
      "Stochastic Gradient Descent(16099): loss=13.84562083631082\n",
      "Stochastic Gradient Descent(16100): loss=0.22574726283009647\n",
      "Stochastic Gradient Descent(16101): loss=1.881639936835222\n",
      "Stochastic Gradient Descent(16102): loss=6.117802191076776\n",
      "Stochastic Gradient Descent(16103): loss=5.199791978734806\n",
      "Stochastic Gradient Descent(16104): loss=0.07170578476577563\n",
      "Stochastic Gradient Descent(16105): loss=4.7527623998387005\n",
      "Stochastic Gradient Descent(16106): loss=2.4003198906815877\n",
      "Stochastic Gradient Descent(16107): loss=2.124086753502764\n",
      "Stochastic Gradient Descent(16108): loss=0.019663215118375255\n",
      "Stochastic Gradient Descent(16109): loss=6.269591839750838\n",
      "Stochastic Gradient Descent(16110): loss=0.030308304302551273\n",
      "Stochastic Gradient Descent(16111): loss=0.8462806149773258\n",
      "Stochastic Gradient Descent(16112): loss=0.48596984367792706\n",
      "Stochastic Gradient Descent(16113): loss=0.7715587327894328\n",
      "Stochastic Gradient Descent(16114): loss=1.033456656746028\n",
      "Stochastic Gradient Descent(16115): loss=0.010194519854633334\n",
      "Stochastic Gradient Descent(16116): loss=0.000769198869928707\n",
      "Stochastic Gradient Descent(16117): loss=13.87627893390212\n",
      "Stochastic Gradient Descent(16118): loss=0.08003346371459166\n",
      "Stochastic Gradient Descent(16119): loss=7.839262423405654\n",
      "Stochastic Gradient Descent(16120): loss=0.20860959695914197\n",
      "Stochastic Gradient Descent(16121): loss=0.21245963724970887\n",
      "Stochastic Gradient Descent(16122): loss=0.1482619545552504\n",
      "Stochastic Gradient Descent(16123): loss=11.582069214492988\n",
      "Stochastic Gradient Descent(16124): loss=0.4201158253040628\n",
      "Stochastic Gradient Descent(16125): loss=2.1896058222835966\n",
      "Stochastic Gradient Descent(16126): loss=4.6632659440994475\n",
      "Stochastic Gradient Descent(16127): loss=3.9854788925864453\n",
      "Stochastic Gradient Descent(16128): loss=14.332206748422863\n",
      "Stochastic Gradient Descent(16129): loss=0.0033659629676314686\n",
      "Stochastic Gradient Descent(16130): loss=0.26819737170936364\n",
      "Stochastic Gradient Descent(16131): loss=0.10503378479741304\n",
      "Stochastic Gradient Descent(16132): loss=0.438689084574214\n",
      "Stochastic Gradient Descent(16133): loss=1.1169189601438392\n",
      "Stochastic Gradient Descent(16134): loss=8.446124753693637\n",
      "Stochastic Gradient Descent(16135): loss=2.557257528376299\n",
      "Stochastic Gradient Descent(16136): loss=0.0564001141543541\n",
      "Stochastic Gradient Descent(16137): loss=7.160197709267065\n",
      "Stochastic Gradient Descent(16138): loss=5.931068541082197\n",
      "Stochastic Gradient Descent(16139): loss=3.8954060284997882\n",
      "Stochastic Gradient Descent(16140): loss=4.084819982531878\n",
      "Stochastic Gradient Descent(16141): loss=0.5980083494932521\n",
      "Stochastic Gradient Descent(16142): loss=0.1178790286164406\n",
      "Stochastic Gradient Descent(16143): loss=0.3180598392806502\n",
      "Stochastic Gradient Descent(16144): loss=5.620444851859889\n",
      "Stochastic Gradient Descent(16145): loss=2.3383585975490844\n",
      "Stochastic Gradient Descent(16146): loss=8.451269937017841\n",
      "Stochastic Gradient Descent(16147): loss=0.017671335885932886\n",
      "Stochastic Gradient Descent(16148): loss=9.589568634377756\n",
      "Stochastic Gradient Descent(16149): loss=2.9356992125627936\n",
      "Stochastic Gradient Descent(16150): loss=1.7987735687096367\n",
      "Stochastic Gradient Descent(16151): loss=36.15432839224995\n",
      "Stochastic Gradient Descent(16152): loss=15.28657378465525\n",
      "Stochastic Gradient Descent(16153): loss=1.3765085206833463\n",
      "Stochastic Gradient Descent(16154): loss=4.33117421641484\n",
      "Stochastic Gradient Descent(16155): loss=0.07007984081072391\n",
      "Stochastic Gradient Descent(16156): loss=0.0007137281963985798\n",
      "Stochastic Gradient Descent(16157): loss=1.5378494634263216\n",
      "Stochastic Gradient Descent(16158): loss=3.922999404318402\n",
      "Stochastic Gradient Descent(16159): loss=2.7019814062549554\n",
      "Stochastic Gradient Descent(16160): loss=14.510589574824383\n",
      "Stochastic Gradient Descent(16161): loss=42.11933923935513\n",
      "Stochastic Gradient Descent(16162): loss=5.1374595777718\n",
      "Stochastic Gradient Descent(16163): loss=1.5015486279488621\n",
      "Stochastic Gradient Descent(16164): loss=0.42491300798160864\n",
      "Stochastic Gradient Descent(16165): loss=54.29095140911824\n",
      "Stochastic Gradient Descent(16166): loss=4.910588986601345\n",
      "Stochastic Gradient Descent(16167): loss=9.673919862301194\n",
      "Stochastic Gradient Descent(16168): loss=0.5457866425385415\n",
      "Stochastic Gradient Descent(16169): loss=5.619451762786045\n",
      "Stochastic Gradient Descent(16170): loss=12.2503229729583\n",
      "Stochastic Gradient Descent(16171): loss=1.0189377351502136\n",
      "Stochastic Gradient Descent(16172): loss=1.4868759220806125\n",
      "Stochastic Gradient Descent(16173): loss=29.346453879269394\n",
      "Stochastic Gradient Descent(16174): loss=1.6011955864844118\n",
      "Stochastic Gradient Descent(16175): loss=3.543700593546154\n",
      "Stochastic Gradient Descent(16176): loss=0.05578808755292417\n",
      "Stochastic Gradient Descent(16177): loss=4.905972466326337\n",
      "Stochastic Gradient Descent(16178): loss=20.176292308714917\n",
      "Stochastic Gradient Descent(16179): loss=2.5997563238508725\n",
      "Stochastic Gradient Descent(16180): loss=18.694246461685335\n",
      "Stochastic Gradient Descent(16181): loss=0.8690568625864186\n",
      "Stochastic Gradient Descent(16182): loss=7.34763934131508\n",
      "Stochastic Gradient Descent(16183): loss=0.4094460311662863\n",
      "Stochastic Gradient Descent(16184): loss=18.9760656970592\n",
      "Stochastic Gradient Descent(16185): loss=0.2642633360974025\n",
      "Stochastic Gradient Descent(16186): loss=2.6937172060190187\n",
      "Stochastic Gradient Descent(16187): loss=4.266587509663193\n",
      "Stochastic Gradient Descent(16188): loss=0.44441326512598234\n",
      "Stochastic Gradient Descent(16189): loss=2.076333604622283\n",
      "Stochastic Gradient Descent(16190): loss=0.09073863669111605\n",
      "Stochastic Gradient Descent(16191): loss=13.181348002083432\n",
      "Stochastic Gradient Descent(16192): loss=1.317563083791629\n",
      "Stochastic Gradient Descent(16193): loss=2.5747942641101282\n",
      "Stochastic Gradient Descent(16194): loss=0.12748663924193787\n",
      "Stochastic Gradient Descent(16195): loss=0.5623626796594006\n",
      "Stochastic Gradient Descent(16196): loss=0.8021535914634341\n",
      "Stochastic Gradient Descent(16197): loss=9.17653335253542\n",
      "Stochastic Gradient Descent(16198): loss=3.9727219771461475\n",
      "Stochastic Gradient Descent(16199): loss=13.48747680942125\n",
      "Stochastic Gradient Descent(16200): loss=0.008331065884631844\n",
      "Stochastic Gradient Descent(16201): loss=3.253323844273841\n",
      "Stochastic Gradient Descent(16202): loss=0.5393739778008662\n",
      "Stochastic Gradient Descent(16203): loss=0.5014220837430111\n",
      "Stochastic Gradient Descent(16204): loss=0.06460809990459751\n",
      "Stochastic Gradient Descent(16205): loss=4.531469052268264\n",
      "Stochastic Gradient Descent(16206): loss=2.715601769683709\n",
      "Stochastic Gradient Descent(16207): loss=2.2426879269741695\n",
      "Stochastic Gradient Descent(16208): loss=3.7983416830209658\n",
      "Stochastic Gradient Descent(16209): loss=3.6948083199182773\n",
      "Stochastic Gradient Descent(16210): loss=4.08235127292781\n",
      "Stochastic Gradient Descent(16211): loss=19.12795047926686\n",
      "Stochastic Gradient Descent(16212): loss=5.3144341020981285\n",
      "Stochastic Gradient Descent(16213): loss=1.5226858941781787\n",
      "Stochastic Gradient Descent(16214): loss=4.0166296058195154\n",
      "Stochastic Gradient Descent(16215): loss=0.005617238810264005\n",
      "Stochastic Gradient Descent(16216): loss=6.756239301200777\n",
      "Stochastic Gradient Descent(16217): loss=9.447644027968927\n",
      "Stochastic Gradient Descent(16218): loss=0.0004492988963264736\n",
      "Stochastic Gradient Descent(16219): loss=1.6415751543823152\n",
      "Stochastic Gradient Descent(16220): loss=0.2835479739517849\n",
      "Stochastic Gradient Descent(16221): loss=2.8628737171378087\n",
      "Stochastic Gradient Descent(16222): loss=0.09718649890800556\n",
      "Stochastic Gradient Descent(16223): loss=6.155723344690035\n",
      "Stochastic Gradient Descent(16224): loss=2.2200590078493434\n",
      "Stochastic Gradient Descent(16225): loss=0.22677756436083457\n",
      "Stochastic Gradient Descent(16226): loss=1.5893791895895157\n",
      "Stochastic Gradient Descent(16227): loss=0.41768748812888556\n",
      "Stochastic Gradient Descent(16228): loss=0.02399943864873845\n",
      "Stochastic Gradient Descent(16229): loss=0.8561027228065756\n",
      "Stochastic Gradient Descent(16230): loss=6.451718249385605\n",
      "Stochastic Gradient Descent(16231): loss=3.296268877602733\n",
      "Stochastic Gradient Descent(16232): loss=1.1885893119302238\n",
      "Stochastic Gradient Descent(16233): loss=0.9693735484982193\n",
      "Stochastic Gradient Descent(16234): loss=8.14721083521902\n",
      "Stochastic Gradient Descent(16235): loss=0.39071858915477864\n",
      "Stochastic Gradient Descent(16236): loss=1.0178997814773392\n",
      "Stochastic Gradient Descent(16237): loss=0.0745839182313553\n",
      "Stochastic Gradient Descent(16238): loss=0.027303479923616935\n",
      "Stochastic Gradient Descent(16239): loss=11.531813271780715\n",
      "Stochastic Gradient Descent(16240): loss=0.6223286175444396\n",
      "Stochastic Gradient Descent(16241): loss=4.27190798079542\n",
      "Stochastic Gradient Descent(16242): loss=1.8108430166982472\n",
      "Stochastic Gradient Descent(16243): loss=1.9560318530396466\n",
      "Stochastic Gradient Descent(16244): loss=1.0260472068536852\n",
      "Stochastic Gradient Descent(16245): loss=3.0819310344610273\n",
      "Stochastic Gradient Descent(16246): loss=2.1235246028177945\n",
      "Stochastic Gradient Descent(16247): loss=0.22799795753562685\n",
      "Stochastic Gradient Descent(16248): loss=3.562424591358216\n",
      "Stochastic Gradient Descent(16249): loss=0.04847814819433851\n",
      "Stochastic Gradient Descent(16250): loss=6.776784006295552\n",
      "Stochastic Gradient Descent(16251): loss=0.10753385244047071\n",
      "Stochastic Gradient Descent(16252): loss=5.236841215617422\n",
      "Stochastic Gradient Descent(16253): loss=7.763074838870668\n",
      "Stochastic Gradient Descent(16254): loss=0.413981509440173\n",
      "Stochastic Gradient Descent(16255): loss=0.3326565073593872\n",
      "Stochastic Gradient Descent(16256): loss=0.17864146625130461\n",
      "Stochastic Gradient Descent(16257): loss=0.6694582865653065\n",
      "Stochastic Gradient Descent(16258): loss=3.1308263636396636\n",
      "Stochastic Gradient Descent(16259): loss=1.468258177681801\n",
      "Stochastic Gradient Descent(16260): loss=0.9039018065788629\n",
      "Stochastic Gradient Descent(16261): loss=3.97523546279191\n",
      "Stochastic Gradient Descent(16262): loss=0.4154160182928248\n",
      "Stochastic Gradient Descent(16263): loss=0.33851590079025995\n",
      "Stochastic Gradient Descent(16264): loss=4.651740241931061\n",
      "Stochastic Gradient Descent(16265): loss=0.4114390862760545\n",
      "Stochastic Gradient Descent(16266): loss=5.394405638638667\n",
      "Stochastic Gradient Descent(16267): loss=1.0502701473137883\n",
      "Stochastic Gradient Descent(16268): loss=1.271193180481214\n",
      "Stochastic Gradient Descent(16269): loss=0.9336696093345717\n",
      "Stochastic Gradient Descent(16270): loss=0.8591955071295091\n",
      "Stochastic Gradient Descent(16271): loss=17.553041984045624\n",
      "Stochastic Gradient Descent(16272): loss=8.156424217070679\n",
      "Stochastic Gradient Descent(16273): loss=0.28044409729697056\n",
      "Stochastic Gradient Descent(16274): loss=8.987331783708564\n",
      "Stochastic Gradient Descent(16275): loss=6.856251358409515\n",
      "Stochastic Gradient Descent(16276): loss=1.0478534683142255\n",
      "Stochastic Gradient Descent(16277): loss=0.49932093712941833\n",
      "Stochastic Gradient Descent(16278): loss=0.45773337863961766\n",
      "Stochastic Gradient Descent(16279): loss=3.101300640872026\n",
      "Stochastic Gradient Descent(16280): loss=0.31972501119381586\n",
      "Stochastic Gradient Descent(16281): loss=0.15773408010642923\n",
      "Stochastic Gradient Descent(16282): loss=1.1175981084107391\n",
      "Stochastic Gradient Descent(16283): loss=0.00027678355869686434\n",
      "Stochastic Gradient Descent(16284): loss=3.222281837716461\n",
      "Stochastic Gradient Descent(16285): loss=5.507753012726912\n",
      "Stochastic Gradient Descent(16286): loss=3.872573656707064\n",
      "Stochastic Gradient Descent(16287): loss=0.613481112935031\n",
      "Stochastic Gradient Descent(16288): loss=6.081883139684689\n",
      "Stochastic Gradient Descent(16289): loss=8.309118188279875\n",
      "Stochastic Gradient Descent(16290): loss=4.064561609083108\n",
      "Stochastic Gradient Descent(16291): loss=5.797714936245767\n",
      "Stochastic Gradient Descent(16292): loss=7.950560615140802\n",
      "Stochastic Gradient Descent(16293): loss=25.46606274225527\n",
      "Stochastic Gradient Descent(16294): loss=9.499467194751286\n",
      "Stochastic Gradient Descent(16295): loss=10.145384278063064\n",
      "Stochastic Gradient Descent(16296): loss=0.954829460284204\n",
      "Stochastic Gradient Descent(16297): loss=3.0668705732681922\n",
      "Stochastic Gradient Descent(16298): loss=5.809685078837935\n",
      "Stochastic Gradient Descent(16299): loss=3.24550050723028\n",
      "Stochastic Gradient Descent(16300): loss=1.0125998127233888\n",
      "Stochastic Gradient Descent(16301): loss=0.5924554134640428\n",
      "Stochastic Gradient Descent(16302): loss=0.05768050873083098\n",
      "Stochastic Gradient Descent(16303): loss=0.03814603905286288\n",
      "Stochastic Gradient Descent(16304): loss=1.5835196905340847\n",
      "Stochastic Gradient Descent(16305): loss=4.266031788527923\n",
      "Stochastic Gradient Descent(16306): loss=8.424927216702162\n",
      "Stochastic Gradient Descent(16307): loss=0.7400638179815463\n",
      "Stochastic Gradient Descent(16308): loss=6.250076047402319\n",
      "Stochastic Gradient Descent(16309): loss=3.1854755645798485\n",
      "Stochastic Gradient Descent(16310): loss=4.6414747689735805\n",
      "Stochastic Gradient Descent(16311): loss=0.284855209735068\n",
      "Stochastic Gradient Descent(16312): loss=7.722347478645105\n",
      "Stochastic Gradient Descent(16313): loss=0.5411748994763562\n",
      "Stochastic Gradient Descent(16314): loss=5.504339079520848\n",
      "Stochastic Gradient Descent(16315): loss=3.099751658233329\n",
      "Stochastic Gradient Descent(16316): loss=6.927257773627791\n",
      "Stochastic Gradient Descent(16317): loss=6.308005021977547\n",
      "Stochastic Gradient Descent(16318): loss=0.047421902017102564\n",
      "Stochastic Gradient Descent(16319): loss=3.5036807321393617\n",
      "Stochastic Gradient Descent(16320): loss=2.0643280273007476\n",
      "Stochastic Gradient Descent(16321): loss=7.857196373351383\n",
      "Stochastic Gradient Descent(16322): loss=11.281241506217356\n",
      "Stochastic Gradient Descent(16323): loss=3.247712353147955\n",
      "Stochastic Gradient Descent(16324): loss=12.829829318133145\n",
      "Stochastic Gradient Descent(16325): loss=0.02351880587272519\n",
      "Stochastic Gradient Descent(16326): loss=3.35178389671765\n",
      "Stochastic Gradient Descent(16327): loss=19.1733412351917\n",
      "Stochastic Gradient Descent(16328): loss=4.422816841455848\n",
      "Stochastic Gradient Descent(16329): loss=46.727892444446915\n",
      "Stochastic Gradient Descent(16330): loss=8.797532918037096\n",
      "Stochastic Gradient Descent(16331): loss=5.388174999962646\n",
      "Stochastic Gradient Descent(16332): loss=3.0839988002339025\n",
      "Stochastic Gradient Descent(16333): loss=1.0784553379996236\n",
      "Stochastic Gradient Descent(16334): loss=47.51076538320406\n",
      "Stochastic Gradient Descent(16335): loss=3.1094884043854925\n",
      "Stochastic Gradient Descent(16336): loss=0.2676743607709554\n",
      "Stochastic Gradient Descent(16337): loss=1.2181104848672417\n",
      "Stochastic Gradient Descent(16338): loss=0.016342890237172933\n",
      "Stochastic Gradient Descent(16339): loss=16.221206735566458\n",
      "Stochastic Gradient Descent(16340): loss=54.44098077913295\n",
      "Stochastic Gradient Descent(16341): loss=12.114735245154195\n",
      "Stochastic Gradient Descent(16342): loss=9.174693156652074\n",
      "Stochastic Gradient Descent(16343): loss=0.009437523171433062\n",
      "Stochastic Gradient Descent(16344): loss=16.96736375199408\n",
      "Stochastic Gradient Descent(16345): loss=6.628413786417221\n",
      "Stochastic Gradient Descent(16346): loss=20.40734637744848\n",
      "Stochastic Gradient Descent(16347): loss=0.08644345224984215\n",
      "Stochastic Gradient Descent(16348): loss=1.2515253535976134\n",
      "Stochastic Gradient Descent(16349): loss=0.002563566461662032\n",
      "Stochastic Gradient Descent(16350): loss=0.30216734954942354\n",
      "Stochastic Gradient Descent(16351): loss=7.802113524710313\n",
      "Stochastic Gradient Descent(16352): loss=22.85187198600104\n",
      "Stochastic Gradient Descent(16353): loss=1.9238965833041752\n",
      "Stochastic Gradient Descent(16354): loss=4.6913077086059785\n",
      "Stochastic Gradient Descent(16355): loss=3.3305167724019262\n",
      "Stochastic Gradient Descent(16356): loss=6.825668660224324\n",
      "Stochastic Gradient Descent(16357): loss=1.7061425661340652\n",
      "Stochastic Gradient Descent(16358): loss=0.51134625784817\n",
      "Stochastic Gradient Descent(16359): loss=1.3087650407546545\n",
      "Stochastic Gradient Descent(16360): loss=0.3794772672708509\n",
      "Stochastic Gradient Descent(16361): loss=3.938649632862912\n",
      "Stochastic Gradient Descent(16362): loss=0.4761828828869609\n",
      "Stochastic Gradient Descent(16363): loss=2.064266253015887\n",
      "Stochastic Gradient Descent(16364): loss=11.28726437524561\n",
      "Stochastic Gradient Descent(16365): loss=0.24897173797430863\n",
      "Stochastic Gradient Descent(16366): loss=18.394219640857926\n",
      "Stochastic Gradient Descent(16367): loss=0.6073829093817892\n",
      "Stochastic Gradient Descent(16368): loss=7.185990784777836\n",
      "Stochastic Gradient Descent(16369): loss=0.26731325499617253\n",
      "Stochastic Gradient Descent(16370): loss=0.02294461107170106\n",
      "Stochastic Gradient Descent(16371): loss=55.476078931649354\n",
      "Stochastic Gradient Descent(16372): loss=1.79947876443906\n",
      "Stochastic Gradient Descent(16373): loss=36.95675903635667\n",
      "Stochastic Gradient Descent(16374): loss=0.03377326856984746\n",
      "Stochastic Gradient Descent(16375): loss=8.953534735457119\n",
      "Stochastic Gradient Descent(16376): loss=2.5452118204687735\n",
      "Stochastic Gradient Descent(16377): loss=3.675583056604031\n",
      "Stochastic Gradient Descent(16378): loss=0.6787085301256228\n",
      "Stochastic Gradient Descent(16379): loss=0.012587022515960996\n",
      "Stochastic Gradient Descent(16380): loss=0.04108546838439623\n",
      "Stochastic Gradient Descent(16381): loss=11.328291137099466\n",
      "Stochastic Gradient Descent(16382): loss=2.9974006856824698\n",
      "Stochastic Gradient Descent(16383): loss=14.552172855483656\n",
      "Stochastic Gradient Descent(16384): loss=2.584365853348124\n",
      "Stochastic Gradient Descent(16385): loss=3.1744317214627316\n",
      "Stochastic Gradient Descent(16386): loss=6.466118234907542\n",
      "Stochastic Gradient Descent(16387): loss=7.579068113361095\n",
      "Stochastic Gradient Descent(16388): loss=7.576238923451717\n",
      "Stochastic Gradient Descent(16389): loss=8.553092660497828\n",
      "Stochastic Gradient Descent(16390): loss=4.665354194502603\n",
      "Stochastic Gradient Descent(16391): loss=0.25940719356440356\n",
      "Stochastic Gradient Descent(16392): loss=0.8437309327176767\n",
      "Stochastic Gradient Descent(16393): loss=4.213308548616155\n",
      "Stochastic Gradient Descent(16394): loss=1.253369777621604\n",
      "Stochastic Gradient Descent(16395): loss=1.0848271027461454\n",
      "Stochastic Gradient Descent(16396): loss=0.4831326125722283\n",
      "Stochastic Gradient Descent(16397): loss=2.9154119361591304\n",
      "Stochastic Gradient Descent(16398): loss=0.6487202834057219\n",
      "Stochastic Gradient Descent(16399): loss=0.5840172309323395\n",
      "Stochastic Gradient Descent(16400): loss=0.3840136202881396\n",
      "Stochastic Gradient Descent(16401): loss=6.110974638981939\n",
      "Stochastic Gradient Descent(16402): loss=3.2525145490223313\n",
      "Stochastic Gradient Descent(16403): loss=0.016702580463877716\n",
      "Stochastic Gradient Descent(16404): loss=2.30076999155664\n",
      "Stochastic Gradient Descent(16405): loss=2.4274198393999256\n",
      "Stochastic Gradient Descent(16406): loss=12.253513500627978\n",
      "Stochastic Gradient Descent(16407): loss=29.698347935901666\n",
      "Stochastic Gradient Descent(16408): loss=1.470478053973983\n",
      "Stochastic Gradient Descent(16409): loss=8.245183842183133\n",
      "Stochastic Gradient Descent(16410): loss=0.13035201142215358\n",
      "Stochastic Gradient Descent(16411): loss=0.6436551686789997\n",
      "Stochastic Gradient Descent(16412): loss=25.643020232916662\n",
      "Stochastic Gradient Descent(16413): loss=1.2935939595919852\n",
      "Stochastic Gradient Descent(16414): loss=2.407669293057667\n",
      "Stochastic Gradient Descent(16415): loss=8.253200332197174\n",
      "Stochastic Gradient Descent(16416): loss=3.0420644728017847\n",
      "Stochastic Gradient Descent(16417): loss=14.237145640831828\n",
      "Stochastic Gradient Descent(16418): loss=0.07058389906686925\n",
      "Stochastic Gradient Descent(16419): loss=0.20265828133611183\n",
      "Stochastic Gradient Descent(16420): loss=1.6232355088205408\n",
      "Stochastic Gradient Descent(16421): loss=0.018516227571829632\n",
      "Stochastic Gradient Descent(16422): loss=0.6094695752633634\n",
      "Stochastic Gradient Descent(16423): loss=11.584409240601344\n",
      "Stochastic Gradient Descent(16424): loss=2.2665020076191817\n",
      "Stochastic Gradient Descent(16425): loss=4.597107714943688\n",
      "Stochastic Gradient Descent(16426): loss=0.39509286629420426\n",
      "Stochastic Gradient Descent(16427): loss=0.131986239552075\n",
      "Stochastic Gradient Descent(16428): loss=0.69113150701859\n",
      "Stochastic Gradient Descent(16429): loss=0.9152184735976935\n",
      "Stochastic Gradient Descent(16430): loss=1.729184496048732\n",
      "Stochastic Gradient Descent(16431): loss=5.406957356177527\n",
      "Stochastic Gradient Descent(16432): loss=3.312318827249542e-05\n",
      "Stochastic Gradient Descent(16433): loss=10.97960584316614\n",
      "Stochastic Gradient Descent(16434): loss=9.71749086333933\n",
      "Stochastic Gradient Descent(16435): loss=3.3454683141917023\n",
      "Stochastic Gradient Descent(16436): loss=1.6228343165254886\n",
      "Stochastic Gradient Descent(16437): loss=2.2107252543514955\n",
      "Stochastic Gradient Descent(16438): loss=9.527075321956318\n",
      "Stochastic Gradient Descent(16439): loss=0.10450714016847666\n",
      "Stochastic Gradient Descent(16440): loss=0.7478247976793813\n",
      "Stochastic Gradient Descent(16441): loss=6.19103809875886\n",
      "Stochastic Gradient Descent(16442): loss=0.2944518770582304\n",
      "Stochastic Gradient Descent(16443): loss=0.5146236094766697\n",
      "Stochastic Gradient Descent(16444): loss=1.07229060852595\n",
      "Stochastic Gradient Descent(16445): loss=2.1481573691470177\n",
      "Stochastic Gradient Descent(16446): loss=2.581763171328927\n",
      "Stochastic Gradient Descent(16447): loss=0.0026566511736987805\n",
      "Stochastic Gradient Descent(16448): loss=1.2539439882704595\n",
      "Stochastic Gradient Descent(16449): loss=0.0017378153922938924\n",
      "Stochastic Gradient Descent(16450): loss=0.025139344519416112\n",
      "Stochastic Gradient Descent(16451): loss=8.492950622598125\n",
      "Stochastic Gradient Descent(16452): loss=1.6201050028324104\n",
      "Stochastic Gradient Descent(16453): loss=5.231713130255954\n",
      "Stochastic Gradient Descent(16454): loss=0.428240395902757\n",
      "Stochastic Gradient Descent(16455): loss=0.10994669922095289\n",
      "Stochastic Gradient Descent(16456): loss=0.2296472781153776\n",
      "Stochastic Gradient Descent(16457): loss=2.8097311333828934\n",
      "Stochastic Gradient Descent(16458): loss=2.789287085714078\n",
      "Stochastic Gradient Descent(16459): loss=5.327101265740886\n",
      "Stochastic Gradient Descent(16460): loss=3.0124093669652776\n",
      "Stochastic Gradient Descent(16461): loss=0.45071643974774084\n",
      "Stochastic Gradient Descent(16462): loss=1.0685234540524169\n",
      "Stochastic Gradient Descent(16463): loss=4.819538403299011\n",
      "Stochastic Gradient Descent(16464): loss=4.6710910718412455\n",
      "Stochastic Gradient Descent(16465): loss=12.14388384321135\n",
      "Stochastic Gradient Descent(16466): loss=0.19784806604023664\n",
      "Stochastic Gradient Descent(16467): loss=1.9987030379164725\n",
      "Stochastic Gradient Descent(16468): loss=17.594457885507396\n",
      "Stochastic Gradient Descent(16469): loss=0.5373869062053903\n",
      "Stochastic Gradient Descent(16470): loss=1.813390765103814\n",
      "Stochastic Gradient Descent(16471): loss=4.1293459411874975\n",
      "Stochastic Gradient Descent(16472): loss=1.549136727890469\n",
      "Stochastic Gradient Descent(16473): loss=5.331925836556621\n",
      "Stochastic Gradient Descent(16474): loss=2.409783635682473\n",
      "Stochastic Gradient Descent(16475): loss=0.20729981487445998\n",
      "Stochastic Gradient Descent(16476): loss=4.576421846139698\n",
      "Stochastic Gradient Descent(16477): loss=4.649311998492331\n",
      "Stochastic Gradient Descent(16478): loss=0.15381393265467727\n",
      "Stochastic Gradient Descent(16479): loss=6.43642948104901\n",
      "Stochastic Gradient Descent(16480): loss=6.832543994915218\n",
      "Stochastic Gradient Descent(16481): loss=9.043149497229793\n",
      "Stochastic Gradient Descent(16482): loss=0.2338562881366455\n",
      "Stochastic Gradient Descent(16483): loss=0.127340229826873\n",
      "Stochastic Gradient Descent(16484): loss=4.973850210662578\n",
      "Stochastic Gradient Descent(16485): loss=6.51077528711713\n",
      "Stochastic Gradient Descent(16486): loss=5.7902479694168045\n",
      "Stochastic Gradient Descent(16487): loss=4.541215409808141\n",
      "Stochastic Gradient Descent(16488): loss=1.7313910947885973\n",
      "Stochastic Gradient Descent(16489): loss=13.268678853994382\n",
      "Stochastic Gradient Descent(16490): loss=25.488751330734473\n",
      "Stochastic Gradient Descent(16491): loss=29.77705534458808\n",
      "Stochastic Gradient Descent(16492): loss=0.031181273992799116\n",
      "Stochastic Gradient Descent(16493): loss=0.09887413907997361\n",
      "Stochastic Gradient Descent(16494): loss=8.449595183937967\n",
      "Stochastic Gradient Descent(16495): loss=1.0831995835897907\n",
      "Stochastic Gradient Descent(16496): loss=37.24518763227223\n",
      "Stochastic Gradient Descent(16497): loss=10.23370105195452\n",
      "Stochastic Gradient Descent(16498): loss=0.004038984346337512\n",
      "Stochastic Gradient Descent(16499): loss=5.474923419974896\n",
      "Stochastic Gradient Descent(16500): loss=6.207627405125771\n",
      "Stochastic Gradient Descent(16501): loss=1.7554195546316773\n",
      "Stochastic Gradient Descent(16502): loss=2.049669484787151\n",
      "Stochastic Gradient Descent(16503): loss=13.724729720655658\n",
      "Stochastic Gradient Descent(16504): loss=0.1381800994697639\n",
      "Stochastic Gradient Descent(16505): loss=0.7221544271776175\n",
      "Stochastic Gradient Descent(16506): loss=1.281049393683944e-05\n",
      "Stochastic Gradient Descent(16507): loss=5.814780263480405\n",
      "Stochastic Gradient Descent(16508): loss=1.898400699372855\n",
      "Stochastic Gradient Descent(16509): loss=3.2198078039018925\n",
      "Stochastic Gradient Descent(16510): loss=0.0004157045811013151\n",
      "Stochastic Gradient Descent(16511): loss=0.37709497692353555\n",
      "Stochastic Gradient Descent(16512): loss=1.5106857838175913\n",
      "Stochastic Gradient Descent(16513): loss=9.739997849770667\n",
      "Stochastic Gradient Descent(16514): loss=2.1234749744163235\n",
      "Stochastic Gradient Descent(16515): loss=8.672231548636903\n",
      "Stochastic Gradient Descent(16516): loss=34.0041404197461\n",
      "Stochastic Gradient Descent(16517): loss=4.5887047664040335\n",
      "Stochastic Gradient Descent(16518): loss=12.25093109542997\n",
      "Stochastic Gradient Descent(16519): loss=0.1844668982069555\n",
      "Stochastic Gradient Descent(16520): loss=0.01681036728794851\n",
      "Stochastic Gradient Descent(16521): loss=0.0035502059694586807\n",
      "Stochastic Gradient Descent(16522): loss=2.0804384986261355\n",
      "Stochastic Gradient Descent(16523): loss=19.748067492901136\n",
      "Stochastic Gradient Descent(16524): loss=6.061637129596471\n",
      "Stochastic Gradient Descent(16525): loss=50.36825132410869\n",
      "Stochastic Gradient Descent(16526): loss=1.0687675687054767\n",
      "Stochastic Gradient Descent(16527): loss=10.102097823608437\n",
      "Stochastic Gradient Descent(16528): loss=2.2459793883262673\n",
      "Stochastic Gradient Descent(16529): loss=2.538257607236992\n",
      "Stochastic Gradient Descent(16530): loss=0.00029832323916677153\n",
      "Stochastic Gradient Descent(16531): loss=3.9412411140334065\n",
      "Stochastic Gradient Descent(16532): loss=3.3000356795003962\n",
      "Stochastic Gradient Descent(16533): loss=0.030607202141623036\n",
      "Stochastic Gradient Descent(16534): loss=0.7255650794689738\n",
      "Stochastic Gradient Descent(16535): loss=2.765090590212795\n",
      "Stochastic Gradient Descent(16536): loss=0.010530117981113184\n",
      "Stochastic Gradient Descent(16537): loss=0.5099875497442315\n",
      "Stochastic Gradient Descent(16538): loss=1.5939126223154192\n",
      "Stochastic Gradient Descent(16539): loss=0.03270833096483654\n",
      "Stochastic Gradient Descent(16540): loss=0.08791258030524818\n",
      "Stochastic Gradient Descent(16541): loss=8.114113078889421\n",
      "Stochastic Gradient Descent(16542): loss=4.385696084244076\n",
      "Stochastic Gradient Descent(16543): loss=3.7445258938043136\n",
      "Stochastic Gradient Descent(16544): loss=4.079418567235178\n",
      "Stochastic Gradient Descent(16545): loss=0.006545335543158124\n",
      "Stochastic Gradient Descent(16546): loss=1.023190081665259\n",
      "Stochastic Gradient Descent(16547): loss=11.523701436287048\n",
      "Stochastic Gradient Descent(16548): loss=0.1664877089344186\n",
      "Stochastic Gradient Descent(16549): loss=5.229842329154299\n",
      "Stochastic Gradient Descent(16550): loss=4.247807384862618\n",
      "Stochastic Gradient Descent(16551): loss=0.01899339453507043\n",
      "Stochastic Gradient Descent(16552): loss=0.02366397831755774\n",
      "Stochastic Gradient Descent(16553): loss=2.979023632377657\n",
      "Stochastic Gradient Descent(16554): loss=45.57741691312405\n",
      "Stochastic Gradient Descent(16555): loss=14.18400810208496\n",
      "Stochastic Gradient Descent(16556): loss=2.9739816145998126\n",
      "Stochastic Gradient Descent(16557): loss=1.2517149762214221\n",
      "Stochastic Gradient Descent(16558): loss=1.1320329510473555\n",
      "Stochastic Gradient Descent(16559): loss=5.221059017351132\n",
      "Stochastic Gradient Descent(16560): loss=0.3256607144834219\n",
      "Stochastic Gradient Descent(16561): loss=6.478241361232238\n",
      "Stochastic Gradient Descent(16562): loss=1.44426387689289\n",
      "Stochastic Gradient Descent(16563): loss=2.5128347530164734\n",
      "Stochastic Gradient Descent(16564): loss=14.042058353664045\n",
      "Stochastic Gradient Descent(16565): loss=5.057988932732066\n",
      "Stochastic Gradient Descent(16566): loss=70.24001251994241\n",
      "Stochastic Gradient Descent(16567): loss=52.25195512757261\n",
      "Stochastic Gradient Descent(16568): loss=0.29483682144160717\n",
      "Stochastic Gradient Descent(16569): loss=2.0125700974191516\n",
      "Stochastic Gradient Descent(16570): loss=0.001459946205097196\n",
      "Stochastic Gradient Descent(16571): loss=1.321830858476718\n",
      "Stochastic Gradient Descent(16572): loss=0.2295226528453898\n",
      "Stochastic Gradient Descent(16573): loss=0.05366777761013547\n",
      "Stochastic Gradient Descent(16574): loss=1.0832865500732516\n",
      "Stochastic Gradient Descent(16575): loss=13.279680983321404\n",
      "Stochastic Gradient Descent(16576): loss=11.374115841960935\n",
      "Stochastic Gradient Descent(16577): loss=0.3866178721528691\n",
      "Stochastic Gradient Descent(16578): loss=1.0582360821096992\n",
      "Stochastic Gradient Descent(16579): loss=0.9751317148643015\n",
      "Stochastic Gradient Descent(16580): loss=0.4500577244492344\n",
      "Stochastic Gradient Descent(16581): loss=5.569993081724683\n",
      "Stochastic Gradient Descent(16582): loss=6.633592824775403\n",
      "Stochastic Gradient Descent(16583): loss=2.713854075644311\n",
      "Stochastic Gradient Descent(16584): loss=1.1120123679310603\n",
      "Stochastic Gradient Descent(16585): loss=0.6454550128825112\n",
      "Stochastic Gradient Descent(16586): loss=2.386458416044449e-06\n",
      "Stochastic Gradient Descent(16587): loss=0.020523183653220273\n",
      "Stochastic Gradient Descent(16588): loss=0.001004966312850644\n",
      "Stochastic Gradient Descent(16589): loss=4.0706421223482\n",
      "Stochastic Gradient Descent(16590): loss=5.903308724883998\n",
      "Stochastic Gradient Descent(16591): loss=0.16022058466718042\n",
      "Stochastic Gradient Descent(16592): loss=0.0168033215842963\n",
      "Stochastic Gradient Descent(16593): loss=5.611493462437207\n",
      "Stochastic Gradient Descent(16594): loss=0.29800279217835196\n",
      "Stochastic Gradient Descent(16595): loss=3.9903137911743403\n",
      "Stochastic Gradient Descent(16596): loss=1.5994790664790175\n",
      "Stochastic Gradient Descent(16597): loss=13.220700695384178\n",
      "Stochastic Gradient Descent(16598): loss=0.01977409255698386\n",
      "Stochastic Gradient Descent(16599): loss=9.146969568970105\n",
      "Stochastic Gradient Descent(16600): loss=0.08324631600468194\n",
      "Stochastic Gradient Descent(16601): loss=9.571986597506053\n",
      "Stochastic Gradient Descent(16602): loss=1.174581275952707\n",
      "Stochastic Gradient Descent(16603): loss=2.2209530051011126\n",
      "Stochastic Gradient Descent(16604): loss=0.8629847418662888\n",
      "Stochastic Gradient Descent(16605): loss=1.7472251921356483\n",
      "Stochastic Gradient Descent(16606): loss=0.927387767832453\n",
      "Stochastic Gradient Descent(16607): loss=1.76300029068526e-05\n",
      "Stochastic Gradient Descent(16608): loss=0.35895288872780534\n",
      "Stochastic Gradient Descent(16609): loss=5.369558225935333\n",
      "Stochastic Gradient Descent(16610): loss=2.75976113954295\n",
      "Stochastic Gradient Descent(16611): loss=21.361410964426966\n",
      "Stochastic Gradient Descent(16612): loss=0.5450110476263127\n",
      "Stochastic Gradient Descent(16613): loss=0.07218567248652893\n",
      "Stochastic Gradient Descent(16614): loss=16.325855243836948\n",
      "Stochastic Gradient Descent(16615): loss=0.08952068482212423\n",
      "Stochastic Gradient Descent(16616): loss=0.6688438607577789\n",
      "Stochastic Gradient Descent(16617): loss=18.423691743969563\n",
      "Stochastic Gradient Descent(16618): loss=11.521192818289737\n",
      "Stochastic Gradient Descent(16619): loss=0.5471315008398958\n",
      "Stochastic Gradient Descent(16620): loss=0.11360974020931609\n",
      "Stochastic Gradient Descent(16621): loss=27.652962509795724\n",
      "Stochastic Gradient Descent(16622): loss=0.13124266876564308\n",
      "Stochastic Gradient Descent(16623): loss=6.675836002467028\n",
      "Stochastic Gradient Descent(16624): loss=4.980525651313671\n",
      "Stochastic Gradient Descent(16625): loss=5.330205710562907\n",
      "Stochastic Gradient Descent(16626): loss=0.0072285089124622314\n",
      "Stochastic Gradient Descent(16627): loss=4.718036494858623\n",
      "Stochastic Gradient Descent(16628): loss=14.162367851899434\n",
      "Stochastic Gradient Descent(16629): loss=3.9959596272563447\n",
      "Stochastic Gradient Descent(16630): loss=0.00335927475775388\n",
      "Stochastic Gradient Descent(16631): loss=0.07192861157690336\n",
      "Stochastic Gradient Descent(16632): loss=0.0007181090708527653\n",
      "Stochastic Gradient Descent(16633): loss=0.3647161252846153\n",
      "Stochastic Gradient Descent(16634): loss=3.3093981722076897\n",
      "Stochastic Gradient Descent(16635): loss=1.5269220167265045\n",
      "Stochastic Gradient Descent(16636): loss=18.477812013681753\n",
      "Stochastic Gradient Descent(16637): loss=0.227014481076329\n",
      "Stochastic Gradient Descent(16638): loss=7.928000619039415\n",
      "Stochastic Gradient Descent(16639): loss=4.929616107505498\n",
      "Stochastic Gradient Descent(16640): loss=1.6509229966895211\n",
      "Stochastic Gradient Descent(16641): loss=4.104184988383332\n",
      "Stochastic Gradient Descent(16642): loss=3.4203916394660183\n",
      "Stochastic Gradient Descent(16643): loss=0.6102468029841861\n",
      "Stochastic Gradient Descent(16644): loss=1.351268446172002\n",
      "Stochastic Gradient Descent(16645): loss=2.933850196631539\n",
      "Stochastic Gradient Descent(16646): loss=1.687620288293598\n",
      "Stochastic Gradient Descent(16647): loss=4.865318225135789\n",
      "Stochastic Gradient Descent(16648): loss=0.63991961091069\n",
      "Stochastic Gradient Descent(16649): loss=0.37539037395163327\n",
      "Stochastic Gradient Descent(16650): loss=4.391434033066335\n",
      "Stochastic Gradient Descent(16651): loss=2.7466508495231214\n",
      "Stochastic Gradient Descent(16652): loss=0.8664889817033606\n",
      "Stochastic Gradient Descent(16653): loss=1.5683892776815058\n",
      "Stochastic Gradient Descent(16654): loss=19.82433635564056\n",
      "Stochastic Gradient Descent(16655): loss=0.1633491417645137\n",
      "Stochastic Gradient Descent(16656): loss=0.729687827001592\n",
      "Stochastic Gradient Descent(16657): loss=5.692521786394188\n",
      "Stochastic Gradient Descent(16658): loss=1.0231486223703576\n",
      "Stochastic Gradient Descent(16659): loss=9.644728159986546\n",
      "Stochastic Gradient Descent(16660): loss=0.20342529389584516\n",
      "Stochastic Gradient Descent(16661): loss=15.947895744584734\n",
      "Stochastic Gradient Descent(16662): loss=5.813631711443667\n",
      "Stochastic Gradient Descent(16663): loss=13.095927690316556\n",
      "Stochastic Gradient Descent(16664): loss=0.001483454798262937\n",
      "Stochastic Gradient Descent(16665): loss=1.1846654705227067\n",
      "Stochastic Gradient Descent(16666): loss=4.554150516395893\n",
      "Stochastic Gradient Descent(16667): loss=1.5647527467712057\n",
      "Stochastic Gradient Descent(16668): loss=23.27325547244418\n",
      "Stochastic Gradient Descent(16669): loss=1.5220388745622333\n",
      "Stochastic Gradient Descent(16670): loss=4.311596994187736\n",
      "Stochastic Gradient Descent(16671): loss=2.088850006636286\n",
      "Stochastic Gradient Descent(16672): loss=0.5725595367922754\n",
      "Stochastic Gradient Descent(16673): loss=0.32444678966176443\n",
      "Stochastic Gradient Descent(16674): loss=0.006062470083970493\n",
      "Stochastic Gradient Descent(16675): loss=2.0277293357109785\n",
      "Stochastic Gradient Descent(16676): loss=0.17537979864514594\n",
      "Stochastic Gradient Descent(16677): loss=1.3124463267966509\n",
      "Stochastic Gradient Descent(16678): loss=5.83753319885476\n",
      "Stochastic Gradient Descent(16679): loss=3.2299925113862993\n",
      "Stochastic Gradient Descent(16680): loss=9.899313738171156\n",
      "Stochastic Gradient Descent(16681): loss=4.008049191682576\n",
      "Stochastic Gradient Descent(16682): loss=1.126554427776538\n",
      "Stochastic Gradient Descent(16683): loss=0.8837061339740566\n",
      "Stochastic Gradient Descent(16684): loss=20.326425876569417\n",
      "Stochastic Gradient Descent(16685): loss=3.5357695615343343\n",
      "Stochastic Gradient Descent(16686): loss=0.011087439302674185\n",
      "Stochastic Gradient Descent(16687): loss=0.5152840920853026\n",
      "Stochastic Gradient Descent(16688): loss=0.6812482100596263\n",
      "Stochastic Gradient Descent(16689): loss=4.684371391947222\n",
      "Stochastic Gradient Descent(16690): loss=5.4908983881322415\n",
      "Stochastic Gradient Descent(16691): loss=7.071588320054867\n",
      "Stochastic Gradient Descent(16692): loss=0.3315124963695252\n",
      "Stochastic Gradient Descent(16693): loss=3.6764954847639695\n",
      "Stochastic Gradient Descent(16694): loss=33.78082966658481\n",
      "Stochastic Gradient Descent(16695): loss=1.1909051370993446\n",
      "Stochastic Gradient Descent(16696): loss=3.288385263386165\n",
      "Stochastic Gradient Descent(16697): loss=11.311924119749632\n",
      "Stochastic Gradient Descent(16698): loss=9.241760588187438\n",
      "Stochastic Gradient Descent(16699): loss=1.6816785519775737\n",
      "Stochastic Gradient Descent(16700): loss=0.03544673443636772\n",
      "Stochastic Gradient Descent(16701): loss=0.3233682852665099\n",
      "Stochastic Gradient Descent(16702): loss=39.55800970100115\n",
      "Stochastic Gradient Descent(16703): loss=20.683395960397302\n",
      "Stochastic Gradient Descent(16704): loss=1.0693875204500556\n",
      "Stochastic Gradient Descent(16705): loss=8.69522028735869\n",
      "Stochastic Gradient Descent(16706): loss=1.8799476343650445\n",
      "Stochastic Gradient Descent(16707): loss=1.9951546840137993\n",
      "Stochastic Gradient Descent(16708): loss=5.248115967922273\n",
      "Stochastic Gradient Descent(16709): loss=0.1976464170141733\n",
      "Stochastic Gradient Descent(16710): loss=0.6919414413841773\n",
      "Stochastic Gradient Descent(16711): loss=0.5347016156846529\n",
      "Stochastic Gradient Descent(16712): loss=0.11266596639235052\n",
      "Stochastic Gradient Descent(16713): loss=0.4409125172731176\n",
      "Stochastic Gradient Descent(16714): loss=8.484838610000109\n",
      "Stochastic Gradient Descent(16715): loss=0.0709622930243914\n",
      "Stochastic Gradient Descent(16716): loss=5.778087229046095\n",
      "Stochastic Gradient Descent(16717): loss=9.352704268055394\n",
      "Stochastic Gradient Descent(16718): loss=0.31420422554282074\n",
      "Stochastic Gradient Descent(16719): loss=4.710496039807925\n",
      "Stochastic Gradient Descent(16720): loss=0.0009649260298779096\n",
      "Stochastic Gradient Descent(16721): loss=3.0073043329004663\n",
      "Stochastic Gradient Descent(16722): loss=3.4474683713283136\n",
      "Stochastic Gradient Descent(16723): loss=0.03251960046250338\n",
      "Stochastic Gradient Descent(16724): loss=0.30241006696021955\n",
      "Stochastic Gradient Descent(16725): loss=27.164699223752127\n",
      "Stochastic Gradient Descent(16726): loss=6.725582328960648\n",
      "Stochastic Gradient Descent(16727): loss=32.31627882603874\n",
      "Stochastic Gradient Descent(16728): loss=6.555821364619214\n",
      "Stochastic Gradient Descent(16729): loss=5.695772295546531\n",
      "Stochastic Gradient Descent(16730): loss=0.12098035952686818\n",
      "Stochastic Gradient Descent(16731): loss=0.5336291682195506\n",
      "Stochastic Gradient Descent(16732): loss=8.842463538746456\n",
      "Stochastic Gradient Descent(16733): loss=0.10654983976222702\n",
      "Stochastic Gradient Descent(16734): loss=8.573311174171323\n",
      "Stochastic Gradient Descent(16735): loss=0.06532341190303698\n",
      "Stochastic Gradient Descent(16736): loss=0.5766067695086373\n",
      "Stochastic Gradient Descent(16737): loss=0.18374059525666658\n",
      "Stochastic Gradient Descent(16738): loss=2.6130101825721277\n",
      "Stochastic Gradient Descent(16739): loss=4.864995534376061\n",
      "Stochastic Gradient Descent(16740): loss=5.851418397233745\n",
      "Stochastic Gradient Descent(16741): loss=11.624260221528733\n",
      "Stochastic Gradient Descent(16742): loss=0.9767002198477276\n",
      "Stochastic Gradient Descent(16743): loss=0.949745157319695\n",
      "Stochastic Gradient Descent(16744): loss=10.878717821836537\n",
      "Stochastic Gradient Descent(16745): loss=5.92473588406538\n",
      "Stochastic Gradient Descent(16746): loss=5.248299967799446\n",
      "Stochastic Gradient Descent(16747): loss=0.040729098014107346\n",
      "Stochastic Gradient Descent(16748): loss=0.006093443511703251\n",
      "Stochastic Gradient Descent(16749): loss=6.335723765976415\n",
      "Stochastic Gradient Descent(16750): loss=0.04668987720295655\n",
      "Stochastic Gradient Descent(16751): loss=0.6488203190946371\n",
      "Stochastic Gradient Descent(16752): loss=4.295557780946861\n",
      "Stochastic Gradient Descent(16753): loss=12.538147934173274\n",
      "Stochastic Gradient Descent(16754): loss=0.13392501842616447\n",
      "Stochastic Gradient Descent(16755): loss=0.001278459965195227\n",
      "Stochastic Gradient Descent(16756): loss=4.903366543888631\n",
      "Stochastic Gradient Descent(16757): loss=3.1114911671587686\n",
      "Stochastic Gradient Descent(16758): loss=9.380594200849954\n",
      "Stochastic Gradient Descent(16759): loss=5.740857800076537\n",
      "Stochastic Gradient Descent(16760): loss=4.491779210288496\n",
      "Stochastic Gradient Descent(16761): loss=0.2560245204172681\n",
      "Stochastic Gradient Descent(16762): loss=0.04477386173117907\n",
      "Stochastic Gradient Descent(16763): loss=1.7450005934938866\n",
      "Stochastic Gradient Descent(16764): loss=4.485356131949076\n",
      "Stochastic Gradient Descent(16765): loss=0.7781426762139968\n",
      "Stochastic Gradient Descent(16766): loss=1.863927313751908\n",
      "Stochastic Gradient Descent(16767): loss=5.530651047769586\n",
      "Stochastic Gradient Descent(16768): loss=2.7259454901047047\n",
      "Stochastic Gradient Descent(16769): loss=4.541064495827642\n",
      "Stochastic Gradient Descent(16770): loss=4.856848314237156\n",
      "Stochastic Gradient Descent(16771): loss=0.2862998388524107\n",
      "Stochastic Gradient Descent(16772): loss=11.897728822501167\n",
      "Stochastic Gradient Descent(16773): loss=6.622384980023194\n",
      "Stochastic Gradient Descent(16774): loss=0.0036692197711699816\n",
      "Stochastic Gradient Descent(16775): loss=23.049388279068175\n",
      "Stochastic Gradient Descent(16776): loss=0.2665554596050032\n",
      "Stochastic Gradient Descent(16777): loss=0.12287832445602316\n",
      "Stochastic Gradient Descent(16778): loss=0.7104691806265255\n",
      "Stochastic Gradient Descent(16779): loss=0.22711742473710558\n",
      "Stochastic Gradient Descent(16780): loss=2.004011240262635\n",
      "Stochastic Gradient Descent(16781): loss=14.60210010437702\n",
      "Stochastic Gradient Descent(16782): loss=0.9465754900545272\n",
      "Stochastic Gradient Descent(16783): loss=0.08780975425978128\n",
      "Stochastic Gradient Descent(16784): loss=0.3680296321164\n",
      "Stochastic Gradient Descent(16785): loss=8.057490019522621\n",
      "Stochastic Gradient Descent(16786): loss=6.59311221663467\n",
      "Stochastic Gradient Descent(16787): loss=0.03323176723737245\n",
      "Stochastic Gradient Descent(16788): loss=3.616400394629887\n",
      "Stochastic Gradient Descent(16789): loss=0.365385587704815\n",
      "Stochastic Gradient Descent(16790): loss=4.014960741488394\n",
      "Stochastic Gradient Descent(16791): loss=12.773424582307067\n",
      "Stochastic Gradient Descent(16792): loss=0.01626480811145311\n",
      "Stochastic Gradient Descent(16793): loss=16.55723067727454\n",
      "Stochastic Gradient Descent(16794): loss=0.5593742567172767\n",
      "Stochastic Gradient Descent(16795): loss=30.971104420972335\n",
      "Stochastic Gradient Descent(16796): loss=13.711194708545104\n",
      "Stochastic Gradient Descent(16797): loss=6.855414466978495\n",
      "Stochastic Gradient Descent(16798): loss=0.28307909145371934\n",
      "Stochastic Gradient Descent(16799): loss=0.6117723961221063\n",
      "Stochastic Gradient Descent(16800): loss=0.8771151390181844\n",
      "Stochastic Gradient Descent(16801): loss=1.1036911812131496\n",
      "Stochastic Gradient Descent(16802): loss=0.4304906849845306\n",
      "Stochastic Gradient Descent(16803): loss=11.55736047689547\n",
      "Stochastic Gradient Descent(16804): loss=0.09741691426416536\n",
      "Stochastic Gradient Descent(16805): loss=3.1095784264925617\n",
      "Stochastic Gradient Descent(16806): loss=0.5411615709604827\n",
      "Stochastic Gradient Descent(16807): loss=6.958323668318955\n",
      "Stochastic Gradient Descent(16808): loss=0.3588828691210225\n",
      "Stochastic Gradient Descent(16809): loss=1.4928446989704107\n",
      "Stochastic Gradient Descent(16810): loss=0.7852678819455068\n",
      "Stochastic Gradient Descent(16811): loss=4.145969594263742\n",
      "Stochastic Gradient Descent(16812): loss=14.199006897804908\n",
      "Stochastic Gradient Descent(16813): loss=0.4548914438116527\n",
      "Stochastic Gradient Descent(16814): loss=1.0227173510893195\n",
      "Stochastic Gradient Descent(16815): loss=1.2729105202799083\n",
      "Stochastic Gradient Descent(16816): loss=11.773897030997945\n",
      "Stochastic Gradient Descent(16817): loss=1.0402576318276253\n",
      "Stochastic Gradient Descent(16818): loss=0.15226504964637796\n",
      "Stochastic Gradient Descent(16819): loss=47.713843143436584\n",
      "Stochastic Gradient Descent(16820): loss=4.961698008943614\n",
      "Stochastic Gradient Descent(16821): loss=0.8982152237480022\n",
      "Stochastic Gradient Descent(16822): loss=0.019417318990982802\n",
      "Stochastic Gradient Descent(16823): loss=1.5030893663571463\n",
      "Stochastic Gradient Descent(16824): loss=2.880697146740237\n",
      "Stochastic Gradient Descent(16825): loss=2.862603449658207\n",
      "Stochastic Gradient Descent(16826): loss=2.630321779300332\n",
      "Stochastic Gradient Descent(16827): loss=0.0007452889643080099\n",
      "Stochastic Gradient Descent(16828): loss=0.4844707741568901\n",
      "Stochastic Gradient Descent(16829): loss=0.09551275688511629\n",
      "Stochastic Gradient Descent(16830): loss=0.5403426641675302\n",
      "Stochastic Gradient Descent(16831): loss=0.6250918897920686\n",
      "Stochastic Gradient Descent(16832): loss=5.127559389539903\n",
      "Stochastic Gradient Descent(16833): loss=11.914044864257713\n",
      "Stochastic Gradient Descent(16834): loss=0.1055938254677557\n",
      "Stochastic Gradient Descent(16835): loss=13.681443204914979\n",
      "Stochastic Gradient Descent(16836): loss=12.524916383423406\n",
      "Stochastic Gradient Descent(16837): loss=1.1668679285900745\n",
      "Stochastic Gradient Descent(16838): loss=4.281970046000788\n",
      "Stochastic Gradient Descent(16839): loss=0.7145605042774471\n",
      "Stochastic Gradient Descent(16840): loss=1.5369669047822172\n",
      "Stochastic Gradient Descent(16841): loss=13.134360601922976\n",
      "Stochastic Gradient Descent(16842): loss=7.92662936969264e-06\n",
      "Stochastic Gradient Descent(16843): loss=31.569994081005877\n",
      "Stochastic Gradient Descent(16844): loss=3.2593682027496484\n",
      "Stochastic Gradient Descent(16845): loss=0.2012766684108218\n",
      "Stochastic Gradient Descent(16846): loss=14.621808494213273\n",
      "Stochastic Gradient Descent(16847): loss=3.388432358056532\n",
      "Stochastic Gradient Descent(16848): loss=1.9062608412082143\n",
      "Stochastic Gradient Descent(16849): loss=3.6298387262419602\n",
      "Stochastic Gradient Descent(16850): loss=0.5794730262109519\n",
      "Stochastic Gradient Descent(16851): loss=0.2864401790205314\n",
      "Stochastic Gradient Descent(16852): loss=0.9121103007108308\n",
      "Stochastic Gradient Descent(16853): loss=3.3616084113168063\n",
      "Stochastic Gradient Descent(16854): loss=8.466929450618423\n",
      "Stochastic Gradient Descent(16855): loss=14.188644166121522\n",
      "Stochastic Gradient Descent(16856): loss=0.5108969411146327\n",
      "Stochastic Gradient Descent(16857): loss=0.2381004794366801\n",
      "Stochastic Gradient Descent(16858): loss=21.865838379091425\n",
      "Stochastic Gradient Descent(16859): loss=3.1051016776437206\n",
      "Stochastic Gradient Descent(16860): loss=4.589535160292104\n",
      "Stochastic Gradient Descent(16861): loss=65.7506072782906\n",
      "Stochastic Gradient Descent(16862): loss=0.46737359790200955\n",
      "Stochastic Gradient Descent(16863): loss=2.466171253091589\n",
      "Stochastic Gradient Descent(16864): loss=0.6476078375295068\n",
      "Stochastic Gradient Descent(16865): loss=60.30846001357091\n",
      "Stochastic Gradient Descent(16866): loss=14.023420609158098\n",
      "Stochastic Gradient Descent(16867): loss=7.40918944102994\n",
      "Stochastic Gradient Descent(16868): loss=1.4489768795450755\n",
      "Stochastic Gradient Descent(16869): loss=9.842504985798914\n",
      "Stochastic Gradient Descent(16870): loss=0.2129510907596431\n",
      "Stochastic Gradient Descent(16871): loss=0.2429307032989069\n",
      "Stochastic Gradient Descent(16872): loss=3.327486630338395\n",
      "Stochastic Gradient Descent(16873): loss=6.344748724965781\n",
      "Stochastic Gradient Descent(16874): loss=1.9234283757377504\n",
      "Stochastic Gradient Descent(16875): loss=8.401373201941597\n",
      "Stochastic Gradient Descent(16876): loss=2.385626567803494\n",
      "Stochastic Gradient Descent(16877): loss=2.2351239375354823\n",
      "Stochastic Gradient Descent(16878): loss=12.589914944153401\n",
      "Stochastic Gradient Descent(16879): loss=5.6956229728953955\n",
      "Stochastic Gradient Descent(16880): loss=8.523937114495117\n",
      "Stochastic Gradient Descent(16881): loss=4.014811096296242\n",
      "Stochastic Gradient Descent(16882): loss=0.49055276223488814\n",
      "Stochastic Gradient Descent(16883): loss=2.573950924726869\n",
      "Stochastic Gradient Descent(16884): loss=3.604314272861777\n",
      "Stochastic Gradient Descent(16885): loss=17.171062975990743\n",
      "Stochastic Gradient Descent(16886): loss=4.3675704282866885\n",
      "Stochastic Gradient Descent(16887): loss=1.999963706815596\n",
      "Stochastic Gradient Descent(16888): loss=1.705513769594816\n",
      "Stochastic Gradient Descent(16889): loss=0.3906953525924279\n",
      "Stochastic Gradient Descent(16890): loss=0.7655526953863938\n",
      "Stochastic Gradient Descent(16891): loss=0.13871927604001452\n",
      "Stochastic Gradient Descent(16892): loss=1.0106604692725532\n",
      "Stochastic Gradient Descent(16893): loss=22.991628934646094\n",
      "Stochastic Gradient Descent(16894): loss=33.69640666479871\n",
      "Stochastic Gradient Descent(16895): loss=1.6061508009453496\n",
      "Stochastic Gradient Descent(16896): loss=2.4033790052003448\n",
      "Stochastic Gradient Descent(16897): loss=0.7510774485479721\n",
      "Stochastic Gradient Descent(16898): loss=5.2532871352886\n",
      "Stochastic Gradient Descent(16899): loss=1.378989971380669\n",
      "Stochastic Gradient Descent(16900): loss=5.945083224910225\n",
      "Stochastic Gradient Descent(16901): loss=3.453662690542829\n",
      "Stochastic Gradient Descent(16902): loss=2.0370345165357815\n",
      "Stochastic Gradient Descent(16903): loss=0.19959880648545017\n",
      "Stochastic Gradient Descent(16904): loss=2.5114239552891506\n",
      "Stochastic Gradient Descent(16905): loss=1.9358641683258009\n",
      "Stochastic Gradient Descent(16906): loss=16.506677069488806\n",
      "Stochastic Gradient Descent(16907): loss=3.303318405852738\n",
      "Stochastic Gradient Descent(16908): loss=1.6970046100129985\n",
      "Stochastic Gradient Descent(16909): loss=0.38418749189693957\n",
      "Stochastic Gradient Descent(16910): loss=7.657544632279889\n",
      "Stochastic Gradient Descent(16911): loss=1.1762042701235191\n",
      "Stochastic Gradient Descent(16912): loss=0.25676610532396626\n",
      "Stochastic Gradient Descent(16913): loss=0.1466994197113844\n",
      "Stochastic Gradient Descent(16914): loss=47.20345630441617\n",
      "Stochastic Gradient Descent(16915): loss=0.8892544846726779\n",
      "Stochastic Gradient Descent(16916): loss=0.5710965920797895\n",
      "Stochastic Gradient Descent(16917): loss=2.1342547014155846\n",
      "Stochastic Gradient Descent(16918): loss=0.23938724972916103\n",
      "Stochastic Gradient Descent(16919): loss=0.9802154476094613\n",
      "Stochastic Gradient Descent(16920): loss=2.8348132649041755\n",
      "Stochastic Gradient Descent(16921): loss=4.11928013418266\n",
      "Stochastic Gradient Descent(16922): loss=39.776688884581354\n",
      "Stochastic Gradient Descent(16923): loss=4.555125375067918\n",
      "Stochastic Gradient Descent(16924): loss=8.665667948690528\n",
      "Stochastic Gradient Descent(16925): loss=0.5079817563290999\n",
      "Stochastic Gradient Descent(16926): loss=1.414617879995505\n",
      "Stochastic Gradient Descent(16927): loss=1.9509875237466803\n",
      "Stochastic Gradient Descent(16928): loss=4.134931791291949\n",
      "Stochastic Gradient Descent(16929): loss=1.2712946461046657\n",
      "Stochastic Gradient Descent(16930): loss=0.5955898261352741\n",
      "Stochastic Gradient Descent(16931): loss=0.4561325739181834\n",
      "Stochastic Gradient Descent(16932): loss=4.137629287213822\n",
      "Stochastic Gradient Descent(16933): loss=0.10815363033290479\n",
      "Stochastic Gradient Descent(16934): loss=34.55642718078741\n",
      "Stochastic Gradient Descent(16935): loss=23.745347081409424\n",
      "Stochastic Gradient Descent(16936): loss=0.0005645388663969558\n",
      "Stochastic Gradient Descent(16937): loss=0.08858594541119368\n",
      "Stochastic Gradient Descent(16938): loss=2.9231002439237272\n",
      "Stochastic Gradient Descent(16939): loss=13.970579365109165\n",
      "Stochastic Gradient Descent(16940): loss=0.049628091792847204\n",
      "Stochastic Gradient Descent(16941): loss=0.037920407574336464\n",
      "Stochastic Gradient Descent(16942): loss=0.09038414896132904\n",
      "Stochastic Gradient Descent(16943): loss=7.705868022574302\n",
      "Stochastic Gradient Descent(16944): loss=6.531916615264138\n",
      "Stochastic Gradient Descent(16945): loss=9.79787336984494\n",
      "Stochastic Gradient Descent(16946): loss=24.47673372693899\n",
      "Stochastic Gradient Descent(16947): loss=24.955765444241923\n",
      "Stochastic Gradient Descent(16948): loss=4.675470960073832\n",
      "Stochastic Gradient Descent(16949): loss=1.6477687497221716\n",
      "Stochastic Gradient Descent(16950): loss=5.724140952074402\n",
      "Stochastic Gradient Descent(16951): loss=0.2896139567333078\n",
      "Stochastic Gradient Descent(16952): loss=2.150795065557473\n",
      "Stochastic Gradient Descent(16953): loss=3.489831382417809\n",
      "Stochastic Gradient Descent(16954): loss=0.05140751248321786\n",
      "Stochastic Gradient Descent(16955): loss=0.3386146354947373\n",
      "Stochastic Gradient Descent(16956): loss=0.014347997954878627\n",
      "Stochastic Gradient Descent(16957): loss=0.9880828787695596\n",
      "Stochastic Gradient Descent(16958): loss=0.12562441604073646\n",
      "Stochastic Gradient Descent(16959): loss=9.214607172185893\n",
      "Stochastic Gradient Descent(16960): loss=0.8659631550236053\n",
      "Stochastic Gradient Descent(16961): loss=0.28503812884780216\n",
      "Stochastic Gradient Descent(16962): loss=0.15241376381402735\n",
      "Stochastic Gradient Descent(16963): loss=0.46729060850353954\n",
      "Stochastic Gradient Descent(16964): loss=13.871877706524273\n",
      "Stochastic Gradient Descent(16965): loss=0.02881443274833364\n",
      "Stochastic Gradient Descent(16966): loss=0.00012105855796342537\n",
      "Stochastic Gradient Descent(16967): loss=0.001518442334565465\n",
      "Stochastic Gradient Descent(16968): loss=4.711850665217338\n",
      "Stochastic Gradient Descent(16969): loss=0.09131118002131923\n",
      "Stochastic Gradient Descent(16970): loss=2.680219106163805\n",
      "Stochastic Gradient Descent(16971): loss=2.558663618911637\n",
      "Stochastic Gradient Descent(16972): loss=0.007982663337449618\n",
      "Stochastic Gradient Descent(16973): loss=1.4878516380092672\n",
      "Stochastic Gradient Descent(16974): loss=2.4158796003773992\n",
      "Stochastic Gradient Descent(16975): loss=33.77219024465211\n",
      "Stochastic Gradient Descent(16976): loss=7.848281953139232\n",
      "Stochastic Gradient Descent(16977): loss=3.366333335883592\n",
      "Stochastic Gradient Descent(16978): loss=1.804249424749979\n",
      "Stochastic Gradient Descent(16979): loss=0.009328207635682305\n",
      "Stochastic Gradient Descent(16980): loss=0.01879954179880414\n",
      "Stochastic Gradient Descent(16981): loss=0.32470879167758615\n",
      "Stochastic Gradient Descent(16982): loss=0.5587276718612567\n",
      "Stochastic Gradient Descent(16983): loss=0.061778401566201485\n",
      "Stochastic Gradient Descent(16984): loss=9.153090303625808\n",
      "Stochastic Gradient Descent(16985): loss=0.04577287883793514\n",
      "Stochastic Gradient Descent(16986): loss=12.190573236662651\n",
      "Stochastic Gradient Descent(16987): loss=1.483821294417458\n",
      "Stochastic Gradient Descent(16988): loss=1.5728902193624483\n",
      "Stochastic Gradient Descent(16989): loss=7.851251204739371\n",
      "Stochastic Gradient Descent(16990): loss=0.0007477596066275659\n",
      "Stochastic Gradient Descent(16991): loss=2.2661072432112617\n",
      "Stochastic Gradient Descent(16992): loss=2.2467892456907568\n",
      "Stochastic Gradient Descent(16993): loss=2.301984678809623\n",
      "Stochastic Gradient Descent(16994): loss=0.2526512028587557\n",
      "Stochastic Gradient Descent(16995): loss=1.8985064944199084\n",
      "Stochastic Gradient Descent(16996): loss=0.0484362461643673\n",
      "Stochastic Gradient Descent(16997): loss=6.755215153203531\n",
      "Stochastic Gradient Descent(16998): loss=3.1324398994814073\n",
      "Stochastic Gradient Descent(16999): loss=0.03470413812081157\n",
      "Stochastic Gradient Descent(17000): loss=5.2477886251886465\n",
      "Stochastic Gradient Descent(17001): loss=15.452463067655488\n",
      "Stochastic Gradient Descent(17002): loss=0.9944243199403345\n",
      "Stochastic Gradient Descent(17003): loss=29.81119489535358\n",
      "Stochastic Gradient Descent(17004): loss=6.152961532031703\n",
      "Stochastic Gradient Descent(17005): loss=0.5983405062442996\n",
      "Stochastic Gradient Descent(17006): loss=0.1155199832955642\n",
      "Stochastic Gradient Descent(17007): loss=0.00694507508606641\n",
      "Stochastic Gradient Descent(17008): loss=4.2786547577332685\n",
      "Stochastic Gradient Descent(17009): loss=0.12267738243998529\n",
      "Stochastic Gradient Descent(17010): loss=16.291059415307952\n",
      "Stochastic Gradient Descent(17011): loss=16.867676886077735\n",
      "Stochastic Gradient Descent(17012): loss=0.04207001340296997\n",
      "Stochastic Gradient Descent(17013): loss=3.3132948676830782\n",
      "Stochastic Gradient Descent(17014): loss=0.7459750943122745\n",
      "Stochastic Gradient Descent(17015): loss=8.083649193214002\n",
      "Stochastic Gradient Descent(17016): loss=2.6032320819650256\n",
      "Stochastic Gradient Descent(17017): loss=7.862613728856039\n",
      "Stochastic Gradient Descent(17018): loss=0.056491792447754416\n",
      "Stochastic Gradient Descent(17019): loss=0.3210189388636029\n",
      "Stochastic Gradient Descent(17020): loss=18.46810045873922\n",
      "Stochastic Gradient Descent(17021): loss=28.01110966289087\n",
      "Stochastic Gradient Descent(17022): loss=3.0971765796698496\n",
      "Stochastic Gradient Descent(17023): loss=1.9321722615815908\n",
      "Stochastic Gradient Descent(17024): loss=1.2927262272700775\n",
      "Stochastic Gradient Descent(17025): loss=0.09273491962270611\n",
      "Stochastic Gradient Descent(17026): loss=12.077930538544752\n",
      "Stochastic Gradient Descent(17027): loss=0.573387607465284\n",
      "Stochastic Gradient Descent(17028): loss=0.48058099809878263\n",
      "Stochastic Gradient Descent(17029): loss=6.507672890549556\n",
      "Stochastic Gradient Descent(17030): loss=2.578895502868036\n",
      "Stochastic Gradient Descent(17031): loss=11.639763993311966\n",
      "Stochastic Gradient Descent(17032): loss=2.0635189435810664\n",
      "Stochastic Gradient Descent(17033): loss=0.3856352333976098\n",
      "Stochastic Gradient Descent(17034): loss=1.7213559925756279\n",
      "Stochastic Gradient Descent(17035): loss=10.904452853485955\n",
      "Stochastic Gradient Descent(17036): loss=6.912959400931452\n",
      "Stochastic Gradient Descent(17037): loss=1.5851601446300787\n",
      "Stochastic Gradient Descent(17038): loss=1.190228697582773\n",
      "Stochastic Gradient Descent(17039): loss=10.24942814101628\n",
      "Stochastic Gradient Descent(17040): loss=1.2943886958542592\n",
      "Stochastic Gradient Descent(17041): loss=2.5567750937762606\n",
      "Stochastic Gradient Descent(17042): loss=1.7679013924576437\n",
      "Stochastic Gradient Descent(17043): loss=7.832739629848729\n",
      "Stochastic Gradient Descent(17044): loss=7.7572411638065395\n",
      "Stochastic Gradient Descent(17045): loss=1.3726790389265195\n",
      "Stochastic Gradient Descent(17046): loss=0.22618968299447428\n",
      "Stochastic Gradient Descent(17047): loss=11.702068978051088\n",
      "Stochastic Gradient Descent(17048): loss=1.2225773242105429\n",
      "Stochastic Gradient Descent(17049): loss=0.2586056569280798\n",
      "Stochastic Gradient Descent(17050): loss=1.3459929690978287\n",
      "Stochastic Gradient Descent(17051): loss=3.2412358701663013\n",
      "Stochastic Gradient Descent(17052): loss=0.04599760797291615\n",
      "Stochastic Gradient Descent(17053): loss=0.271864184357051\n",
      "Stochastic Gradient Descent(17054): loss=0.32142662621054696\n",
      "Stochastic Gradient Descent(17055): loss=3.146150986441508\n",
      "Stochastic Gradient Descent(17056): loss=0.6493657820011014\n",
      "Stochastic Gradient Descent(17057): loss=0.00701375073886238\n",
      "Stochastic Gradient Descent(17058): loss=16.14455920614309\n",
      "Stochastic Gradient Descent(17059): loss=1.1494934504148295\n",
      "Stochastic Gradient Descent(17060): loss=6.701618793731153e-05\n",
      "Stochastic Gradient Descent(17061): loss=2.0883309903066385\n",
      "Stochastic Gradient Descent(17062): loss=3.702280011098877\n",
      "Stochastic Gradient Descent(17063): loss=0.6891613711122243\n",
      "Stochastic Gradient Descent(17064): loss=4.145931602199203\n",
      "Stochastic Gradient Descent(17065): loss=17.850054121935653\n",
      "Stochastic Gradient Descent(17066): loss=14.844097391150703\n",
      "Stochastic Gradient Descent(17067): loss=0.5465722743070949\n",
      "Stochastic Gradient Descent(17068): loss=3.5403345347977675\n",
      "Stochastic Gradient Descent(17069): loss=4.800578330636682\n",
      "Stochastic Gradient Descent(17070): loss=3.3881089518758025\n",
      "Stochastic Gradient Descent(17071): loss=0.23601950264147406\n",
      "Stochastic Gradient Descent(17072): loss=0.47446746738528917\n",
      "Stochastic Gradient Descent(17073): loss=0.3123669983537932\n",
      "Stochastic Gradient Descent(17074): loss=0.07760678718432182\n",
      "Stochastic Gradient Descent(17075): loss=0.05693664162576256\n",
      "Stochastic Gradient Descent(17076): loss=2.775405425121037\n",
      "Stochastic Gradient Descent(17077): loss=0.01741440032798966\n",
      "Stochastic Gradient Descent(17078): loss=5.386900978580888\n",
      "Stochastic Gradient Descent(17079): loss=1.3129533715283521\n",
      "Stochastic Gradient Descent(17080): loss=4.572141791573038\n",
      "Stochastic Gradient Descent(17081): loss=5.811497182437865\n",
      "Stochastic Gradient Descent(17082): loss=17.717164422421167\n",
      "Stochastic Gradient Descent(17083): loss=15.405405463806552\n",
      "Stochastic Gradient Descent(17084): loss=4.6655417434113655\n",
      "Stochastic Gradient Descent(17085): loss=41.84780150581044\n",
      "Stochastic Gradient Descent(17086): loss=0.16896891102728662\n",
      "Stochastic Gradient Descent(17087): loss=0.8060993500236769\n",
      "Stochastic Gradient Descent(17088): loss=2.4100020139962512\n",
      "Stochastic Gradient Descent(17089): loss=0.679729318810685\n",
      "Stochastic Gradient Descent(17090): loss=1.020094681417712\n",
      "Stochastic Gradient Descent(17091): loss=2.650735823941106\n",
      "Stochastic Gradient Descent(17092): loss=0.0504194172937809\n",
      "Stochastic Gradient Descent(17093): loss=0.3192404127997839\n",
      "Stochastic Gradient Descent(17094): loss=2.8239643675306274\n",
      "Stochastic Gradient Descent(17095): loss=8.042311149906563\n",
      "Stochastic Gradient Descent(17096): loss=2.7116940224561437\n",
      "Stochastic Gradient Descent(17097): loss=13.129249681442396\n",
      "Stochastic Gradient Descent(17098): loss=8.755767340140851\n",
      "Stochastic Gradient Descent(17099): loss=8.80620623457723\n",
      "Stochastic Gradient Descent(17100): loss=9.760560128201876\n",
      "Stochastic Gradient Descent(17101): loss=0.23068782527261103\n",
      "Stochastic Gradient Descent(17102): loss=42.29941681994314\n",
      "Stochastic Gradient Descent(17103): loss=0.025365395454709973\n",
      "Stochastic Gradient Descent(17104): loss=0.24661156986571628\n",
      "Stochastic Gradient Descent(17105): loss=51.47597578569633\n",
      "Stochastic Gradient Descent(17106): loss=5.328081879548333\n",
      "Stochastic Gradient Descent(17107): loss=1.0736390507002325\n",
      "Stochastic Gradient Descent(17108): loss=0.04751308912554279\n",
      "Stochastic Gradient Descent(17109): loss=6.277709145600289\n",
      "Stochastic Gradient Descent(17110): loss=0.1835613516640434\n",
      "Stochastic Gradient Descent(17111): loss=3.0941588081628755\n",
      "Stochastic Gradient Descent(17112): loss=4.205619345823199\n",
      "Stochastic Gradient Descent(17113): loss=6.8123116745477805\n",
      "Stochastic Gradient Descent(17114): loss=4.486420728802503\n",
      "Stochastic Gradient Descent(17115): loss=2.3531282113092424\n",
      "Stochastic Gradient Descent(17116): loss=0.003931406369456728\n",
      "Stochastic Gradient Descent(17117): loss=4.816916967447862\n",
      "Stochastic Gradient Descent(17118): loss=40.71055353872215\n",
      "Stochastic Gradient Descent(17119): loss=2.947304543263377\n",
      "Stochastic Gradient Descent(17120): loss=26.57417435610618\n",
      "Stochastic Gradient Descent(17121): loss=26.390180505979387\n",
      "Stochastic Gradient Descent(17122): loss=3.3609903909466046\n",
      "Stochastic Gradient Descent(17123): loss=8.08275022928751\n",
      "Stochastic Gradient Descent(17124): loss=0.02406183926097483\n",
      "Stochastic Gradient Descent(17125): loss=4.422770731896399\n",
      "Stochastic Gradient Descent(17126): loss=1.180505468572259\n",
      "Stochastic Gradient Descent(17127): loss=48.64942680289515\n",
      "Stochastic Gradient Descent(17128): loss=9.08850105586728\n",
      "Stochastic Gradient Descent(17129): loss=0.2479860826929966\n",
      "Stochastic Gradient Descent(17130): loss=4.166947744652097\n",
      "Stochastic Gradient Descent(17131): loss=0.9564145653741155\n",
      "Stochastic Gradient Descent(17132): loss=9.36609969080918\n",
      "Stochastic Gradient Descent(17133): loss=8.564848968644396\n",
      "Stochastic Gradient Descent(17134): loss=1.220148553628367\n",
      "Stochastic Gradient Descent(17135): loss=4.873233780013414\n",
      "Stochastic Gradient Descent(17136): loss=0.4324119438532878\n",
      "Stochastic Gradient Descent(17137): loss=1.238384875940402\n",
      "Stochastic Gradient Descent(17138): loss=0.39013572800507157\n",
      "Stochastic Gradient Descent(17139): loss=9.967314809667219\n",
      "Stochastic Gradient Descent(17140): loss=4.71589489880324\n",
      "Stochastic Gradient Descent(17141): loss=0.013617460511134657\n",
      "Stochastic Gradient Descent(17142): loss=10.918189609015489\n",
      "Stochastic Gradient Descent(17143): loss=37.37723793156003\n",
      "Stochastic Gradient Descent(17144): loss=3.0930696834907407\n",
      "Stochastic Gradient Descent(17145): loss=2.949727475165938\n",
      "Stochastic Gradient Descent(17146): loss=1.5638661295599334\n",
      "Stochastic Gradient Descent(17147): loss=20.669582457138727\n",
      "Stochastic Gradient Descent(17148): loss=0.9484943215603724\n",
      "Stochastic Gradient Descent(17149): loss=24.650721600768854\n",
      "Stochastic Gradient Descent(17150): loss=4.8979975457920375\n",
      "Stochastic Gradient Descent(17151): loss=0.41463973232397733\n",
      "Stochastic Gradient Descent(17152): loss=0.4762886962502469\n",
      "Stochastic Gradient Descent(17153): loss=7.131278987431207\n",
      "Stochastic Gradient Descent(17154): loss=17.731687660860672\n",
      "Stochastic Gradient Descent(17155): loss=0.8380176340110337\n",
      "Stochastic Gradient Descent(17156): loss=0.43866322336962277\n",
      "Stochastic Gradient Descent(17157): loss=1.927560182053619\n",
      "Stochastic Gradient Descent(17158): loss=14.468157663346995\n",
      "Stochastic Gradient Descent(17159): loss=10.661855170152457\n",
      "Stochastic Gradient Descent(17160): loss=0.01478044570563455\n",
      "Stochastic Gradient Descent(17161): loss=36.64248810302287\n",
      "Stochastic Gradient Descent(17162): loss=0.00017804975983490897\n",
      "Stochastic Gradient Descent(17163): loss=0.6200340661603961\n",
      "Stochastic Gradient Descent(17164): loss=0.0030572204634825902\n",
      "Stochastic Gradient Descent(17165): loss=2.1049679659277153e-05\n",
      "Stochastic Gradient Descent(17166): loss=1.499927703482097\n",
      "Stochastic Gradient Descent(17167): loss=9.830933461551378\n",
      "Stochastic Gradient Descent(17168): loss=1.00454761209653\n",
      "Stochastic Gradient Descent(17169): loss=9.726591191603543\n",
      "Stochastic Gradient Descent(17170): loss=8.804024181166353\n",
      "Stochastic Gradient Descent(17171): loss=10.195435178861173\n",
      "Stochastic Gradient Descent(17172): loss=0.7846306464182364\n",
      "Stochastic Gradient Descent(17173): loss=4.54488775252541\n",
      "Stochastic Gradient Descent(17174): loss=19.927721688793515\n",
      "Stochastic Gradient Descent(17175): loss=5.826790673374363\n",
      "Stochastic Gradient Descent(17176): loss=2.1088610057176105\n",
      "Stochastic Gradient Descent(17177): loss=1.0168134984247923\n",
      "Stochastic Gradient Descent(17178): loss=0.047120820204651824\n",
      "Stochastic Gradient Descent(17179): loss=0.08875097165890797\n",
      "Stochastic Gradient Descent(17180): loss=0.7943046900698733\n",
      "Stochastic Gradient Descent(17181): loss=8.2081672289961\n",
      "Stochastic Gradient Descent(17182): loss=0.24891516864190308\n",
      "Stochastic Gradient Descent(17183): loss=5.302864952632092\n",
      "Stochastic Gradient Descent(17184): loss=1.423630652626641\n",
      "Stochastic Gradient Descent(17185): loss=2.3558248446955354\n",
      "Stochastic Gradient Descent(17186): loss=17.206639189075737\n",
      "Stochastic Gradient Descent(17187): loss=0.8929645263500788\n",
      "Stochastic Gradient Descent(17188): loss=5.251872792332291\n",
      "Stochastic Gradient Descent(17189): loss=0.19330358994770927\n",
      "Stochastic Gradient Descent(17190): loss=23.051811521480378\n",
      "Stochastic Gradient Descent(17191): loss=4.9543019917453375\n",
      "Stochastic Gradient Descent(17192): loss=10.011736940639668\n",
      "Stochastic Gradient Descent(17193): loss=0.3467966841300276\n",
      "Stochastic Gradient Descent(17194): loss=9.72456213084744\n",
      "Stochastic Gradient Descent(17195): loss=32.869956820035036\n",
      "Stochastic Gradient Descent(17196): loss=44.14305499291977\n",
      "Stochastic Gradient Descent(17197): loss=1.8305909718135263\n",
      "Stochastic Gradient Descent(17198): loss=0.5418140902427657\n",
      "Stochastic Gradient Descent(17199): loss=0.18007829126577896\n",
      "Stochastic Gradient Descent(17200): loss=2.8593871786673035\n",
      "Stochastic Gradient Descent(17201): loss=2.8917715836226376\n",
      "Stochastic Gradient Descent(17202): loss=0.05812909282854624\n",
      "Stochastic Gradient Descent(17203): loss=13.088662406050105\n",
      "Stochastic Gradient Descent(17204): loss=1.568092594597954\n",
      "Stochastic Gradient Descent(17205): loss=1.2383575219247362\n",
      "Stochastic Gradient Descent(17206): loss=8.213980234134805\n",
      "Stochastic Gradient Descent(17207): loss=3.120958536951214\n",
      "Stochastic Gradient Descent(17208): loss=5.366603336338309\n",
      "Stochastic Gradient Descent(17209): loss=5.429196282381241\n",
      "Stochastic Gradient Descent(17210): loss=12.954611495089985\n",
      "Stochastic Gradient Descent(17211): loss=1.578681236422524\n",
      "Stochastic Gradient Descent(17212): loss=2.1414190652252163\n",
      "Stochastic Gradient Descent(17213): loss=8.937447682830916\n",
      "Stochastic Gradient Descent(17214): loss=2.537423595018072\n",
      "Stochastic Gradient Descent(17215): loss=19.584538665324086\n",
      "Stochastic Gradient Descent(17216): loss=3.945431430081963\n",
      "Stochastic Gradient Descent(17217): loss=0.2607658839093216\n",
      "Stochastic Gradient Descent(17218): loss=12.396454117540364\n",
      "Stochastic Gradient Descent(17219): loss=8.398311909696965\n",
      "Stochastic Gradient Descent(17220): loss=0.6362707103759456\n",
      "Stochastic Gradient Descent(17221): loss=20.677963870436407\n",
      "Stochastic Gradient Descent(17222): loss=0.05709790011211844\n",
      "Stochastic Gradient Descent(17223): loss=1.5216077669594115\n",
      "Stochastic Gradient Descent(17224): loss=1.006848761110011\n",
      "Stochastic Gradient Descent(17225): loss=0.011323859956403462\n",
      "Stochastic Gradient Descent(17226): loss=6.500878047567413\n",
      "Stochastic Gradient Descent(17227): loss=2.6355731042634285\n",
      "Stochastic Gradient Descent(17228): loss=10.111471722958134\n",
      "Stochastic Gradient Descent(17229): loss=0.029792180876271182\n",
      "Stochastic Gradient Descent(17230): loss=5.580959825224286\n",
      "Stochastic Gradient Descent(17231): loss=0.6094132392063133\n",
      "Stochastic Gradient Descent(17232): loss=14.510094466190763\n",
      "Stochastic Gradient Descent(17233): loss=2.441322314352113\n",
      "Stochastic Gradient Descent(17234): loss=8.213555012374432\n",
      "Stochastic Gradient Descent(17235): loss=0.5338907945299185\n",
      "Stochastic Gradient Descent(17236): loss=0.506732923283749\n",
      "Stochastic Gradient Descent(17237): loss=2.952729779242608\n",
      "Stochastic Gradient Descent(17238): loss=0.39890680051137867\n",
      "Stochastic Gradient Descent(17239): loss=7.269180599694666\n",
      "Stochastic Gradient Descent(17240): loss=0.11375733993741827\n",
      "Stochastic Gradient Descent(17241): loss=8.552050038031627\n",
      "Stochastic Gradient Descent(17242): loss=0.31049902400921836\n",
      "Stochastic Gradient Descent(17243): loss=0.26401155258031456\n",
      "Stochastic Gradient Descent(17244): loss=1.3696486111215171\n",
      "Stochastic Gradient Descent(17245): loss=2.3506953419137298\n",
      "Stochastic Gradient Descent(17246): loss=13.893075049311166\n",
      "Stochastic Gradient Descent(17247): loss=4.05225793612339\n",
      "Stochastic Gradient Descent(17248): loss=0.0729826952051302\n",
      "Stochastic Gradient Descent(17249): loss=8.02484837329562\n",
      "Stochastic Gradient Descent(17250): loss=3.7458826677714887\n",
      "Stochastic Gradient Descent(17251): loss=2.62271270676152\n",
      "Stochastic Gradient Descent(17252): loss=0.40086894689863056\n",
      "Stochastic Gradient Descent(17253): loss=0.0651958137056496\n",
      "Stochastic Gradient Descent(17254): loss=0.7848962188715961\n",
      "Stochastic Gradient Descent(17255): loss=12.379544023544595\n",
      "Stochastic Gradient Descent(17256): loss=4.952105226969009\n",
      "Stochastic Gradient Descent(17257): loss=0.33942295865980415\n",
      "Stochastic Gradient Descent(17258): loss=5.0215632658522384\n",
      "Stochastic Gradient Descent(17259): loss=4.280500800795216\n",
      "Stochastic Gradient Descent(17260): loss=2.852580520768228\n",
      "Stochastic Gradient Descent(17261): loss=3.563070820928132\n",
      "Stochastic Gradient Descent(17262): loss=0.3213894832732178\n",
      "Stochastic Gradient Descent(17263): loss=19.039109135855703\n",
      "Stochastic Gradient Descent(17264): loss=12.631452247666862\n",
      "Stochastic Gradient Descent(17265): loss=15.12125269178089\n",
      "Stochastic Gradient Descent(17266): loss=9.061983469691896\n",
      "Stochastic Gradient Descent(17267): loss=3.6452993610725852\n",
      "Stochastic Gradient Descent(17268): loss=26.510578490683148\n",
      "Stochastic Gradient Descent(17269): loss=2.0861478612959687\n",
      "Stochastic Gradient Descent(17270): loss=1.977230377470812\n",
      "Stochastic Gradient Descent(17271): loss=0.7816407921430553\n",
      "Stochastic Gradient Descent(17272): loss=1.6505660566908733\n",
      "Stochastic Gradient Descent(17273): loss=5.262814552728312\n",
      "Stochastic Gradient Descent(17274): loss=2.0758207772669786\n",
      "Stochastic Gradient Descent(17275): loss=0.30722693430960163\n",
      "Stochastic Gradient Descent(17276): loss=4.959314550028049\n",
      "Stochastic Gradient Descent(17277): loss=3.3795420698843723\n",
      "Stochastic Gradient Descent(17278): loss=0.2681376967909828\n",
      "Stochastic Gradient Descent(17279): loss=0.057586375157988964\n",
      "Stochastic Gradient Descent(17280): loss=0.38784534672022364\n",
      "Stochastic Gradient Descent(17281): loss=1.3210503579446666\n",
      "Stochastic Gradient Descent(17282): loss=7.196695077721183\n",
      "Stochastic Gradient Descent(17283): loss=1.1133213325995726\n",
      "Stochastic Gradient Descent(17284): loss=0.2943359353309236\n",
      "Stochastic Gradient Descent(17285): loss=0.8509491743358795\n",
      "Stochastic Gradient Descent(17286): loss=0.14379806155943467\n",
      "Stochastic Gradient Descent(17287): loss=34.207578359388755\n",
      "Stochastic Gradient Descent(17288): loss=0.018549181801447476\n",
      "Stochastic Gradient Descent(17289): loss=1.2760688041546013\n",
      "Stochastic Gradient Descent(17290): loss=8.538145894120799\n",
      "Stochastic Gradient Descent(17291): loss=1.0058281966362443\n",
      "Stochastic Gradient Descent(17292): loss=0.6030949397588065\n",
      "Stochastic Gradient Descent(17293): loss=1.1771677591233212\n",
      "Stochastic Gradient Descent(17294): loss=4.363129020472747\n",
      "Stochastic Gradient Descent(17295): loss=3.1374951123715324\n",
      "Stochastic Gradient Descent(17296): loss=0.41475856494012603\n",
      "Stochastic Gradient Descent(17297): loss=6.756516896120462\n",
      "Stochastic Gradient Descent(17298): loss=11.011951217245675\n",
      "Stochastic Gradient Descent(17299): loss=2.2995038858025594\n",
      "Stochastic Gradient Descent(17300): loss=5.824781265690505\n",
      "Stochastic Gradient Descent(17301): loss=0.08781968580389013\n",
      "Stochastic Gradient Descent(17302): loss=8.568183193618495\n",
      "Stochastic Gradient Descent(17303): loss=10.30713372183143\n",
      "Stochastic Gradient Descent(17304): loss=3.8508287508216994\n",
      "Stochastic Gradient Descent(17305): loss=1.762525454743762\n",
      "Stochastic Gradient Descent(17306): loss=0.6149806096561595\n",
      "Stochastic Gradient Descent(17307): loss=1.9400783601276073\n",
      "Stochastic Gradient Descent(17308): loss=4.254545908354968\n",
      "Stochastic Gradient Descent(17309): loss=3.47034996625527\n",
      "Stochastic Gradient Descent(17310): loss=8.265306391540907\n",
      "Stochastic Gradient Descent(17311): loss=0.0873935536021037\n",
      "Stochastic Gradient Descent(17312): loss=2.4728750174719942\n",
      "Stochastic Gradient Descent(17313): loss=0.3786201629001022\n",
      "Stochastic Gradient Descent(17314): loss=0.04075556607778229\n",
      "Stochastic Gradient Descent(17315): loss=10.051426063054494\n",
      "Stochastic Gradient Descent(17316): loss=3.1923033407449775\n",
      "Stochastic Gradient Descent(17317): loss=4.300473183611944\n",
      "Stochastic Gradient Descent(17318): loss=0.006342074333020211\n",
      "Stochastic Gradient Descent(17319): loss=7.977959338263084\n",
      "Stochastic Gradient Descent(17320): loss=10.007481251529246\n",
      "Stochastic Gradient Descent(17321): loss=2.4005281773929914\n",
      "Stochastic Gradient Descent(17322): loss=0.25746863512861523\n",
      "Stochastic Gradient Descent(17323): loss=0.06607309540468398\n",
      "Stochastic Gradient Descent(17324): loss=0.7468702601328726\n",
      "Stochastic Gradient Descent(17325): loss=0.049292849655032556\n",
      "Stochastic Gradient Descent(17326): loss=4.998115364525177\n",
      "Stochastic Gradient Descent(17327): loss=3.435065014798394\n",
      "Stochastic Gradient Descent(17328): loss=10.437145321752856\n",
      "Stochastic Gradient Descent(17329): loss=0.7864175755830489\n",
      "Stochastic Gradient Descent(17330): loss=7.364685966136641\n",
      "Stochastic Gradient Descent(17331): loss=0.41789170499269546\n",
      "Stochastic Gradient Descent(17332): loss=0.825821736910292\n",
      "Stochastic Gradient Descent(17333): loss=0.30414590557974985\n",
      "Stochastic Gradient Descent(17334): loss=4.34033231406178\n",
      "Stochastic Gradient Descent(17335): loss=10.091923921790118\n",
      "Stochastic Gradient Descent(17336): loss=1.7223313801276199\n",
      "Stochastic Gradient Descent(17337): loss=3.7736579584587675\n",
      "Stochastic Gradient Descent(17338): loss=1.8323146750553345\n",
      "Stochastic Gradient Descent(17339): loss=5.359228703371711\n",
      "Stochastic Gradient Descent(17340): loss=0.10077848002470326\n",
      "Stochastic Gradient Descent(17341): loss=0.9011800520738269\n",
      "Stochastic Gradient Descent(17342): loss=14.190564381575658\n",
      "Stochastic Gradient Descent(17343): loss=1.3134098384808122\n",
      "Stochastic Gradient Descent(17344): loss=0.09028250094650794\n",
      "Stochastic Gradient Descent(17345): loss=32.83576929098271\n",
      "Stochastic Gradient Descent(17346): loss=6.199692381375173\n",
      "Stochastic Gradient Descent(17347): loss=10.672640408311272\n",
      "Stochastic Gradient Descent(17348): loss=1.3151488110540355\n",
      "Stochastic Gradient Descent(17349): loss=7.315339284361769\n",
      "Stochastic Gradient Descent(17350): loss=12.340767257904814\n",
      "Stochastic Gradient Descent(17351): loss=11.77725724194674\n",
      "Stochastic Gradient Descent(17352): loss=19.027941069199876\n",
      "Stochastic Gradient Descent(17353): loss=33.72928975942476\n",
      "Stochastic Gradient Descent(17354): loss=7.713429035720964\n",
      "Stochastic Gradient Descent(17355): loss=0.09084994348179833\n",
      "Stochastic Gradient Descent(17356): loss=6.668849110940825\n",
      "Stochastic Gradient Descent(17357): loss=7.0453858257399515\n",
      "Stochastic Gradient Descent(17358): loss=4.506458006660799\n",
      "Stochastic Gradient Descent(17359): loss=0.07670788820804968\n",
      "Stochastic Gradient Descent(17360): loss=0.4948933306093901\n",
      "Stochastic Gradient Descent(17361): loss=3.212076561919537\n",
      "Stochastic Gradient Descent(17362): loss=3.650940676750477\n",
      "Stochastic Gradient Descent(17363): loss=0.30658823305289784\n",
      "Stochastic Gradient Descent(17364): loss=1.8565290829066392\n",
      "Stochastic Gradient Descent(17365): loss=1.3010126170147225\n",
      "Stochastic Gradient Descent(17366): loss=3.564730635612722\n",
      "Stochastic Gradient Descent(17367): loss=3.121552533534029\n",
      "Stochastic Gradient Descent(17368): loss=15.763372147531033\n",
      "Stochastic Gradient Descent(17369): loss=4.927341176281191\n",
      "Stochastic Gradient Descent(17370): loss=1.6078566565885613\n",
      "Stochastic Gradient Descent(17371): loss=38.38993017077293\n",
      "Stochastic Gradient Descent(17372): loss=13.458265506942405\n",
      "Stochastic Gradient Descent(17373): loss=21.592589930029934\n",
      "Stochastic Gradient Descent(17374): loss=0.004044344063899437\n",
      "Stochastic Gradient Descent(17375): loss=1.6165281569486825\n",
      "Stochastic Gradient Descent(17376): loss=1.5669195543231098\n",
      "Stochastic Gradient Descent(17377): loss=0.5814509252541127\n",
      "Stochastic Gradient Descent(17378): loss=3.7254700621830845\n",
      "Stochastic Gradient Descent(17379): loss=26.035029463560758\n",
      "Stochastic Gradient Descent(17380): loss=0.012249416254561982\n",
      "Stochastic Gradient Descent(17381): loss=17.472856387703537\n",
      "Stochastic Gradient Descent(17382): loss=2.4664957722102407\n",
      "Stochastic Gradient Descent(17383): loss=0.5947754784897749\n",
      "Stochastic Gradient Descent(17384): loss=2.3442092515034063\n",
      "Stochastic Gradient Descent(17385): loss=0.22664153979201557\n",
      "Stochastic Gradient Descent(17386): loss=10.265106501282293\n",
      "Stochastic Gradient Descent(17387): loss=2.1323848158569336\n",
      "Stochastic Gradient Descent(17388): loss=0.8600870163877045\n",
      "Stochastic Gradient Descent(17389): loss=13.051193693565317\n",
      "Stochastic Gradient Descent(17390): loss=0.3341797271219899\n",
      "Stochastic Gradient Descent(17391): loss=0.08819977583404946\n",
      "Stochastic Gradient Descent(17392): loss=2.5796201749486958\n",
      "Stochastic Gradient Descent(17393): loss=4.482096766694143\n",
      "Stochastic Gradient Descent(17394): loss=0.005048330218265902\n",
      "Stochastic Gradient Descent(17395): loss=5.586631952705316\n",
      "Stochastic Gradient Descent(17396): loss=0.05321833046813512\n",
      "Stochastic Gradient Descent(17397): loss=0.3408844149599124\n",
      "Stochastic Gradient Descent(17398): loss=2.0759279790722327\n",
      "Stochastic Gradient Descent(17399): loss=0.15698454125925745\n",
      "Stochastic Gradient Descent(17400): loss=5.361111856242735\n",
      "Stochastic Gradient Descent(17401): loss=6.229853370439294\n",
      "Stochastic Gradient Descent(17402): loss=0.058468518039471264\n",
      "Stochastic Gradient Descent(17403): loss=0.5520256342026415\n",
      "Stochastic Gradient Descent(17404): loss=0.8721437209201536\n",
      "Stochastic Gradient Descent(17405): loss=10.312470399186537\n",
      "Stochastic Gradient Descent(17406): loss=0.08131406311182618\n",
      "Stochastic Gradient Descent(17407): loss=3.1588945619403925\n",
      "Stochastic Gradient Descent(17408): loss=0.23920357651849578\n",
      "Stochastic Gradient Descent(17409): loss=3.4328733677345884\n",
      "Stochastic Gradient Descent(17410): loss=0.5578708202755505\n",
      "Stochastic Gradient Descent(17411): loss=24.388303487048976\n",
      "Stochastic Gradient Descent(17412): loss=2.6050366399981386\n",
      "Stochastic Gradient Descent(17413): loss=10.020424198063424\n",
      "Stochastic Gradient Descent(17414): loss=2.699185907653546\n",
      "Stochastic Gradient Descent(17415): loss=23.283288084425177\n",
      "Stochastic Gradient Descent(17416): loss=0.4029580498271123\n",
      "Stochastic Gradient Descent(17417): loss=0.14669543419325046\n",
      "Stochastic Gradient Descent(17418): loss=1.7174930066038996\n",
      "Stochastic Gradient Descent(17419): loss=1.228165938192142\n",
      "Stochastic Gradient Descent(17420): loss=0.00011858877275821179\n",
      "Stochastic Gradient Descent(17421): loss=0.8017549787577188\n",
      "Stochastic Gradient Descent(17422): loss=3.1401577144913317\n",
      "Stochastic Gradient Descent(17423): loss=17.02998203334459\n",
      "Stochastic Gradient Descent(17424): loss=3.205445024387967\n",
      "Stochastic Gradient Descent(17425): loss=8.666504552739795\n",
      "Stochastic Gradient Descent(17426): loss=22.355010188892205\n",
      "Stochastic Gradient Descent(17427): loss=1.432470705618738\n",
      "Stochastic Gradient Descent(17428): loss=2.7690307865518395\n",
      "Stochastic Gradient Descent(17429): loss=5.72288009330051\n",
      "Stochastic Gradient Descent(17430): loss=3.3210506275168785\n",
      "Stochastic Gradient Descent(17431): loss=0.021973427522514372\n",
      "Stochastic Gradient Descent(17432): loss=0.6582950902210672\n",
      "Stochastic Gradient Descent(17433): loss=4.113979830816376\n",
      "Stochastic Gradient Descent(17434): loss=2.323164735979812\n",
      "Stochastic Gradient Descent(17435): loss=3.371218817316644\n",
      "Stochastic Gradient Descent(17436): loss=8.525474743278554\n",
      "Stochastic Gradient Descent(17437): loss=5.738486898499024\n",
      "Stochastic Gradient Descent(17438): loss=0.6945736634176218\n",
      "Stochastic Gradient Descent(17439): loss=0.26193681607295555\n",
      "Stochastic Gradient Descent(17440): loss=0.15006114106666033\n",
      "Stochastic Gradient Descent(17441): loss=0.08708040007469252\n",
      "Stochastic Gradient Descent(17442): loss=18.2797395202653\n",
      "Stochastic Gradient Descent(17443): loss=0.04240031720015562\n",
      "Stochastic Gradient Descent(17444): loss=1.484891002753816\n",
      "Stochastic Gradient Descent(17445): loss=2.3766520967295475\n",
      "Stochastic Gradient Descent(17446): loss=2.6970189440461545\n",
      "Stochastic Gradient Descent(17447): loss=0.3284661722354073\n",
      "Stochastic Gradient Descent(17448): loss=5.4303725883243725\n",
      "Stochastic Gradient Descent(17449): loss=3.975305514173408\n",
      "Stochastic Gradient Descent(17450): loss=0.10619636226559974\n",
      "Stochastic Gradient Descent(17451): loss=1.7438036781625366\n",
      "Stochastic Gradient Descent(17452): loss=2.555395052837897\n",
      "Stochastic Gradient Descent(17453): loss=2.440195442604675\n",
      "Stochastic Gradient Descent(17454): loss=2.3797046871488243\n",
      "Stochastic Gradient Descent(17455): loss=0.9466163537785477\n",
      "Stochastic Gradient Descent(17456): loss=2.5409986914333045\n",
      "Stochastic Gradient Descent(17457): loss=0.7036179577151535\n",
      "Stochastic Gradient Descent(17458): loss=0.4984902746513378\n",
      "Stochastic Gradient Descent(17459): loss=9.32116335535751\n",
      "Stochastic Gradient Descent(17460): loss=6.048309511191301\n",
      "Stochastic Gradient Descent(17461): loss=4.1276453935579145\n",
      "Stochastic Gradient Descent(17462): loss=7.60476320041871\n",
      "Stochastic Gradient Descent(17463): loss=4.744035008075349\n",
      "Stochastic Gradient Descent(17464): loss=0.027507809480300374\n",
      "Stochastic Gradient Descent(17465): loss=6.0231094344647165\n",
      "Stochastic Gradient Descent(17466): loss=2.964052916185025\n",
      "Stochastic Gradient Descent(17467): loss=23.052602494541958\n",
      "Stochastic Gradient Descent(17468): loss=8.159106640960392\n",
      "Stochastic Gradient Descent(17469): loss=0.6591217746392924\n",
      "Stochastic Gradient Descent(17470): loss=1.8354558487294779\n",
      "Stochastic Gradient Descent(17471): loss=2.163983109641997\n",
      "Stochastic Gradient Descent(17472): loss=0.02394409364745208\n",
      "Stochastic Gradient Descent(17473): loss=1.9058225640651913\n",
      "Stochastic Gradient Descent(17474): loss=2.3195960960585693\n",
      "Stochastic Gradient Descent(17475): loss=1.792260947717744\n",
      "Stochastic Gradient Descent(17476): loss=8.378486909144588\n",
      "Stochastic Gradient Descent(17477): loss=0.9893553889975218\n",
      "Stochastic Gradient Descent(17478): loss=0.08255776649222349\n",
      "Stochastic Gradient Descent(17479): loss=2.02018972436071\n",
      "Stochastic Gradient Descent(17480): loss=4.445590484378432\n",
      "Stochastic Gradient Descent(17481): loss=2.686772512765468\n",
      "Stochastic Gradient Descent(17482): loss=0.4241875549515882\n",
      "Stochastic Gradient Descent(17483): loss=11.367493987997541\n",
      "Stochastic Gradient Descent(17484): loss=4.777957897700344\n",
      "Stochastic Gradient Descent(17485): loss=0.7598170830743491\n",
      "Stochastic Gradient Descent(17486): loss=7.50201160607563\n",
      "Stochastic Gradient Descent(17487): loss=1.4295629060061117\n",
      "Stochastic Gradient Descent(17488): loss=0.002162307767102679\n",
      "Stochastic Gradient Descent(17489): loss=8.303363082895261\n",
      "Stochastic Gradient Descent(17490): loss=7.962983452945002\n",
      "Stochastic Gradient Descent(17491): loss=0.2839283121927308\n",
      "Stochastic Gradient Descent(17492): loss=3.2230574375543357\n",
      "Stochastic Gradient Descent(17493): loss=0.04142416375930914\n",
      "Stochastic Gradient Descent(17494): loss=4.843967622654954e-06\n",
      "Stochastic Gradient Descent(17495): loss=29.07067445502071\n",
      "Stochastic Gradient Descent(17496): loss=0.16136269451593893\n",
      "Stochastic Gradient Descent(17497): loss=82.98235671808389\n",
      "Stochastic Gradient Descent(17498): loss=8.941395493266432\n",
      "Stochastic Gradient Descent(17499): loss=0.4858980927807769\n",
      "Stochastic Gradient Descent(17500): loss=0.04323797407485034\n",
      "Stochastic Gradient Descent(17501): loss=8.433737484614824\n",
      "Stochastic Gradient Descent(17502): loss=6.5242124975132345\n",
      "Stochastic Gradient Descent(17503): loss=1.4516471945173952\n",
      "Stochastic Gradient Descent(17504): loss=7.79612316463141\n",
      "Stochastic Gradient Descent(17505): loss=19.982667992678323\n",
      "Stochastic Gradient Descent(17506): loss=0.2470943318646115\n",
      "Stochastic Gradient Descent(17507): loss=2.712804289470131\n",
      "Stochastic Gradient Descent(17508): loss=9.9885191017759\n",
      "Stochastic Gradient Descent(17509): loss=11.22082417307049\n",
      "Stochastic Gradient Descent(17510): loss=2.9667329931364548\n",
      "Stochastic Gradient Descent(17511): loss=6.16285294767505\n",
      "Stochastic Gradient Descent(17512): loss=13.673193226028479\n",
      "Stochastic Gradient Descent(17513): loss=0.3278868613371739\n",
      "Stochastic Gradient Descent(17514): loss=0.6250573771785296\n",
      "Stochastic Gradient Descent(17515): loss=0.5679839743428325\n",
      "Stochastic Gradient Descent(17516): loss=2.0932184072688194\n",
      "Stochastic Gradient Descent(17517): loss=0.3463201412380506\n",
      "Stochastic Gradient Descent(17518): loss=1.2713969673782688\n",
      "Stochastic Gradient Descent(17519): loss=7.963398205792984\n",
      "Stochastic Gradient Descent(17520): loss=7.360860512772913\n",
      "Stochastic Gradient Descent(17521): loss=0.7226035583953684\n",
      "Stochastic Gradient Descent(17522): loss=0.6380738531401867\n",
      "Stochastic Gradient Descent(17523): loss=1.6609351729890869\n",
      "Stochastic Gradient Descent(17524): loss=11.12852789815217\n",
      "Stochastic Gradient Descent(17525): loss=2.8229238437180135\n",
      "Stochastic Gradient Descent(17526): loss=0.11188976472218506\n",
      "Stochastic Gradient Descent(17527): loss=0.06064367092303968\n",
      "Stochastic Gradient Descent(17528): loss=8.213848598761777\n",
      "Stochastic Gradient Descent(17529): loss=2.312176276932417\n",
      "Stochastic Gradient Descent(17530): loss=0.4181411024789748\n",
      "Stochastic Gradient Descent(17531): loss=2.595098643744161\n",
      "Stochastic Gradient Descent(17532): loss=0.060241272733852755\n",
      "Stochastic Gradient Descent(17533): loss=1.2026518376664608\n",
      "Stochastic Gradient Descent(17534): loss=0.2750735921003754\n",
      "Stochastic Gradient Descent(17535): loss=2.723825590811018\n",
      "Stochastic Gradient Descent(17536): loss=2.9739448787163387\n",
      "Stochastic Gradient Descent(17537): loss=6.157557760462082\n",
      "Stochastic Gradient Descent(17538): loss=0.19068117749068378\n",
      "Stochastic Gradient Descent(17539): loss=0.14002016304563586\n",
      "Stochastic Gradient Descent(17540): loss=9.424318149879598\n",
      "Stochastic Gradient Descent(17541): loss=9.325387510428065\n",
      "Stochastic Gradient Descent(17542): loss=2.267923819842384\n",
      "Stochastic Gradient Descent(17543): loss=0.5987622924433598\n",
      "Stochastic Gradient Descent(17544): loss=4.673727978005864\n",
      "Stochastic Gradient Descent(17545): loss=13.395352888207292\n",
      "Stochastic Gradient Descent(17546): loss=2.2866562531299746\n",
      "Stochastic Gradient Descent(17547): loss=30.599813409815038\n",
      "Stochastic Gradient Descent(17548): loss=0.1479982262731829\n",
      "Stochastic Gradient Descent(17549): loss=5.125816602982053\n",
      "Stochastic Gradient Descent(17550): loss=1.0150562531030698\n",
      "Stochastic Gradient Descent(17551): loss=5.31425387700691\n",
      "Stochastic Gradient Descent(17552): loss=0.010163319464492209\n",
      "Stochastic Gradient Descent(17553): loss=1.3841377614407067\n",
      "Stochastic Gradient Descent(17554): loss=12.3781453336038\n",
      "Stochastic Gradient Descent(17555): loss=13.031551883995178\n",
      "Stochastic Gradient Descent(17556): loss=1.2243504710137545\n",
      "Stochastic Gradient Descent(17557): loss=0.06565771524429705\n",
      "Stochastic Gradient Descent(17558): loss=8.031290928308973\n",
      "Stochastic Gradient Descent(17559): loss=12.649102401334217\n",
      "Stochastic Gradient Descent(17560): loss=0.1711870857064389\n",
      "Stochastic Gradient Descent(17561): loss=0.14751091565808253\n",
      "Stochastic Gradient Descent(17562): loss=2.070621830331494\n",
      "Stochastic Gradient Descent(17563): loss=0.05116775886412991\n",
      "Stochastic Gradient Descent(17564): loss=7.949796361913605\n",
      "Stochastic Gradient Descent(17565): loss=4.572471957974403\n",
      "Stochastic Gradient Descent(17566): loss=1.0327673860228868\n",
      "Stochastic Gradient Descent(17567): loss=28.866603467560658\n",
      "Stochastic Gradient Descent(17568): loss=5.086783713447982\n",
      "Stochastic Gradient Descent(17569): loss=0.004500398297748376\n",
      "Stochastic Gradient Descent(17570): loss=2.3510245951801836\n",
      "Stochastic Gradient Descent(17571): loss=9.325983406778217\n",
      "Stochastic Gradient Descent(17572): loss=0.02305040288597784\n",
      "Stochastic Gradient Descent(17573): loss=0.12362629223449564\n",
      "Stochastic Gradient Descent(17574): loss=0.9064122588665832\n",
      "Stochastic Gradient Descent(17575): loss=13.0115970037703\n",
      "Stochastic Gradient Descent(17576): loss=7.02650294545996\n",
      "Stochastic Gradient Descent(17577): loss=2.131582909126729\n",
      "Stochastic Gradient Descent(17578): loss=1.9162388897297817\n",
      "Stochastic Gradient Descent(17579): loss=9.237713115959519\n",
      "Stochastic Gradient Descent(17580): loss=0.17544378086503692\n",
      "Stochastic Gradient Descent(17581): loss=26.410488839211716\n",
      "Stochastic Gradient Descent(17582): loss=25.880269551851\n",
      "Stochastic Gradient Descent(17583): loss=6.1737444905602565\n",
      "Stochastic Gradient Descent(17584): loss=1.9133372249589555\n",
      "Stochastic Gradient Descent(17585): loss=8.016601744635244\n",
      "Stochastic Gradient Descent(17586): loss=0.9830926832600583\n",
      "Stochastic Gradient Descent(17587): loss=7.396682020381344\n",
      "Stochastic Gradient Descent(17588): loss=1.3940321721176432\n",
      "Stochastic Gradient Descent(17589): loss=2.3775540459649465\n",
      "Stochastic Gradient Descent(17590): loss=1.9741199018473614\n",
      "Stochastic Gradient Descent(17591): loss=0.41283231152089145\n",
      "Stochastic Gradient Descent(17592): loss=1.3345245373126613\n",
      "Stochastic Gradient Descent(17593): loss=6.358664732110245\n",
      "Stochastic Gradient Descent(17594): loss=13.340186026575772\n",
      "Stochastic Gradient Descent(17595): loss=0.0018707248271684965\n",
      "Stochastic Gradient Descent(17596): loss=2.047020898520255\n",
      "Stochastic Gradient Descent(17597): loss=0.3580638512133659\n",
      "Stochastic Gradient Descent(17598): loss=19.544144976420537\n",
      "Stochastic Gradient Descent(17599): loss=0.895902094403253\n",
      "Stochastic Gradient Descent(17600): loss=29.999497417720246\n",
      "Stochastic Gradient Descent(17601): loss=10.44600623775269\n",
      "Stochastic Gradient Descent(17602): loss=0.010746520071320523\n",
      "Stochastic Gradient Descent(17603): loss=1.9793351608443612\n",
      "Stochastic Gradient Descent(17604): loss=6.837106568810276\n",
      "Stochastic Gradient Descent(17605): loss=5.228299903680309\n",
      "Stochastic Gradient Descent(17606): loss=11.01858511212438\n",
      "Stochastic Gradient Descent(17607): loss=0.15421310452822576\n",
      "Stochastic Gradient Descent(17608): loss=9.362984694829224\n",
      "Stochastic Gradient Descent(17609): loss=0.5056469646095257\n",
      "Stochastic Gradient Descent(17610): loss=7.725072768785414\n",
      "Stochastic Gradient Descent(17611): loss=7.228945331508518\n",
      "Stochastic Gradient Descent(17612): loss=7.589093540686972\n",
      "Stochastic Gradient Descent(17613): loss=8.945777811204724\n",
      "Stochastic Gradient Descent(17614): loss=0.8425521985891289\n",
      "Stochastic Gradient Descent(17615): loss=8.376703169774764\n",
      "Stochastic Gradient Descent(17616): loss=10.23887407290006\n",
      "Stochastic Gradient Descent(17617): loss=0.038889358061991064\n",
      "Stochastic Gradient Descent(17618): loss=1.365592204791641\n",
      "Stochastic Gradient Descent(17619): loss=8.374816122235277\n",
      "Stochastic Gradient Descent(17620): loss=4.274699894876973\n",
      "Stochastic Gradient Descent(17621): loss=0.014456361903662228\n",
      "Stochastic Gradient Descent(17622): loss=1.772162203351723\n",
      "Stochastic Gradient Descent(17623): loss=0.21949116102281088\n",
      "Stochastic Gradient Descent(17624): loss=18.237084847491175\n",
      "Stochastic Gradient Descent(17625): loss=3.6661915171705908\n",
      "Stochastic Gradient Descent(17626): loss=0.14027319943059663\n",
      "Stochastic Gradient Descent(17627): loss=2.1748475343100413\n",
      "Stochastic Gradient Descent(17628): loss=6.2660147307893395\n",
      "Stochastic Gradient Descent(17629): loss=1.123542495373747\n",
      "Stochastic Gradient Descent(17630): loss=3.7676957075073427\n",
      "Stochastic Gradient Descent(17631): loss=6.024948368163356\n",
      "Stochastic Gradient Descent(17632): loss=1.1917449808537086\n",
      "Stochastic Gradient Descent(17633): loss=0.9636735832486539\n",
      "Stochastic Gradient Descent(17634): loss=4.370051219141051\n",
      "Stochastic Gradient Descent(17635): loss=4.548927410107462\n",
      "Stochastic Gradient Descent(17636): loss=0.0071868130107828445\n",
      "Stochastic Gradient Descent(17637): loss=0.19855677293272028\n",
      "Stochastic Gradient Descent(17638): loss=1.63368727737201\n",
      "Stochastic Gradient Descent(17639): loss=4.585428724374782\n",
      "Stochastic Gradient Descent(17640): loss=4.136034098560578\n",
      "Stochastic Gradient Descent(17641): loss=0.02181808218740565\n",
      "Stochastic Gradient Descent(17642): loss=0.08795504085789814\n",
      "Stochastic Gradient Descent(17643): loss=1.447701491539752\n",
      "Stochastic Gradient Descent(17644): loss=0.004889816679688958\n",
      "Stochastic Gradient Descent(17645): loss=5.334828815869622\n",
      "Stochastic Gradient Descent(17646): loss=3.5411677513931016e-05\n",
      "Stochastic Gradient Descent(17647): loss=4.896864962884742\n",
      "Stochastic Gradient Descent(17648): loss=0.8129546376861281\n",
      "Stochastic Gradient Descent(17649): loss=0.07572475104239222\n",
      "Stochastic Gradient Descent(17650): loss=0.752630776731299\n",
      "Stochastic Gradient Descent(17651): loss=0.677150466283238\n",
      "Stochastic Gradient Descent(17652): loss=6.750937502034328\n",
      "Stochastic Gradient Descent(17653): loss=14.263872904084895\n",
      "Stochastic Gradient Descent(17654): loss=5.824906980054499\n",
      "Stochastic Gradient Descent(17655): loss=3.9795553434664894\n",
      "Stochastic Gradient Descent(17656): loss=10.943390391931986\n",
      "Stochastic Gradient Descent(17657): loss=5.8630514447608455\n",
      "Stochastic Gradient Descent(17658): loss=1.321138215413522\n",
      "Stochastic Gradient Descent(17659): loss=0.09336353728968777\n",
      "Stochastic Gradient Descent(17660): loss=0.30545480943524894\n",
      "Stochastic Gradient Descent(17661): loss=0.9344165477605697\n",
      "Stochastic Gradient Descent(17662): loss=0.20558065223680375\n",
      "Stochastic Gradient Descent(17663): loss=0.03681239340634902\n",
      "Stochastic Gradient Descent(17664): loss=0.19125442788323133\n",
      "Stochastic Gradient Descent(17665): loss=1.7389110137370378\n",
      "Stochastic Gradient Descent(17666): loss=0.03836414084862103\n",
      "Stochastic Gradient Descent(17667): loss=2.243755591899809\n",
      "Stochastic Gradient Descent(17668): loss=0.0002153705978813724\n",
      "Stochastic Gradient Descent(17669): loss=2.538969887138548\n",
      "Stochastic Gradient Descent(17670): loss=1.8613475328679785\n",
      "Stochastic Gradient Descent(17671): loss=0.35163967541642366\n",
      "Stochastic Gradient Descent(17672): loss=6.183567842007743\n",
      "Stochastic Gradient Descent(17673): loss=3.1914941069560516\n",
      "Stochastic Gradient Descent(17674): loss=0.06975871959833137\n",
      "Stochastic Gradient Descent(17675): loss=2.2444467129016306\n",
      "Stochastic Gradient Descent(17676): loss=3.623533934622349\n",
      "Stochastic Gradient Descent(17677): loss=7.019445682336928\n",
      "Stochastic Gradient Descent(17678): loss=2.5820411573163664\n",
      "Stochastic Gradient Descent(17679): loss=44.19097647921794\n",
      "Stochastic Gradient Descent(17680): loss=12.395291248954697\n",
      "Stochastic Gradient Descent(17681): loss=26.51979168314241\n",
      "Stochastic Gradient Descent(17682): loss=8.049128948988116\n",
      "Stochastic Gradient Descent(17683): loss=52.931092949594756\n",
      "Stochastic Gradient Descent(17684): loss=0.012632134678985711\n",
      "Stochastic Gradient Descent(17685): loss=0.8908162472457993\n",
      "Stochastic Gradient Descent(17686): loss=3.074381374481049\n",
      "Stochastic Gradient Descent(17687): loss=5.593905338358109\n",
      "Stochastic Gradient Descent(17688): loss=7.817197262314331\n",
      "Stochastic Gradient Descent(17689): loss=0.6391113019629069\n",
      "Stochastic Gradient Descent(17690): loss=2.3989572431657136\n",
      "Stochastic Gradient Descent(17691): loss=1.5062146832298162\n",
      "Stochastic Gradient Descent(17692): loss=0.20677513920054613\n",
      "Stochastic Gradient Descent(17693): loss=0.405063065728329\n",
      "Stochastic Gradient Descent(17694): loss=1.730731908212555\n",
      "Stochastic Gradient Descent(17695): loss=1.1919880327178178\n",
      "Stochastic Gradient Descent(17696): loss=0.14052512072521803\n",
      "Stochastic Gradient Descent(17697): loss=4.071468117626287\n",
      "Stochastic Gradient Descent(17698): loss=0.830296225889033\n",
      "Stochastic Gradient Descent(17699): loss=7.3004697785641595\n",
      "Stochastic Gradient Descent(17700): loss=3.3887612424470106\n",
      "Stochastic Gradient Descent(17701): loss=10.335123253835135\n",
      "Stochastic Gradient Descent(17702): loss=12.240380270431906\n",
      "Stochastic Gradient Descent(17703): loss=4.3923451100383275\n",
      "Stochastic Gradient Descent(17704): loss=0.0620285640223134\n",
      "Stochastic Gradient Descent(17705): loss=0.31836756058793014\n",
      "Stochastic Gradient Descent(17706): loss=6.434320890713921\n",
      "Stochastic Gradient Descent(17707): loss=28.547039083627123\n",
      "Stochastic Gradient Descent(17708): loss=0.17838956378002702\n",
      "Stochastic Gradient Descent(17709): loss=1.9292808434459945\n",
      "Stochastic Gradient Descent(17710): loss=0.4327478827792625\n",
      "Stochastic Gradient Descent(17711): loss=4.016985046493959\n",
      "Stochastic Gradient Descent(17712): loss=0.3755754349509388\n",
      "Stochastic Gradient Descent(17713): loss=47.41433905922622\n",
      "Stochastic Gradient Descent(17714): loss=4.083195917206409\n",
      "Stochastic Gradient Descent(17715): loss=0.11550872129050851\n",
      "Stochastic Gradient Descent(17716): loss=0.8178464133946115\n",
      "Stochastic Gradient Descent(17717): loss=0.9025783584686419\n",
      "Stochastic Gradient Descent(17718): loss=1.2100453323755846\n",
      "Stochastic Gradient Descent(17719): loss=0.36191159400399947\n",
      "Stochastic Gradient Descent(17720): loss=19.662449198083895\n",
      "Stochastic Gradient Descent(17721): loss=1.5255429690488889\n",
      "Stochastic Gradient Descent(17722): loss=9.371164277048567\n",
      "Stochastic Gradient Descent(17723): loss=22.174707958477477\n",
      "Stochastic Gradient Descent(17724): loss=37.95482493941748\n",
      "Stochastic Gradient Descent(17725): loss=12.187199545234437\n",
      "Stochastic Gradient Descent(17726): loss=1.6004012439941895\n",
      "Stochastic Gradient Descent(17727): loss=0.3068962086901986\n",
      "Stochastic Gradient Descent(17728): loss=0.12149043013834251\n",
      "Stochastic Gradient Descent(17729): loss=5.716603388669493\n",
      "Stochastic Gradient Descent(17730): loss=0.01384414149052925\n",
      "Stochastic Gradient Descent(17731): loss=3.134554614753648\n",
      "Stochastic Gradient Descent(17732): loss=0.22404413575737947\n",
      "Stochastic Gradient Descent(17733): loss=0.033517345875009465\n",
      "Stochastic Gradient Descent(17734): loss=10.558639952295392\n",
      "Stochastic Gradient Descent(17735): loss=27.237327281573435\n",
      "Stochastic Gradient Descent(17736): loss=0.8023991812428313\n",
      "Stochastic Gradient Descent(17737): loss=0.3841061276481068\n",
      "Stochastic Gradient Descent(17738): loss=2.1814312921161814\n",
      "Stochastic Gradient Descent(17739): loss=3.324221632704672\n",
      "Stochastic Gradient Descent(17740): loss=8.763573707480045\n",
      "Stochastic Gradient Descent(17741): loss=0.9395481453838113\n",
      "Stochastic Gradient Descent(17742): loss=0.004863491917965881\n",
      "Stochastic Gradient Descent(17743): loss=5.871836378862755\n",
      "Stochastic Gradient Descent(17744): loss=21.743782418343155\n",
      "Stochastic Gradient Descent(17745): loss=2.1033478786597195\n",
      "Stochastic Gradient Descent(17746): loss=2.027137039010385\n",
      "Stochastic Gradient Descent(17747): loss=2.6129563012026753\n",
      "Stochastic Gradient Descent(17748): loss=0.8297997973423346\n",
      "Stochastic Gradient Descent(17749): loss=0.02616091660938193\n",
      "Stochastic Gradient Descent(17750): loss=0.35127095176410433\n",
      "Stochastic Gradient Descent(17751): loss=0.11984656422232384\n",
      "Stochastic Gradient Descent(17752): loss=0.01972083713625949\n",
      "Stochastic Gradient Descent(17753): loss=8.611078662794807\n",
      "Stochastic Gradient Descent(17754): loss=3.828582416221529\n",
      "Stochastic Gradient Descent(17755): loss=6.708305878672654\n",
      "Stochastic Gradient Descent(17756): loss=6.195623222599098\n",
      "Stochastic Gradient Descent(17757): loss=0.09237168527590463\n",
      "Stochastic Gradient Descent(17758): loss=1.5235878364452597\n",
      "Stochastic Gradient Descent(17759): loss=1.9655132355014362\n",
      "Stochastic Gradient Descent(17760): loss=0.0075783829492589065\n",
      "Stochastic Gradient Descent(17761): loss=3.543303813034191\n",
      "Stochastic Gradient Descent(17762): loss=0.12305412653714543\n",
      "Stochastic Gradient Descent(17763): loss=0.39916831172847145\n",
      "Stochastic Gradient Descent(17764): loss=1.7031461523967995\n",
      "Stochastic Gradient Descent(17765): loss=0.33418928157796635\n",
      "Stochastic Gradient Descent(17766): loss=30.939692133455598\n",
      "Stochastic Gradient Descent(17767): loss=0.00014587675663670245\n",
      "Stochastic Gradient Descent(17768): loss=7.6980348708514175\n",
      "Stochastic Gradient Descent(17769): loss=2.8437455798094744\n",
      "Stochastic Gradient Descent(17770): loss=6.544539636311659\n",
      "Stochastic Gradient Descent(17771): loss=8.494450918558078\n",
      "Stochastic Gradient Descent(17772): loss=1.7875722617052023\n",
      "Stochastic Gradient Descent(17773): loss=10.786637387736794\n",
      "Stochastic Gradient Descent(17774): loss=0.12986243081643087\n",
      "Stochastic Gradient Descent(17775): loss=0.26920675214830897\n",
      "Stochastic Gradient Descent(17776): loss=0.36368772541816535\n",
      "Stochastic Gradient Descent(17777): loss=6.3044445177191895\n",
      "Stochastic Gradient Descent(17778): loss=3.3716113003050174\n",
      "Stochastic Gradient Descent(17779): loss=1.7260391684018297\n",
      "Stochastic Gradient Descent(17780): loss=0.00542931185815941\n",
      "Stochastic Gradient Descent(17781): loss=3.496311771910998\n",
      "Stochastic Gradient Descent(17782): loss=3.3422727367785203\n",
      "Stochastic Gradient Descent(17783): loss=0.14293339466512417\n",
      "Stochastic Gradient Descent(17784): loss=7.261752880509598\n",
      "Stochastic Gradient Descent(17785): loss=0.04132177900629861\n",
      "Stochastic Gradient Descent(17786): loss=0.024556365743423816\n",
      "Stochastic Gradient Descent(17787): loss=2.7121925909881517\n",
      "Stochastic Gradient Descent(17788): loss=0.32805014453469444\n",
      "Stochastic Gradient Descent(17789): loss=0.056156504107460446\n",
      "Stochastic Gradient Descent(17790): loss=0.17142287480319066\n",
      "Stochastic Gradient Descent(17791): loss=2.006365139355033\n",
      "Stochastic Gradient Descent(17792): loss=4.682760408764473\n",
      "Stochastic Gradient Descent(17793): loss=0.23519844564528672\n",
      "Stochastic Gradient Descent(17794): loss=0.23843265036948727\n",
      "Stochastic Gradient Descent(17795): loss=10.748821735576366\n",
      "Stochastic Gradient Descent(17796): loss=5.802416203378191\n",
      "Stochastic Gradient Descent(17797): loss=1.3073900415202873\n",
      "Stochastic Gradient Descent(17798): loss=0.7155073903673029\n",
      "Stochastic Gradient Descent(17799): loss=1.8756055428337661\n",
      "Stochastic Gradient Descent(17800): loss=0.1413485183650762\n",
      "Stochastic Gradient Descent(17801): loss=3.4465408627503904\n",
      "Stochastic Gradient Descent(17802): loss=39.607882780966285\n",
      "Stochastic Gradient Descent(17803): loss=0.06397712281461464\n",
      "Stochastic Gradient Descent(17804): loss=11.817548255076407\n",
      "Stochastic Gradient Descent(17805): loss=0.9668025170479339\n",
      "Stochastic Gradient Descent(17806): loss=1.2408899685281918\n",
      "Stochastic Gradient Descent(17807): loss=1.5892531592075125\n",
      "Stochastic Gradient Descent(17808): loss=20.338192212470037\n",
      "Stochastic Gradient Descent(17809): loss=1.8294598817912653\n",
      "Stochastic Gradient Descent(17810): loss=4.841927148140809\n",
      "Stochastic Gradient Descent(17811): loss=1.9727207681232442\n",
      "Stochastic Gradient Descent(17812): loss=0.3504653719254547\n",
      "Stochastic Gradient Descent(17813): loss=0.0008103384094776454\n",
      "Stochastic Gradient Descent(17814): loss=0.8910947815658489\n",
      "Stochastic Gradient Descent(17815): loss=2.412826504949508\n",
      "Stochastic Gradient Descent(17816): loss=0.2998702722932021\n",
      "Stochastic Gradient Descent(17817): loss=3.2157213708984216\n",
      "Stochastic Gradient Descent(17818): loss=0.6326263458701241\n",
      "Stochastic Gradient Descent(17819): loss=3.69231393117824\n",
      "Stochastic Gradient Descent(17820): loss=1.7566585839341267\n",
      "Stochastic Gradient Descent(17821): loss=0.38636529482120047\n",
      "Stochastic Gradient Descent(17822): loss=0.22809744156800843\n",
      "Stochastic Gradient Descent(17823): loss=1.0147193489500406\n",
      "Stochastic Gradient Descent(17824): loss=2.5380027053458267\n",
      "Stochastic Gradient Descent(17825): loss=0.022561853561306848\n",
      "Stochastic Gradient Descent(17826): loss=0.4571348354929224\n",
      "Stochastic Gradient Descent(17827): loss=0.3311464547768579\n",
      "Stochastic Gradient Descent(17828): loss=1.999879372426213\n",
      "Stochastic Gradient Descent(17829): loss=1.5313937689168475\n",
      "Stochastic Gradient Descent(17830): loss=0.05996758530407631\n",
      "Stochastic Gradient Descent(17831): loss=0.5173548859920822\n",
      "Stochastic Gradient Descent(17832): loss=0.02738070745053227\n",
      "Stochastic Gradient Descent(17833): loss=5.395450886246916\n",
      "Stochastic Gradient Descent(17834): loss=0.1604207593532102\n",
      "Stochastic Gradient Descent(17835): loss=28.320852368748398\n",
      "Stochastic Gradient Descent(17836): loss=0.5780570657378818\n",
      "Stochastic Gradient Descent(17837): loss=16.12586551005405\n",
      "Stochastic Gradient Descent(17838): loss=5.089565312790787\n",
      "Stochastic Gradient Descent(17839): loss=7.781948938243654\n",
      "Stochastic Gradient Descent(17840): loss=1.2131481326597193\n",
      "Stochastic Gradient Descent(17841): loss=51.29691060883697\n",
      "Stochastic Gradient Descent(17842): loss=15.091734688135324\n",
      "Stochastic Gradient Descent(17843): loss=3.381484579224732\n",
      "Stochastic Gradient Descent(17844): loss=0.5502080659568472\n",
      "Stochastic Gradient Descent(17845): loss=2.1241013675758333\n",
      "Stochastic Gradient Descent(17846): loss=14.10424581146784\n",
      "Stochastic Gradient Descent(17847): loss=11.383833423389815\n",
      "Stochastic Gradient Descent(17848): loss=2.180657680590009\n",
      "Stochastic Gradient Descent(17849): loss=2.7090158403573956\n",
      "Stochastic Gradient Descent(17850): loss=0.160530043739889\n",
      "Stochastic Gradient Descent(17851): loss=31.018360458676487\n",
      "Stochastic Gradient Descent(17852): loss=1.1494412843819943\n",
      "Stochastic Gradient Descent(17853): loss=1.1329333039494276\n",
      "Stochastic Gradient Descent(17854): loss=6.276246226111715\n",
      "Stochastic Gradient Descent(17855): loss=0.01612369841970465\n",
      "Stochastic Gradient Descent(17856): loss=1.3108699238467825\n",
      "Stochastic Gradient Descent(17857): loss=1.7946123321906875\n",
      "Stochastic Gradient Descent(17858): loss=1.4396261122263065\n",
      "Stochastic Gradient Descent(17859): loss=2.4423535546968917\n",
      "Stochastic Gradient Descent(17860): loss=23.54517978205836\n",
      "Stochastic Gradient Descent(17861): loss=3.4943504612072194\n",
      "Stochastic Gradient Descent(17862): loss=5.087569259622282\n",
      "Stochastic Gradient Descent(17863): loss=0.5328425179147288\n",
      "Stochastic Gradient Descent(17864): loss=0.3225935262327425\n",
      "Stochastic Gradient Descent(17865): loss=5.389039697495382\n",
      "Stochastic Gradient Descent(17866): loss=0.16647967863494598\n",
      "Stochastic Gradient Descent(17867): loss=3.1205578569907613\n",
      "Stochastic Gradient Descent(17868): loss=1.5328148149726382\n",
      "Stochastic Gradient Descent(17869): loss=4.854263424483737\n",
      "Stochastic Gradient Descent(17870): loss=2.036088164301547\n",
      "Stochastic Gradient Descent(17871): loss=4.506445779171308\n",
      "Stochastic Gradient Descent(17872): loss=1.2354291776959965\n",
      "Stochastic Gradient Descent(17873): loss=3.2320618309913693\n",
      "Stochastic Gradient Descent(17874): loss=1.3952367355484099\n",
      "Stochastic Gradient Descent(17875): loss=5.022249816064606\n",
      "Stochastic Gradient Descent(17876): loss=0.34827795166740877\n",
      "Stochastic Gradient Descent(17877): loss=0.3195974542503888\n",
      "Stochastic Gradient Descent(17878): loss=0.17972191152936817\n",
      "Stochastic Gradient Descent(17879): loss=2.0714839471786295\n",
      "Stochastic Gradient Descent(17880): loss=0.3811031435313273\n",
      "Stochastic Gradient Descent(17881): loss=5.07432789249821\n",
      "Stochastic Gradient Descent(17882): loss=0.3087545404706064\n",
      "Stochastic Gradient Descent(17883): loss=5.25484169776767\n",
      "Stochastic Gradient Descent(17884): loss=10.130138731165378\n",
      "Stochastic Gradient Descent(17885): loss=0.6344307352520083\n",
      "Stochastic Gradient Descent(17886): loss=0.25237953929675533\n",
      "Stochastic Gradient Descent(17887): loss=6.341204801150985\n",
      "Stochastic Gradient Descent(17888): loss=0.4277237584166862\n",
      "Stochastic Gradient Descent(17889): loss=1.5115965263739939\n",
      "Stochastic Gradient Descent(17890): loss=5.764555771811188\n",
      "Stochastic Gradient Descent(17891): loss=23.496675009366278\n",
      "Stochastic Gradient Descent(17892): loss=4.398118941724704\n",
      "Stochastic Gradient Descent(17893): loss=4.546866017779352\n",
      "Stochastic Gradient Descent(17894): loss=0.616482131099154\n",
      "Stochastic Gradient Descent(17895): loss=6.441609187493158\n",
      "Stochastic Gradient Descent(17896): loss=0.07801879395849072\n",
      "Stochastic Gradient Descent(17897): loss=0.17335251247932057\n",
      "Stochastic Gradient Descent(17898): loss=4.207177241560995\n",
      "Stochastic Gradient Descent(17899): loss=2.7226082427852325\n",
      "Stochastic Gradient Descent(17900): loss=3.8750895124390445\n",
      "Stochastic Gradient Descent(17901): loss=8.198921645269607\n",
      "Stochastic Gradient Descent(17902): loss=7.319669033485066\n",
      "Stochastic Gradient Descent(17903): loss=2.806061493946287\n",
      "Stochastic Gradient Descent(17904): loss=2.1297785112096452\n",
      "Stochastic Gradient Descent(17905): loss=16.493439493399357\n",
      "Stochastic Gradient Descent(17906): loss=1.011814534687601\n",
      "Stochastic Gradient Descent(17907): loss=10.334345584897182\n",
      "Stochastic Gradient Descent(17908): loss=0.023428444155192494\n",
      "Stochastic Gradient Descent(17909): loss=0.37572087718563446\n",
      "Stochastic Gradient Descent(17910): loss=5.052446560030349\n",
      "Stochastic Gradient Descent(17911): loss=0.14132882317822273\n",
      "Stochastic Gradient Descent(17912): loss=4.069387450172221\n",
      "Stochastic Gradient Descent(17913): loss=4.6878133377804\n",
      "Stochastic Gradient Descent(17914): loss=0.5683522495796604\n",
      "Stochastic Gradient Descent(17915): loss=2.1133077291328832\n",
      "Stochastic Gradient Descent(17916): loss=0.18345572725002296\n",
      "Stochastic Gradient Descent(17917): loss=1.9952164649774788\n",
      "Stochastic Gradient Descent(17918): loss=5.349236956218584\n",
      "Stochastic Gradient Descent(17919): loss=1.0769680635865475\n",
      "Stochastic Gradient Descent(17920): loss=17.287045656708365\n",
      "Stochastic Gradient Descent(17921): loss=0.36512736621018804\n",
      "Stochastic Gradient Descent(17922): loss=1.7270904328836605\n",
      "Stochastic Gradient Descent(17923): loss=0.021938537303227253\n",
      "Stochastic Gradient Descent(17924): loss=8.032684886963576\n",
      "Stochastic Gradient Descent(17925): loss=2.533396954994592\n",
      "Stochastic Gradient Descent(17926): loss=0.3329233854306024\n",
      "Stochastic Gradient Descent(17927): loss=2.1623186314990175\n",
      "Stochastic Gradient Descent(17928): loss=0.024586782249599443\n",
      "Stochastic Gradient Descent(17929): loss=0.0044770532117073235\n",
      "Stochastic Gradient Descent(17930): loss=0.7897346525254132\n",
      "Stochastic Gradient Descent(17931): loss=3.677010375979406\n",
      "Stochastic Gradient Descent(17932): loss=1.1172448732740907\n",
      "Stochastic Gradient Descent(17933): loss=4.565589348504709\n",
      "Stochastic Gradient Descent(17934): loss=9.739895630368391\n",
      "Stochastic Gradient Descent(17935): loss=18.29616834395597\n",
      "Stochastic Gradient Descent(17936): loss=1.2693944997827604\n",
      "Stochastic Gradient Descent(17937): loss=1.8999678644434346\n",
      "Stochastic Gradient Descent(17938): loss=4.1410746054961525\n",
      "Stochastic Gradient Descent(17939): loss=1.111671182105576\n",
      "Stochastic Gradient Descent(17940): loss=0.3493901900385992\n",
      "Stochastic Gradient Descent(17941): loss=0.5383719365669235\n",
      "Stochastic Gradient Descent(17942): loss=0.09815535726775981\n",
      "Stochastic Gradient Descent(17943): loss=4.226890679167865\n",
      "Stochastic Gradient Descent(17944): loss=3.6088886720560085\n",
      "Stochastic Gradient Descent(17945): loss=8.977937491545871\n",
      "Stochastic Gradient Descent(17946): loss=0.4780539913165981\n",
      "Stochastic Gradient Descent(17947): loss=0.0769455737294223\n",
      "Stochastic Gradient Descent(17948): loss=0.03856169990334692\n",
      "Stochastic Gradient Descent(17949): loss=2.887421366237874\n",
      "Stochastic Gradient Descent(17950): loss=0.007758903766752774\n",
      "Stochastic Gradient Descent(17951): loss=14.460573148054545\n",
      "Stochastic Gradient Descent(17952): loss=0.27522889832652114\n",
      "Stochastic Gradient Descent(17953): loss=2.393515628296935\n",
      "Stochastic Gradient Descent(17954): loss=0.01864701488983961\n",
      "Stochastic Gradient Descent(17955): loss=4.021192946225264\n",
      "Stochastic Gradient Descent(17956): loss=0.28987049013000415\n",
      "Stochastic Gradient Descent(17957): loss=0.31049768366325137\n",
      "Stochastic Gradient Descent(17958): loss=3.446376249095205\n",
      "Stochastic Gradient Descent(17959): loss=2.9303346042767573\n",
      "Stochastic Gradient Descent(17960): loss=7.9298340878020905\n",
      "Stochastic Gradient Descent(17961): loss=5.663860666010911\n",
      "Stochastic Gradient Descent(17962): loss=2.2331561113314606\n",
      "Stochastic Gradient Descent(17963): loss=6.239448344099639\n",
      "Stochastic Gradient Descent(17964): loss=0.1734113816843354\n",
      "Stochastic Gradient Descent(17965): loss=0.4563736005199491\n",
      "Stochastic Gradient Descent(17966): loss=2.608757169285007\n",
      "Stochastic Gradient Descent(17967): loss=7.48718624581569\n",
      "Stochastic Gradient Descent(17968): loss=0.0003750174191146355\n",
      "Stochastic Gradient Descent(17969): loss=0.054402918197392375\n",
      "Stochastic Gradient Descent(17970): loss=0.4016724660287759\n",
      "Stochastic Gradient Descent(17971): loss=2.568918525091532\n",
      "Stochastic Gradient Descent(17972): loss=0.11915095326623021\n",
      "Stochastic Gradient Descent(17973): loss=2.273220240805369\n",
      "Stochastic Gradient Descent(17974): loss=0.8853377988667468\n",
      "Stochastic Gradient Descent(17975): loss=0.06509459301696026\n",
      "Stochastic Gradient Descent(17976): loss=23.21555349334394\n",
      "Stochastic Gradient Descent(17977): loss=3.4504801320543974\n",
      "Stochastic Gradient Descent(17978): loss=9.90160995697052\n",
      "Stochastic Gradient Descent(17979): loss=2.418405235059269\n",
      "Stochastic Gradient Descent(17980): loss=13.053927567518503\n",
      "Stochastic Gradient Descent(17981): loss=0.32633160617164214\n",
      "Stochastic Gradient Descent(17982): loss=22.32032069431826\n",
      "Stochastic Gradient Descent(17983): loss=0.1379806418279893\n",
      "Stochastic Gradient Descent(17984): loss=3.2219170920482516\n",
      "Stochastic Gradient Descent(17985): loss=2.69322150685886\n",
      "Stochastic Gradient Descent(17986): loss=1.3151761302160456\n",
      "Stochastic Gradient Descent(17987): loss=3.6281943576713775\n",
      "Stochastic Gradient Descent(17988): loss=0.26430363764441206\n",
      "Stochastic Gradient Descent(17989): loss=6.466653402131541\n",
      "Stochastic Gradient Descent(17990): loss=3.7718787609522866\n",
      "Stochastic Gradient Descent(17991): loss=1.5025472900278918\n",
      "Stochastic Gradient Descent(17992): loss=0.2069045427486507\n",
      "Stochastic Gradient Descent(17993): loss=2.0108857532639925\n",
      "Stochastic Gradient Descent(17994): loss=1.3347064332856224\n",
      "Stochastic Gradient Descent(17995): loss=0.9384202352699427\n",
      "Stochastic Gradient Descent(17996): loss=10.264751560548993\n",
      "Stochastic Gradient Descent(17997): loss=0.5117239077392299\n",
      "Stochastic Gradient Descent(17998): loss=0.1643682325250942\n",
      "Stochastic Gradient Descent(17999): loss=1.8199574366021105\n",
      "Stochastic Gradient Descent(18000): loss=2.224061733749299\n",
      "Stochastic Gradient Descent(18001): loss=2.246399394526889\n",
      "Stochastic Gradient Descent(18002): loss=2.5089296085107127\n",
      "Stochastic Gradient Descent(18003): loss=0.8569982935726506\n",
      "Stochastic Gradient Descent(18004): loss=1.9331417716701544\n",
      "Stochastic Gradient Descent(18005): loss=1.6709831880566905\n",
      "Stochastic Gradient Descent(18006): loss=2.057679515488448\n",
      "Stochastic Gradient Descent(18007): loss=0.2669099985616326\n",
      "Stochastic Gradient Descent(18008): loss=16.92627811714705\n",
      "Stochastic Gradient Descent(18009): loss=9.48514414780971\n",
      "Stochastic Gradient Descent(18010): loss=0.048581159235667325\n",
      "Stochastic Gradient Descent(18011): loss=13.535867221996243\n",
      "Stochastic Gradient Descent(18012): loss=0.05916798144363146\n",
      "Stochastic Gradient Descent(18013): loss=0.877893738291467\n",
      "Stochastic Gradient Descent(18014): loss=0.6116923399098686\n",
      "Stochastic Gradient Descent(18015): loss=15.87512115923907\n",
      "Stochastic Gradient Descent(18016): loss=5.7660466380219235\n",
      "Stochastic Gradient Descent(18017): loss=0.32971794829508616\n",
      "Stochastic Gradient Descent(18018): loss=3.3171141610125825\n",
      "Stochastic Gradient Descent(18019): loss=7.497764647087326\n",
      "Stochastic Gradient Descent(18020): loss=2.2650320651568165\n",
      "Stochastic Gradient Descent(18021): loss=6.335517592420402\n",
      "Stochastic Gradient Descent(18022): loss=20.988787697232347\n",
      "Stochastic Gradient Descent(18023): loss=4.5948408032316275\n",
      "Stochastic Gradient Descent(18024): loss=0.1850997575271824\n",
      "Stochastic Gradient Descent(18025): loss=0.10570320289451324\n",
      "Stochastic Gradient Descent(18026): loss=0.05586064059973102\n",
      "Stochastic Gradient Descent(18027): loss=1.7888390056126615\n",
      "Stochastic Gradient Descent(18028): loss=14.325601263538035\n",
      "Stochastic Gradient Descent(18029): loss=0.6307322442732742\n",
      "Stochastic Gradient Descent(18030): loss=12.830158425708488\n",
      "Stochastic Gradient Descent(18031): loss=0.8989075443181578\n",
      "Stochastic Gradient Descent(18032): loss=5.134143030717103\n",
      "Stochastic Gradient Descent(18033): loss=0.036818066925832546\n",
      "Stochastic Gradient Descent(18034): loss=1.5110192817072279\n",
      "Stochastic Gradient Descent(18035): loss=4.336901992409902\n",
      "Stochastic Gradient Descent(18036): loss=0.5065915649039253\n",
      "Stochastic Gradient Descent(18037): loss=0.4631717787597858\n",
      "Stochastic Gradient Descent(18038): loss=5.423124783750807\n",
      "Stochastic Gradient Descent(18039): loss=17.316203377920395\n",
      "Stochastic Gradient Descent(18040): loss=0.015604388630725967\n",
      "Stochastic Gradient Descent(18041): loss=10.289543318460707\n",
      "Stochastic Gradient Descent(18042): loss=10.731335568641699\n",
      "Stochastic Gradient Descent(18043): loss=1.5904935068825685\n",
      "Stochastic Gradient Descent(18044): loss=0.1750897797329601\n",
      "Stochastic Gradient Descent(18045): loss=17.271498113426983\n",
      "Stochastic Gradient Descent(18046): loss=8.760127351760655\n",
      "Stochastic Gradient Descent(18047): loss=1.5165203377908516\n",
      "Stochastic Gradient Descent(18048): loss=1.226031167203159\n",
      "Stochastic Gradient Descent(18049): loss=0.9935078849539767\n",
      "Stochastic Gradient Descent(18050): loss=1.4282998191718814\n",
      "Stochastic Gradient Descent(18051): loss=1.8197640955376106\n",
      "Stochastic Gradient Descent(18052): loss=0.6167176916609449\n",
      "Stochastic Gradient Descent(18053): loss=0.06698093970666026\n",
      "Stochastic Gradient Descent(18054): loss=0.3889624397574405\n",
      "Stochastic Gradient Descent(18055): loss=11.831161436201212\n",
      "Stochastic Gradient Descent(18056): loss=3.709292700121878\n",
      "Stochastic Gradient Descent(18057): loss=0.04943002523733119\n",
      "Stochastic Gradient Descent(18058): loss=4.13172723649167\n",
      "Stochastic Gradient Descent(18059): loss=0.4330616713710124\n",
      "Stochastic Gradient Descent(18060): loss=5.089109769160807\n",
      "Stochastic Gradient Descent(18061): loss=4.280022648513844\n",
      "Stochastic Gradient Descent(18062): loss=0.2533826477666499\n",
      "Stochastic Gradient Descent(18063): loss=0.007996780030961109\n",
      "Stochastic Gradient Descent(18064): loss=1.6383617562902306\n",
      "Stochastic Gradient Descent(18065): loss=0.030916299109815217\n",
      "Stochastic Gradient Descent(18066): loss=8.04981307235752\n",
      "Stochastic Gradient Descent(18067): loss=16.93202194709476\n",
      "Stochastic Gradient Descent(18068): loss=0.125306151287312\n",
      "Stochastic Gradient Descent(18069): loss=23.442165979441448\n",
      "Stochastic Gradient Descent(18070): loss=7.944869799677237\n",
      "Stochastic Gradient Descent(18071): loss=2.3015358688757765\n",
      "Stochastic Gradient Descent(18072): loss=14.14378034571813\n",
      "Stochastic Gradient Descent(18073): loss=6.962957348231935\n",
      "Stochastic Gradient Descent(18074): loss=0.9483864852400417\n",
      "Stochastic Gradient Descent(18075): loss=0.7395053418554487\n",
      "Stochastic Gradient Descent(18076): loss=13.438649003619362\n",
      "Stochastic Gradient Descent(18077): loss=0.5152886680007391\n",
      "Stochastic Gradient Descent(18078): loss=4.427081974311161\n",
      "Stochastic Gradient Descent(18079): loss=4.69606977902581\n",
      "Stochastic Gradient Descent(18080): loss=2.3303234995026347\n",
      "Stochastic Gradient Descent(18081): loss=9.003192696618678\n",
      "Stochastic Gradient Descent(18082): loss=0.025225403846580553\n",
      "Stochastic Gradient Descent(18083): loss=7.867083293676797\n",
      "Stochastic Gradient Descent(18084): loss=1.9304245162748062\n",
      "Stochastic Gradient Descent(18085): loss=0.0057305162364540575\n",
      "Stochastic Gradient Descent(18086): loss=0.15700961295545324\n",
      "Stochastic Gradient Descent(18087): loss=0.028213435892833153\n",
      "Stochastic Gradient Descent(18088): loss=0.334093867821778\n",
      "Stochastic Gradient Descent(18089): loss=1.7344363261820632\n",
      "Stochastic Gradient Descent(18090): loss=2.84493546200765\n",
      "Stochastic Gradient Descent(18091): loss=8.435917268178546\n",
      "Stochastic Gradient Descent(18092): loss=4.122061902300324\n",
      "Stochastic Gradient Descent(18093): loss=0.0011596252405174008\n",
      "Stochastic Gradient Descent(18094): loss=0.3472748719963333\n",
      "Stochastic Gradient Descent(18095): loss=0.08807599472685608\n",
      "Stochastic Gradient Descent(18096): loss=6.089678610588062\n",
      "Stochastic Gradient Descent(18097): loss=5.1872540269956323e-05\n",
      "Stochastic Gradient Descent(18098): loss=0.10368402015333032\n",
      "Stochastic Gradient Descent(18099): loss=0.5163506367051054\n",
      "Stochastic Gradient Descent(18100): loss=2.0130966761088653\n",
      "Stochastic Gradient Descent(18101): loss=6.751549931035856\n",
      "Stochastic Gradient Descent(18102): loss=12.795607823330219\n",
      "Stochastic Gradient Descent(18103): loss=2.27738989356472\n",
      "Stochastic Gradient Descent(18104): loss=0.06688953658605176\n",
      "Stochastic Gradient Descent(18105): loss=8.466175988268775\n",
      "Stochastic Gradient Descent(18106): loss=0.0018307569599647546\n",
      "Stochastic Gradient Descent(18107): loss=2.7102072726809907\n",
      "Stochastic Gradient Descent(18108): loss=0.609273133349663\n",
      "Stochastic Gradient Descent(18109): loss=4.68235022211512\n",
      "Stochastic Gradient Descent(18110): loss=1.3056354543222797\n",
      "Stochastic Gradient Descent(18111): loss=5.087444042622989\n",
      "Stochastic Gradient Descent(18112): loss=0.018831006480378\n",
      "Stochastic Gradient Descent(18113): loss=0.9595193451525291\n",
      "Stochastic Gradient Descent(18114): loss=0.06210011243341042\n",
      "Stochastic Gradient Descent(18115): loss=4.725753098454611\n",
      "Stochastic Gradient Descent(18116): loss=2.3548821811355283\n",
      "Stochastic Gradient Descent(18117): loss=0.7270150041645406\n",
      "Stochastic Gradient Descent(18118): loss=0.10013781478002243\n",
      "Stochastic Gradient Descent(18119): loss=6.965120033418927\n",
      "Stochastic Gradient Descent(18120): loss=2.0020033300073505\n",
      "Stochastic Gradient Descent(18121): loss=38.80259516344931\n",
      "Stochastic Gradient Descent(18122): loss=2.0865938018915204\n",
      "Stochastic Gradient Descent(18123): loss=2.459165683765497\n",
      "Stochastic Gradient Descent(18124): loss=37.8942014269996\n",
      "Stochastic Gradient Descent(18125): loss=80.8608249850648\n",
      "Stochastic Gradient Descent(18126): loss=3.2038174797571815\n",
      "Stochastic Gradient Descent(18127): loss=2.7695562495330175\n",
      "Stochastic Gradient Descent(18128): loss=0.5698633214320322\n",
      "Stochastic Gradient Descent(18129): loss=0.7563350622859584\n",
      "Stochastic Gradient Descent(18130): loss=5.576053954555253\n",
      "Stochastic Gradient Descent(18131): loss=0.03646345866836502\n",
      "Stochastic Gradient Descent(18132): loss=1.3388394470606204\n",
      "Stochastic Gradient Descent(18133): loss=1.4508042219601804\n",
      "Stochastic Gradient Descent(18134): loss=4.763831971939269\n",
      "Stochastic Gradient Descent(18135): loss=1.214622638046407\n",
      "Stochastic Gradient Descent(18136): loss=5.961282824129644\n",
      "Stochastic Gradient Descent(18137): loss=3.463455577713035\n",
      "Stochastic Gradient Descent(18138): loss=1.5072813585623375\n",
      "Stochastic Gradient Descent(18139): loss=43.31856210471723\n",
      "Stochastic Gradient Descent(18140): loss=6.511093735214585\n",
      "Stochastic Gradient Descent(18141): loss=4.6865748704246135\n",
      "Stochastic Gradient Descent(18142): loss=0.02767656494702814\n",
      "Stochastic Gradient Descent(18143): loss=8.360619156817474\n",
      "Stochastic Gradient Descent(18144): loss=1.7417230338895873\n",
      "Stochastic Gradient Descent(18145): loss=8.161370893605316\n",
      "Stochastic Gradient Descent(18146): loss=1.2290889012130837\n",
      "Stochastic Gradient Descent(18147): loss=7.040293488072822\n",
      "Stochastic Gradient Descent(18148): loss=0.9059158711821158\n",
      "Stochastic Gradient Descent(18149): loss=0.3712916627701314\n",
      "Stochastic Gradient Descent(18150): loss=6.692688164019627\n",
      "Stochastic Gradient Descent(18151): loss=0.40169075194004406\n",
      "Stochastic Gradient Descent(18152): loss=9.613331598898904\n",
      "Stochastic Gradient Descent(18153): loss=0.10413477735286523\n",
      "Stochastic Gradient Descent(18154): loss=4.2459163651446294\n",
      "Stochastic Gradient Descent(18155): loss=0.9039655054276088\n",
      "Stochastic Gradient Descent(18156): loss=2.9682819216710423\n",
      "Stochastic Gradient Descent(18157): loss=1.7824286197489898\n",
      "Stochastic Gradient Descent(18158): loss=0.2276595597822757\n",
      "Stochastic Gradient Descent(18159): loss=6.047086732866186\n",
      "Stochastic Gradient Descent(18160): loss=1.517806343608492\n",
      "Stochastic Gradient Descent(18161): loss=1.415031768927914\n",
      "Stochastic Gradient Descent(18162): loss=0.702164200423356\n",
      "Stochastic Gradient Descent(18163): loss=15.293686263516962\n",
      "Stochastic Gradient Descent(18164): loss=36.00088703372876\n",
      "Stochastic Gradient Descent(18165): loss=0.030957361127514776\n",
      "Stochastic Gradient Descent(18166): loss=6.108311298674453\n",
      "Stochastic Gradient Descent(18167): loss=5.374470276112588\n",
      "Stochastic Gradient Descent(18168): loss=4.749704997198288\n",
      "Stochastic Gradient Descent(18169): loss=5.142070003203831\n",
      "Stochastic Gradient Descent(18170): loss=9.719760098894476\n",
      "Stochastic Gradient Descent(18171): loss=7.414714324303113\n",
      "Stochastic Gradient Descent(18172): loss=25.21724811495427\n",
      "Stochastic Gradient Descent(18173): loss=0.5388796552810685\n",
      "Stochastic Gradient Descent(18174): loss=12.421546013558823\n",
      "Stochastic Gradient Descent(18175): loss=1.5709245793253628\n",
      "Stochastic Gradient Descent(18176): loss=1.081132905467471\n",
      "Stochastic Gradient Descent(18177): loss=0.020878356188867388\n",
      "Stochastic Gradient Descent(18178): loss=0.3883950216001182\n",
      "Stochastic Gradient Descent(18179): loss=32.83959606166222\n",
      "Stochastic Gradient Descent(18180): loss=11.5035694395971\n",
      "Stochastic Gradient Descent(18181): loss=2.5389954656612335\n",
      "Stochastic Gradient Descent(18182): loss=1.2533058851203742\n",
      "Stochastic Gradient Descent(18183): loss=0.7830164568506748\n",
      "Stochastic Gradient Descent(18184): loss=6.198097446288159\n",
      "Stochastic Gradient Descent(18185): loss=2.459608396490919\n",
      "Stochastic Gradient Descent(18186): loss=0.062283848176130086\n",
      "Stochastic Gradient Descent(18187): loss=1.3436424708076449\n",
      "Stochastic Gradient Descent(18188): loss=21.199798886994795\n",
      "Stochastic Gradient Descent(18189): loss=0.44561607391130664\n",
      "Stochastic Gradient Descent(18190): loss=3.3992286160211975\n",
      "Stochastic Gradient Descent(18191): loss=29.067911021242747\n",
      "Stochastic Gradient Descent(18192): loss=6.79379314009511\n",
      "Stochastic Gradient Descent(18193): loss=27.747208001552266\n",
      "Stochastic Gradient Descent(18194): loss=1.1505023748086722\n",
      "Stochastic Gradient Descent(18195): loss=0.272135178487905\n",
      "Stochastic Gradient Descent(18196): loss=22.94021012495232\n",
      "Stochastic Gradient Descent(18197): loss=12.067045199044687\n",
      "Stochastic Gradient Descent(18198): loss=6.14727612643015\n",
      "Stochastic Gradient Descent(18199): loss=24.206692630032585\n",
      "Stochastic Gradient Descent(18200): loss=4.269836460296455\n",
      "Stochastic Gradient Descent(18201): loss=1.7446967916829734\n",
      "Stochastic Gradient Descent(18202): loss=1.491341337729837\n",
      "Stochastic Gradient Descent(18203): loss=0.27522891873984945\n",
      "Stochastic Gradient Descent(18204): loss=9.717118715760462\n",
      "Stochastic Gradient Descent(18205): loss=0.10039104200832683\n",
      "Stochastic Gradient Descent(18206): loss=5.871393224469169\n",
      "Stochastic Gradient Descent(18207): loss=0.3391382443266887\n",
      "Stochastic Gradient Descent(18208): loss=1.9443327962112968\n",
      "Stochastic Gradient Descent(18209): loss=31.863446888877604\n",
      "Stochastic Gradient Descent(18210): loss=25.5457128624734\n",
      "Stochastic Gradient Descent(18211): loss=2.0476632730309134\n",
      "Stochastic Gradient Descent(18212): loss=0.18431023618360742\n",
      "Stochastic Gradient Descent(18213): loss=0.7819262727760681\n",
      "Stochastic Gradient Descent(18214): loss=28.267055261038927\n",
      "Stochastic Gradient Descent(18215): loss=17.98069992367433\n",
      "Stochastic Gradient Descent(18216): loss=7.497840151204564\n",
      "Stochastic Gradient Descent(18217): loss=9.401024084599896\n",
      "Stochastic Gradient Descent(18218): loss=0.3660319827429137\n",
      "Stochastic Gradient Descent(18219): loss=0.00024429104825185634\n",
      "Stochastic Gradient Descent(18220): loss=2.5073837689654153\n",
      "Stochastic Gradient Descent(18221): loss=0.03916087453959259\n",
      "Stochastic Gradient Descent(18222): loss=7.311624022119081\n",
      "Stochastic Gradient Descent(18223): loss=3.0675906705735945\n",
      "Stochastic Gradient Descent(18224): loss=0.6987661923217723\n",
      "Stochastic Gradient Descent(18225): loss=12.411686085567025\n",
      "Stochastic Gradient Descent(18226): loss=1.0185089747762124\n",
      "Stochastic Gradient Descent(18227): loss=10.408577020809682\n",
      "Stochastic Gradient Descent(18228): loss=6.436793160212894\n",
      "Stochastic Gradient Descent(18229): loss=2.9609421684765844\n",
      "Stochastic Gradient Descent(18230): loss=1.4575982253023738\n",
      "Stochastic Gradient Descent(18231): loss=6.133385986127066\n",
      "Stochastic Gradient Descent(18232): loss=6.173677714964645\n",
      "Stochastic Gradient Descent(18233): loss=0.026534771956935856\n",
      "Stochastic Gradient Descent(18234): loss=0.10206683770678697\n",
      "Stochastic Gradient Descent(18235): loss=0.6071447001353298\n",
      "Stochastic Gradient Descent(18236): loss=2.4327351278323683\n",
      "Stochastic Gradient Descent(18237): loss=4.363767800440908\n",
      "Stochastic Gradient Descent(18238): loss=6.9337847667521135\n",
      "Stochastic Gradient Descent(18239): loss=12.974489153408426\n",
      "Stochastic Gradient Descent(18240): loss=0.020678350072007558\n",
      "Stochastic Gradient Descent(18241): loss=0.3847910779674855\n",
      "Stochastic Gradient Descent(18242): loss=8.75208254902916\n",
      "Stochastic Gradient Descent(18243): loss=3.205682437322305\n",
      "Stochastic Gradient Descent(18244): loss=3.5621885323921827\n",
      "Stochastic Gradient Descent(18245): loss=0.030291412301535224\n",
      "Stochastic Gradient Descent(18246): loss=1.5159588327628104\n",
      "Stochastic Gradient Descent(18247): loss=0.5013504738237177\n",
      "Stochastic Gradient Descent(18248): loss=4.812842094002986\n",
      "Stochastic Gradient Descent(18249): loss=13.066147256691261\n",
      "Stochastic Gradient Descent(18250): loss=2.538441732975617\n",
      "Stochastic Gradient Descent(18251): loss=4.06727003976332\n",
      "Stochastic Gradient Descent(18252): loss=13.158803474272483\n",
      "Stochastic Gradient Descent(18253): loss=4.24800217092478\n",
      "Stochastic Gradient Descent(18254): loss=7.233777928254851\n",
      "Stochastic Gradient Descent(18255): loss=10.143770453548825\n",
      "Stochastic Gradient Descent(18256): loss=3.2274101470682877\n",
      "Stochastic Gradient Descent(18257): loss=1.9598715964942237\n",
      "Stochastic Gradient Descent(18258): loss=4.8499267749282815\n",
      "Stochastic Gradient Descent(18259): loss=43.11928039737953\n",
      "Stochastic Gradient Descent(18260): loss=14.332961151736198\n",
      "Stochastic Gradient Descent(18261): loss=1.4703616474887982\n",
      "Stochastic Gradient Descent(18262): loss=6.221889556548823\n",
      "Stochastic Gradient Descent(18263): loss=11.661725626081449\n",
      "Stochastic Gradient Descent(18264): loss=1.317374379520537\n",
      "Stochastic Gradient Descent(18265): loss=10.300232805987456\n",
      "Stochastic Gradient Descent(18266): loss=0.6167302436041854\n",
      "Stochastic Gradient Descent(18267): loss=13.888713030245757\n",
      "Stochastic Gradient Descent(18268): loss=0.006149684008141611\n",
      "Stochastic Gradient Descent(18269): loss=1.4520276323181878\n",
      "Stochastic Gradient Descent(18270): loss=8.700445869186721\n",
      "Stochastic Gradient Descent(18271): loss=2.334933660072808\n",
      "Stochastic Gradient Descent(18272): loss=1.9995539142379049\n",
      "Stochastic Gradient Descent(18273): loss=1.1207562278178596\n",
      "Stochastic Gradient Descent(18274): loss=6.4069498815894494\n",
      "Stochastic Gradient Descent(18275): loss=0.8693272731509868\n",
      "Stochastic Gradient Descent(18276): loss=37.68033270141075\n",
      "Stochastic Gradient Descent(18277): loss=2.39420478710594\n",
      "Stochastic Gradient Descent(18278): loss=5.9696072572065795\n",
      "Stochastic Gradient Descent(18279): loss=5.230977737938514\n",
      "Stochastic Gradient Descent(18280): loss=10.790208935232501\n",
      "Stochastic Gradient Descent(18281): loss=3.687273675317589\n",
      "Stochastic Gradient Descent(18282): loss=4.105415954908545\n",
      "Stochastic Gradient Descent(18283): loss=8.425742332873753e-05\n",
      "Stochastic Gradient Descent(18284): loss=2.9366481981958166\n",
      "Stochastic Gradient Descent(18285): loss=2.8900659896779395\n",
      "Stochastic Gradient Descent(18286): loss=0.30474491541976984\n",
      "Stochastic Gradient Descent(18287): loss=1.632300659533177\n",
      "Stochastic Gradient Descent(18288): loss=0.3301295494498459\n",
      "Stochastic Gradient Descent(18289): loss=0.3050547408809698\n",
      "Stochastic Gradient Descent(18290): loss=3.634813541827121\n",
      "Stochastic Gradient Descent(18291): loss=6.507617137673079\n",
      "Stochastic Gradient Descent(18292): loss=1.062308483773406\n",
      "Stochastic Gradient Descent(18293): loss=5.302948750672395\n",
      "Stochastic Gradient Descent(18294): loss=0.5990781322448396\n",
      "Stochastic Gradient Descent(18295): loss=0.20014108032483455\n",
      "Stochastic Gradient Descent(18296): loss=5.728653684053738\n",
      "Stochastic Gradient Descent(18297): loss=29.72428311558887\n",
      "Stochastic Gradient Descent(18298): loss=1.1143694081064566\n",
      "Stochastic Gradient Descent(18299): loss=7.414225754815052\n",
      "Stochastic Gradient Descent(18300): loss=1.774572807933755\n",
      "Stochastic Gradient Descent(18301): loss=4.535304080045949\n",
      "Stochastic Gradient Descent(18302): loss=11.408339290551744\n",
      "Stochastic Gradient Descent(18303): loss=0.0875392348079283\n",
      "Stochastic Gradient Descent(18304): loss=3.3945226346329425\n",
      "Stochastic Gradient Descent(18305): loss=1.427582356660779\n",
      "Stochastic Gradient Descent(18306): loss=2.532666633631637\n",
      "Stochastic Gradient Descent(18307): loss=1.223696710997545\n",
      "Stochastic Gradient Descent(18308): loss=0.03225202707419724\n",
      "Stochastic Gradient Descent(18309): loss=11.178019715754358\n",
      "Stochastic Gradient Descent(18310): loss=0.04103910023931156\n",
      "Stochastic Gradient Descent(18311): loss=18.767217147355264\n",
      "Stochastic Gradient Descent(18312): loss=27.558325145082343\n",
      "Stochastic Gradient Descent(18313): loss=3.401972142857294\n",
      "Stochastic Gradient Descent(18314): loss=0.6634684182979491\n",
      "Stochastic Gradient Descent(18315): loss=1.1258937028930198\n",
      "Stochastic Gradient Descent(18316): loss=0.015922444674050892\n",
      "Stochastic Gradient Descent(18317): loss=14.134657048809867\n",
      "Stochastic Gradient Descent(18318): loss=1.486461274018907\n",
      "Stochastic Gradient Descent(18319): loss=0.6940947842642258\n",
      "Stochastic Gradient Descent(18320): loss=0.3585527248295055\n",
      "Stochastic Gradient Descent(18321): loss=0.06973611161820183\n",
      "Stochastic Gradient Descent(18322): loss=12.037251032204024\n",
      "Stochastic Gradient Descent(18323): loss=9.784892567422121\n",
      "Stochastic Gradient Descent(18324): loss=1.6972361329149626\n",
      "Stochastic Gradient Descent(18325): loss=0.08744331755052419\n",
      "Stochastic Gradient Descent(18326): loss=14.941914914171047\n",
      "Stochastic Gradient Descent(18327): loss=1.1018568182874942\n",
      "Stochastic Gradient Descent(18328): loss=1.7973150526587596\n",
      "Stochastic Gradient Descent(18329): loss=0.5946764343310756\n",
      "Stochastic Gradient Descent(18330): loss=0.6777432049405537\n",
      "Stochastic Gradient Descent(18331): loss=0.750182161384888\n",
      "Stochastic Gradient Descent(18332): loss=6.420493537587462\n",
      "Stochastic Gradient Descent(18333): loss=0.32431593182980817\n",
      "Stochastic Gradient Descent(18334): loss=1.5589068771391732\n",
      "Stochastic Gradient Descent(18335): loss=7.8657765718960375\n",
      "Stochastic Gradient Descent(18336): loss=4.632286864512791\n",
      "Stochastic Gradient Descent(18337): loss=3.0332312323849626\n",
      "Stochastic Gradient Descent(18338): loss=1.9678867646161373\n",
      "Stochastic Gradient Descent(18339): loss=17.45390535591846\n",
      "Stochastic Gradient Descent(18340): loss=8.942594860650809\n",
      "Stochastic Gradient Descent(18341): loss=4.754174188588134\n",
      "Stochastic Gradient Descent(18342): loss=2.9686355175805414\n",
      "Stochastic Gradient Descent(18343): loss=1.9014389783553791\n",
      "Stochastic Gradient Descent(18344): loss=1.8392414312458927\n",
      "Stochastic Gradient Descent(18345): loss=1.7629873073081854\n",
      "Stochastic Gradient Descent(18346): loss=13.207644773930022\n",
      "Stochastic Gradient Descent(18347): loss=0.6575791002197233\n",
      "Stochastic Gradient Descent(18348): loss=0.06360724911460051\n",
      "Stochastic Gradient Descent(18349): loss=0.009563860720020335\n",
      "Stochastic Gradient Descent(18350): loss=16.95334173133544\n",
      "Stochastic Gradient Descent(18351): loss=0.15871593743511442\n",
      "Stochastic Gradient Descent(18352): loss=19.13792471724635\n",
      "Stochastic Gradient Descent(18353): loss=14.047493938091693\n",
      "Stochastic Gradient Descent(18354): loss=0.028830788466122876\n",
      "Stochastic Gradient Descent(18355): loss=9.619299202473995\n",
      "Stochastic Gradient Descent(18356): loss=3.2567899049442093\n",
      "Stochastic Gradient Descent(18357): loss=7.81239197114653\n",
      "Stochastic Gradient Descent(18358): loss=3.107169625281465\n",
      "Stochastic Gradient Descent(18359): loss=5.455020256225774\n",
      "Stochastic Gradient Descent(18360): loss=21.655112408661164\n",
      "Stochastic Gradient Descent(18361): loss=4.318295749512522\n",
      "Stochastic Gradient Descent(18362): loss=4.803453887919848\n",
      "Stochastic Gradient Descent(18363): loss=3.8950917329815877\n",
      "Stochastic Gradient Descent(18364): loss=116.8959717355592\n",
      "Stochastic Gradient Descent(18365): loss=1.7423642868412967\n",
      "Stochastic Gradient Descent(18366): loss=2.941599744109233\n",
      "Stochastic Gradient Descent(18367): loss=1.7260624157608475\n",
      "Stochastic Gradient Descent(18368): loss=11.372314885001591\n",
      "Stochastic Gradient Descent(18369): loss=1.260687675511208\n",
      "Stochastic Gradient Descent(18370): loss=20.52744455327856\n",
      "Stochastic Gradient Descent(18371): loss=24.4211794073802\n",
      "Stochastic Gradient Descent(18372): loss=5.778837852712567\n",
      "Stochastic Gradient Descent(18373): loss=1.6978154603954643\n",
      "Stochastic Gradient Descent(18374): loss=0.18429105608120086\n",
      "Stochastic Gradient Descent(18375): loss=4.929492344094842\n",
      "Stochastic Gradient Descent(18376): loss=7.047351386134106\n",
      "Stochastic Gradient Descent(18377): loss=1.6359304907289105\n",
      "Stochastic Gradient Descent(18378): loss=1.5611696078260273\n",
      "Stochastic Gradient Descent(18379): loss=5.302075428527225\n",
      "Stochastic Gradient Descent(18380): loss=5.024846296649135\n",
      "Stochastic Gradient Descent(18381): loss=0.6856675558830383\n",
      "Stochastic Gradient Descent(18382): loss=0.003451053420592842\n",
      "Stochastic Gradient Descent(18383): loss=1.8467047287673795\n",
      "Stochastic Gradient Descent(18384): loss=0.8432420339403881\n",
      "Stochastic Gradient Descent(18385): loss=0.002429374818138479\n",
      "Stochastic Gradient Descent(18386): loss=2.985111662166523\n",
      "Stochastic Gradient Descent(18387): loss=6.2945866758004065\n",
      "Stochastic Gradient Descent(18388): loss=17.035125009488794\n",
      "Stochastic Gradient Descent(18389): loss=4.628075805573789\n",
      "Stochastic Gradient Descent(18390): loss=0.5466987563406309\n",
      "Stochastic Gradient Descent(18391): loss=0.004629000371867612\n",
      "Stochastic Gradient Descent(18392): loss=0.03525217416319346\n",
      "Stochastic Gradient Descent(18393): loss=1.3898246348489809\n",
      "Stochastic Gradient Descent(18394): loss=0.3822925389831029\n",
      "Stochastic Gradient Descent(18395): loss=1.9375019065745192\n",
      "Stochastic Gradient Descent(18396): loss=7.635065992455754\n",
      "Stochastic Gradient Descent(18397): loss=3.717922920187688\n",
      "Stochastic Gradient Descent(18398): loss=0.4288496024461212\n",
      "Stochastic Gradient Descent(18399): loss=9.754401590026433\n",
      "Stochastic Gradient Descent(18400): loss=0.23719917974840787\n",
      "Stochastic Gradient Descent(18401): loss=8.359079832024785\n",
      "Stochastic Gradient Descent(18402): loss=5.503408684186613\n",
      "Stochastic Gradient Descent(18403): loss=9.307761879628416\n",
      "Stochastic Gradient Descent(18404): loss=5.665710161725359\n",
      "Stochastic Gradient Descent(18405): loss=0.5971666941848578\n",
      "Stochastic Gradient Descent(18406): loss=4.799586105352571\n",
      "Stochastic Gradient Descent(18407): loss=4.1832794226435155\n",
      "Stochastic Gradient Descent(18408): loss=5.393732859105371\n",
      "Stochastic Gradient Descent(18409): loss=0.8655428920179705\n",
      "Stochastic Gradient Descent(18410): loss=2.0628706619373065\n",
      "Stochastic Gradient Descent(18411): loss=6.59914060822755\n",
      "Stochastic Gradient Descent(18412): loss=0.32535969871175324\n",
      "Stochastic Gradient Descent(18413): loss=0.27495917173503115\n",
      "Stochastic Gradient Descent(18414): loss=0.03249019903644907\n",
      "Stochastic Gradient Descent(18415): loss=2.6011901643142012\n",
      "Stochastic Gradient Descent(18416): loss=24.766516988509828\n",
      "Stochastic Gradient Descent(18417): loss=1.4920924903749246\n",
      "Stochastic Gradient Descent(18418): loss=3.610347081609518\n",
      "Stochastic Gradient Descent(18419): loss=0.6598146029476291\n",
      "Stochastic Gradient Descent(18420): loss=1.2175424154399352\n",
      "Stochastic Gradient Descent(18421): loss=1.1604090678885663\n",
      "Stochastic Gradient Descent(18422): loss=2.0050425057960934\n",
      "Stochastic Gradient Descent(18423): loss=2.1354243829513364\n",
      "Stochastic Gradient Descent(18424): loss=2.656051113337001\n",
      "Stochastic Gradient Descent(18425): loss=0.5921803526641048\n",
      "Stochastic Gradient Descent(18426): loss=0.7802205270535533\n",
      "Stochastic Gradient Descent(18427): loss=7.932199003948163\n",
      "Stochastic Gradient Descent(18428): loss=3.777095294249244\n",
      "Stochastic Gradient Descent(18429): loss=0.5368229406725519\n",
      "Stochastic Gradient Descent(18430): loss=0.004504378950853615\n",
      "Stochastic Gradient Descent(18431): loss=1.6990175089204127\n",
      "Stochastic Gradient Descent(18432): loss=1.9044073882067198\n",
      "Stochastic Gradient Descent(18433): loss=5.680259723992122\n",
      "Stochastic Gradient Descent(18434): loss=2.159382654089758\n",
      "Stochastic Gradient Descent(18435): loss=2.948083292425346\n",
      "Stochastic Gradient Descent(18436): loss=2.288894255927187\n",
      "Stochastic Gradient Descent(18437): loss=0.38754698076917954\n",
      "Stochastic Gradient Descent(18438): loss=0.5927242986673509\n",
      "Stochastic Gradient Descent(18439): loss=37.96360872291433\n",
      "Stochastic Gradient Descent(18440): loss=1.6600745295189894\n",
      "Stochastic Gradient Descent(18441): loss=5.000141631610714\n",
      "Stochastic Gradient Descent(18442): loss=41.35468335109356\n",
      "Stochastic Gradient Descent(18443): loss=14.386924037626622\n",
      "Stochastic Gradient Descent(18444): loss=11.971114263564353\n",
      "Stochastic Gradient Descent(18445): loss=0.2106247696858394\n",
      "Stochastic Gradient Descent(18446): loss=2.1894266398354705\n",
      "Stochastic Gradient Descent(18447): loss=0.22753568451168843\n",
      "Stochastic Gradient Descent(18448): loss=1.4417486999227518\n",
      "Stochastic Gradient Descent(18449): loss=8.720324516264377\n",
      "Stochastic Gradient Descent(18450): loss=3.8848359730699578\n",
      "Stochastic Gradient Descent(18451): loss=10.938150402807514\n",
      "Stochastic Gradient Descent(18452): loss=0.9636800609696494\n",
      "Stochastic Gradient Descent(18453): loss=2.0093473861079323\n",
      "Stochastic Gradient Descent(18454): loss=0.18857552894100904\n",
      "Stochastic Gradient Descent(18455): loss=0.006352586390021737\n",
      "Stochastic Gradient Descent(18456): loss=3.61423664168089\n",
      "Stochastic Gradient Descent(18457): loss=0.5323017020785465\n",
      "Stochastic Gradient Descent(18458): loss=0.048045964621718625\n",
      "Stochastic Gradient Descent(18459): loss=2.56998144096031\n",
      "Stochastic Gradient Descent(18460): loss=0.1558395053524062\n",
      "Stochastic Gradient Descent(18461): loss=1.6323612261945724\n",
      "Stochastic Gradient Descent(18462): loss=2.085540977496234\n",
      "Stochastic Gradient Descent(18463): loss=10.55311819058845\n",
      "Stochastic Gradient Descent(18464): loss=12.749893865773807\n",
      "Stochastic Gradient Descent(18465): loss=7.446732977595327\n",
      "Stochastic Gradient Descent(18466): loss=0.06948329620721551\n",
      "Stochastic Gradient Descent(18467): loss=0.06742281763628556\n",
      "Stochastic Gradient Descent(18468): loss=4.253703080205842\n",
      "Stochastic Gradient Descent(18469): loss=7.299055134420312\n",
      "Stochastic Gradient Descent(18470): loss=8.196526155885747\n",
      "Stochastic Gradient Descent(18471): loss=3.182608265829959\n",
      "Stochastic Gradient Descent(18472): loss=1.1900912172179468\n",
      "Stochastic Gradient Descent(18473): loss=13.236146609712044\n",
      "Stochastic Gradient Descent(18474): loss=14.323976026160604\n",
      "Stochastic Gradient Descent(18475): loss=16.987118490776012\n",
      "Stochastic Gradient Descent(18476): loss=2.5529762052205403\n",
      "Stochastic Gradient Descent(18477): loss=0.14669417890134756\n",
      "Stochastic Gradient Descent(18478): loss=5.116008109359095\n",
      "Stochastic Gradient Descent(18479): loss=0.7052646563845953\n",
      "Stochastic Gradient Descent(18480): loss=2.7813827624026852\n",
      "Stochastic Gradient Descent(18481): loss=2.2686913685909385e-06\n",
      "Stochastic Gradient Descent(18482): loss=1.5845269764933416\n",
      "Stochastic Gradient Descent(18483): loss=11.34827514332116\n",
      "Stochastic Gradient Descent(18484): loss=4.6908139529308\n",
      "Stochastic Gradient Descent(18485): loss=56.934389122662914\n",
      "Stochastic Gradient Descent(18486): loss=8.233692396891487e-05\n",
      "Stochastic Gradient Descent(18487): loss=43.42619243383797\n",
      "Stochastic Gradient Descent(18488): loss=26.295298210934586\n",
      "Stochastic Gradient Descent(18489): loss=0.865134136040712\n",
      "Stochastic Gradient Descent(18490): loss=2.4827580504094007\n",
      "Stochastic Gradient Descent(18491): loss=55.56849677682348\n",
      "Stochastic Gradient Descent(18492): loss=0.02985082177897554\n",
      "Stochastic Gradient Descent(18493): loss=1.5231391359073625\n",
      "Stochastic Gradient Descent(18494): loss=3.2390768662764553\n",
      "Stochastic Gradient Descent(18495): loss=0.5173367164939652\n",
      "Stochastic Gradient Descent(18496): loss=5.792647554940413\n",
      "Stochastic Gradient Descent(18497): loss=1.2447366948926926\n",
      "Stochastic Gradient Descent(18498): loss=12.996318424387468\n",
      "Stochastic Gradient Descent(18499): loss=1.266390893045023\n",
      "Stochastic Gradient Descent(18500): loss=0.5817002765535823\n",
      "Stochastic Gradient Descent(18501): loss=0.02998634283655394\n",
      "Stochastic Gradient Descent(18502): loss=20.01845509383651\n",
      "Stochastic Gradient Descent(18503): loss=0.516445367528972\n",
      "Stochastic Gradient Descent(18504): loss=6.403542143666542\n",
      "Stochastic Gradient Descent(18505): loss=1.0793347266994127\n",
      "Stochastic Gradient Descent(18506): loss=0.004700557076260872\n",
      "Stochastic Gradient Descent(18507): loss=2.6653146533057526\n",
      "Stochastic Gradient Descent(18508): loss=0.021305980260815732\n",
      "Stochastic Gradient Descent(18509): loss=0.9205976467267641\n",
      "Stochastic Gradient Descent(18510): loss=3.6739224623902444\n",
      "Stochastic Gradient Descent(18511): loss=0.5182636603072287\n",
      "Stochastic Gradient Descent(18512): loss=0.21051314716768602\n",
      "Stochastic Gradient Descent(18513): loss=5.694495544322693\n",
      "Stochastic Gradient Descent(18514): loss=21.04004849402692\n",
      "Stochastic Gradient Descent(18515): loss=4.0012362273385\n",
      "Stochastic Gradient Descent(18516): loss=0.0026745733611262235\n",
      "Stochastic Gradient Descent(18517): loss=3.3092355277049594\n",
      "Stochastic Gradient Descent(18518): loss=4.887972593591807\n",
      "Stochastic Gradient Descent(18519): loss=0.005056643306082878\n",
      "Stochastic Gradient Descent(18520): loss=4.943717220637454\n",
      "Stochastic Gradient Descent(18521): loss=2.143257356281872\n",
      "Stochastic Gradient Descent(18522): loss=0.45314799697946395\n",
      "Stochastic Gradient Descent(18523): loss=2.191556722381579\n",
      "Stochastic Gradient Descent(18524): loss=3.209983632101267\n",
      "Stochastic Gradient Descent(18525): loss=10.77037805512263\n",
      "Stochastic Gradient Descent(18526): loss=0.7206047273754893\n",
      "Stochastic Gradient Descent(18527): loss=1.8443968094867156\n",
      "Stochastic Gradient Descent(18528): loss=0.05614338288385521\n",
      "Stochastic Gradient Descent(18529): loss=10.542244689153486\n",
      "Stochastic Gradient Descent(18530): loss=2.4121294902552783\n",
      "Stochastic Gradient Descent(18531): loss=21.623238639152003\n",
      "Stochastic Gradient Descent(18532): loss=22.23990293157611\n",
      "Stochastic Gradient Descent(18533): loss=109.8073876716008\n",
      "Stochastic Gradient Descent(18534): loss=4.257969710106906\n",
      "Stochastic Gradient Descent(18535): loss=8.19762560806207\n",
      "Stochastic Gradient Descent(18536): loss=4.543687666377397\n",
      "Stochastic Gradient Descent(18537): loss=0.29000793808660014\n",
      "Stochastic Gradient Descent(18538): loss=12.008579953725572\n",
      "Stochastic Gradient Descent(18539): loss=0.6654872126325808\n",
      "Stochastic Gradient Descent(18540): loss=0.013073221350103643\n",
      "Stochastic Gradient Descent(18541): loss=14.032932095762515\n",
      "Stochastic Gradient Descent(18542): loss=5.110598948575962\n",
      "Stochastic Gradient Descent(18543): loss=0.19180351018185055\n",
      "Stochastic Gradient Descent(18544): loss=12.739586354747802\n",
      "Stochastic Gradient Descent(18545): loss=10.952413679030023\n",
      "Stochastic Gradient Descent(18546): loss=12.178966987239917\n",
      "Stochastic Gradient Descent(18547): loss=2.2844879801414146\n",
      "Stochastic Gradient Descent(18548): loss=10.789364588549386\n",
      "Stochastic Gradient Descent(18549): loss=0.9896744910560102\n",
      "Stochastic Gradient Descent(18550): loss=1.0058488893663573\n",
      "Stochastic Gradient Descent(18551): loss=11.780203464650064\n",
      "Stochastic Gradient Descent(18552): loss=4.705554063291892\n",
      "Stochastic Gradient Descent(18553): loss=7.647138378564822\n",
      "Stochastic Gradient Descent(18554): loss=3.8173606824690576\n",
      "Stochastic Gradient Descent(18555): loss=1.2701926363092297\n",
      "Stochastic Gradient Descent(18556): loss=5.226492684067351\n",
      "Stochastic Gradient Descent(18557): loss=45.1356753342024\n",
      "Stochastic Gradient Descent(18558): loss=12.325533327050014\n",
      "Stochastic Gradient Descent(18559): loss=13.155236616257621\n",
      "Stochastic Gradient Descent(18560): loss=0.11694520520052892\n",
      "Stochastic Gradient Descent(18561): loss=2.0274304512144545\n",
      "Stochastic Gradient Descent(18562): loss=0.6892843640581098\n",
      "Stochastic Gradient Descent(18563): loss=17.78266843182606\n",
      "Stochastic Gradient Descent(18564): loss=35.92467305040731\n",
      "Stochastic Gradient Descent(18565): loss=4.021082122408074\n",
      "Stochastic Gradient Descent(18566): loss=2.333597545198977\n",
      "Stochastic Gradient Descent(18567): loss=7.975626358805846\n",
      "Stochastic Gradient Descent(18568): loss=1.2817911267174944\n",
      "Stochastic Gradient Descent(18569): loss=0.34942890600627885\n",
      "Stochastic Gradient Descent(18570): loss=0.10033497411221054\n",
      "Stochastic Gradient Descent(18571): loss=0.9902732653338325\n",
      "Stochastic Gradient Descent(18572): loss=13.096551962023817\n",
      "Stochastic Gradient Descent(18573): loss=4.274258576845504\n",
      "Stochastic Gradient Descent(18574): loss=5.940047448499665\n",
      "Stochastic Gradient Descent(18575): loss=15.490699364931245\n",
      "Stochastic Gradient Descent(18576): loss=2.934206775451577\n",
      "Stochastic Gradient Descent(18577): loss=1.504624628070274\n",
      "Stochastic Gradient Descent(18578): loss=0.2768397241460383\n",
      "Stochastic Gradient Descent(18579): loss=11.50512595255232\n",
      "Stochastic Gradient Descent(18580): loss=0.10479609183926698\n",
      "Stochastic Gradient Descent(18581): loss=1.3203139018696959\n",
      "Stochastic Gradient Descent(18582): loss=1.7718173480620292\n",
      "Stochastic Gradient Descent(18583): loss=2.151551543901419\n",
      "Stochastic Gradient Descent(18584): loss=0.03833528188786948\n",
      "Stochastic Gradient Descent(18585): loss=3.3881054874519263\n",
      "Stochastic Gradient Descent(18586): loss=1.9152611948862424\n",
      "Stochastic Gradient Descent(18587): loss=1.0844228514566714\n",
      "Stochastic Gradient Descent(18588): loss=1.467925970573935\n",
      "Stochastic Gradient Descent(18589): loss=0.0732223710469099\n",
      "Stochastic Gradient Descent(18590): loss=4.050321967916563\n",
      "Stochastic Gradient Descent(18591): loss=8.28002760067958\n",
      "Stochastic Gradient Descent(18592): loss=25.320616414363002\n",
      "Stochastic Gradient Descent(18593): loss=0.003816319252854481\n",
      "Stochastic Gradient Descent(18594): loss=0.32536830402873257\n",
      "Stochastic Gradient Descent(18595): loss=2.5745180329753796\n",
      "Stochastic Gradient Descent(18596): loss=0.007457437481509063\n",
      "Stochastic Gradient Descent(18597): loss=26.691926451459345\n",
      "Stochastic Gradient Descent(18598): loss=0.012670439851654385\n",
      "Stochastic Gradient Descent(18599): loss=1.4788277840828392\n",
      "Stochastic Gradient Descent(18600): loss=0.04473359619866344\n",
      "Stochastic Gradient Descent(18601): loss=1.0470393489588874\n",
      "Stochastic Gradient Descent(18602): loss=4.4096355625441\n",
      "Stochastic Gradient Descent(18603): loss=0.21273885699965164\n",
      "Stochastic Gradient Descent(18604): loss=0.15550851578292163\n",
      "Stochastic Gradient Descent(18605): loss=0.0022526065407959986\n",
      "Stochastic Gradient Descent(18606): loss=0.7120780032257175\n",
      "Stochastic Gradient Descent(18607): loss=0.5574854221653316\n",
      "Stochastic Gradient Descent(18608): loss=3.716101053689223\n",
      "Stochastic Gradient Descent(18609): loss=0.37547401868345764\n",
      "Stochastic Gradient Descent(18610): loss=2.947006172361684\n",
      "Stochastic Gradient Descent(18611): loss=6.432870329547848\n",
      "Stochastic Gradient Descent(18612): loss=0.12373978122871782\n",
      "Stochastic Gradient Descent(18613): loss=0.1961331546644941\n",
      "Stochastic Gradient Descent(18614): loss=2.3621642373861054\n",
      "Stochastic Gradient Descent(18615): loss=1.5353803807943462\n",
      "Stochastic Gradient Descent(18616): loss=9.671349135258573\n",
      "Stochastic Gradient Descent(18617): loss=2.336771144607524\n",
      "Stochastic Gradient Descent(18618): loss=4.765540501452171\n",
      "Stochastic Gradient Descent(18619): loss=0.22751485173457106\n",
      "Stochastic Gradient Descent(18620): loss=5.443955299885631\n",
      "Stochastic Gradient Descent(18621): loss=0.7800290180065171\n",
      "Stochastic Gradient Descent(18622): loss=3.1291063521842144\n",
      "Stochastic Gradient Descent(18623): loss=0.28298765151830774\n",
      "Stochastic Gradient Descent(18624): loss=0.4049908041846369\n",
      "Stochastic Gradient Descent(18625): loss=0.2643110741431529\n",
      "Stochastic Gradient Descent(18626): loss=0.3960528681552392\n",
      "Stochastic Gradient Descent(18627): loss=0.3959472130473721\n",
      "Stochastic Gradient Descent(18628): loss=5.531862818804008\n",
      "Stochastic Gradient Descent(18629): loss=1.3782642885571397\n",
      "Stochastic Gradient Descent(18630): loss=11.536046342211446\n",
      "Stochastic Gradient Descent(18631): loss=2.5706974280894546\n",
      "Stochastic Gradient Descent(18632): loss=4.759586180879414\n",
      "Stochastic Gradient Descent(18633): loss=5.409991468212679\n",
      "Stochastic Gradient Descent(18634): loss=9.24037122659263\n",
      "Stochastic Gradient Descent(18635): loss=1.0166666610270314e-05\n",
      "Stochastic Gradient Descent(18636): loss=9.472682528773941\n",
      "Stochastic Gradient Descent(18637): loss=0.01734253793946502\n",
      "Stochastic Gradient Descent(18638): loss=25.243770053349433\n",
      "Stochastic Gradient Descent(18639): loss=55.60718834039791\n",
      "Stochastic Gradient Descent(18640): loss=0.7330288665628565\n",
      "Stochastic Gradient Descent(18641): loss=4.052370829480648\n",
      "Stochastic Gradient Descent(18642): loss=0.03097665307281671\n",
      "Stochastic Gradient Descent(18643): loss=5.684550953189041\n",
      "Stochastic Gradient Descent(18644): loss=9.382986136270723\n",
      "Stochastic Gradient Descent(18645): loss=1.4331740800176431\n",
      "Stochastic Gradient Descent(18646): loss=3.9380152973781755\n",
      "Stochastic Gradient Descent(18647): loss=0.3049958515130383\n",
      "Stochastic Gradient Descent(18648): loss=2.2741038064190455\n",
      "Stochastic Gradient Descent(18649): loss=0.17406548765987395\n",
      "Stochastic Gradient Descent(18650): loss=2.964779301733345\n",
      "Stochastic Gradient Descent(18651): loss=9.687866969562009\n",
      "Stochastic Gradient Descent(18652): loss=0.23609532087776167\n",
      "Stochastic Gradient Descent(18653): loss=16.800946407204208\n",
      "Stochastic Gradient Descent(18654): loss=1.4195082145218705\n",
      "Stochastic Gradient Descent(18655): loss=0.5051356104624992\n",
      "Stochastic Gradient Descent(18656): loss=5.612245059034426\n",
      "Stochastic Gradient Descent(18657): loss=0.014457110131710735\n",
      "Stochastic Gradient Descent(18658): loss=0.9595776784769366\n",
      "Stochastic Gradient Descent(18659): loss=1.2711628409114912\n",
      "Stochastic Gradient Descent(18660): loss=3.6338373859786057\n",
      "Stochastic Gradient Descent(18661): loss=3.802576822468493\n",
      "Stochastic Gradient Descent(18662): loss=3.2334057873408115\n",
      "Stochastic Gradient Descent(18663): loss=5.409167779899656\n",
      "Stochastic Gradient Descent(18664): loss=2.5936107311134107\n",
      "Stochastic Gradient Descent(18665): loss=6.889475628023593\n",
      "Stochastic Gradient Descent(18666): loss=1.9284098523273396\n",
      "Stochastic Gradient Descent(18667): loss=0.49609941216131126\n",
      "Stochastic Gradient Descent(18668): loss=20.104188568834687\n",
      "Stochastic Gradient Descent(18669): loss=2.7110346465088306\n",
      "Stochastic Gradient Descent(18670): loss=1.3531141686499535\n",
      "Stochastic Gradient Descent(18671): loss=14.300262630709524\n",
      "Stochastic Gradient Descent(18672): loss=0.58749140210778\n",
      "Stochastic Gradient Descent(18673): loss=0.534073651445116\n",
      "Stochastic Gradient Descent(18674): loss=9.461667362943619\n",
      "Stochastic Gradient Descent(18675): loss=4.903374658388074\n",
      "Stochastic Gradient Descent(18676): loss=0.6698804874084978\n",
      "Stochastic Gradient Descent(18677): loss=0.4720231678286443\n",
      "Stochastic Gradient Descent(18678): loss=13.963296007348927\n",
      "Stochastic Gradient Descent(18679): loss=2.3588955711208786\n",
      "Stochastic Gradient Descent(18680): loss=5.065930567698493\n",
      "Stochastic Gradient Descent(18681): loss=1.8713418328318263\n",
      "Stochastic Gradient Descent(18682): loss=1.768336919181764\n",
      "Stochastic Gradient Descent(18683): loss=0.7849250811355258\n",
      "Stochastic Gradient Descent(18684): loss=0.33604453872534606\n",
      "Stochastic Gradient Descent(18685): loss=0.15303455718507156\n",
      "Stochastic Gradient Descent(18686): loss=9.021149369149237\n",
      "Stochastic Gradient Descent(18687): loss=0.1943200447236604\n",
      "Stochastic Gradient Descent(18688): loss=3.358396090950479\n",
      "Stochastic Gradient Descent(18689): loss=0.6656815603405813\n",
      "Stochastic Gradient Descent(18690): loss=5.633992240030579\n",
      "Stochastic Gradient Descent(18691): loss=0.7121146989195047\n",
      "Stochastic Gradient Descent(18692): loss=5.955613029758831\n",
      "Stochastic Gradient Descent(18693): loss=0.9449760697245218\n",
      "Stochastic Gradient Descent(18694): loss=0.55892550508213\n",
      "Stochastic Gradient Descent(18695): loss=0.033953882118181025\n",
      "Stochastic Gradient Descent(18696): loss=9.645245704484896\n",
      "Stochastic Gradient Descent(18697): loss=3.1798512047621528\n",
      "Stochastic Gradient Descent(18698): loss=0.012466031863345052\n",
      "Stochastic Gradient Descent(18699): loss=0.2331765596967478\n",
      "Stochastic Gradient Descent(18700): loss=9.730895223342495\n",
      "Stochastic Gradient Descent(18701): loss=6.504612205150374\n",
      "Stochastic Gradient Descent(18702): loss=24.078621861122166\n",
      "Stochastic Gradient Descent(18703): loss=4.283192387295821\n",
      "Stochastic Gradient Descent(18704): loss=1.0536210455270156\n",
      "Stochastic Gradient Descent(18705): loss=1.1951500197877782\n",
      "Stochastic Gradient Descent(18706): loss=0.00040282231452414154\n",
      "Stochastic Gradient Descent(18707): loss=1.0853429835044162\n",
      "Stochastic Gradient Descent(18708): loss=0.1273769117369483\n",
      "Stochastic Gradient Descent(18709): loss=3.53441746751367\n",
      "Stochastic Gradient Descent(18710): loss=0.024041288463530996\n",
      "Stochastic Gradient Descent(18711): loss=18.509483256467142\n",
      "Stochastic Gradient Descent(18712): loss=0.2583703033969386\n",
      "Stochastic Gradient Descent(18713): loss=0.2279903050756393\n",
      "Stochastic Gradient Descent(18714): loss=10.092268417574253\n",
      "Stochastic Gradient Descent(18715): loss=19.314836369051616\n",
      "Stochastic Gradient Descent(18716): loss=6.99124731402023\n",
      "Stochastic Gradient Descent(18717): loss=2.275892576356734\n",
      "Stochastic Gradient Descent(18718): loss=1.0188419810953944\n",
      "Stochastic Gradient Descent(18719): loss=7.406655032918503\n",
      "Stochastic Gradient Descent(18720): loss=0.09667174957372557\n",
      "Stochastic Gradient Descent(18721): loss=1.5403749217149059\n",
      "Stochastic Gradient Descent(18722): loss=0.6171327279423897\n",
      "Stochastic Gradient Descent(18723): loss=1.2912845332290712\n",
      "Stochastic Gradient Descent(18724): loss=1.3899897157029035\n",
      "Stochastic Gradient Descent(18725): loss=0.004304872570492292\n",
      "Stochastic Gradient Descent(18726): loss=0.38225394626624204\n",
      "Stochastic Gradient Descent(18727): loss=33.87019979951604\n",
      "Stochastic Gradient Descent(18728): loss=0.0493250831453776\n",
      "Stochastic Gradient Descent(18729): loss=0.1548935149883592\n",
      "Stochastic Gradient Descent(18730): loss=2.639766816210095\n",
      "Stochastic Gradient Descent(18731): loss=2.1653520775036412\n",
      "Stochastic Gradient Descent(18732): loss=0.5368205184841124\n",
      "Stochastic Gradient Descent(18733): loss=0.8254276837160223\n",
      "Stochastic Gradient Descent(18734): loss=0.7293978548995537\n",
      "Stochastic Gradient Descent(18735): loss=3.2114599380066093\n",
      "Stochastic Gradient Descent(18736): loss=2.3683342391105473\n",
      "Stochastic Gradient Descent(18737): loss=14.890192879680885\n",
      "Stochastic Gradient Descent(18738): loss=5.473711243018453\n",
      "Stochastic Gradient Descent(18739): loss=0.10801506850785189\n",
      "Stochastic Gradient Descent(18740): loss=2.057471872765443\n",
      "Stochastic Gradient Descent(18741): loss=7.7910393908475\n",
      "Stochastic Gradient Descent(18742): loss=22.23421658921483\n",
      "Stochastic Gradient Descent(18743): loss=16.986733621369293\n",
      "Stochastic Gradient Descent(18744): loss=7.589376022983265\n",
      "Stochastic Gradient Descent(18745): loss=1.1491272855660033\n",
      "Stochastic Gradient Descent(18746): loss=8.375806528899437\n",
      "Stochastic Gradient Descent(18747): loss=6.689771454926163\n",
      "Stochastic Gradient Descent(18748): loss=4.2863561156966625\n",
      "Stochastic Gradient Descent(18749): loss=0.003168539830694681\n",
      "Stochastic Gradient Descent(18750): loss=9.52018460801522\n",
      "Stochastic Gradient Descent(18751): loss=3.8547911599336326\n",
      "Stochastic Gradient Descent(18752): loss=5.47301226859847\n",
      "Stochastic Gradient Descent(18753): loss=4.336210189116986\n",
      "Stochastic Gradient Descent(18754): loss=1.41228051538828\n",
      "Stochastic Gradient Descent(18755): loss=4.504526447621665\n",
      "Stochastic Gradient Descent(18756): loss=10.249684223567998\n",
      "Stochastic Gradient Descent(18757): loss=0.351282460932492\n",
      "Stochastic Gradient Descent(18758): loss=0.07322553244119145\n",
      "Stochastic Gradient Descent(18759): loss=0.7909806745943954\n",
      "Stochastic Gradient Descent(18760): loss=3.5107811098469903\n",
      "Stochastic Gradient Descent(18761): loss=0.5199660611443062\n",
      "Stochastic Gradient Descent(18762): loss=7.861493658160737\n",
      "Stochastic Gradient Descent(18763): loss=2.9324972265691005\n",
      "Stochastic Gradient Descent(18764): loss=7.897377458197109\n",
      "Stochastic Gradient Descent(18765): loss=0.9634017697673141\n",
      "Stochastic Gradient Descent(18766): loss=8.456980961453342\n",
      "Stochastic Gradient Descent(18767): loss=2.3778248102160013\n",
      "Stochastic Gradient Descent(18768): loss=0.48981559261871954\n",
      "Stochastic Gradient Descent(18769): loss=1.8969704966152263\n",
      "Stochastic Gradient Descent(18770): loss=13.05695266834016\n",
      "Stochastic Gradient Descent(18771): loss=6.041871393876198\n",
      "Stochastic Gradient Descent(18772): loss=0.3615610623081732\n",
      "Stochastic Gradient Descent(18773): loss=2.3620534995397806\n",
      "Stochastic Gradient Descent(18774): loss=4.523745829995022\n",
      "Stochastic Gradient Descent(18775): loss=0.027230099928734798\n",
      "Stochastic Gradient Descent(18776): loss=3.476345996724787\n",
      "Stochastic Gradient Descent(18777): loss=0.05174391349214389\n",
      "Stochastic Gradient Descent(18778): loss=10.223623381221321\n",
      "Stochastic Gradient Descent(18779): loss=3.437643891905987\n",
      "Stochastic Gradient Descent(18780): loss=0.7393718351336748\n",
      "Stochastic Gradient Descent(18781): loss=0.14130053528125397\n",
      "Stochastic Gradient Descent(18782): loss=2.8384216045975728\n",
      "Stochastic Gradient Descent(18783): loss=0.19691935455261822\n",
      "Stochastic Gradient Descent(18784): loss=18.4798832934478\n",
      "Stochastic Gradient Descent(18785): loss=4.462943847593658\n",
      "Stochastic Gradient Descent(18786): loss=0.03563179175631562\n",
      "Stochastic Gradient Descent(18787): loss=0.4578762813274259\n",
      "Stochastic Gradient Descent(18788): loss=1.163039085055529\n",
      "Stochastic Gradient Descent(18789): loss=12.4443316864994\n",
      "Stochastic Gradient Descent(18790): loss=0.035317870277124665\n",
      "Stochastic Gradient Descent(18791): loss=0.2332197954482005\n",
      "Stochastic Gradient Descent(18792): loss=4.90732728737729\n",
      "Stochastic Gradient Descent(18793): loss=0.8337135283819019\n",
      "Stochastic Gradient Descent(18794): loss=0.6551020885761035\n",
      "Stochastic Gradient Descent(18795): loss=0.301111447243576\n",
      "Stochastic Gradient Descent(18796): loss=0.3361853884425625\n",
      "Stochastic Gradient Descent(18797): loss=1.9799894130597533\n",
      "Stochastic Gradient Descent(18798): loss=10.054837966084007\n",
      "Stochastic Gradient Descent(18799): loss=12.046434949933793\n",
      "Stochastic Gradient Descent(18800): loss=4.721066662632245\n",
      "Stochastic Gradient Descent(18801): loss=0.16153679965025053\n",
      "Stochastic Gradient Descent(18802): loss=0.006870208921969358\n",
      "Stochastic Gradient Descent(18803): loss=6.004989000387364\n",
      "Stochastic Gradient Descent(18804): loss=0.4013300430764893\n",
      "Stochastic Gradient Descent(18805): loss=0.7797118719248117\n",
      "Stochastic Gradient Descent(18806): loss=2.6815251266534457\n",
      "Stochastic Gradient Descent(18807): loss=4.277792838131656\n",
      "Stochastic Gradient Descent(18808): loss=0.15340973335817112\n",
      "Stochastic Gradient Descent(18809): loss=6.750148796351277\n",
      "Stochastic Gradient Descent(18810): loss=5.6411909704077745\n",
      "Stochastic Gradient Descent(18811): loss=14.002116711127453\n",
      "Stochastic Gradient Descent(18812): loss=4.889410863553313\n",
      "Stochastic Gradient Descent(18813): loss=0.005933353676722016\n",
      "Stochastic Gradient Descent(18814): loss=0.2103671581772996\n",
      "Stochastic Gradient Descent(18815): loss=8.053766519294573\n",
      "Stochastic Gradient Descent(18816): loss=8.05665021240869\n",
      "Stochastic Gradient Descent(18817): loss=1.494083000388343\n",
      "Stochastic Gradient Descent(18818): loss=0.018038975468812186\n",
      "Stochastic Gradient Descent(18819): loss=17.077260961247177\n",
      "Stochastic Gradient Descent(18820): loss=12.978482522783114\n",
      "Stochastic Gradient Descent(18821): loss=4.6667579624697835\n",
      "Stochastic Gradient Descent(18822): loss=3.4529600690595372\n",
      "Stochastic Gradient Descent(18823): loss=5.216927261597079\n",
      "Stochastic Gradient Descent(18824): loss=2.391799460606052\n",
      "Stochastic Gradient Descent(18825): loss=4.730690001397545\n",
      "Stochastic Gradient Descent(18826): loss=4.785370083454029\n",
      "Stochastic Gradient Descent(18827): loss=5.030212852799243\n",
      "Stochastic Gradient Descent(18828): loss=0.7544403487214776\n",
      "Stochastic Gradient Descent(18829): loss=0.00017926235078017966\n",
      "Stochastic Gradient Descent(18830): loss=0.5126055757384234\n",
      "Stochastic Gradient Descent(18831): loss=1.2607700696865816\n",
      "Stochastic Gradient Descent(18832): loss=16.112113340932368\n",
      "Stochastic Gradient Descent(18833): loss=0.7562068270361387\n",
      "Stochastic Gradient Descent(18834): loss=15.189059239618468\n",
      "Stochastic Gradient Descent(18835): loss=3.2760858510976076\n",
      "Stochastic Gradient Descent(18836): loss=12.57464543227818\n",
      "Stochastic Gradient Descent(18837): loss=20.30409879448363\n",
      "Stochastic Gradient Descent(18838): loss=0.6014026697507456\n",
      "Stochastic Gradient Descent(18839): loss=0.04628735818425014\n",
      "Stochastic Gradient Descent(18840): loss=0.32892282672882156\n",
      "Stochastic Gradient Descent(18841): loss=2.3407816258320655\n",
      "Stochastic Gradient Descent(18842): loss=2.0505207048392053\n",
      "Stochastic Gradient Descent(18843): loss=29.97095283486135\n",
      "Stochastic Gradient Descent(18844): loss=19.597519594636765\n",
      "Stochastic Gradient Descent(18845): loss=0.9513537487230875\n",
      "Stochastic Gradient Descent(18846): loss=16.96101831414096\n",
      "Stochastic Gradient Descent(18847): loss=0.004855776832744375\n",
      "Stochastic Gradient Descent(18848): loss=1.653255183204361\n",
      "Stochastic Gradient Descent(18849): loss=2.1024846884619364\n",
      "Stochastic Gradient Descent(18850): loss=1.7023035653304643\n",
      "Stochastic Gradient Descent(18851): loss=0.10225866784640965\n",
      "Stochastic Gradient Descent(18852): loss=0.0008987889128207176\n",
      "Stochastic Gradient Descent(18853): loss=1.0530293151972099\n",
      "Stochastic Gradient Descent(18854): loss=0.3257113574072867\n",
      "Stochastic Gradient Descent(18855): loss=0.49973825823790424\n",
      "Stochastic Gradient Descent(18856): loss=0.7590003510165743\n",
      "Stochastic Gradient Descent(18857): loss=1.7883353676078508\n",
      "Stochastic Gradient Descent(18858): loss=5.1781311356657\n",
      "Stochastic Gradient Descent(18859): loss=1.440670327651556\n",
      "Stochastic Gradient Descent(18860): loss=0.363630858829944\n",
      "Stochastic Gradient Descent(18861): loss=0.2668267946933394\n",
      "Stochastic Gradient Descent(18862): loss=0.624523317571695\n",
      "Stochastic Gradient Descent(18863): loss=0.13361600135055626\n",
      "Stochastic Gradient Descent(18864): loss=7.839330742119479\n",
      "Stochastic Gradient Descent(18865): loss=5.073807358873579\n",
      "Stochastic Gradient Descent(18866): loss=0.5315976924743452\n",
      "Stochastic Gradient Descent(18867): loss=5.551070945939822\n",
      "Stochastic Gradient Descent(18868): loss=10.079503255523557\n",
      "Stochastic Gradient Descent(18869): loss=0.17454155655312806\n",
      "Stochastic Gradient Descent(18870): loss=3.185236483044814\n",
      "Stochastic Gradient Descent(18871): loss=5.5629573328094715\n",
      "Stochastic Gradient Descent(18872): loss=0.008016566849146704\n",
      "Stochastic Gradient Descent(18873): loss=0.6702645427971408\n",
      "Stochastic Gradient Descent(18874): loss=0.6096990059328881\n",
      "Stochastic Gradient Descent(18875): loss=5.209229601191172\n",
      "Stochastic Gradient Descent(18876): loss=8.422551076932796\n",
      "Stochastic Gradient Descent(18877): loss=0.010865840652295322\n",
      "Stochastic Gradient Descent(18878): loss=12.101849869995084\n",
      "Stochastic Gradient Descent(18879): loss=0.8015012449613979\n",
      "Stochastic Gradient Descent(18880): loss=0.2822038734463585\n",
      "Stochastic Gradient Descent(18881): loss=3.998060660997156\n",
      "Stochastic Gradient Descent(18882): loss=8.028601431444923\n",
      "Stochastic Gradient Descent(18883): loss=3.773553605971348e-05\n",
      "Stochastic Gradient Descent(18884): loss=1.6656111027230003\n",
      "Stochastic Gradient Descent(18885): loss=2.0273919179230275\n",
      "Stochastic Gradient Descent(18886): loss=1.4847592183144245\n",
      "Stochastic Gradient Descent(18887): loss=2.502170548396728\n",
      "Stochastic Gradient Descent(18888): loss=1.4443426585235806\n",
      "Stochastic Gradient Descent(18889): loss=8.713522016344267\n",
      "Stochastic Gradient Descent(18890): loss=0.026765551266723867\n",
      "Stochastic Gradient Descent(18891): loss=1.4632859744296705\n",
      "Stochastic Gradient Descent(18892): loss=0.5513140069847078\n",
      "Stochastic Gradient Descent(18893): loss=0.08076485657931207\n",
      "Stochastic Gradient Descent(18894): loss=0.9780721147760679\n",
      "Stochastic Gradient Descent(18895): loss=0.5978104663336146\n",
      "Stochastic Gradient Descent(18896): loss=0.7238718572482864\n",
      "Stochastic Gradient Descent(18897): loss=9.943010178696642\n",
      "Stochastic Gradient Descent(18898): loss=0.09576444930049943\n",
      "Stochastic Gradient Descent(18899): loss=1.1261404565975013\n",
      "Stochastic Gradient Descent(18900): loss=2.695083998000579\n",
      "Stochastic Gradient Descent(18901): loss=2.117153711450259\n",
      "Stochastic Gradient Descent(18902): loss=2.50457055286051\n",
      "Stochastic Gradient Descent(18903): loss=0.4160917829470522\n",
      "Stochastic Gradient Descent(18904): loss=18.32910405696698\n",
      "Stochastic Gradient Descent(18905): loss=0.5277442954732839\n",
      "Stochastic Gradient Descent(18906): loss=3.7217213104000963\n",
      "Stochastic Gradient Descent(18907): loss=2.2636233313550544\n",
      "Stochastic Gradient Descent(18908): loss=0.26255427939162\n",
      "Stochastic Gradient Descent(18909): loss=2.0165859783086537\n",
      "Stochastic Gradient Descent(18910): loss=0.9179578896924518\n",
      "Stochastic Gradient Descent(18911): loss=3.8572795537178917\n",
      "Stochastic Gradient Descent(18912): loss=1.8188343834660177\n",
      "Stochastic Gradient Descent(18913): loss=5.818474727192714\n",
      "Stochastic Gradient Descent(18914): loss=1.384558863763473\n",
      "Stochastic Gradient Descent(18915): loss=22.72128589866616\n",
      "Stochastic Gradient Descent(18916): loss=2.0944257528106975\n",
      "Stochastic Gradient Descent(18917): loss=0.412313197585091\n",
      "Stochastic Gradient Descent(18918): loss=0.009914950822651241\n",
      "Stochastic Gradient Descent(18919): loss=0.7274260280645694\n",
      "Stochastic Gradient Descent(18920): loss=3.521342737299515\n",
      "Stochastic Gradient Descent(18921): loss=2.464487655812535\n",
      "Stochastic Gradient Descent(18922): loss=2.95850644841934\n",
      "Stochastic Gradient Descent(18923): loss=4.06187106792641\n",
      "Stochastic Gradient Descent(18924): loss=6.298105138947838\n",
      "Stochastic Gradient Descent(18925): loss=2.626401924633358\n",
      "Stochastic Gradient Descent(18926): loss=0.16967029357597405\n",
      "Stochastic Gradient Descent(18927): loss=9.069356304394923\n",
      "Stochastic Gradient Descent(18928): loss=4.7767883736655445\n",
      "Stochastic Gradient Descent(18929): loss=0.001075048973697358\n",
      "Stochastic Gradient Descent(18930): loss=5.913448986547519\n",
      "Stochastic Gradient Descent(18931): loss=1.7269783721225305\n",
      "Stochastic Gradient Descent(18932): loss=2.2577985093606214\n",
      "Stochastic Gradient Descent(18933): loss=2.466395179031043\n",
      "Stochastic Gradient Descent(18934): loss=9.361051979903287\n",
      "Stochastic Gradient Descent(18935): loss=0.5619635735241338\n",
      "Stochastic Gradient Descent(18936): loss=1.251807543192643\n",
      "Stochastic Gradient Descent(18937): loss=0.1166609969480863\n",
      "Stochastic Gradient Descent(18938): loss=12.926709792089337\n",
      "Stochastic Gradient Descent(18939): loss=0.39686611878974976\n",
      "Stochastic Gradient Descent(18940): loss=0.4955861167560771\n",
      "Stochastic Gradient Descent(18941): loss=9.474949894220632\n",
      "Stochastic Gradient Descent(18942): loss=1.4670697226666463\n",
      "Stochastic Gradient Descent(18943): loss=0.06355064469473257\n",
      "Stochastic Gradient Descent(18944): loss=0.8956561747923717\n",
      "Stochastic Gradient Descent(18945): loss=0.3807677246334901\n",
      "Stochastic Gradient Descent(18946): loss=0.9471392912402755\n",
      "Stochastic Gradient Descent(18947): loss=14.189194581375624\n",
      "Stochastic Gradient Descent(18948): loss=0.6334366999099449\n",
      "Stochastic Gradient Descent(18949): loss=7.422844261111335\n",
      "Stochastic Gradient Descent(18950): loss=1.2922779409351064\n",
      "Stochastic Gradient Descent(18951): loss=0.8638829271780737\n",
      "Stochastic Gradient Descent(18952): loss=1.0037138804545034\n",
      "Stochastic Gradient Descent(18953): loss=0.018571267741791724\n",
      "Stochastic Gradient Descent(18954): loss=10.778414329703121\n",
      "Stochastic Gradient Descent(18955): loss=7.458919960175421\n",
      "Stochastic Gradient Descent(18956): loss=7.358857638723738\n",
      "Stochastic Gradient Descent(18957): loss=19.908767793065117\n",
      "Stochastic Gradient Descent(18958): loss=1.7492326244197371\n",
      "Stochastic Gradient Descent(18959): loss=3.166697946638739\n",
      "Stochastic Gradient Descent(18960): loss=1.9364194069283598\n",
      "Stochastic Gradient Descent(18961): loss=0.03934561191139119\n",
      "Stochastic Gradient Descent(18962): loss=1.1101321758539347e-05\n",
      "Stochastic Gradient Descent(18963): loss=5.6154588939015015\n",
      "Stochastic Gradient Descent(18964): loss=5.355929434042289\n",
      "Stochastic Gradient Descent(18965): loss=0.49877727433956687\n",
      "Stochastic Gradient Descent(18966): loss=13.037108574443993\n",
      "Stochastic Gradient Descent(18967): loss=1.9371169059441211\n",
      "Stochastic Gradient Descent(18968): loss=10.908999622118426\n",
      "Stochastic Gradient Descent(18969): loss=0.2835704630458717\n",
      "Stochastic Gradient Descent(18970): loss=0.03214255369305439\n",
      "Stochastic Gradient Descent(18971): loss=6.403685960583206\n",
      "Stochastic Gradient Descent(18972): loss=0.7570272157716778\n",
      "Stochastic Gradient Descent(18973): loss=2.2887772895585585\n",
      "Stochastic Gradient Descent(18974): loss=8.658751187564212\n",
      "Stochastic Gradient Descent(18975): loss=1.2875703800436542\n",
      "Stochastic Gradient Descent(18976): loss=3.5319853246319095\n",
      "Stochastic Gradient Descent(18977): loss=0.00046404988051118064\n",
      "Stochastic Gradient Descent(18978): loss=0.8444324367764403\n",
      "Stochastic Gradient Descent(18979): loss=0.05008773432659781\n",
      "Stochastic Gradient Descent(18980): loss=0.1875192460851943\n",
      "Stochastic Gradient Descent(18981): loss=0.865782606455011\n",
      "Stochastic Gradient Descent(18982): loss=8.013617081536454\n",
      "Stochastic Gradient Descent(18983): loss=36.10967579117718\n",
      "Stochastic Gradient Descent(18984): loss=0.3130853533816295\n",
      "Stochastic Gradient Descent(18985): loss=0.8782582246029954\n",
      "Stochastic Gradient Descent(18986): loss=0.29577696707988504\n",
      "Stochastic Gradient Descent(18987): loss=9.960394538679596\n",
      "Stochastic Gradient Descent(18988): loss=0.5570180897349541\n",
      "Stochastic Gradient Descent(18989): loss=0.40022339023685344\n",
      "Stochastic Gradient Descent(18990): loss=0.1009598472389126\n",
      "Stochastic Gradient Descent(18991): loss=0.09112579353001644\n",
      "Stochastic Gradient Descent(18992): loss=0.019814929644463786\n",
      "Stochastic Gradient Descent(18993): loss=25.02577078337743\n",
      "Stochastic Gradient Descent(18994): loss=1.6782397761636272\n",
      "Stochastic Gradient Descent(18995): loss=4.29028097426953\n",
      "Stochastic Gradient Descent(18996): loss=1.7515491945129709\n",
      "Stochastic Gradient Descent(18997): loss=0.42848368467189496\n",
      "Stochastic Gradient Descent(18998): loss=1.146754710624093\n",
      "Stochastic Gradient Descent(18999): loss=2.317135588452195\n",
      "Stochastic Gradient Descent(19000): loss=0.6273827527621054\n",
      "Stochastic Gradient Descent(19001): loss=11.544466155727367\n",
      "Stochastic Gradient Descent(19002): loss=17.341368158506548\n",
      "Stochastic Gradient Descent(19003): loss=0.03335980126379925\n",
      "Stochastic Gradient Descent(19004): loss=1.1370426326000624\n",
      "Stochastic Gradient Descent(19005): loss=0.7196564674603951\n",
      "Stochastic Gradient Descent(19006): loss=1.6870445713991429\n",
      "Stochastic Gradient Descent(19007): loss=2.230558476420339\n",
      "Stochastic Gradient Descent(19008): loss=0.16357527161499588\n",
      "Stochastic Gradient Descent(19009): loss=2.930378939181864\n",
      "Stochastic Gradient Descent(19010): loss=0.23480022698204345\n",
      "Stochastic Gradient Descent(19011): loss=0.13111798179136505\n",
      "Stochastic Gradient Descent(19012): loss=54.13522577646186\n",
      "Stochastic Gradient Descent(19013): loss=19.343541371902507\n",
      "Stochastic Gradient Descent(19014): loss=4.465566329699667\n",
      "Stochastic Gradient Descent(19015): loss=0.0023514546810265544\n",
      "Stochastic Gradient Descent(19016): loss=8.73467274056438\n",
      "Stochastic Gradient Descent(19017): loss=1.8601571059380118\n",
      "Stochastic Gradient Descent(19018): loss=2.736893513308004\n",
      "Stochastic Gradient Descent(19019): loss=6.910633065680681\n",
      "Stochastic Gradient Descent(19020): loss=7.55056531842748\n",
      "Stochastic Gradient Descent(19021): loss=14.994044463990999\n",
      "Stochastic Gradient Descent(19022): loss=0.7167711473458378\n",
      "Stochastic Gradient Descent(19023): loss=1.0577079781383143\n",
      "Stochastic Gradient Descent(19024): loss=3.860263412052049\n",
      "Stochastic Gradient Descent(19025): loss=0.30043586454009735\n",
      "Stochastic Gradient Descent(19026): loss=0.8217202388623965\n",
      "Stochastic Gradient Descent(19027): loss=0.7814215197745215\n",
      "Stochastic Gradient Descent(19028): loss=15.25071819684672\n",
      "Stochastic Gradient Descent(19029): loss=4.340220080007612\n",
      "Stochastic Gradient Descent(19030): loss=0.7463522074697214\n",
      "Stochastic Gradient Descent(19031): loss=3.032618263323998\n",
      "Stochastic Gradient Descent(19032): loss=1.7332679898429093\n",
      "Stochastic Gradient Descent(19033): loss=0.8811292962919925\n",
      "Stochastic Gradient Descent(19034): loss=27.106263337394903\n",
      "Stochastic Gradient Descent(19035): loss=0.06534312701111031\n",
      "Stochastic Gradient Descent(19036): loss=5.538365086461628\n",
      "Stochastic Gradient Descent(19037): loss=2.6616825800112958\n",
      "Stochastic Gradient Descent(19038): loss=0.048273195530017604\n",
      "Stochastic Gradient Descent(19039): loss=4.970434627366884\n",
      "Stochastic Gradient Descent(19040): loss=2.2808277562782995\n",
      "Stochastic Gradient Descent(19041): loss=0.6102422587369947\n",
      "Stochastic Gradient Descent(19042): loss=0.014952870429237613\n",
      "Stochastic Gradient Descent(19043): loss=3.291114940958075\n",
      "Stochastic Gradient Descent(19044): loss=0.6113001700178387\n",
      "Stochastic Gradient Descent(19045): loss=12.162979991092168\n",
      "Stochastic Gradient Descent(19046): loss=9.3213430498702\n",
      "Stochastic Gradient Descent(19047): loss=2.7510794401795726\n",
      "Stochastic Gradient Descent(19048): loss=2.9029259596028725\n",
      "Stochastic Gradient Descent(19049): loss=0.6293589459333119\n",
      "Stochastic Gradient Descent(19050): loss=2.1084901481307443\n",
      "Stochastic Gradient Descent(19051): loss=6.761517699006508\n",
      "Stochastic Gradient Descent(19052): loss=1.2711012081282589\n",
      "Stochastic Gradient Descent(19053): loss=0.3095772777055682\n",
      "Stochastic Gradient Descent(19054): loss=4.201884156859596\n",
      "Stochastic Gradient Descent(19055): loss=1.9166376547259416\n",
      "Stochastic Gradient Descent(19056): loss=6.595115045050363\n",
      "Stochastic Gradient Descent(19057): loss=10.854316080425903\n",
      "Stochastic Gradient Descent(19058): loss=0.503519439264305\n",
      "Stochastic Gradient Descent(19059): loss=4.527318397515158\n",
      "Stochastic Gradient Descent(19060): loss=10.82539126175014\n",
      "Stochastic Gradient Descent(19061): loss=0.17151237978946596\n",
      "Stochastic Gradient Descent(19062): loss=35.4277680769872\n",
      "Stochastic Gradient Descent(19063): loss=15.577019068223281\n",
      "Stochastic Gradient Descent(19064): loss=3.2242607080316024\n",
      "Stochastic Gradient Descent(19065): loss=32.91066792159265\n",
      "Stochastic Gradient Descent(19066): loss=2.8590216021388946\n",
      "Stochastic Gradient Descent(19067): loss=24.671361233310773\n",
      "Stochastic Gradient Descent(19068): loss=10.054819004845827\n",
      "Stochastic Gradient Descent(19069): loss=5.13290756553571\n",
      "Stochastic Gradient Descent(19070): loss=0.040726022655796466\n",
      "Stochastic Gradient Descent(19071): loss=4.795026851189422\n",
      "Stochastic Gradient Descent(19072): loss=0.1428008044598626\n",
      "Stochastic Gradient Descent(19073): loss=0.9374537747785927\n",
      "Stochastic Gradient Descent(19074): loss=2.4261624554268915\n",
      "Stochastic Gradient Descent(19075): loss=2.1808279164544477\n",
      "Stochastic Gradient Descent(19076): loss=0.017916797968736933\n",
      "Stochastic Gradient Descent(19077): loss=2.3871812639854157\n",
      "Stochastic Gradient Descent(19078): loss=3.790872537179819\n",
      "Stochastic Gradient Descent(19079): loss=4.444584499314663\n",
      "Stochastic Gradient Descent(19080): loss=0.5190325859846439\n",
      "Stochastic Gradient Descent(19081): loss=12.410132473890009\n",
      "Stochastic Gradient Descent(19082): loss=0.1050084551728823\n",
      "Stochastic Gradient Descent(19083): loss=0.34177261265643555\n",
      "Stochastic Gradient Descent(19084): loss=8.892623568780161\n",
      "Stochastic Gradient Descent(19085): loss=15.25481649901393\n",
      "Stochastic Gradient Descent(19086): loss=0.3978637485703418\n",
      "Stochastic Gradient Descent(19087): loss=0.04884304483432935\n",
      "Stochastic Gradient Descent(19088): loss=6.867088491204942\n",
      "Stochastic Gradient Descent(19089): loss=4.338893763339788\n",
      "Stochastic Gradient Descent(19090): loss=5.7901791437864745\n",
      "Stochastic Gradient Descent(19091): loss=1.3342387672364406\n",
      "Stochastic Gradient Descent(19092): loss=3.5664829988033118\n",
      "Stochastic Gradient Descent(19093): loss=0.07370607279820536\n",
      "Stochastic Gradient Descent(19094): loss=14.418234495145477\n",
      "Stochastic Gradient Descent(19095): loss=0.9905051903660639\n",
      "Stochastic Gradient Descent(19096): loss=2.533617459526629\n",
      "Stochastic Gradient Descent(19097): loss=0.0003524977565384713\n",
      "Stochastic Gradient Descent(19098): loss=3.1181037659956963\n",
      "Stochastic Gradient Descent(19099): loss=0.8014105078829027\n",
      "Stochastic Gradient Descent(19100): loss=3.01891180573137\n",
      "Stochastic Gradient Descent(19101): loss=4.743985915041891\n",
      "Stochastic Gradient Descent(19102): loss=0.40327412762539033\n",
      "Stochastic Gradient Descent(19103): loss=0.9364049853028491\n",
      "Stochastic Gradient Descent(19104): loss=8.97356148113216\n",
      "Stochastic Gradient Descent(19105): loss=0.048151643450813374\n",
      "Stochastic Gradient Descent(19106): loss=1.996356799943661\n",
      "Stochastic Gradient Descent(19107): loss=2.012411768625674\n",
      "Stochastic Gradient Descent(19108): loss=0.6470423826904603\n",
      "Stochastic Gradient Descent(19109): loss=2.049461180455665\n",
      "Stochastic Gradient Descent(19110): loss=3.3509379437428835\n",
      "Stochastic Gradient Descent(19111): loss=0.4731001891871408\n",
      "Stochastic Gradient Descent(19112): loss=2.596967207122631\n",
      "Stochastic Gradient Descent(19113): loss=0.042707159939556476\n",
      "Stochastic Gradient Descent(19114): loss=0.7696649261319674\n",
      "Stochastic Gradient Descent(19115): loss=0.9324943551971833\n",
      "Stochastic Gradient Descent(19116): loss=4.048092517724787\n",
      "Stochastic Gradient Descent(19117): loss=0.4380876068017296\n",
      "Stochastic Gradient Descent(19118): loss=5.812188047822873\n",
      "Stochastic Gradient Descent(19119): loss=1.5556984948846615\n",
      "Stochastic Gradient Descent(19120): loss=31.837999591810377\n",
      "Stochastic Gradient Descent(19121): loss=4.9353430879443\n",
      "Stochastic Gradient Descent(19122): loss=12.500677144293057\n",
      "Stochastic Gradient Descent(19123): loss=4.843747794378001\n",
      "Stochastic Gradient Descent(19124): loss=20.6379608171645\n",
      "Stochastic Gradient Descent(19125): loss=0.2971077421132598\n",
      "Stochastic Gradient Descent(19126): loss=5.140814033942838\n",
      "Stochastic Gradient Descent(19127): loss=6.181666495484136\n",
      "Stochastic Gradient Descent(19128): loss=3.6966239357607793\n",
      "Stochastic Gradient Descent(19129): loss=0.06788990606797643\n",
      "Stochastic Gradient Descent(19130): loss=0.3996553158402621\n",
      "Stochastic Gradient Descent(19131): loss=10.159517639917759\n",
      "Stochastic Gradient Descent(19132): loss=0.021649925667943774\n",
      "Stochastic Gradient Descent(19133): loss=2.614993369480351\n",
      "Stochastic Gradient Descent(19134): loss=13.728649435919548\n",
      "Stochastic Gradient Descent(19135): loss=0.5282370286982105\n",
      "Stochastic Gradient Descent(19136): loss=0.13785870840167105\n",
      "Stochastic Gradient Descent(19137): loss=5.4454324488493\n",
      "Stochastic Gradient Descent(19138): loss=12.05223357932656\n",
      "Stochastic Gradient Descent(19139): loss=3.9188551719155393\n",
      "Stochastic Gradient Descent(19140): loss=0.20643948607616053\n",
      "Stochastic Gradient Descent(19141): loss=0.04283340048160152\n",
      "Stochastic Gradient Descent(19142): loss=0.0022161634268150765\n",
      "Stochastic Gradient Descent(19143): loss=0.0764649650017173\n",
      "Stochastic Gradient Descent(19144): loss=25.880186541674302\n",
      "Stochastic Gradient Descent(19145): loss=15.038505413626435\n",
      "Stochastic Gradient Descent(19146): loss=3.214347098840772\n",
      "Stochastic Gradient Descent(19147): loss=0.49343723693070024\n",
      "Stochastic Gradient Descent(19148): loss=5.528542902298066\n",
      "Stochastic Gradient Descent(19149): loss=1.0176409108218558\n",
      "Stochastic Gradient Descent(19150): loss=0.29825002165828\n",
      "Stochastic Gradient Descent(19151): loss=0.16724677523468967\n",
      "Stochastic Gradient Descent(19152): loss=1.187751870990706\n",
      "Stochastic Gradient Descent(19153): loss=6.21082154747743\n",
      "Stochastic Gradient Descent(19154): loss=2.1002358617402503\n",
      "Stochastic Gradient Descent(19155): loss=2.518341669115266\n",
      "Stochastic Gradient Descent(19156): loss=5.67288044931794\n",
      "Stochastic Gradient Descent(19157): loss=0.15854496036038276\n",
      "Stochastic Gradient Descent(19158): loss=8.286431035533603\n",
      "Stochastic Gradient Descent(19159): loss=0.009623265314610736\n",
      "Stochastic Gradient Descent(19160): loss=4.455678715885082\n",
      "Stochastic Gradient Descent(19161): loss=17.478835139391364\n",
      "Stochastic Gradient Descent(19162): loss=0.5313188859094278\n",
      "Stochastic Gradient Descent(19163): loss=1.1403823825110848\n",
      "Stochastic Gradient Descent(19164): loss=2.0847115366046665\n",
      "Stochastic Gradient Descent(19165): loss=4.896229070061134\n",
      "Stochastic Gradient Descent(19166): loss=3.2822823447432197\n",
      "Stochastic Gradient Descent(19167): loss=0.42923470943998737\n",
      "Stochastic Gradient Descent(19168): loss=0.0036754078874017765\n",
      "Stochastic Gradient Descent(19169): loss=0.15368481576548912\n",
      "Stochastic Gradient Descent(19170): loss=1.2073973714250432\n",
      "Stochastic Gradient Descent(19171): loss=11.658587528495474\n",
      "Stochastic Gradient Descent(19172): loss=3.042603046458673\n",
      "Stochastic Gradient Descent(19173): loss=21.556343157571153\n",
      "Stochastic Gradient Descent(19174): loss=0.43060433957550664\n",
      "Stochastic Gradient Descent(19175): loss=0.4211343468126216\n",
      "Stochastic Gradient Descent(19176): loss=16.329781417180666\n",
      "Stochastic Gradient Descent(19177): loss=0.3758971325253697\n",
      "Stochastic Gradient Descent(19178): loss=1.6705449532803498\n",
      "Stochastic Gradient Descent(19179): loss=1.6386044895239724\n",
      "Stochastic Gradient Descent(19180): loss=0.34193769498102666\n",
      "Stochastic Gradient Descent(19181): loss=7.485205087371635\n",
      "Stochastic Gradient Descent(19182): loss=2.9988652270946234\n",
      "Stochastic Gradient Descent(19183): loss=0.09251933354164309\n",
      "Stochastic Gradient Descent(19184): loss=0.29843959026359135\n",
      "Stochastic Gradient Descent(19185): loss=2.2780238522871303\n",
      "Stochastic Gradient Descent(19186): loss=8.650228347742722\n",
      "Stochastic Gradient Descent(19187): loss=6.0392837152480405\n",
      "Stochastic Gradient Descent(19188): loss=2.3041842404390933\n",
      "Stochastic Gradient Descent(19189): loss=0.14243313273012778\n",
      "Stochastic Gradient Descent(19190): loss=1.2150803035024293\n",
      "Stochastic Gradient Descent(19191): loss=1.0924471740285147\n",
      "Stochastic Gradient Descent(19192): loss=2.7260162157909438\n",
      "Stochastic Gradient Descent(19193): loss=0.8813773407629871\n",
      "Stochastic Gradient Descent(19194): loss=1.9285846861534641\n",
      "Stochastic Gradient Descent(19195): loss=0.15560232743491215\n",
      "Stochastic Gradient Descent(19196): loss=11.629851896235348\n",
      "Stochastic Gradient Descent(19197): loss=1.611265773189942\n",
      "Stochastic Gradient Descent(19198): loss=0.8362769588787311\n",
      "Stochastic Gradient Descent(19199): loss=0.2591989434572289\n",
      "Stochastic Gradient Descent(19200): loss=0.13593594098587397\n",
      "Stochastic Gradient Descent(19201): loss=7.341595469393021\n",
      "Stochastic Gradient Descent(19202): loss=17.213421639576964\n",
      "Stochastic Gradient Descent(19203): loss=17.853415855950285\n",
      "Stochastic Gradient Descent(19204): loss=13.972011364306475\n",
      "Stochastic Gradient Descent(19205): loss=9.412755563314462\n",
      "Stochastic Gradient Descent(19206): loss=0.9482685414906373\n",
      "Stochastic Gradient Descent(19207): loss=0.09553822790205951\n",
      "Stochastic Gradient Descent(19208): loss=1.4908145500971504\n",
      "Stochastic Gradient Descent(19209): loss=0.0059111681826853465\n",
      "Stochastic Gradient Descent(19210): loss=1.1312229928406021\n",
      "Stochastic Gradient Descent(19211): loss=10.93570644530095\n",
      "Stochastic Gradient Descent(19212): loss=0.279233814322858\n",
      "Stochastic Gradient Descent(19213): loss=2.422101523576004\n",
      "Stochastic Gradient Descent(19214): loss=0.08473524126720094\n",
      "Stochastic Gradient Descent(19215): loss=1.0667507267890668\n",
      "Stochastic Gradient Descent(19216): loss=0.44514313606258843\n",
      "Stochastic Gradient Descent(19217): loss=12.931180077234517\n",
      "Stochastic Gradient Descent(19218): loss=0.5298938914448822\n",
      "Stochastic Gradient Descent(19219): loss=0.2983110694514502\n",
      "Stochastic Gradient Descent(19220): loss=6.681966413124793\n",
      "Stochastic Gradient Descent(19221): loss=12.230726658175806\n",
      "Stochastic Gradient Descent(19222): loss=0.3325826517011555\n",
      "Stochastic Gradient Descent(19223): loss=0.37671284698153057\n",
      "Stochastic Gradient Descent(19224): loss=4.374060029301632\n",
      "Stochastic Gradient Descent(19225): loss=0.7720479157303676\n",
      "Stochastic Gradient Descent(19226): loss=30.05107984410728\n",
      "Stochastic Gradient Descent(19227): loss=96.6011258838633\n",
      "Stochastic Gradient Descent(19228): loss=49.85407185599306\n",
      "Stochastic Gradient Descent(19229): loss=56.41514490349725\n",
      "Stochastic Gradient Descent(19230): loss=16.059231936001243\n",
      "Stochastic Gradient Descent(19231): loss=4.292980417813575\n",
      "Stochastic Gradient Descent(19232): loss=171.07878055647856\n",
      "Stochastic Gradient Descent(19233): loss=3.674442014203744\n",
      "Stochastic Gradient Descent(19234): loss=0.23485413127316862\n",
      "Stochastic Gradient Descent(19235): loss=0.3535846165008321\n",
      "Stochastic Gradient Descent(19236): loss=7.715499447523198\n",
      "Stochastic Gradient Descent(19237): loss=5.753181829443563\n",
      "Stochastic Gradient Descent(19238): loss=21.63154584355056\n",
      "Stochastic Gradient Descent(19239): loss=1.3766540043646414\n",
      "Stochastic Gradient Descent(19240): loss=6.786170281461083\n",
      "Stochastic Gradient Descent(19241): loss=3.737564916484794\n",
      "Stochastic Gradient Descent(19242): loss=13.98392139393112\n",
      "Stochastic Gradient Descent(19243): loss=4.094221926360465\n",
      "Stochastic Gradient Descent(19244): loss=0.8371292911791411\n",
      "Stochastic Gradient Descent(19245): loss=0.47838486134932323\n",
      "Stochastic Gradient Descent(19246): loss=5.299181596580205\n",
      "Stochastic Gradient Descent(19247): loss=0.1807610619468967\n",
      "Stochastic Gradient Descent(19248): loss=1.3911538205086609\n",
      "Stochastic Gradient Descent(19249): loss=0.2514308338655178\n",
      "Stochastic Gradient Descent(19250): loss=0.08710930908447577\n",
      "Stochastic Gradient Descent(19251): loss=6.644590376810703\n",
      "Stochastic Gradient Descent(19252): loss=0.7830686677323264\n",
      "Stochastic Gradient Descent(19253): loss=0.3426678284718093\n",
      "Stochastic Gradient Descent(19254): loss=0.44512654533908086\n",
      "Stochastic Gradient Descent(19255): loss=47.34828040536808\n",
      "Stochastic Gradient Descent(19256): loss=5.270774182362872\n",
      "Stochastic Gradient Descent(19257): loss=14.27539720189819\n",
      "Stochastic Gradient Descent(19258): loss=2.93724621217717\n",
      "Stochastic Gradient Descent(19259): loss=6.268888178396156\n",
      "Stochastic Gradient Descent(19260): loss=11.949692393768032\n",
      "Stochastic Gradient Descent(19261): loss=2.3068218786980146\n",
      "Stochastic Gradient Descent(19262): loss=5.753669298515519\n",
      "Stochastic Gradient Descent(19263): loss=0.022139232810890638\n",
      "Stochastic Gradient Descent(19264): loss=4.716861577070837\n",
      "Stochastic Gradient Descent(19265): loss=3.45147429849882\n",
      "Stochastic Gradient Descent(19266): loss=0.4757109754647022\n",
      "Stochastic Gradient Descent(19267): loss=0.6211186600447656\n",
      "Stochastic Gradient Descent(19268): loss=3.679662157345218\n",
      "Stochastic Gradient Descent(19269): loss=0.36401698203537647\n",
      "Stochastic Gradient Descent(19270): loss=12.268136197933815\n",
      "Stochastic Gradient Descent(19271): loss=6.958889239993057\n",
      "Stochastic Gradient Descent(19272): loss=3.224791085791528\n",
      "Stochastic Gradient Descent(19273): loss=2.38717724763699\n",
      "Stochastic Gradient Descent(19274): loss=3.451680912603446\n",
      "Stochastic Gradient Descent(19275): loss=3.9909164799036536\n",
      "Stochastic Gradient Descent(19276): loss=20.873944036021385\n",
      "Stochastic Gradient Descent(19277): loss=0.07908100677817273\n",
      "Stochastic Gradient Descent(19278): loss=22.120233852325043\n",
      "Stochastic Gradient Descent(19279): loss=1.6744688290109142\n",
      "Stochastic Gradient Descent(19280): loss=6.868367464539406\n",
      "Stochastic Gradient Descent(19281): loss=5.496272546548849\n",
      "Stochastic Gradient Descent(19282): loss=5.9190056896992385\n",
      "Stochastic Gradient Descent(19283): loss=9.877090103969142\n",
      "Stochastic Gradient Descent(19284): loss=7.622580517961185\n",
      "Stochastic Gradient Descent(19285): loss=0.8747935544893389\n",
      "Stochastic Gradient Descent(19286): loss=3.174010083373057\n",
      "Stochastic Gradient Descent(19287): loss=0.5620377885566111\n",
      "Stochastic Gradient Descent(19288): loss=5.8274570957686995\n",
      "Stochastic Gradient Descent(19289): loss=2.348404289555134\n",
      "Stochastic Gradient Descent(19290): loss=6.329418889502281\n",
      "Stochastic Gradient Descent(19291): loss=4.4101118247172035\n",
      "Stochastic Gradient Descent(19292): loss=1.1132797686566867\n",
      "Stochastic Gradient Descent(19293): loss=13.046480006828267\n",
      "Stochastic Gradient Descent(19294): loss=0.23785693175729766\n",
      "Stochastic Gradient Descent(19295): loss=1.8847640860610964\n",
      "Stochastic Gradient Descent(19296): loss=23.24126528818172\n",
      "Stochastic Gradient Descent(19297): loss=6.217805709688431\n",
      "Stochastic Gradient Descent(19298): loss=2.3875598815168653\n",
      "Stochastic Gradient Descent(19299): loss=28.272552917147827\n",
      "Stochastic Gradient Descent(19300): loss=0.1681931315784706\n",
      "Stochastic Gradient Descent(19301): loss=1.4533618657402325\n",
      "Stochastic Gradient Descent(19302): loss=11.542145242288623\n",
      "Stochastic Gradient Descent(19303): loss=1.071535788589395\n",
      "Stochastic Gradient Descent(19304): loss=3.1206394425267336\n",
      "Stochastic Gradient Descent(19305): loss=10.360680631037173\n",
      "Stochastic Gradient Descent(19306): loss=0.510367144610867\n",
      "Stochastic Gradient Descent(19307): loss=6.077836057334766\n",
      "Stochastic Gradient Descent(19308): loss=0.7138584555749118\n",
      "Stochastic Gradient Descent(19309): loss=12.44682833314582\n",
      "Stochastic Gradient Descent(19310): loss=5.23968711597509\n",
      "Stochastic Gradient Descent(19311): loss=2.6525075934626545\n",
      "Stochastic Gradient Descent(19312): loss=12.770931403269207\n",
      "Stochastic Gradient Descent(19313): loss=0.39979372678302383\n",
      "Stochastic Gradient Descent(19314): loss=6.033693695099966\n",
      "Stochastic Gradient Descent(19315): loss=2.716113371486269\n",
      "Stochastic Gradient Descent(19316): loss=22.345099803420556\n",
      "Stochastic Gradient Descent(19317): loss=0.008953978637864008\n",
      "Stochastic Gradient Descent(19318): loss=4.169049212726226\n",
      "Stochastic Gradient Descent(19319): loss=9.828262045885324\n",
      "Stochastic Gradient Descent(19320): loss=2.426877924640684\n",
      "Stochastic Gradient Descent(19321): loss=1.9673050333263586\n",
      "Stochastic Gradient Descent(19322): loss=0.8627932572202586\n",
      "Stochastic Gradient Descent(19323): loss=0.11603113652011457\n",
      "Stochastic Gradient Descent(19324): loss=1.1722072649442081\n",
      "Stochastic Gradient Descent(19325): loss=7.7017833677440874\n",
      "Stochastic Gradient Descent(19326): loss=7.2978396620073305\n",
      "Stochastic Gradient Descent(19327): loss=4.259914033009064\n",
      "Stochastic Gradient Descent(19328): loss=0.1717360955566613\n",
      "Stochastic Gradient Descent(19329): loss=0.9513347858281499\n",
      "Stochastic Gradient Descent(19330): loss=1.9617211698597827\n",
      "Stochastic Gradient Descent(19331): loss=2.1413083237237616\n",
      "Stochastic Gradient Descent(19332): loss=3.238329770284666\n",
      "Stochastic Gradient Descent(19333): loss=3.364931453586334\n",
      "Stochastic Gradient Descent(19334): loss=20.649453716108585\n",
      "Stochastic Gradient Descent(19335): loss=2.0491590304283274\n",
      "Stochastic Gradient Descent(19336): loss=0.029779475802324165\n",
      "Stochastic Gradient Descent(19337): loss=4.882240264527759\n",
      "Stochastic Gradient Descent(19338): loss=1.8393349869003124\n",
      "Stochastic Gradient Descent(19339): loss=0.07026439330529426\n",
      "Stochastic Gradient Descent(19340): loss=1.2870161627074725\n",
      "Stochastic Gradient Descent(19341): loss=4.920420296182886\n",
      "Stochastic Gradient Descent(19342): loss=0.3286414553920467\n",
      "Stochastic Gradient Descent(19343): loss=13.219791828999032\n",
      "Stochastic Gradient Descent(19344): loss=0.053622651001846745\n",
      "Stochastic Gradient Descent(19345): loss=0.32582379633144287\n",
      "Stochastic Gradient Descent(19346): loss=2.2700916414402674\n",
      "Stochastic Gradient Descent(19347): loss=0.02643456306007999\n",
      "Stochastic Gradient Descent(19348): loss=0.08435246558992976\n",
      "Stochastic Gradient Descent(19349): loss=0.01654215101141277\n",
      "Stochastic Gradient Descent(19350): loss=5.955197861840991\n",
      "Stochastic Gradient Descent(19351): loss=1.2393515952351781\n",
      "Stochastic Gradient Descent(19352): loss=44.149399229035346\n",
      "Stochastic Gradient Descent(19353): loss=5.286630512012694\n",
      "Stochastic Gradient Descent(19354): loss=0.7704045939553608\n",
      "Stochastic Gradient Descent(19355): loss=7.879364211407343\n",
      "Stochastic Gradient Descent(19356): loss=2.5204185508330155\n",
      "Stochastic Gradient Descent(19357): loss=0.387217337355679\n",
      "Stochastic Gradient Descent(19358): loss=2.5739602708386626\n",
      "Stochastic Gradient Descent(19359): loss=3.697087994530016\n",
      "Stochastic Gradient Descent(19360): loss=0.017148908397510522\n",
      "Stochastic Gradient Descent(19361): loss=3.617634698301577\n",
      "Stochastic Gradient Descent(19362): loss=0.8945430153052051\n",
      "Stochastic Gradient Descent(19363): loss=1.1430529170458463\n",
      "Stochastic Gradient Descent(19364): loss=0.5220691870937464\n",
      "Stochastic Gradient Descent(19365): loss=11.336514446333021\n",
      "Stochastic Gradient Descent(19366): loss=0.21839810833205034\n",
      "Stochastic Gradient Descent(19367): loss=4.247037979676681\n",
      "Stochastic Gradient Descent(19368): loss=1.2074291403000204\n",
      "Stochastic Gradient Descent(19369): loss=0.2863994265039226\n",
      "Stochastic Gradient Descent(19370): loss=6.847002586839802\n",
      "Stochastic Gradient Descent(19371): loss=1.8278712420210905\n",
      "Stochastic Gradient Descent(19372): loss=6.734958782183666\n",
      "Stochastic Gradient Descent(19373): loss=0.07761355634448698\n",
      "Stochastic Gradient Descent(19374): loss=0.9408674645657169\n",
      "Stochastic Gradient Descent(19375): loss=10.986199861444488\n",
      "Stochastic Gradient Descent(19376): loss=0.9323438695840496\n",
      "Stochastic Gradient Descent(19377): loss=1.5323165389743625\n",
      "Stochastic Gradient Descent(19378): loss=1.3584957431172604\n",
      "Stochastic Gradient Descent(19379): loss=11.231329423896653\n",
      "Stochastic Gradient Descent(19380): loss=6.352810856415707\n",
      "Stochastic Gradient Descent(19381): loss=0.3607303920369119\n",
      "Stochastic Gradient Descent(19382): loss=2.2760515903717944\n",
      "Stochastic Gradient Descent(19383): loss=12.917785401083153\n",
      "Stochastic Gradient Descent(19384): loss=1.5992395350077855\n",
      "Stochastic Gradient Descent(19385): loss=0.6037901819779272\n",
      "Stochastic Gradient Descent(19386): loss=2.997799384423906\n",
      "Stochastic Gradient Descent(19387): loss=0.43387380333995657\n",
      "Stochastic Gradient Descent(19388): loss=2.2412857999369273\n",
      "Stochastic Gradient Descent(19389): loss=6.904158884352028\n",
      "Stochastic Gradient Descent(19390): loss=0.3623070545789098\n",
      "Stochastic Gradient Descent(19391): loss=11.479631861132075\n",
      "Stochastic Gradient Descent(19392): loss=0.09591848973925288\n",
      "Stochastic Gradient Descent(19393): loss=6.582946854854223\n",
      "Stochastic Gradient Descent(19394): loss=9.52397062649363\n",
      "Stochastic Gradient Descent(19395): loss=6.986728597854753\n",
      "Stochastic Gradient Descent(19396): loss=3.2864886457329834\n",
      "Stochastic Gradient Descent(19397): loss=0.5967585249638674\n",
      "Stochastic Gradient Descent(19398): loss=4.8066453251807\n",
      "Stochastic Gradient Descent(19399): loss=12.28291002610444\n",
      "Stochastic Gradient Descent(19400): loss=9.284927057960816\n",
      "Stochastic Gradient Descent(19401): loss=17.95549078462821\n",
      "Stochastic Gradient Descent(19402): loss=5.949460471969273\n",
      "Stochastic Gradient Descent(19403): loss=4.928804608780923\n",
      "Stochastic Gradient Descent(19404): loss=0.685820399340214\n",
      "Stochastic Gradient Descent(19405): loss=0.13375514670112082\n",
      "Stochastic Gradient Descent(19406): loss=0.47697940117273624\n",
      "Stochastic Gradient Descent(19407): loss=4.760506367748461\n",
      "Stochastic Gradient Descent(19408): loss=0.439568530109759\n",
      "Stochastic Gradient Descent(19409): loss=1.7233137534972631\n",
      "Stochastic Gradient Descent(19410): loss=4.6353697840004795\n",
      "Stochastic Gradient Descent(19411): loss=3.43463718411876\n",
      "Stochastic Gradient Descent(19412): loss=1.7360930526559264\n",
      "Stochastic Gradient Descent(19413): loss=2.370392002763769\n",
      "Stochastic Gradient Descent(19414): loss=0.35359658015167084\n",
      "Stochastic Gradient Descent(19415): loss=1.5941466824445838\n",
      "Stochastic Gradient Descent(19416): loss=53.76842928154473\n",
      "Stochastic Gradient Descent(19417): loss=2.1397721105453766\n",
      "Stochastic Gradient Descent(19418): loss=5.602927163028664\n",
      "Stochastic Gradient Descent(19419): loss=0.027041679905338543\n",
      "Stochastic Gradient Descent(19420): loss=4.751871149913539\n",
      "Stochastic Gradient Descent(19421): loss=12.27199871567286\n",
      "Stochastic Gradient Descent(19422): loss=0.36794618719132255\n",
      "Stochastic Gradient Descent(19423): loss=0.46406562262506873\n",
      "Stochastic Gradient Descent(19424): loss=0.041091496138731844\n",
      "Stochastic Gradient Descent(19425): loss=7.090379953352415\n",
      "Stochastic Gradient Descent(19426): loss=13.856423086406782\n",
      "Stochastic Gradient Descent(19427): loss=2.0127210694192916\n",
      "Stochastic Gradient Descent(19428): loss=1.2589578852602847\n",
      "Stochastic Gradient Descent(19429): loss=12.747247217333197\n",
      "Stochastic Gradient Descent(19430): loss=0.7189589809627619\n",
      "Stochastic Gradient Descent(19431): loss=9.96849549768463\n",
      "Stochastic Gradient Descent(19432): loss=0.5598377152342345\n",
      "Stochastic Gradient Descent(19433): loss=0.3824784869202953\n",
      "Stochastic Gradient Descent(19434): loss=0.040899547796375\n",
      "Stochastic Gradient Descent(19435): loss=3.5656384336405598\n",
      "Stochastic Gradient Descent(19436): loss=10.553968918424937\n",
      "Stochastic Gradient Descent(19437): loss=6.757672069811769\n",
      "Stochastic Gradient Descent(19438): loss=9.814755018433537\n",
      "Stochastic Gradient Descent(19439): loss=7.673453904894379\n",
      "Stochastic Gradient Descent(19440): loss=14.93879393691467\n",
      "Stochastic Gradient Descent(19441): loss=0.3352086653706786\n",
      "Stochastic Gradient Descent(19442): loss=1.2312683086172431\n",
      "Stochastic Gradient Descent(19443): loss=29.12164985094119\n",
      "Stochastic Gradient Descent(19444): loss=25.084910408234954\n",
      "Stochastic Gradient Descent(19445): loss=4.266613860395639\n",
      "Stochastic Gradient Descent(19446): loss=2.5653020871143206\n",
      "Stochastic Gradient Descent(19447): loss=12.376419498925193\n",
      "Stochastic Gradient Descent(19448): loss=7.703862065767223\n",
      "Stochastic Gradient Descent(19449): loss=0.3233459214810523\n",
      "Stochastic Gradient Descent(19450): loss=4.19461706585123\n",
      "Stochastic Gradient Descent(19451): loss=26.835338227150856\n",
      "Stochastic Gradient Descent(19452): loss=0.0018418726934993006\n",
      "Stochastic Gradient Descent(19453): loss=1.213023282694894\n",
      "Stochastic Gradient Descent(19454): loss=0.2947308511714485\n",
      "Stochastic Gradient Descent(19455): loss=1.5269045604290796\n",
      "Stochastic Gradient Descent(19456): loss=0.6817565088756605\n",
      "Stochastic Gradient Descent(19457): loss=0.19645291925164451\n",
      "Stochastic Gradient Descent(19458): loss=0.48913806931500825\n",
      "Stochastic Gradient Descent(19459): loss=1.335349727657643\n",
      "Stochastic Gradient Descent(19460): loss=8.63688893151974\n",
      "Stochastic Gradient Descent(19461): loss=1.110881887573558\n",
      "Stochastic Gradient Descent(19462): loss=16.688683991823407\n",
      "Stochastic Gradient Descent(19463): loss=2.6151827115104176\n",
      "Stochastic Gradient Descent(19464): loss=2.4502519682165445\n",
      "Stochastic Gradient Descent(19465): loss=9.029947753108358\n",
      "Stochastic Gradient Descent(19466): loss=6.672443292610377\n",
      "Stochastic Gradient Descent(19467): loss=6.96310588300167\n",
      "Stochastic Gradient Descent(19468): loss=0.9766275030016637\n",
      "Stochastic Gradient Descent(19469): loss=0.31425240283974537\n",
      "Stochastic Gradient Descent(19470): loss=24.350865586895967\n",
      "Stochastic Gradient Descent(19471): loss=4.715440959423899\n",
      "Stochastic Gradient Descent(19472): loss=0.6859480610224177\n",
      "Stochastic Gradient Descent(19473): loss=2.4009623617908202\n",
      "Stochastic Gradient Descent(19474): loss=0.7871202217772929\n",
      "Stochastic Gradient Descent(19475): loss=0.032285065327821515\n",
      "Stochastic Gradient Descent(19476): loss=8.541175339798095\n",
      "Stochastic Gradient Descent(19477): loss=10.150224397974773\n",
      "Stochastic Gradient Descent(19478): loss=4.287805995045452\n",
      "Stochastic Gradient Descent(19479): loss=0.6587096287183946\n",
      "Stochastic Gradient Descent(19480): loss=4.558302033994919\n",
      "Stochastic Gradient Descent(19481): loss=0.051346952101273415\n",
      "Stochastic Gradient Descent(19482): loss=7.897489027825493\n",
      "Stochastic Gradient Descent(19483): loss=0.3184952521796268\n",
      "Stochastic Gradient Descent(19484): loss=3.5004951337315284\n",
      "Stochastic Gradient Descent(19485): loss=0.06828320247449243\n",
      "Stochastic Gradient Descent(19486): loss=0.35618868525322306\n",
      "Stochastic Gradient Descent(19487): loss=3.990379555596424\n",
      "Stochastic Gradient Descent(19488): loss=0.2645981030159574\n",
      "Stochastic Gradient Descent(19489): loss=1.20012734571523\n",
      "Stochastic Gradient Descent(19490): loss=3.2788100489392873\n",
      "Stochastic Gradient Descent(19491): loss=0.02578069339658127\n",
      "Stochastic Gradient Descent(19492): loss=8.101651687987934\n",
      "Stochastic Gradient Descent(19493): loss=1.9384544198077138\n",
      "Stochastic Gradient Descent(19494): loss=14.259607378235211\n",
      "Stochastic Gradient Descent(19495): loss=0.2214717499549112\n",
      "Stochastic Gradient Descent(19496): loss=0.45777500258393966\n",
      "Stochastic Gradient Descent(19497): loss=8.04651980577768\n",
      "Stochastic Gradient Descent(19498): loss=8.588494494689924\n",
      "Stochastic Gradient Descent(19499): loss=2.4808208324914913\n",
      "Stochastic Gradient Descent(19500): loss=3.605682420526273\n",
      "Stochastic Gradient Descent(19501): loss=7.35767687345914\n",
      "Stochastic Gradient Descent(19502): loss=13.71046019089774\n",
      "Stochastic Gradient Descent(19503): loss=6.902416599582942\n",
      "Stochastic Gradient Descent(19504): loss=1.1299379006211223\n",
      "Stochastic Gradient Descent(19505): loss=3.2754974826799064\n",
      "Stochastic Gradient Descent(19506): loss=10.59496307582227\n",
      "Stochastic Gradient Descent(19507): loss=0.9564722328444175\n",
      "Stochastic Gradient Descent(19508): loss=4.217813441504385\n",
      "Stochastic Gradient Descent(19509): loss=16.010388153974\n",
      "Stochastic Gradient Descent(19510): loss=3.439963845456117\n",
      "Stochastic Gradient Descent(19511): loss=1.294770570502486\n",
      "Stochastic Gradient Descent(19512): loss=7.1330560187664025\n",
      "Stochastic Gradient Descent(19513): loss=0.01621116630928133\n",
      "Stochastic Gradient Descent(19514): loss=0.016762417267468528\n",
      "Stochastic Gradient Descent(19515): loss=1.0499355731098974\n",
      "Stochastic Gradient Descent(19516): loss=1.090321197270461\n",
      "Stochastic Gradient Descent(19517): loss=0.9460599197582306\n",
      "Stochastic Gradient Descent(19518): loss=0.08408577693527121\n",
      "Stochastic Gradient Descent(19519): loss=0.0002544891855417591\n",
      "Stochastic Gradient Descent(19520): loss=1.5439832647530884\n",
      "Stochastic Gradient Descent(19521): loss=0.20185161717026937\n",
      "Stochastic Gradient Descent(19522): loss=0.2615644750372162\n",
      "Stochastic Gradient Descent(19523): loss=1.269841784660655\n",
      "Stochastic Gradient Descent(19524): loss=0.12476686839952403\n",
      "Stochastic Gradient Descent(19525): loss=3.5086014662363922\n",
      "Stochastic Gradient Descent(19526): loss=8.327269414338458\n",
      "Stochastic Gradient Descent(19527): loss=13.088109964704373\n",
      "Stochastic Gradient Descent(19528): loss=0.27653006911222766\n",
      "Stochastic Gradient Descent(19529): loss=1.997372639810389\n",
      "Stochastic Gradient Descent(19530): loss=0.8326806405088284\n",
      "Stochastic Gradient Descent(19531): loss=18.47730724681873\n",
      "Stochastic Gradient Descent(19532): loss=6.124758229567963\n",
      "Stochastic Gradient Descent(19533): loss=4.54317622927699\n",
      "Stochastic Gradient Descent(19534): loss=4.886703206615638\n",
      "Stochastic Gradient Descent(19535): loss=1.0386680070538432\n",
      "Stochastic Gradient Descent(19536): loss=17.18904563435861\n",
      "Stochastic Gradient Descent(19537): loss=6.19588227196394\n",
      "Stochastic Gradient Descent(19538): loss=0.7708976014030758\n",
      "Stochastic Gradient Descent(19539): loss=0.0014304667762104425\n",
      "Stochastic Gradient Descent(19540): loss=6.495137770878783\n",
      "Stochastic Gradient Descent(19541): loss=0.2254037768857994\n",
      "Stochastic Gradient Descent(19542): loss=9.389699210877955\n",
      "Stochastic Gradient Descent(19543): loss=5.645913690746334\n",
      "Stochastic Gradient Descent(19544): loss=0.699257115381941\n",
      "Stochastic Gradient Descent(19545): loss=0.023732141970639546\n",
      "Stochastic Gradient Descent(19546): loss=1.9913584852310875\n",
      "Stochastic Gradient Descent(19547): loss=1.2837043139534174\n",
      "Stochastic Gradient Descent(19548): loss=2.3632620767090207\n",
      "Stochastic Gradient Descent(19549): loss=0.9902038506250554\n",
      "Stochastic Gradient Descent(19550): loss=1.3097044184634852\n",
      "Stochastic Gradient Descent(19551): loss=0.11628995301138542\n",
      "Stochastic Gradient Descent(19552): loss=5.242844116926773\n",
      "Stochastic Gradient Descent(19553): loss=1.984573754694119\n",
      "Stochastic Gradient Descent(19554): loss=4.054013132141269\n",
      "Stochastic Gradient Descent(19555): loss=1.3476115591157578\n",
      "Stochastic Gradient Descent(19556): loss=4.314472904082687\n",
      "Stochastic Gradient Descent(19557): loss=7.0251186992294725\n",
      "Stochastic Gradient Descent(19558): loss=0.0004025169535880552\n",
      "Stochastic Gradient Descent(19559): loss=2.765773104517\n",
      "Stochastic Gradient Descent(19560): loss=5.357796158183862\n",
      "Stochastic Gradient Descent(19561): loss=0.8960975089463936\n",
      "Stochastic Gradient Descent(19562): loss=0.4333598444652871\n",
      "Stochastic Gradient Descent(19563): loss=10.463805321897093\n",
      "Stochastic Gradient Descent(19564): loss=0.9542313425286406\n",
      "Stochastic Gradient Descent(19565): loss=0.19200033266887934\n",
      "Stochastic Gradient Descent(19566): loss=28.1549616748428\n",
      "Stochastic Gradient Descent(19567): loss=7.2875990345803165\n",
      "Stochastic Gradient Descent(19568): loss=79.49047511585647\n",
      "Stochastic Gradient Descent(19569): loss=0.024910488748581962\n",
      "Stochastic Gradient Descent(19570): loss=15.166375096836402\n",
      "Stochastic Gradient Descent(19571): loss=1.4611226766947192\n",
      "Stochastic Gradient Descent(19572): loss=0.7929803528836613\n",
      "Stochastic Gradient Descent(19573): loss=1.4915579356225444\n",
      "Stochastic Gradient Descent(19574): loss=19.99448331156247\n",
      "Stochastic Gradient Descent(19575): loss=49.68585689003793\n",
      "Stochastic Gradient Descent(19576): loss=27.153636535547836\n",
      "Stochastic Gradient Descent(19577): loss=1.2495259011117716\n",
      "Stochastic Gradient Descent(19578): loss=6.291833002108899\n",
      "Stochastic Gradient Descent(19579): loss=0.0911809003350993\n",
      "Stochastic Gradient Descent(19580): loss=6.872284908675144\n",
      "Stochastic Gradient Descent(19581): loss=0.14763687138690798\n",
      "Stochastic Gradient Descent(19582): loss=10.47170666452665\n",
      "Stochastic Gradient Descent(19583): loss=9.476678371959405\n",
      "Stochastic Gradient Descent(19584): loss=3.4043314814740735\n",
      "Stochastic Gradient Descent(19585): loss=2.5393794565494656\n",
      "Stochastic Gradient Descent(19586): loss=2.0245255278537613\n",
      "Stochastic Gradient Descent(19587): loss=0.8859421306373652\n",
      "Stochastic Gradient Descent(19588): loss=0.004290995529338963\n",
      "Stochastic Gradient Descent(19589): loss=3.6941047218049126\n",
      "Stochastic Gradient Descent(19590): loss=2.607501548374313\n",
      "Stochastic Gradient Descent(19591): loss=5.712671888700457\n",
      "Stochastic Gradient Descent(19592): loss=0.9586603455948972\n",
      "Stochastic Gradient Descent(19593): loss=0.7072670263345626\n",
      "Stochastic Gradient Descent(19594): loss=7.848335328987106\n",
      "Stochastic Gradient Descent(19595): loss=0.405789751567548\n",
      "Stochastic Gradient Descent(19596): loss=0.3362988746925799\n",
      "Stochastic Gradient Descent(19597): loss=1.015232557646128\n",
      "Stochastic Gradient Descent(19598): loss=5.259769537683701\n",
      "Stochastic Gradient Descent(19599): loss=0.04031656773766142\n",
      "Stochastic Gradient Descent(19600): loss=0.14932244450048637\n",
      "Stochastic Gradient Descent(19601): loss=8.093296154832052\n",
      "Stochastic Gradient Descent(19602): loss=0.2745722037838458\n",
      "Stochastic Gradient Descent(19603): loss=0.7556892193147526\n",
      "Stochastic Gradient Descent(19604): loss=2.79993800524112\n",
      "Stochastic Gradient Descent(19605): loss=0.02302801833201232\n",
      "Stochastic Gradient Descent(19606): loss=5.426833874046575\n",
      "Stochastic Gradient Descent(19607): loss=2.364206758426552\n",
      "Stochastic Gradient Descent(19608): loss=20.643098385613094\n",
      "Stochastic Gradient Descent(19609): loss=0.25454792124730263\n",
      "Stochastic Gradient Descent(19610): loss=2.120026948576307\n",
      "Stochastic Gradient Descent(19611): loss=0.02571763657165006\n",
      "Stochastic Gradient Descent(19612): loss=6.4442669658891525\n",
      "Stochastic Gradient Descent(19613): loss=35.824780602293714\n",
      "Stochastic Gradient Descent(19614): loss=0.7388356742240121\n",
      "Stochastic Gradient Descent(19615): loss=7.860775055335402\n",
      "Stochastic Gradient Descent(19616): loss=28.15370997559232\n",
      "Stochastic Gradient Descent(19617): loss=14.758798246866482\n",
      "Stochastic Gradient Descent(19618): loss=17.813673257039042\n",
      "Stochastic Gradient Descent(19619): loss=6.140319664719995\n",
      "Stochastic Gradient Descent(19620): loss=0.5071410095246518\n",
      "Stochastic Gradient Descent(19621): loss=2.3671890638101822\n",
      "Stochastic Gradient Descent(19622): loss=4.5564879725622145\n",
      "Stochastic Gradient Descent(19623): loss=5.567398339998888\n",
      "Stochastic Gradient Descent(19624): loss=1.194023344314731\n",
      "Stochastic Gradient Descent(19625): loss=0.4942828014444461\n",
      "Stochastic Gradient Descent(19626): loss=0.34760239832662687\n",
      "Stochastic Gradient Descent(19627): loss=7.176045165131575\n",
      "Stochastic Gradient Descent(19628): loss=2.5122848860528433\n",
      "Stochastic Gradient Descent(19629): loss=7.479121712015923\n",
      "Stochastic Gradient Descent(19630): loss=2.688156001395454\n",
      "Stochastic Gradient Descent(19631): loss=0.9333376221305217\n",
      "Stochastic Gradient Descent(19632): loss=2.978622630532219\n",
      "Stochastic Gradient Descent(19633): loss=1.4900831966016646\n",
      "Stochastic Gradient Descent(19634): loss=0.005621112601209476\n",
      "Stochastic Gradient Descent(19635): loss=8.570759606817102\n",
      "Stochastic Gradient Descent(19636): loss=2.1257883672742097\n",
      "Stochastic Gradient Descent(19637): loss=3.5330588182008977\n",
      "Stochastic Gradient Descent(19638): loss=9.44735459470224\n",
      "Stochastic Gradient Descent(19639): loss=5.759952497680358\n",
      "Stochastic Gradient Descent(19640): loss=2.219855793562131\n",
      "Stochastic Gradient Descent(19641): loss=6.076240821902998\n",
      "Stochastic Gradient Descent(19642): loss=6.964127100892451\n",
      "Stochastic Gradient Descent(19643): loss=10.315919542722433\n",
      "Stochastic Gradient Descent(19644): loss=10.531033933944961\n",
      "Stochastic Gradient Descent(19645): loss=0.06274737591073476\n",
      "Stochastic Gradient Descent(19646): loss=1.2770141577058007\n",
      "Stochastic Gradient Descent(19647): loss=0.025073359499905978\n",
      "Stochastic Gradient Descent(19648): loss=5.329211547903966\n",
      "Stochastic Gradient Descent(19649): loss=1.1001141049369525\n",
      "Stochastic Gradient Descent(19650): loss=0.005144147460066049\n",
      "Stochastic Gradient Descent(19651): loss=0.5709756780427941\n",
      "Stochastic Gradient Descent(19652): loss=4.341898791118824\n",
      "Stochastic Gradient Descent(19653): loss=7.55978742417086\n",
      "Stochastic Gradient Descent(19654): loss=0.0680481371723136\n",
      "Stochastic Gradient Descent(19655): loss=2.6785995328107766\n",
      "Stochastic Gradient Descent(19656): loss=1.0059777423427232\n",
      "Stochastic Gradient Descent(19657): loss=2.586808293838098\n",
      "Stochastic Gradient Descent(19658): loss=7.946632977338774\n",
      "Stochastic Gradient Descent(19659): loss=17.039891901697967\n",
      "Stochastic Gradient Descent(19660): loss=0.062202986860077455\n",
      "Stochastic Gradient Descent(19661): loss=28.890462419656227\n",
      "Stochastic Gradient Descent(19662): loss=5.541380431724688\n",
      "Stochastic Gradient Descent(19663): loss=17.656754246248607\n",
      "Stochastic Gradient Descent(19664): loss=1.243193034906532\n",
      "Stochastic Gradient Descent(19665): loss=0.40821287847027177\n",
      "Stochastic Gradient Descent(19666): loss=0.0011561173488979285\n",
      "Stochastic Gradient Descent(19667): loss=2.145042797901613\n",
      "Stochastic Gradient Descent(19668): loss=2.264730137813349\n",
      "Stochastic Gradient Descent(19669): loss=6.292585629758534\n",
      "Stochastic Gradient Descent(19670): loss=0.17725478018120283\n",
      "Stochastic Gradient Descent(19671): loss=0.3656466500364957\n",
      "Stochastic Gradient Descent(19672): loss=0.012768035219942648\n",
      "Stochastic Gradient Descent(19673): loss=0.01453403635360127\n",
      "Stochastic Gradient Descent(19674): loss=0.062386282420571156\n",
      "Stochastic Gradient Descent(19675): loss=0.14214880130924448\n",
      "Stochastic Gradient Descent(19676): loss=7.738235272893005\n",
      "Stochastic Gradient Descent(19677): loss=1.171786021934652\n",
      "Stochastic Gradient Descent(19678): loss=3.3725037949135563\n",
      "Stochastic Gradient Descent(19679): loss=5.333248689765456\n",
      "Stochastic Gradient Descent(19680): loss=9.398385222659833\n",
      "Stochastic Gradient Descent(19681): loss=0.5665925836942908\n",
      "Stochastic Gradient Descent(19682): loss=6.629184229449977\n",
      "Stochastic Gradient Descent(19683): loss=12.438395703407844\n",
      "Stochastic Gradient Descent(19684): loss=0.6423063237711927\n",
      "Stochastic Gradient Descent(19685): loss=0.05667047860356218\n",
      "Stochastic Gradient Descent(19686): loss=29.29791366912936\n",
      "Stochastic Gradient Descent(19687): loss=0.0020350920196191896\n",
      "Stochastic Gradient Descent(19688): loss=4.844326095712813\n",
      "Stochastic Gradient Descent(19689): loss=0.9477005539383083\n",
      "Stochastic Gradient Descent(19690): loss=5.915413761393209\n",
      "Stochastic Gradient Descent(19691): loss=79.92332005909164\n",
      "Stochastic Gradient Descent(19692): loss=9.992206710314777\n",
      "Stochastic Gradient Descent(19693): loss=0.5528653871650355\n",
      "Stochastic Gradient Descent(19694): loss=1.6300005971550267\n",
      "Stochastic Gradient Descent(19695): loss=0.22497771052649676\n",
      "Stochastic Gradient Descent(19696): loss=2.2486809536097816\n",
      "Stochastic Gradient Descent(19697): loss=7.507179004465539\n",
      "Stochastic Gradient Descent(19698): loss=10.530609638074203\n",
      "Stochastic Gradient Descent(19699): loss=3.7700206623696597\n",
      "Stochastic Gradient Descent(19700): loss=7.644694613071259\n",
      "Stochastic Gradient Descent(19701): loss=0.3336498525990994\n",
      "Stochastic Gradient Descent(19702): loss=15.148045579351066\n",
      "Stochastic Gradient Descent(19703): loss=0.8838660029785158\n",
      "Stochastic Gradient Descent(19704): loss=8.544217970495948\n",
      "Stochastic Gradient Descent(19705): loss=3.307644515268923\n",
      "Stochastic Gradient Descent(19706): loss=0.6287107716568742\n",
      "Stochastic Gradient Descent(19707): loss=0.9971753628243741\n",
      "Stochastic Gradient Descent(19708): loss=3.1883574585994725\n",
      "Stochastic Gradient Descent(19709): loss=2.1086685444920765\n",
      "Stochastic Gradient Descent(19710): loss=2.6864893504176384\n",
      "Stochastic Gradient Descent(19711): loss=1.2186763214123193\n",
      "Stochastic Gradient Descent(19712): loss=0.08318028806531798\n",
      "Stochastic Gradient Descent(19713): loss=5.953778588168919\n",
      "Stochastic Gradient Descent(19714): loss=0.8148215455806905\n",
      "Stochastic Gradient Descent(19715): loss=1.5167698331094073\n",
      "Stochastic Gradient Descent(19716): loss=1.7066261227645108\n",
      "Stochastic Gradient Descent(19717): loss=1.3357581034923096\n",
      "Stochastic Gradient Descent(19718): loss=4.71233979065267\n",
      "Stochastic Gradient Descent(19719): loss=10.033423288524702\n",
      "Stochastic Gradient Descent(19720): loss=6.951370748528756\n",
      "Stochastic Gradient Descent(19721): loss=1.1593873156512131\n",
      "Stochastic Gradient Descent(19722): loss=0.9466910776842539\n",
      "Stochastic Gradient Descent(19723): loss=0.7717055312947202\n",
      "Stochastic Gradient Descent(19724): loss=0.014223126575583096\n",
      "Stochastic Gradient Descent(19725): loss=0.0058745352968256675\n",
      "Stochastic Gradient Descent(19726): loss=0.06712751460587331\n",
      "Stochastic Gradient Descent(19727): loss=0.032972089354985065\n",
      "Stochastic Gradient Descent(19728): loss=0.09988645622402642\n",
      "Stochastic Gradient Descent(19729): loss=12.437639925681955\n",
      "Stochastic Gradient Descent(19730): loss=0.0005507417578251863\n",
      "Stochastic Gradient Descent(19731): loss=10.702274585811649\n",
      "Stochastic Gradient Descent(19732): loss=14.39040400255749\n",
      "Stochastic Gradient Descent(19733): loss=3.206980312202926\n",
      "Stochastic Gradient Descent(19734): loss=1.4878882045668027\n",
      "Stochastic Gradient Descent(19735): loss=2.2599913008211425\n",
      "Stochastic Gradient Descent(19736): loss=0.10237317627406817\n",
      "Stochastic Gradient Descent(19737): loss=9.694811244202068\n",
      "Stochastic Gradient Descent(19738): loss=0.4807857673924762\n",
      "Stochastic Gradient Descent(19739): loss=1.205617383209716\n",
      "Stochastic Gradient Descent(19740): loss=0.0474198174041267\n",
      "Stochastic Gradient Descent(19741): loss=1.0318666203959512\n",
      "Stochastic Gradient Descent(19742): loss=24.159830285457456\n",
      "Stochastic Gradient Descent(19743): loss=0.028376544183389592\n",
      "Stochastic Gradient Descent(19744): loss=0.3633757375365724\n",
      "Stochastic Gradient Descent(19745): loss=6.068648688883147\n",
      "Stochastic Gradient Descent(19746): loss=2.0236534475127113\n",
      "Stochastic Gradient Descent(19747): loss=0.3799784141224105\n",
      "Stochastic Gradient Descent(19748): loss=10.53429926750491\n",
      "Stochastic Gradient Descent(19749): loss=1.2224062931098623\n",
      "Stochastic Gradient Descent(19750): loss=0.13959231749839013\n",
      "Stochastic Gradient Descent(19751): loss=0.35228877675978537\n",
      "Stochastic Gradient Descent(19752): loss=7.082229446586748\n",
      "Stochastic Gradient Descent(19753): loss=0.1323843526846058\n",
      "Stochastic Gradient Descent(19754): loss=0.036452591685225376\n",
      "Stochastic Gradient Descent(19755): loss=0.007020809132847918\n",
      "Stochastic Gradient Descent(19756): loss=2.919801086515931\n",
      "Stochastic Gradient Descent(19757): loss=2.1881669107102795\n",
      "Stochastic Gradient Descent(19758): loss=1.1520476175520973\n",
      "Stochastic Gradient Descent(19759): loss=11.527937003406599\n",
      "Stochastic Gradient Descent(19760): loss=0.3813491327954988\n",
      "Stochastic Gradient Descent(19761): loss=19.241934021218732\n",
      "Stochastic Gradient Descent(19762): loss=0.6901238409049731\n",
      "Stochastic Gradient Descent(19763): loss=2.137638749162939\n",
      "Stochastic Gradient Descent(19764): loss=10.9422551985727\n",
      "Stochastic Gradient Descent(19765): loss=0.232147720982189\n",
      "Stochastic Gradient Descent(19766): loss=0.0027616685653798546\n",
      "Stochastic Gradient Descent(19767): loss=1.0643528696121554\n",
      "Stochastic Gradient Descent(19768): loss=1.7842274770037896\n",
      "Stochastic Gradient Descent(19769): loss=2.910251221073129e-05\n",
      "Stochastic Gradient Descent(19770): loss=10.553802874963198\n",
      "Stochastic Gradient Descent(19771): loss=7.242972952306132\n",
      "Stochastic Gradient Descent(19772): loss=3.159747506843121\n",
      "Stochastic Gradient Descent(19773): loss=0.007457197514640936\n",
      "Stochastic Gradient Descent(19774): loss=4.390487518724867\n",
      "Stochastic Gradient Descent(19775): loss=0.11499482896013721\n",
      "Stochastic Gradient Descent(19776): loss=0.003411002007082197\n",
      "Stochastic Gradient Descent(19777): loss=21.972921762569655\n",
      "Stochastic Gradient Descent(19778): loss=2.877763233573611\n",
      "Stochastic Gradient Descent(19779): loss=0.1797603793961997\n",
      "Stochastic Gradient Descent(19780): loss=0.4911834677333715\n",
      "Stochastic Gradient Descent(19781): loss=16.007233452212752\n",
      "Stochastic Gradient Descent(19782): loss=19.82207843995913\n",
      "Stochastic Gradient Descent(19783): loss=8.040691497887108\n",
      "Stochastic Gradient Descent(19784): loss=3.928539526963667\n",
      "Stochastic Gradient Descent(19785): loss=0.0010389609002339048\n",
      "Stochastic Gradient Descent(19786): loss=6.53223444187217\n",
      "Stochastic Gradient Descent(19787): loss=11.087514695739063\n",
      "Stochastic Gradient Descent(19788): loss=0.02492728753227282\n",
      "Stochastic Gradient Descent(19789): loss=1.3668085102829783\n",
      "Stochastic Gradient Descent(19790): loss=1.3882172531554045\n",
      "Stochastic Gradient Descent(19791): loss=0.15362739477352258\n",
      "Stochastic Gradient Descent(19792): loss=0.47627885518034685\n",
      "Stochastic Gradient Descent(19793): loss=1.1089949569441202\n",
      "Stochastic Gradient Descent(19794): loss=0.002622923693675194\n",
      "Stochastic Gradient Descent(19795): loss=1.1342528789741437\n",
      "Stochastic Gradient Descent(19796): loss=23.43936320194279\n",
      "Stochastic Gradient Descent(19797): loss=0.8791383724749949\n",
      "Stochastic Gradient Descent(19798): loss=78.35235781137284\n",
      "Stochastic Gradient Descent(19799): loss=6.78266356316469\n",
      "Stochastic Gradient Descent(19800): loss=0.12240975357371824\n",
      "Stochastic Gradient Descent(19801): loss=1.4524264686077617\n",
      "Stochastic Gradient Descent(19802): loss=10.01219142469039\n",
      "Stochastic Gradient Descent(19803): loss=1.6554685210439732\n",
      "Stochastic Gradient Descent(19804): loss=0.030260008729488173\n",
      "Stochastic Gradient Descent(19805): loss=8.80717026608957\n",
      "Stochastic Gradient Descent(19806): loss=1.2411769736769367\n",
      "Stochastic Gradient Descent(19807): loss=3.78975618593609\n",
      "Stochastic Gradient Descent(19808): loss=19.600537339703234\n",
      "Stochastic Gradient Descent(19809): loss=2.229686708429636\n",
      "Stochastic Gradient Descent(19810): loss=3.6542254331518396\n",
      "Stochastic Gradient Descent(19811): loss=0.2713630996165641\n",
      "Stochastic Gradient Descent(19812): loss=4.364468822980084\n",
      "Stochastic Gradient Descent(19813): loss=1.6269869289147134\n",
      "Stochastic Gradient Descent(19814): loss=0.8869493948916538\n",
      "Stochastic Gradient Descent(19815): loss=0.0014585371307953\n",
      "Stochastic Gradient Descent(19816): loss=29.47745193361375\n",
      "Stochastic Gradient Descent(19817): loss=10.457331130394305\n",
      "Stochastic Gradient Descent(19818): loss=0.44817439142390103\n",
      "Stochastic Gradient Descent(19819): loss=13.620939536541872\n",
      "Stochastic Gradient Descent(19820): loss=3.6019862748903266\n",
      "Stochastic Gradient Descent(19821): loss=0.0012735177651636894\n",
      "Stochastic Gradient Descent(19822): loss=0.06681161966775691\n",
      "Stochastic Gradient Descent(19823): loss=18.41098174580522\n",
      "Stochastic Gradient Descent(19824): loss=16.99503688300378\n",
      "Stochastic Gradient Descent(19825): loss=4.32830669290948\n",
      "Stochastic Gradient Descent(19826): loss=0.6144070052714884\n",
      "Stochastic Gradient Descent(19827): loss=0.00012875470694967932\n",
      "Stochastic Gradient Descent(19828): loss=20.269566956953167\n",
      "Stochastic Gradient Descent(19829): loss=2.866168293922681\n",
      "Stochastic Gradient Descent(19830): loss=1.211274485476539\n",
      "Stochastic Gradient Descent(19831): loss=2.599363165757521\n",
      "Stochastic Gradient Descent(19832): loss=0.14522545579873503\n",
      "Stochastic Gradient Descent(19833): loss=23.195861245875687\n",
      "Stochastic Gradient Descent(19834): loss=0.21870448610867738\n",
      "Stochastic Gradient Descent(19835): loss=4.211423418706213\n",
      "Stochastic Gradient Descent(19836): loss=28.187777797777507\n",
      "Stochastic Gradient Descent(19837): loss=0.5198966188763382\n",
      "Stochastic Gradient Descent(19838): loss=6.9287658314429645\n",
      "Stochastic Gradient Descent(19839): loss=0.2305057751720768\n",
      "Stochastic Gradient Descent(19840): loss=12.491680395936772\n",
      "Stochastic Gradient Descent(19841): loss=0.03409485635629336\n",
      "Stochastic Gradient Descent(19842): loss=3.0046325857228022\n",
      "Stochastic Gradient Descent(19843): loss=0.04612137540971231\n",
      "Stochastic Gradient Descent(19844): loss=8.41589988004425\n",
      "Stochastic Gradient Descent(19845): loss=10.384116112792366\n",
      "Stochastic Gradient Descent(19846): loss=0.6913941817917881\n",
      "Stochastic Gradient Descent(19847): loss=1.4078075176942095\n",
      "Stochastic Gradient Descent(19848): loss=19.418463885873848\n",
      "Stochastic Gradient Descent(19849): loss=1.235187167167009\n",
      "Stochastic Gradient Descent(19850): loss=0.2644383646777167\n",
      "Stochastic Gradient Descent(19851): loss=5.765982075775223\n",
      "Stochastic Gradient Descent(19852): loss=4.0454010831557685\n",
      "Stochastic Gradient Descent(19853): loss=24.30614468494196\n",
      "Stochastic Gradient Descent(19854): loss=9.959860689654946\n",
      "Stochastic Gradient Descent(19855): loss=1.2826013634273077\n",
      "Stochastic Gradient Descent(19856): loss=17.502056737758195\n",
      "Stochastic Gradient Descent(19857): loss=4.220725015198779\n",
      "Stochastic Gradient Descent(19858): loss=0.00010765408924075198\n",
      "Stochastic Gradient Descent(19859): loss=0.0029699260219078665\n",
      "Stochastic Gradient Descent(19860): loss=7.131532768159322\n",
      "Stochastic Gradient Descent(19861): loss=1.908406560560851\n",
      "Stochastic Gradient Descent(19862): loss=20.908860385051266\n",
      "Stochastic Gradient Descent(19863): loss=0.0006207159159083128\n",
      "Stochastic Gradient Descent(19864): loss=1.849231159891696\n",
      "Stochastic Gradient Descent(19865): loss=0.506275210649083\n",
      "Stochastic Gradient Descent(19866): loss=8.014501708798405e-05\n",
      "Stochastic Gradient Descent(19867): loss=0.445741691528715\n",
      "Stochastic Gradient Descent(19868): loss=1.3257604568856125\n",
      "Stochastic Gradient Descent(19869): loss=0.5919057121409277\n",
      "Stochastic Gradient Descent(19870): loss=0.020978658714843813\n",
      "Stochastic Gradient Descent(19871): loss=2.6256799577867382\n",
      "Stochastic Gradient Descent(19872): loss=11.36448168629981\n",
      "Stochastic Gradient Descent(19873): loss=0.862073627532809\n",
      "Stochastic Gradient Descent(19874): loss=0.11683866288164423\n",
      "Stochastic Gradient Descent(19875): loss=0.6706351191463918\n",
      "Stochastic Gradient Descent(19876): loss=1.4001692199083247\n",
      "Stochastic Gradient Descent(19877): loss=0.21006519795916645\n",
      "Stochastic Gradient Descent(19878): loss=0.3534184198244333\n",
      "Stochastic Gradient Descent(19879): loss=3.4455476126828968\n",
      "Stochastic Gradient Descent(19880): loss=20.76570400554393\n",
      "Stochastic Gradient Descent(19881): loss=6.91199952956485\n",
      "Stochastic Gradient Descent(19882): loss=8.29337360973832\n",
      "Stochastic Gradient Descent(19883): loss=5.920017486386474\n",
      "Stochastic Gradient Descent(19884): loss=15.340473335071925\n",
      "Stochastic Gradient Descent(19885): loss=5.684119505154062\n",
      "Stochastic Gradient Descent(19886): loss=0.0019589517447667977\n",
      "Stochastic Gradient Descent(19887): loss=0.050202908913191055\n",
      "Stochastic Gradient Descent(19888): loss=11.746842818908286\n",
      "Stochastic Gradient Descent(19889): loss=2.65045780036856\n",
      "Stochastic Gradient Descent(19890): loss=1.7003521276314955\n",
      "Stochastic Gradient Descent(19891): loss=3.738374783874917\n",
      "Stochastic Gradient Descent(19892): loss=3.4252044579743526\n",
      "Stochastic Gradient Descent(19893): loss=0.6088422509261301\n",
      "Stochastic Gradient Descent(19894): loss=0.05342203187814892\n",
      "Stochastic Gradient Descent(19895): loss=1.0277674667544582e-05\n",
      "Stochastic Gradient Descent(19896): loss=0.0009759477383312599\n",
      "Stochastic Gradient Descent(19897): loss=5.672533355866659\n",
      "Stochastic Gradient Descent(19898): loss=43.175739710251236\n",
      "Stochastic Gradient Descent(19899): loss=0.00013728829312357728\n",
      "Stochastic Gradient Descent(19900): loss=1.4891128626271468\n",
      "Stochastic Gradient Descent(19901): loss=4.742738721387845\n",
      "Stochastic Gradient Descent(19902): loss=0.41787906114091744\n",
      "Stochastic Gradient Descent(19903): loss=15.256969378424374\n",
      "Stochastic Gradient Descent(19904): loss=0.8535048499454402\n",
      "Stochastic Gradient Descent(19905): loss=0.3052017126234421\n",
      "Stochastic Gradient Descent(19906): loss=8.95064775768405\n",
      "Stochastic Gradient Descent(19907): loss=6.9560560177488\n",
      "Stochastic Gradient Descent(19908): loss=0.809929497340113\n",
      "Stochastic Gradient Descent(19909): loss=3.6145609651740025\n",
      "Stochastic Gradient Descent(19910): loss=1.6473093403445516\n",
      "Stochastic Gradient Descent(19911): loss=0.09552452113736691\n",
      "Stochastic Gradient Descent(19912): loss=0.45557547168854756\n",
      "Stochastic Gradient Descent(19913): loss=0.6009177264900024\n",
      "Stochastic Gradient Descent(19914): loss=9.864739652128621\n",
      "Stochastic Gradient Descent(19915): loss=0.8409645101670853\n",
      "Stochastic Gradient Descent(19916): loss=0.22643998416120084\n",
      "Stochastic Gradient Descent(19917): loss=1.2812468084188786\n",
      "Stochastic Gradient Descent(19918): loss=1.0775466669481382\n",
      "Stochastic Gradient Descent(19919): loss=11.958327577470923\n",
      "Stochastic Gradient Descent(19920): loss=3.21812733713675\n",
      "Stochastic Gradient Descent(19921): loss=22.10693908347396\n",
      "Stochastic Gradient Descent(19922): loss=2.0915374691038138\n",
      "Stochastic Gradient Descent(19923): loss=0.17596843745471125\n",
      "Stochastic Gradient Descent(19924): loss=2.4036511937826557\n",
      "Stochastic Gradient Descent(19925): loss=0.2383866183733007\n",
      "Stochastic Gradient Descent(19926): loss=5.397477535094976\n",
      "Stochastic Gradient Descent(19927): loss=0.22166331623056856\n",
      "Stochastic Gradient Descent(19928): loss=0.7447077216335409\n",
      "Stochastic Gradient Descent(19929): loss=6.214961346264506\n",
      "Stochastic Gradient Descent(19930): loss=3.834833216932298\n",
      "Stochastic Gradient Descent(19931): loss=16.72109206378162\n",
      "Stochastic Gradient Descent(19932): loss=0.3012315282011191\n",
      "Stochastic Gradient Descent(19933): loss=3.3540307868787553\n",
      "Stochastic Gradient Descent(19934): loss=2.263296641828467\n",
      "Stochastic Gradient Descent(19935): loss=6.477813139803329\n",
      "Stochastic Gradient Descent(19936): loss=1.759575202491786\n",
      "Stochastic Gradient Descent(19937): loss=2.847206726574979\n",
      "Stochastic Gradient Descent(19938): loss=1.7212066636133991\n",
      "Stochastic Gradient Descent(19939): loss=0.024842623113004045\n",
      "Stochastic Gradient Descent(19940): loss=0.8154958876101006\n",
      "Stochastic Gradient Descent(19941): loss=9.441886441943971\n",
      "Stochastic Gradient Descent(19942): loss=0.9968715873489052\n",
      "Stochastic Gradient Descent(19943): loss=3.2794610863074993\n",
      "Stochastic Gradient Descent(19944): loss=14.070387981503831\n",
      "Stochastic Gradient Descent(19945): loss=10.704858616256644\n",
      "Stochastic Gradient Descent(19946): loss=0.8053373386535846\n",
      "Stochastic Gradient Descent(19947): loss=0.22957644505667021\n",
      "Stochastic Gradient Descent(19948): loss=7.476258107333132\n",
      "Stochastic Gradient Descent(19949): loss=0.009263136077900795\n",
      "Stochastic Gradient Descent(19950): loss=9.473892834162953\n",
      "Stochastic Gradient Descent(19951): loss=0.14048842610697507\n",
      "Stochastic Gradient Descent(19952): loss=8.728913609188814\n",
      "Stochastic Gradient Descent(19953): loss=26.66356775010962\n",
      "Stochastic Gradient Descent(19954): loss=12.86730991209651\n",
      "Stochastic Gradient Descent(19955): loss=1.5094072098517195\n",
      "Stochastic Gradient Descent(19956): loss=6.802087695914664\n",
      "Stochastic Gradient Descent(19957): loss=0.29373068953542103\n",
      "Stochastic Gradient Descent(19958): loss=2.55229353170436\n",
      "Stochastic Gradient Descent(19959): loss=3.9108914122792937\n",
      "Stochastic Gradient Descent(19960): loss=1.276994330775081\n",
      "Stochastic Gradient Descent(19961): loss=0.13365680267378943\n",
      "Stochastic Gradient Descent(19962): loss=0.024672627900719515\n",
      "Stochastic Gradient Descent(19963): loss=5.800573957440071\n",
      "Stochastic Gradient Descent(19964): loss=8.63263820752924\n",
      "Stochastic Gradient Descent(19965): loss=7.89723284566483\n",
      "Stochastic Gradient Descent(19966): loss=2.3806918823699186\n",
      "Stochastic Gradient Descent(19967): loss=0.6251050679209336\n",
      "Stochastic Gradient Descent(19968): loss=7.479622264137872\n",
      "Stochastic Gradient Descent(19969): loss=11.849181148869205\n",
      "Stochastic Gradient Descent(19970): loss=0.7910693275427829\n",
      "Stochastic Gradient Descent(19971): loss=3.6301562007856467\n",
      "Stochastic Gradient Descent(19972): loss=0.2760458076928128\n",
      "Stochastic Gradient Descent(19973): loss=0.9333460633730514\n",
      "Stochastic Gradient Descent(19974): loss=4.699895173334545\n",
      "Stochastic Gradient Descent(19975): loss=6.519722546138003\n",
      "Stochastic Gradient Descent(19976): loss=17.908533983391923\n",
      "Stochastic Gradient Descent(19977): loss=0.023805737675551098\n",
      "Stochastic Gradient Descent(19978): loss=9.543458840660163\n",
      "Stochastic Gradient Descent(19979): loss=11.323398420734057\n",
      "Stochastic Gradient Descent(19980): loss=0.12067720203074812\n",
      "Stochastic Gradient Descent(19981): loss=7.117852789805673\n",
      "Stochastic Gradient Descent(19982): loss=0.47709206094720624\n",
      "Stochastic Gradient Descent(19983): loss=5.640524830036879\n",
      "Stochastic Gradient Descent(19984): loss=12.637224130529795\n",
      "Stochastic Gradient Descent(19985): loss=8.804030813025102\n",
      "Stochastic Gradient Descent(19986): loss=1.8161813778158693\n",
      "Stochastic Gradient Descent(19987): loss=1.041952084183761\n",
      "Stochastic Gradient Descent(19988): loss=1.040781771954986\n",
      "Stochastic Gradient Descent(19989): loss=1.1657635331988423\n",
      "Stochastic Gradient Descent(19990): loss=3.5240836183521886\n",
      "Stochastic Gradient Descent(19991): loss=0.45628689282863827\n",
      "Stochastic Gradient Descent(19992): loss=1.1692677546887853\n",
      "Stochastic Gradient Descent(19993): loss=26.313354658768215\n",
      "Stochastic Gradient Descent(19994): loss=3.8311481398129583\n",
      "Stochastic Gradient Descent(19995): loss=14.918603018567454\n",
      "Stochastic Gradient Descent(19996): loss=2.631741779968957\n",
      "Stochastic Gradient Descent(19997): loss=5.276353626035119\n",
      "Stochastic Gradient Descent(19998): loss=0.045456237304876974\n",
      "Stochastic Gradient Descent(19999): loss=0.009300722543699227\n",
      "Stochastic Gradient Descent(20000): loss=8.502857870641332\n",
      "Stochastic Gradient Descent(20001): loss=3.192540393988952\n",
      "Stochastic Gradient Descent(20002): loss=7.032218252399544\n",
      "Stochastic Gradient Descent(20003): loss=4.304086029307132\n",
      "Stochastic Gradient Descent(20004): loss=3.975066955802666\n",
      "Stochastic Gradient Descent(20005): loss=1.2773676453256857\n",
      "Stochastic Gradient Descent(20006): loss=6.975063768374281\n",
      "Stochastic Gradient Descent(20007): loss=0.5871614164514449\n",
      "Stochastic Gradient Descent(20008): loss=2.3679556053862347\n",
      "Stochastic Gradient Descent(20009): loss=0.3957327342984472\n",
      "Stochastic Gradient Descent(20010): loss=1.6657069942094458\n",
      "Stochastic Gradient Descent(20011): loss=0.18570235179707675\n",
      "Stochastic Gradient Descent(20012): loss=3.034258248684362\n",
      "Stochastic Gradient Descent(20013): loss=1.373203890583022\n",
      "Stochastic Gradient Descent(20014): loss=0.4808212903298488\n",
      "Stochastic Gradient Descent(20015): loss=0.0366260850514059\n",
      "Stochastic Gradient Descent(20016): loss=10.365223549558177\n",
      "Stochastic Gradient Descent(20017): loss=8.122281425586833\n",
      "Stochastic Gradient Descent(20018): loss=3.3505813040493613\n",
      "Stochastic Gradient Descent(20019): loss=3.4087200583984716\n",
      "Stochastic Gradient Descent(20020): loss=3.9769561752149625\n",
      "Stochastic Gradient Descent(20021): loss=0.06891924975506902\n",
      "Stochastic Gradient Descent(20022): loss=4.770870736100894\n",
      "Stochastic Gradient Descent(20023): loss=4.449165258136304\n",
      "Stochastic Gradient Descent(20024): loss=7.619461805670132\n",
      "Stochastic Gradient Descent(20025): loss=14.36928904351348\n",
      "Stochastic Gradient Descent(20026): loss=16.68009411632059\n",
      "Stochastic Gradient Descent(20027): loss=0.8905087937262354\n",
      "Stochastic Gradient Descent(20028): loss=0.1442126504864643\n",
      "Stochastic Gradient Descent(20029): loss=0.535942509716934\n",
      "Stochastic Gradient Descent(20030): loss=0.7445652607318566\n",
      "Stochastic Gradient Descent(20031): loss=2.8606705627233504\n",
      "Stochastic Gradient Descent(20032): loss=0.12659390123128497\n",
      "Stochastic Gradient Descent(20033): loss=2.0275797539757487\n",
      "Stochastic Gradient Descent(20034): loss=0.16468870558556287\n",
      "Stochastic Gradient Descent(20035): loss=11.852841054184756\n",
      "Stochastic Gradient Descent(20036): loss=0.8739762681151982\n",
      "Stochastic Gradient Descent(20037): loss=0.8574821587953694\n",
      "Stochastic Gradient Descent(20038): loss=7.44321750648705\n",
      "Stochastic Gradient Descent(20039): loss=23.005867801024657\n",
      "Stochastic Gradient Descent(20040): loss=38.46613937389103\n",
      "Stochastic Gradient Descent(20041): loss=2.1451155317214603\n",
      "Stochastic Gradient Descent(20042): loss=1.7180869412178832\n",
      "Stochastic Gradient Descent(20043): loss=14.413180904084395\n",
      "Stochastic Gradient Descent(20044): loss=0.09201839157225404\n",
      "Stochastic Gradient Descent(20045): loss=11.776517635321957\n",
      "Stochastic Gradient Descent(20046): loss=1.36907077920442\n",
      "Stochastic Gradient Descent(20047): loss=6.4626671449022615\n",
      "Stochastic Gradient Descent(20048): loss=11.88516997670427\n",
      "Stochastic Gradient Descent(20049): loss=0.016902253360548285\n",
      "Stochastic Gradient Descent(20050): loss=5.346841397921822\n",
      "Stochastic Gradient Descent(20051): loss=10.358781680404586\n",
      "Stochastic Gradient Descent(20052): loss=37.108001090378096\n",
      "Stochastic Gradient Descent(20053): loss=2.0359633715160697\n",
      "Stochastic Gradient Descent(20054): loss=3.739438346617769\n",
      "Stochastic Gradient Descent(20055): loss=2.330282593599277\n",
      "Stochastic Gradient Descent(20056): loss=0.05469039852516172\n",
      "Stochastic Gradient Descent(20057): loss=1.2914435783398883\n",
      "Stochastic Gradient Descent(20058): loss=1.9650333835572178\n",
      "Stochastic Gradient Descent(20059): loss=2.289058493361689\n",
      "Stochastic Gradient Descent(20060): loss=30.604889305094098\n",
      "Stochastic Gradient Descent(20061): loss=3.2812007764300537\n",
      "Stochastic Gradient Descent(20062): loss=0.02207414639895968\n",
      "Stochastic Gradient Descent(20063): loss=1.3833231558340973\n",
      "Stochastic Gradient Descent(20064): loss=12.226050415629386\n",
      "Stochastic Gradient Descent(20065): loss=1.7966665053546\n",
      "Stochastic Gradient Descent(20066): loss=4.797662757803095\n",
      "Stochastic Gradient Descent(20067): loss=7.390070463487116\n",
      "Stochastic Gradient Descent(20068): loss=0.12213626118381303\n",
      "Stochastic Gradient Descent(20069): loss=5.052658341042794\n",
      "Stochastic Gradient Descent(20070): loss=3.442160518866676\n",
      "Stochastic Gradient Descent(20071): loss=17.95722018082605\n",
      "Stochastic Gradient Descent(20072): loss=13.363024395743958\n",
      "Stochastic Gradient Descent(20073): loss=8.37022047475683\n",
      "Stochastic Gradient Descent(20074): loss=0.1772173411320643\n",
      "Stochastic Gradient Descent(20075): loss=32.315279729945914\n",
      "Stochastic Gradient Descent(20076): loss=46.93491960921047\n",
      "Stochastic Gradient Descent(20077): loss=0.5388775316434831\n",
      "Stochastic Gradient Descent(20078): loss=8.2552701860639\n",
      "Stochastic Gradient Descent(20079): loss=4.457964035686044\n",
      "Stochastic Gradient Descent(20080): loss=4.529217970117758\n",
      "Stochastic Gradient Descent(20081): loss=1.0087118749566766\n",
      "Stochastic Gradient Descent(20082): loss=10.984378688617406\n",
      "Stochastic Gradient Descent(20083): loss=3.9582766114085093\n",
      "Stochastic Gradient Descent(20084): loss=1.2689839912044703\n",
      "Stochastic Gradient Descent(20085): loss=0.30192132055673954\n",
      "Stochastic Gradient Descent(20086): loss=0.008557720751900118\n",
      "Stochastic Gradient Descent(20087): loss=0.8987565796568928\n",
      "Stochastic Gradient Descent(20088): loss=1.9716500918080035\n",
      "Stochastic Gradient Descent(20089): loss=0.7246888399113376\n",
      "Stochastic Gradient Descent(20090): loss=61.5242991288253\n",
      "Stochastic Gradient Descent(20091): loss=18.44833660302718\n",
      "Stochastic Gradient Descent(20092): loss=9.079736807626517\n",
      "Stochastic Gradient Descent(20093): loss=10.517044943978588\n",
      "Stochastic Gradient Descent(20094): loss=4.569680225044711\n",
      "Stochastic Gradient Descent(20095): loss=1.3494959557029964\n",
      "Stochastic Gradient Descent(20096): loss=0.48800872938696777\n",
      "Stochastic Gradient Descent(20097): loss=1.0836049054368522\n",
      "Stochastic Gradient Descent(20098): loss=0.20632410661181208\n",
      "Stochastic Gradient Descent(20099): loss=1.750008400448515\n",
      "Stochastic Gradient Descent(20100): loss=0.1084772386995602\n",
      "Stochastic Gradient Descent(20101): loss=1.8367962240964189\n",
      "Stochastic Gradient Descent(20102): loss=1.536549910452237\n",
      "Stochastic Gradient Descent(20103): loss=0.7384597069610772\n",
      "Stochastic Gradient Descent(20104): loss=2.345029143662034\n",
      "Stochastic Gradient Descent(20105): loss=0.41446044135806\n",
      "Stochastic Gradient Descent(20106): loss=9.067690408930842\n",
      "Stochastic Gradient Descent(20107): loss=0.03400179201821701\n",
      "Stochastic Gradient Descent(20108): loss=1.3444648053087116\n",
      "Stochastic Gradient Descent(20109): loss=0.6963345019745056\n",
      "Stochastic Gradient Descent(20110): loss=2.3125757130367774\n",
      "Stochastic Gradient Descent(20111): loss=0.29063937971787507\n",
      "Stochastic Gradient Descent(20112): loss=0.8108844173866612\n",
      "Stochastic Gradient Descent(20113): loss=1.8300545949166682\n",
      "Stochastic Gradient Descent(20114): loss=1.834532801358844\n",
      "Stochastic Gradient Descent(20115): loss=2.732662301991542\n",
      "Stochastic Gradient Descent(20116): loss=2.2423083932050503\n",
      "Stochastic Gradient Descent(20117): loss=0.005340070535593571\n",
      "Stochastic Gradient Descent(20118): loss=3.1802708662893284\n",
      "Stochastic Gradient Descent(20119): loss=0.463377441164866\n",
      "Stochastic Gradient Descent(20120): loss=0.42041850481476906\n",
      "Stochastic Gradient Descent(20121): loss=19.616388441199245\n",
      "Stochastic Gradient Descent(20122): loss=0.801823223180629\n",
      "Stochastic Gradient Descent(20123): loss=1.1606118343361338\n",
      "Stochastic Gradient Descent(20124): loss=6.8251870930970115\n",
      "Stochastic Gradient Descent(20125): loss=2.5088670565257307\n",
      "Stochastic Gradient Descent(20126): loss=0.5652459356295338\n",
      "Stochastic Gradient Descent(20127): loss=1.2942905921222718\n",
      "Stochastic Gradient Descent(20128): loss=0.022508234373486338\n",
      "Stochastic Gradient Descent(20129): loss=5.189745038444251\n",
      "Stochastic Gradient Descent(20130): loss=3.052032408456322\n",
      "Stochastic Gradient Descent(20131): loss=5.3991560141112505\n",
      "Stochastic Gradient Descent(20132): loss=1.5345405574996285\n",
      "Stochastic Gradient Descent(20133): loss=13.5185819007671\n",
      "Stochastic Gradient Descent(20134): loss=6.159345007650605\n",
      "Stochastic Gradient Descent(20135): loss=0.660803322553574\n",
      "Stochastic Gradient Descent(20136): loss=3.976778872055311\n",
      "Stochastic Gradient Descent(20137): loss=12.208043480438386\n",
      "Stochastic Gradient Descent(20138): loss=1.3146016889932957\n",
      "Stochastic Gradient Descent(20139): loss=9.833469687690908\n",
      "Stochastic Gradient Descent(20140): loss=7.494057503018437\n",
      "Stochastic Gradient Descent(20141): loss=0.005020246562956619\n",
      "Stochastic Gradient Descent(20142): loss=10.852147158680951\n",
      "Stochastic Gradient Descent(20143): loss=2.342372631137952\n",
      "Stochastic Gradient Descent(20144): loss=6.50670048671545\n",
      "Stochastic Gradient Descent(20145): loss=17.472610138873222\n",
      "Stochastic Gradient Descent(20146): loss=38.89389463016098\n",
      "Stochastic Gradient Descent(20147): loss=7.189904811830289\n",
      "Stochastic Gradient Descent(20148): loss=1.4718918264955168\n",
      "Stochastic Gradient Descent(20149): loss=0.40600445098567806\n",
      "Stochastic Gradient Descent(20150): loss=0.001112974447539694\n",
      "Stochastic Gradient Descent(20151): loss=16.36184370235471\n",
      "Stochastic Gradient Descent(20152): loss=6.244489717321386\n",
      "Stochastic Gradient Descent(20153): loss=4.626820866037306\n",
      "Stochastic Gradient Descent(20154): loss=2.635557311718847\n",
      "Stochastic Gradient Descent(20155): loss=1.700045159881927\n",
      "Stochastic Gradient Descent(20156): loss=1.068536058675341\n",
      "Stochastic Gradient Descent(20157): loss=22.83111046080145\n",
      "Stochastic Gradient Descent(20158): loss=0.9328046453863353\n",
      "Stochastic Gradient Descent(20159): loss=5.880587764748144\n",
      "Stochastic Gradient Descent(20160): loss=9.14515998322767\n",
      "Stochastic Gradient Descent(20161): loss=24.526361780275607\n",
      "Stochastic Gradient Descent(20162): loss=1.6403579622215274\n",
      "Stochastic Gradient Descent(20163): loss=3.2353982626390327\n",
      "Stochastic Gradient Descent(20164): loss=13.140354542935007\n",
      "Stochastic Gradient Descent(20165): loss=0.12337998048242743\n",
      "Stochastic Gradient Descent(20166): loss=2.377603428284967\n",
      "Stochastic Gradient Descent(20167): loss=0.16434102401388223\n",
      "Stochastic Gradient Descent(20168): loss=1.3990034269736664\n",
      "Stochastic Gradient Descent(20169): loss=28.415251904708317\n",
      "Stochastic Gradient Descent(20170): loss=0.39455179760067444\n",
      "Stochastic Gradient Descent(20171): loss=0.6300681125187418\n",
      "Stochastic Gradient Descent(20172): loss=10.23863344093347\n",
      "Stochastic Gradient Descent(20173): loss=5.065518772352213\n",
      "Stochastic Gradient Descent(20174): loss=5.166844364671356\n",
      "Stochastic Gradient Descent(20175): loss=0.45944072974214656\n",
      "Stochastic Gradient Descent(20176): loss=0.11057943003557691\n",
      "Stochastic Gradient Descent(20177): loss=1.2234938233717139\n",
      "Stochastic Gradient Descent(20178): loss=13.473713918325268\n",
      "Stochastic Gradient Descent(20179): loss=14.624745535283648\n",
      "Stochastic Gradient Descent(20180): loss=0.35179903810263585\n",
      "Stochastic Gradient Descent(20181): loss=2.2601467433707096\n",
      "Stochastic Gradient Descent(20182): loss=6.08210172858207\n",
      "Stochastic Gradient Descent(20183): loss=0.2994067408293505\n",
      "Stochastic Gradient Descent(20184): loss=0.5886010263868656\n",
      "Stochastic Gradient Descent(20185): loss=9.261684855110872\n",
      "Stochastic Gradient Descent(20186): loss=30.594114255337498\n",
      "Stochastic Gradient Descent(20187): loss=0.31693645324005404\n",
      "Stochastic Gradient Descent(20188): loss=0.3876204465164308\n",
      "Stochastic Gradient Descent(20189): loss=3.729044693441983\n",
      "Stochastic Gradient Descent(20190): loss=0.12483711278745066\n",
      "Stochastic Gradient Descent(20191): loss=6.2404550720335825\n",
      "Stochastic Gradient Descent(20192): loss=7.202791484465095\n",
      "Stochastic Gradient Descent(20193): loss=8.59260839516567\n",
      "Stochastic Gradient Descent(20194): loss=0.11687592053785865\n",
      "Stochastic Gradient Descent(20195): loss=2.1348712837854498\n",
      "Stochastic Gradient Descent(20196): loss=0.5895164430740251\n",
      "Stochastic Gradient Descent(20197): loss=0.41590306410959166\n",
      "Stochastic Gradient Descent(20198): loss=0.01885167972089789\n",
      "Stochastic Gradient Descent(20199): loss=3.0674020267975\n",
      "Stochastic Gradient Descent(20200): loss=4.844649935879752\n",
      "Stochastic Gradient Descent(20201): loss=22.54270018712204\n",
      "Stochastic Gradient Descent(20202): loss=7.578194140243672\n",
      "Stochastic Gradient Descent(20203): loss=14.695518879843156\n",
      "Stochastic Gradient Descent(20204): loss=0.95667524203667\n",
      "Stochastic Gradient Descent(20205): loss=5.633784901243215\n",
      "Stochastic Gradient Descent(20206): loss=21.575461170491636\n",
      "Stochastic Gradient Descent(20207): loss=8.174770199719708\n",
      "Stochastic Gradient Descent(20208): loss=0.03867960568289981\n",
      "Stochastic Gradient Descent(20209): loss=13.31296391813705\n",
      "Stochastic Gradient Descent(20210): loss=12.281869635143341\n",
      "Stochastic Gradient Descent(20211): loss=1.498702749910582\n",
      "Stochastic Gradient Descent(20212): loss=3.502303749269991\n",
      "Stochastic Gradient Descent(20213): loss=1.1826452746442686\n",
      "Stochastic Gradient Descent(20214): loss=0.24840798014631368\n",
      "Stochastic Gradient Descent(20215): loss=0.65939210636189\n",
      "Stochastic Gradient Descent(20216): loss=13.176255344023426\n",
      "Stochastic Gradient Descent(20217): loss=12.133620949632562\n",
      "Stochastic Gradient Descent(20218): loss=0.9314514924444328\n",
      "Stochastic Gradient Descent(20219): loss=35.65219801619417\n",
      "Stochastic Gradient Descent(20220): loss=3.2398137060530403\n",
      "Stochastic Gradient Descent(20221): loss=2.255773323899423\n",
      "Stochastic Gradient Descent(20222): loss=29.681132120959596\n",
      "Stochastic Gradient Descent(20223): loss=3.721707743994699\n",
      "Stochastic Gradient Descent(20224): loss=8.962740131396404e-05\n",
      "Stochastic Gradient Descent(20225): loss=0.0018218380488713316\n",
      "Stochastic Gradient Descent(20226): loss=2.238896365730711\n",
      "Stochastic Gradient Descent(20227): loss=0.013582940365731454\n",
      "Stochastic Gradient Descent(20228): loss=0.30844133944025326\n",
      "Stochastic Gradient Descent(20229): loss=1.0307946473167564\n",
      "Stochastic Gradient Descent(20230): loss=0.09824933058782616\n",
      "Stochastic Gradient Descent(20231): loss=17.00204300180761\n",
      "Stochastic Gradient Descent(20232): loss=0.23989568613233614\n",
      "Stochastic Gradient Descent(20233): loss=0.3147899297860764\n",
      "Stochastic Gradient Descent(20234): loss=1.437597721783351\n",
      "Stochastic Gradient Descent(20235): loss=1.97458646557965\n",
      "Stochastic Gradient Descent(20236): loss=4.481360674097137\n",
      "Stochastic Gradient Descent(20237): loss=0.2915327133478071\n",
      "Stochastic Gradient Descent(20238): loss=0.8582452933444567\n",
      "Stochastic Gradient Descent(20239): loss=9.227298522055712\n",
      "Stochastic Gradient Descent(20240): loss=1.6203900142932637\n",
      "Stochastic Gradient Descent(20241): loss=9.268497151541435\n",
      "Stochastic Gradient Descent(20242): loss=4.606295818470439\n",
      "Stochastic Gradient Descent(20243): loss=3.2247398756561974\n",
      "Stochastic Gradient Descent(20244): loss=4.7472349396429445\n",
      "Stochastic Gradient Descent(20245): loss=2.6225520070367745\n",
      "Stochastic Gradient Descent(20246): loss=28.924744262565433\n",
      "Stochastic Gradient Descent(20247): loss=3.3324146261182226\n",
      "Stochastic Gradient Descent(20248): loss=10.456388236755561\n",
      "Stochastic Gradient Descent(20249): loss=1.5428086535561922\n",
      "Stochastic Gradient Descent(20250): loss=5.09658864110653\n",
      "Stochastic Gradient Descent(20251): loss=4.630475248759221\n",
      "Stochastic Gradient Descent(20252): loss=11.94535660507174\n",
      "Stochastic Gradient Descent(20253): loss=0.12055454759108711\n",
      "Stochastic Gradient Descent(20254): loss=0.2705404977377308\n",
      "Stochastic Gradient Descent(20255): loss=8.645132966955158\n",
      "Stochastic Gradient Descent(20256): loss=0.17053177310827597\n",
      "Stochastic Gradient Descent(20257): loss=2.609783585289521\n",
      "Stochastic Gradient Descent(20258): loss=0.4003862080620208\n",
      "Stochastic Gradient Descent(20259): loss=7.273551821386753\n",
      "Stochastic Gradient Descent(20260): loss=9.433181556730075\n",
      "Stochastic Gradient Descent(20261): loss=6.618434326085474\n",
      "Stochastic Gradient Descent(20262): loss=16.75086123059763\n",
      "Stochastic Gradient Descent(20263): loss=0.9890851356656144\n",
      "Stochastic Gradient Descent(20264): loss=0.6057737832459027\n",
      "Stochastic Gradient Descent(20265): loss=8.623260637833082\n",
      "Stochastic Gradient Descent(20266): loss=2.4186333766326547\n",
      "Stochastic Gradient Descent(20267): loss=4.446274548914303\n",
      "Stochastic Gradient Descent(20268): loss=21.535281903706064\n",
      "Stochastic Gradient Descent(20269): loss=0.04065844668708842\n",
      "Stochastic Gradient Descent(20270): loss=5.592130049951982\n",
      "Stochastic Gradient Descent(20271): loss=1.005723739285115\n",
      "Stochastic Gradient Descent(20272): loss=3.660194216820786\n",
      "Stochastic Gradient Descent(20273): loss=0.0022717499080873475\n",
      "Stochastic Gradient Descent(20274): loss=2.1184120645958164\n",
      "Stochastic Gradient Descent(20275): loss=16.636721614003662\n",
      "Stochastic Gradient Descent(20276): loss=0.06448444603002922\n",
      "Stochastic Gradient Descent(20277): loss=0.14034919195685164\n",
      "Stochastic Gradient Descent(20278): loss=2.357628436995971\n",
      "Stochastic Gradient Descent(20279): loss=1.3216859084130002\n",
      "Stochastic Gradient Descent(20280): loss=14.141304284640828\n",
      "Stochastic Gradient Descent(20281): loss=15.191049738974032\n",
      "Stochastic Gradient Descent(20282): loss=0.002693049481686999\n",
      "Stochastic Gradient Descent(20283): loss=1.3626982363265663\n",
      "Stochastic Gradient Descent(20284): loss=0.681132293797032\n",
      "Stochastic Gradient Descent(20285): loss=2.469774416780054\n",
      "Stochastic Gradient Descent(20286): loss=1.219825032492278\n",
      "Stochastic Gradient Descent(20287): loss=6.609083940592014\n",
      "Stochastic Gradient Descent(20288): loss=4.2626721380961525\n",
      "Stochastic Gradient Descent(20289): loss=1.5389563153309362\n",
      "Stochastic Gradient Descent(20290): loss=1.2302238191264208\n",
      "Stochastic Gradient Descent(20291): loss=2.354386155953667\n",
      "Stochastic Gradient Descent(20292): loss=0.18097510431910924\n",
      "Stochastic Gradient Descent(20293): loss=3.3676867458590376\n",
      "Stochastic Gradient Descent(20294): loss=0.08628384742753195\n",
      "Stochastic Gradient Descent(20295): loss=50.133188419708425\n",
      "Stochastic Gradient Descent(20296): loss=18.47523286405584\n",
      "Stochastic Gradient Descent(20297): loss=49.89881797576061\n",
      "Stochastic Gradient Descent(20298): loss=16.83461759645456\n",
      "Stochastic Gradient Descent(20299): loss=28.315180895096546\n",
      "Stochastic Gradient Descent(20300): loss=0.09234661165822387\n",
      "Stochastic Gradient Descent(20301): loss=0.7175188435065195\n",
      "Stochastic Gradient Descent(20302): loss=7.8507267372366405\n",
      "Stochastic Gradient Descent(20303): loss=0.04831841279422748\n",
      "Stochastic Gradient Descent(20304): loss=0.645233925046217\n",
      "Stochastic Gradient Descent(20305): loss=0.09575069296150869\n",
      "Stochastic Gradient Descent(20306): loss=0.0064796897528534275\n",
      "Stochastic Gradient Descent(20307): loss=3.744835810348407\n",
      "Stochastic Gradient Descent(20308): loss=0.5200136024859323\n",
      "Stochastic Gradient Descent(20309): loss=0.0028308637640426936\n",
      "Stochastic Gradient Descent(20310): loss=0.8379141463308848\n",
      "Stochastic Gradient Descent(20311): loss=3.5662706448756682\n",
      "Stochastic Gradient Descent(20312): loss=23.367017279280283\n",
      "Stochastic Gradient Descent(20313): loss=0.4652313867529568\n",
      "Stochastic Gradient Descent(20314): loss=2.3535163339639498\n",
      "Stochastic Gradient Descent(20315): loss=3.418723053371417\n",
      "Stochastic Gradient Descent(20316): loss=6.712189700589755\n",
      "Stochastic Gradient Descent(20317): loss=0.31647316300671585\n",
      "Stochastic Gradient Descent(20318): loss=0.08806836779514428\n",
      "Stochastic Gradient Descent(20319): loss=0.8182364393202926\n",
      "Stochastic Gradient Descent(20320): loss=1.0542884128474672\n",
      "Stochastic Gradient Descent(20321): loss=2.7098082894617104\n",
      "Stochastic Gradient Descent(20322): loss=0.45039975316774894\n",
      "Stochastic Gradient Descent(20323): loss=0.39699027581854635\n",
      "Stochastic Gradient Descent(20324): loss=0.23238805831312367\n",
      "Stochastic Gradient Descent(20325): loss=1.6733011820671986\n",
      "Stochastic Gradient Descent(20326): loss=17.43151811117822\n",
      "Stochastic Gradient Descent(20327): loss=15.396657074902748\n",
      "Stochastic Gradient Descent(20328): loss=1.1082415295821382\n",
      "Stochastic Gradient Descent(20329): loss=1.727128805598885\n",
      "Stochastic Gradient Descent(20330): loss=0.32758829387012217\n",
      "Stochastic Gradient Descent(20331): loss=13.88315013464328\n",
      "Stochastic Gradient Descent(20332): loss=0.7784678162873638\n",
      "Stochastic Gradient Descent(20333): loss=12.999208075920196\n",
      "Stochastic Gradient Descent(20334): loss=0.985009145914539\n",
      "Stochastic Gradient Descent(20335): loss=2.910915011942757\n",
      "Stochastic Gradient Descent(20336): loss=11.902463371590546\n",
      "Stochastic Gradient Descent(20337): loss=5.183303030406101\n",
      "Stochastic Gradient Descent(20338): loss=2.492743070450081\n",
      "Stochastic Gradient Descent(20339): loss=0.009714485020047731\n",
      "Stochastic Gradient Descent(20340): loss=1.660430949977203\n",
      "Stochastic Gradient Descent(20341): loss=11.226264748895963\n",
      "Stochastic Gradient Descent(20342): loss=2.0965256766002867\n",
      "Stochastic Gradient Descent(20343): loss=0.8142785341835873\n",
      "Stochastic Gradient Descent(20344): loss=1.9116653620240776\n",
      "Stochastic Gradient Descent(20345): loss=0.10719140661352987\n",
      "Stochastic Gradient Descent(20346): loss=16.340299799434973\n",
      "Stochastic Gradient Descent(20347): loss=1.3962880375172066\n",
      "Stochastic Gradient Descent(20348): loss=25.496604623021668\n",
      "Stochastic Gradient Descent(20349): loss=0.22896549012274625\n",
      "Stochastic Gradient Descent(20350): loss=0.9555955525867937\n",
      "Stochastic Gradient Descent(20351): loss=2.05871280004838\n",
      "Stochastic Gradient Descent(20352): loss=6.963786521875466\n",
      "Stochastic Gradient Descent(20353): loss=1.9167769138694257\n",
      "Stochastic Gradient Descent(20354): loss=3.4273969787171574\n",
      "Stochastic Gradient Descent(20355): loss=16.905225335281905\n",
      "Stochastic Gradient Descent(20356): loss=17.545691959496796\n",
      "Stochastic Gradient Descent(20357): loss=0.2447339242562829\n",
      "Stochastic Gradient Descent(20358): loss=0.17114349558462102\n",
      "Stochastic Gradient Descent(20359): loss=23.31427578576678\n",
      "Stochastic Gradient Descent(20360): loss=0.5771277180059596\n",
      "Stochastic Gradient Descent(20361): loss=4.067731654738248\n",
      "Stochastic Gradient Descent(20362): loss=0.10054973839024031\n",
      "Stochastic Gradient Descent(20363): loss=0.7505425573874551\n",
      "Stochastic Gradient Descent(20364): loss=12.201873298626523\n",
      "Stochastic Gradient Descent(20365): loss=1.6194522913370495\n",
      "Stochastic Gradient Descent(20366): loss=8.445411117598232\n",
      "Stochastic Gradient Descent(20367): loss=2.6132951926402566\n",
      "Stochastic Gradient Descent(20368): loss=0.004513619363666314\n",
      "Stochastic Gradient Descent(20369): loss=12.758385161738039\n",
      "Stochastic Gradient Descent(20370): loss=7.5934382177473445\n",
      "Stochastic Gradient Descent(20371): loss=4.3014194332860605\n",
      "Stochastic Gradient Descent(20372): loss=43.74101424870969\n",
      "Stochastic Gradient Descent(20373): loss=17.036729942736343\n",
      "Stochastic Gradient Descent(20374): loss=2.351226823922089\n",
      "Stochastic Gradient Descent(20375): loss=10.551580299238937\n",
      "Stochastic Gradient Descent(20376): loss=30.081449951494925\n",
      "Stochastic Gradient Descent(20377): loss=0.16781961097999898\n",
      "Stochastic Gradient Descent(20378): loss=2.800593636761005\n",
      "Stochastic Gradient Descent(20379): loss=0.060659997884975496\n",
      "Stochastic Gradient Descent(20380): loss=0.0004644865300073502\n",
      "Stochastic Gradient Descent(20381): loss=4.486115266957877\n",
      "Stochastic Gradient Descent(20382): loss=0.03653181752640878\n",
      "Stochastic Gradient Descent(20383): loss=0.005951316664149315\n",
      "Stochastic Gradient Descent(20384): loss=17.443608711546418\n",
      "Stochastic Gradient Descent(20385): loss=0.3375399983398939\n",
      "Stochastic Gradient Descent(20386): loss=0.5300627577408237\n",
      "Stochastic Gradient Descent(20387): loss=1.2208260760781164\n",
      "Stochastic Gradient Descent(20388): loss=0.31120758036293933\n",
      "Stochastic Gradient Descent(20389): loss=3.762809914954482\n",
      "Stochastic Gradient Descent(20390): loss=0.08591537827067484\n",
      "Stochastic Gradient Descent(20391): loss=0.8362442418583115\n",
      "Stochastic Gradient Descent(20392): loss=1.6788904250251206\n",
      "Stochastic Gradient Descent(20393): loss=1.026907987024832\n",
      "Stochastic Gradient Descent(20394): loss=1.0762420683818674\n",
      "Stochastic Gradient Descent(20395): loss=0.9252756533894325\n",
      "Stochastic Gradient Descent(20396): loss=2.0311707952658082\n",
      "Stochastic Gradient Descent(20397): loss=1.2351608951357629\n",
      "Stochastic Gradient Descent(20398): loss=0.793031375326866\n",
      "Stochastic Gradient Descent(20399): loss=1.587755068586399\n",
      "Stochastic Gradient Descent(20400): loss=8.91851102580223e-05\n",
      "Stochastic Gradient Descent(20401): loss=1.001032586543528\n",
      "Stochastic Gradient Descent(20402): loss=5.327763334074834\n",
      "Stochastic Gradient Descent(20403): loss=1.8668101966253674\n",
      "Stochastic Gradient Descent(20404): loss=6.010284321713299\n",
      "Stochastic Gradient Descent(20405): loss=1.1446713253604306\n",
      "Stochastic Gradient Descent(20406): loss=2.9621472955868815\n",
      "Stochastic Gradient Descent(20407): loss=0.20786521070431738\n",
      "Stochastic Gradient Descent(20408): loss=3.369138871922019\n",
      "Stochastic Gradient Descent(20409): loss=0.35520715289175936\n",
      "Stochastic Gradient Descent(20410): loss=1.812259931820263\n",
      "Stochastic Gradient Descent(20411): loss=11.220504822261733\n",
      "Stochastic Gradient Descent(20412): loss=3.0093311170877493\n",
      "Stochastic Gradient Descent(20413): loss=0.0059706474106296465\n",
      "Stochastic Gradient Descent(20414): loss=3.548048267448387\n",
      "Stochastic Gradient Descent(20415): loss=0.5743037934921905\n",
      "Stochastic Gradient Descent(20416): loss=1.5081977988323572\n",
      "Stochastic Gradient Descent(20417): loss=2.325440553793321\n",
      "Stochastic Gradient Descent(20418): loss=0.10179128687224585\n",
      "Stochastic Gradient Descent(20419): loss=0.6609041906630667\n",
      "Stochastic Gradient Descent(20420): loss=2.999913105526241\n",
      "Stochastic Gradient Descent(20421): loss=2.761866767674091\n",
      "Stochastic Gradient Descent(20422): loss=1.780779231893058\n",
      "Stochastic Gradient Descent(20423): loss=16.06382366091696\n",
      "Stochastic Gradient Descent(20424): loss=0.028746486503734518\n",
      "Stochastic Gradient Descent(20425): loss=0.28084434957037563\n",
      "Stochastic Gradient Descent(20426): loss=0.7605939368233899\n",
      "Stochastic Gradient Descent(20427): loss=5.090765343461484\n",
      "Stochastic Gradient Descent(20428): loss=9.536405220685625\n",
      "Stochastic Gradient Descent(20429): loss=0.14672667479815513\n",
      "Stochastic Gradient Descent(20430): loss=1.2963531237034012\n",
      "Stochastic Gradient Descent(20431): loss=0.370610925307973\n",
      "Stochastic Gradient Descent(20432): loss=0.39271427076920384\n",
      "Stochastic Gradient Descent(20433): loss=0.482373406774255\n",
      "Stochastic Gradient Descent(20434): loss=0.020117068472880688\n",
      "Stochastic Gradient Descent(20435): loss=1.092860185736912\n",
      "Stochastic Gradient Descent(20436): loss=0.21967679762081405\n",
      "Stochastic Gradient Descent(20437): loss=6.080220396907083\n",
      "Stochastic Gradient Descent(20438): loss=15.413050976547488\n",
      "Stochastic Gradient Descent(20439): loss=1.9472658610790385\n",
      "Stochastic Gradient Descent(20440): loss=2.537369713759736\n",
      "Stochastic Gradient Descent(20441): loss=7.742081671504273\n",
      "Stochastic Gradient Descent(20442): loss=30.59966999562937\n",
      "Stochastic Gradient Descent(20443): loss=6.166909938377757\n",
      "Stochastic Gradient Descent(20444): loss=9.300463243580987\n",
      "Stochastic Gradient Descent(20445): loss=5.41491854843575\n",
      "Stochastic Gradient Descent(20446): loss=0.5687439797686463\n",
      "Stochastic Gradient Descent(20447): loss=0.5856784994844703\n",
      "Stochastic Gradient Descent(20448): loss=0.23128990735378321\n",
      "Stochastic Gradient Descent(20449): loss=5.92088998394246\n",
      "Stochastic Gradient Descent(20450): loss=0.14842113827651865\n",
      "Stochastic Gradient Descent(20451): loss=0.6061138055154786\n",
      "Stochastic Gradient Descent(20452): loss=0.17906952973737278\n",
      "Stochastic Gradient Descent(20453): loss=1.76920988120941\n",
      "Stochastic Gradient Descent(20454): loss=0.6536853892462556\n",
      "Stochastic Gradient Descent(20455): loss=0.5011300730023885\n",
      "Stochastic Gradient Descent(20456): loss=9.309080067576492\n",
      "Stochastic Gradient Descent(20457): loss=0.4631840640537685\n",
      "Stochastic Gradient Descent(20458): loss=2.4644096118051277\n",
      "Stochastic Gradient Descent(20459): loss=0.9751957957146073\n",
      "Stochastic Gradient Descent(20460): loss=0.3146152907701664\n",
      "Stochastic Gradient Descent(20461): loss=0.4548796611728727\n",
      "Stochastic Gradient Descent(20462): loss=18.414423547562436\n",
      "Stochastic Gradient Descent(20463): loss=3.5242830087071964\n",
      "Stochastic Gradient Descent(20464): loss=2.9390066377316706\n",
      "Stochastic Gradient Descent(20465): loss=0.4507183870852954\n",
      "Stochastic Gradient Descent(20466): loss=6.026871285359994\n",
      "Stochastic Gradient Descent(20467): loss=0.5842231342836889\n",
      "Stochastic Gradient Descent(20468): loss=1.7479707871728656\n",
      "Stochastic Gradient Descent(20469): loss=0.07337376117415309\n",
      "Stochastic Gradient Descent(20470): loss=12.578986304401278\n",
      "Stochastic Gradient Descent(20471): loss=0.43049831069740957\n",
      "Stochastic Gradient Descent(20472): loss=1.596320669134248\n",
      "Stochastic Gradient Descent(20473): loss=0.07070046897492659\n",
      "Stochastic Gradient Descent(20474): loss=2.2183153426957887\n",
      "Stochastic Gradient Descent(20475): loss=3.106142580081862\n",
      "Stochastic Gradient Descent(20476): loss=0.8812079937617178\n",
      "Stochastic Gradient Descent(20477): loss=6.2813230842022305\n",
      "Stochastic Gradient Descent(20478): loss=7.362828484163169\n",
      "Stochastic Gradient Descent(20479): loss=0.8121653516740336\n",
      "Stochastic Gradient Descent(20480): loss=0.5675338854399615\n",
      "Stochastic Gradient Descent(20481): loss=3.1915992567448797\n",
      "Stochastic Gradient Descent(20482): loss=0.8920036730397891\n",
      "Stochastic Gradient Descent(20483): loss=0.0006954648969958597\n",
      "Stochastic Gradient Descent(20484): loss=2.1040771378640137\n",
      "Stochastic Gradient Descent(20485): loss=0.29703976457758036\n",
      "Stochastic Gradient Descent(20486): loss=6.828957701732189\n",
      "Stochastic Gradient Descent(20487): loss=9.16314952105921\n",
      "Stochastic Gradient Descent(20488): loss=1.3985661803735299\n",
      "Stochastic Gradient Descent(20489): loss=2.2647867365338485\n",
      "Stochastic Gradient Descent(20490): loss=0.5072205011561238\n",
      "Stochastic Gradient Descent(20491): loss=4.054156665205991\n",
      "Stochastic Gradient Descent(20492): loss=1.5815717065741608\n",
      "Stochastic Gradient Descent(20493): loss=7.055806666001756\n",
      "Stochastic Gradient Descent(20494): loss=0.0173965672751677\n",
      "Stochastic Gradient Descent(20495): loss=0.27369766892954206\n",
      "Stochastic Gradient Descent(20496): loss=2.1828419347637475\n",
      "Stochastic Gradient Descent(20497): loss=0.06458906412338251\n",
      "Stochastic Gradient Descent(20498): loss=4.385227898557288\n",
      "Stochastic Gradient Descent(20499): loss=1.511934659878493\n",
      "Stochastic Gradient Descent(20500): loss=3.3228362079569482\n",
      "Stochastic Gradient Descent(20501): loss=0.4416498258160425\n",
      "Stochastic Gradient Descent(20502): loss=2.7378250677165052\n",
      "Stochastic Gradient Descent(20503): loss=3.8996970845977708\n",
      "Stochastic Gradient Descent(20504): loss=1.4285354771399064\n",
      "Stochastic Gradient Descent(20505): loss=2.8881985539915425\n",
      "Stochastic Gradient Descent(20506): loss=1.8157051619987572\n",
      "Stochastic Gradient Descent(20507): loss=2.3263313029909374\n",
      "Stochastic Gradient Descent(20508): loss=4.004455581349194\n",
      "Stochastic Gradient Descent(20509): loss=1.019199354922623\n",
      "Stochastic Gradient Descent(20510): loss=4.472835475754857\n",
      "Stochastic Gradient Descent(20511): loss=0.7073547387406925\n",
      "Stochastic Gradient Descent(20512): loss=1.1978920441124472\n",
      "Stochastic Gradient Descent(20513): loss=0.6617599302402954\n",
      "Stochastic Gradient Descent(20514): loss=0.1381438616423035\n",
      "Stochastic Gradient Descent(20515): loss=1.3213941310071078\n",
      "Stochastic Gradient Descent(20516): loss=0.4144478121146753\n",
      "Stochastic Gradient Descent(20517): loss=0.012441081812955735\n",
      "Stochastic Gradient Descent(20518): loss=2.126426731946396\n",
      "Stochastic Gradient Descent(20519): loss=0.37585016422719375\n",
      "Stochastic Gradient Descent(20520): loss=3.592784314455919\n",
      "Stochastic Gradient Descent(20521): loss=4.272902451491816\n",
      "Stochastic Gradient Descent(20522): loss=6.0335926402691715\n",
      "Stochastic Gradient Descent(20523): loss=3.010142190310124\n",
      "Stochastic Gradient Descent(20524): loss=2.719535962026032\n",
      "Stochastic Gradient Descent(20525): loss=3.6420163783823987\n",
      "Stochastic Gradient Descent(20526): loss=0.04031488614416877\n",
      "Stochastic Gradient Descent(20527): loss=1.2980510869606157\n",
      "Stochastic Gradient Descent(20528): loss=4.851104864819948e-06\n",
      "Stochastic Gradient Descent(20529): loss=0.03086822023622678\n",
      "Stochastic Gradient Descent(20530): loss=0.7478610625598082\n",
      "Stochastic Gradient Descent(20531): loss=0.4878183637098892\n",
      "Stochastic Gradient Descent(20532): loss=0.008375404392215161\n",
      "Stochastic Gradient Descent(20533): loss=9.050006683063586\n",
      "Stochastic Gradient Descent(20534): loss=4.531453509979116\n",
      "Stochastic Gradient Descent(20535): loss=3.5372468383702227\n",
      "Stochastic Gradient Descent(20536): loss=10.773491976392172\n",
      "Stochastic Gradient Descent(20537): loss=4.0888622663225815\n",
      "Stochastic Gradient Descent(20538): loss=1.1209307411074914\n",
      "Stochastic Gradient Descent(20539): loss=0.06502919252970654\n",
      "Stochastic Gradient Descent(20540): loss=0.26682898240488\n",
      "Stochastic Gradient Descent(20541): loss=0.13101263451970208\n",
      "Stochastic Gradient Descent(20542): loss=1.0452004305799203\n",
      "Stochastic Gradient Descent(20543): loss=5.951185195450013\n",
      "Stochastic Gradient Descent(20544): loss=2.0774080767396983\n",
      "Stochastic Gradient Descent(20545): loss=18.523899843563203\n",
      "Stochastic Gradient Descent(20546): loss=0.13775910151306417\n",
      "Stochastic Gradient Descent(20547): loss=8.638928314111435\n",
      "Stochastic Gradient Descent(20548): loss=0.1285629474446937\n",
      "Stochastic Gradient Descent(20549): loss=1.186673025359426\n",
      "Stochastic Gradient Descent(20550): loss=0.0551360244974032\n",
      "Stochastic Gradient Descent(20551): loss=3.315643509851701\n",
      "Stochastic Gradient Descent(20552): loss=1.1536745921380438\n",
      "Stochastic Gradient Descent(20553): loss=1.2728277629157352\n",
      "Stochastic Gradient Descent(20554): loss=3.2244991818205553\n",
      "Stochastic Gradient Descent(20555): loss=0.3105335895342888\n",
      "Stochastic Gradient Descent(20556): loss=0.37404487520408014\n",
      "Stochastic Gradient Descent(20557): loss=54.50618491535441\n",
      "Stochastic Gradient Descent(20558): loss=155.75357587717184\n",
      "Stochastic Gradient Descent(20559): loss=21.117625151394595\n",
      "Stochastic Gradient Descent(20560): loss=5.41367490760853\n",
      "Stochastic Gradient Descent(20561): loss=10.117799732893184\n",
      "Stochastic Gradient Descent(20562): loss=3.0506698587854992\n",
      "Stochastic Gradient Descent(20563): loss=0.005775444456932945\n",
      "Stochastic Gradient Descent(20564): loss=1.8158244490167632\n",
      "Stochastic Gradient Descent(20565): loss=0.24140650752634152\n",
      "Stochastic Gradient Descent(20566): loss=0.21153709666722198\n",
      "Stochastic Gradient Descent(20567): loss=0.3563169413192309\n",
      "Stochastic Gradient Descent(20568): loss=0.05583297347126287\n",
      "Stochastic Gradient Descent(20569): loss=24.768873071737847\n",
      "Stochastic Gradient Descent(20570): loss=0.844222269008765\n",
      "Stochastic Gradient Descent(20571): loss=1.2042942225969453\n",
      "Stochastic Gradient Descent(20572): loss=4.69277116749688\n",
      "Stochastic Gradient Descent(20573): loss=3.0972036737867166\n",
      "Stochastic Gradient Descent(20574): loss=13.537510358874382\n",
      "Stochastic Gradient Descent(20575): loss=12.599601788428538\n",
      "Stochastic Gradient Descent(20576): loss=2.355059825023582\n",
      "Stochastic Gradient Descent(20577): loss=0.008947902827681951\n",
      "Stochastic Gradient Descent(20578): loss=0.6375730459317375\n",
      "Stochastic Gradient Descent(20579): loss=3.304680285802173\n",
      "Stochastic Gradient Descent(20580): loss=0.8855492423144562\n",
      "Stochastic Gradient Descent(20581): loss=1.8162049605903052\n",
      "Stochastic Gradient Descent(20582): loss=5.720134791807229\n",
      "Stochastic Gradient Descent(20583): loss=2.725959615848078\n",
      "Stochastic Gradient Descent(20584): loss=6.369892066321832\n",
      "Stochastic Gradient Descent(20585): loss=8.43915288391084e-09\n",
      "Stochastic Gradient Descent(20586): loss=0.14781228542970618\n",
      "Stochastic Gradient Descent(20587): loss=6.159490047193444\n",
      "Stochastic Gradient Descent(20588): loss=2.614101112137359\n",
      "Stochastic Gradient Descent(20589): loss=0.5057122415942927\n",
      "Stochastic Gradient Descent(20590): loss=3.547427403194056\n",
      "Stochastic Gradient Descent(20591): loss=2.257039168940008\n",
      "Stochastic Gradient Descent(20592): loss=0.21902868768718717\n",
      "Stochastic Gradient Descent(20593): loss=3.088004470318929\n",
      "Stochastic Gradient Descent(20594): loss=1.2593224850280669\n",
      "Stochastic Gradient Descent(20595): loss=1.5028979433834244\n",
      "Stochastic Gradient Descent(20596): loss=0.15576168754616573\n",
      "Stochastic Gradient Descent(20597): loss=0.38118492254874364\n",
      "Stochastic Gradient Descent(20598): loss=3.5890873266058834\n",
      "Stochastic Gradient Descent(20599): loss=10.217908966031906\n",
      "Stochastic Gradient Descent(20600): loss=2.9528166262432287\n",
      "Stochastic Gradient Descent(20601): loss=0.2692867304926177\n",
      "Stochastic Gradient Descent(20602): loss=3.182035070727001\n",
      "Stochastic Gradient Descent(20603): loss=6.464560821636225\n",
      "Stochastic Gradient Descent(20604): loss=3.8667129806261245\n",
      "Stochastic Gradient Descent(20605): loss=1.0106935985400807\n",
      "Stochastic Gradient Descent(20606): loss=7.782073408718042\n",
      "Stochastic Gradient Descent(20607): loss=8.679955989152177\n",
      "Stochastic Gradient Descent(20608): loss=0.2292821274526862\n",
      "Stochastic Gradient Descent(20609): loss=5.757159262453438\n",
      "Stochastic Gradient Descent(20610): loss=8.967124189448555\n",
      "Stochastic Gradient Descent(20611): loss=0.2519872795432692\n",
      "Stochastic Gradient Descent(20612): loss=10.166971082777552\n",
      "Stochastic Gradient Descent(20613): loss=7.061495633760332\n",
      "Stochastic Gradient Descent(20614): loss=0.004512713299610071\n",
      "Stochastic Gradient Descent(20615): loss=0.4851522297229719\n",
      "Stochastic Gradient Descent(20616): loss=2.5339828324552927\n",
      "Stochastic Gradient Descent(20617): loss=1.0047690237725477\n",
      "Stochastic Gradient Descent(20618): loss=12.948601727980673\n",
      "Stochastic Gradient Descent(20619): loss=1.4964183669732545\n",
      "Stochastic Gradient Descent(20620): loss=10.307986286547793\n",
      "Stochastic Gradient Descent(20621): loss=5.1792429884202535\n",
      "Stochastic Gradient Descent(20622): loss=36.965285750120785\n",
      "Stochastic Gradient Descent(20623): loss=0.012907034471618105\n",
      "Stochastic Gradient Descent(20624): loss=1.907981584728463\n",
      "Stochastic Gradient Descent(20625): loss=2.012985754905575\n",
      "Stochastic Gradient Descent(20626): loss=5.478020685202397\n",
      "Stochastic Gradient Descent(20627): loss=0.6665579059552481\n",
      "Stochastic Gradient Descent(20628): loss=0.6112658839214167\n",
      "Stochastic Gradient Descent(20629): loss=5.702551279061739\n",
      "Stochastic Gradient Descent(20630): loss=8.941808246213443\n",
      "Stochastic Gradient Descent(20631): loss=0.08722122506347973\n",
      "Stochastic Gradient Descent(20632): loss=1.8296441242973103\n",
      "Stochastic Gradient Descent(20633): loss=0.7916583557015486\n",
      "Stochastic Gradient Descent(20634): loss=5.736366900668693\n",
      "Stochastic Gradient Descent(20635): loss=1.5491462083214174\n",
      "Stochastic Gradient Descent(20636): loss=2.779530854539864\n",
      "Stochastic Gradient Descent(20637): loss=9.963984440793247\n",
      "Stochastic Gradient Descent(20638): loss=0.7200404878598259\n",
      "Stochastic Gradient Descent(20639): loss=3.5251737126086256\n",
      "Stochastic Gradient Descent(20640): loss=8.257201114093576\n",
      "Stochastic Gradient Descent(20641): loss=17.089818080445166\n",
      "Stochastic Gradient Descent(20642): loss=0.20383139097211775\n",
      "Stochastic Gradient Descent(20643): loss=0.2287409784655117\n",
      "Stochastic Gradient Descent(20644): loss=5.95249595286358\n",
      "Stochastic Gradient Descent(20645): loss=4.72019228114111\n",
      "Stochastic Gradient Descent(20646): loss=2.1811208673501596\n",
      "Stochastic Gradient Descent(20647): loss=0.00724968220451818\n",
      "Stochastic Gradient Descent(20648): loss=12.666149394673694\n",
      "Stochastic Gradient Descent(20649): loss=1.7819684886370748\n",
      "Stochastic Gradient Descent(20650): loss=1.455265712924851\n",
      "Stochastic Gradient Descent(20651): loss=3.3053026815044166\n",
      "Stochastic Gradient Descent(20652): loss=16.150110292901648\n",
      "Stochastic Gradient Descent(20653): loss=0.5146300887985759\n",
      "Stochastic Gradient Descent(20654): loss=2.589354007510975\n",
      "Stochastic Gradient Descent(20655): loss=0.07072914455988426\n",
      "Stochastic Gradient Descent(20656): loss=6.094837272822712\n",
      "Stochastic Gradient Descent(20657): loss=1.315921629261139\n",
      "Stochastic Gradient Descent(20658): loss=0.06514080292128648\n",
      "Stochastic Gradient Descent(20659): loss=0.6814833066456562\n",
      "Stochastic Gradient Descent(20660): loss=7.956048330879308\n",
      "Stochastic Gradient Descent(20661): loss=9.015605940821263\n",
      "Stochastic Gradient Descent(20662): loss=10.04439753497014\n",
      "Stochastic Gradient Descent(20663): loss=1.2960098229757628\n",
      "Stochastic Gradient Descent(20664): loss=0.004359417285472113\n",
      "Stochastic Gradient Descent(20665): loss=3.4784546040912647\n",
      "Stochastic Gradient Descent(20666): loss=0.03375996699617322\n",
      "Stochastic Gradient Descent(20667): loss=28.041790783182478\n",
      "Stochastic Gradient Descent(20668): loss=5.09476532309826\n",
      "Stochastic Gradient Descent(20669): loss=10.181953204567732\n",
      "Stochastic Gradient Descent(20670): loss=3.227123712941053\n",
      "Stochastic Gradient Descent(20671): loss=5.154619838315502\n",
      "Stochastic Gradient Descent(20672): loss=0.3511888261187651\n",
      "Stochastic Gradient Descent(20673): loss=0.15734229562632462\n",
      "Stochastic Gradient Descent(20674): loss=0.3899458651941858\n",
      "Stochastic Gradient Descent(20675): loss=0.4015401947707948\n",
      "Stochastic Gradient Descent(20676): loss=1.3341222010860474\n",
      "Stochastic Gradient Descent(20677): loss=0.06295533810343597\n",
      "Stochastic Gradient Descent(20678): loss=0.4857307352967682\n",
      "Stochastic Gradient Descent(20679): loss=33.18292894612239\n",
      "Stochastic Gradient Descent(20680): loss=0.6249984268999661\n",
      "Stochastic Gradient Descent(20681): loss=12.656385685462169\n",
      "Stochastic Gradient Descent(20682): loss=1.6722619155847869\n",
      "Stochastic Gradient Descent(20683): loss=0.0798553974797706\n",
      "Stochastic Gradient Descent(20684): loss=29.415897094504448\n",
      "Stochastic Gradient Descent(20685): loss=1.5520491316783624\n",
      "Stochastic Gradient Descent(20686): loss=2.069959949935703\n",
      "Stochastic Gradient Descent(20687): loss=0.09794765591330634\n",
      "Stochastic Gradient Descent(20688): loss=15.699603190506151\n",
      "Stochastic Gradient Descent(20689): loss=1.4520364425528385\n",
      "Stochastic Gradient Descent(20690): loss=0.025291620080575457\n",
      "Stochastic Gradient Descent(20691): loss=6.0570066459692615\n",
      "Stochastic Gradient Descent(20692): loss=0.9205133864092302\n",
      "Stochastic Gradient Descent(20693): loss=4.193226609140707\n",
      "Stochastic Gradient Descent(20694): loss=40.09637186661543\n",
      "Stochastic Gradient Descent(20695): loss=0.4621362271334934\n",
      "Stochastic Gradient Descent(20696): loss=0.004856217325344547\n",
      "Stochastic Gradient Descent(20697): loss=6.1930397217663655\n",
      "Stochastic Gradient Descent(20698): loss=1.4454010401032114\n",
      "Stochastic Gradient Descent(20699): loss=0.7614041836722202\n",
      "Stochastic Gradient Descent(20700): loss=3.0922243609108535\n",
      "Stochastic Gradient Descent(20701): loss=2.4931517455649885\n",
      "Stochastic Gradient Descent(20702): loss=0.0026653977468761142\n",
      "Stochastic Gradient Descent(20703): loss=0.30042001291017517\n",
      "Stochastic Gradient Descent(20704): loss=1.030263957232615\n",
      "Stochastic Gradient Descent(20705): loss=5.9823499646689955\n",
      "Stochastic Gradient Descent(20706): loss=6.132090179785831\n",
      "Stochastic Gradient Descent(20707): loss=9.188240998558138\n",
      "Stochastic Gradient Descent(20708): loss=4.5775316749262195e-07\n",
      "Stochastic Gradient Descent(20709): loss=4.4555783135292835\n",
      "Stochastic Gradient Descent(20710): loss=5.461698296228571\n",
      "Stochastic Gradient Descent(20711): loss=3.6374886197883565\n",
      "Stochastic Gradient Descent(20712): loss=4.236441077582013\n",
      "Stochastic Gradient Descent(20713): loss=0.17325664050570155\n",
      "Stochastic Gradient Descent(20714): loss=10.080999926739153\n",
      "Stochastic Gradient Descent(20715): loss=1.4197393234294677\n",
      "Stochastic Gradient Descent(20716): loss=11.320385856102835\n",
      "Stochastic Gradient Descent(20717): loss=0.8976360043631496\n",
      "Stochastic Gradient Descent(20718): loss=14.932605011261586\n",
      "Stochastic Gradient Descent(20719): loss=10.436968988705338\n",
      "Stochastic Gradient Descent(20720): loss=11.406233067729222\n",
      "Stochastic Gradient Descent(20721): loss=2.648913531653246\n",
      "Stochastic Gradient Descent(20722): loss=7.967209838473217\n",
      "Stochastic Gradient Descent(20723): loss=1.716023952247007\n",
      "Stochastic Gradient Descent(20724): loss=0.8781120836057383\n",
      "Stochastic Gradient Descent(20725): loss=2.3221741622285936\n",
      "Stochastic Gradient Descent(20726): loss=1.5297592624611553\n",
      "Stochastic Gradient Descent(20727): loss=1.478428121781283\n",
      "Stochastic Gradient Descent(20728): loss=0.756783209774672\n",
      "Stochastic Gradient Descent(20729): loss=5.244689183739342\n",
      "Stochastic Gradient Descent(20730): loss=8.684016255665787\n",
      "Stochastic Gradient Descent(20731): loss=0.1924748179603245\n",
      "Stochastic Gradient Descent(20732): loss=1.155669444604364\n",
      "Stochastic Gradient Descent(20733): loss=3.2526529638515336\n",
      "Stochastic Gradient Descent(20734): loss=29.01468692475068\n",
      "Stochastic Gradient Descent(20735): loss=20.19173747365221\n",
      "Stochastic Gradient Descent(20736): loss=5.55926142180053\n",
      "Stochastic Gradient Descent(20737): loss=1.4866455702063244\n",
      "Stochastic Gradient Descent(20738): loss=0.3422954139384781\n",
      "Stochastic Gradient Descent(20739): loss=2.6912507513229857\n",
      "Stochastic Gradient Descent(20740): loss=26.863514661133113\n",
      "Stochastic Gradient Descent(20741): loss=4.670453703489199\n",
      "Stochastic Gradient Descent(20742): loss=1.4587410213905898\n",
      "Stochastic Gradient Descent(20743): loss=21.94915749528077\n",
      "Stochastic Gradient Descent(20744): loss=6.043288130451611\n",
      "Stochastic Gradient Descent(20745): loss=0.5761710053178853\n",
      "Stochastic Gradient Descent(20746): loss=1.9008479923665116\n",
      "Stochastic Gradient Descent(20747): loss=5.525357674003932\n",
      "Stochastic Gradient Descent(20748): loss=8.375979471609238\n",
      "Stochastic Gradient Descent(20749): loss=8.793612230241187\n",
      "Stochastic Gradient Descent(20750): loss=39.32267433580182\n",
      "Stochastic Gradient Descent(20751): loss=0.43002570286170033\n",
      "Stochastic Gradient Descent(20752): loss=2.979962637797402\n",
      "Stochastic Gradient Descent(20753): loss=1.3250846478473823\n",
      "Stochastic Gradient Descent(20754): loss=0.943469166679695\n",
      "Stochastic Gradient Descent(20755): loss=1.333269643512973\n",
      "Stochastic Gradient Descent(20756): loss=18.975274643809882\n",
      "Stochastic Gradient Descent(20757): loss=0.38513125635949447\n",
      "Stochastic Gradient Descent(20758): loss=0.8860833128071827\n",
      "Stochastic Gradient Descent(20759): loss=0.6878098730512542\n",
      "Stochastic Gradient Descent(20760): loss=0.07530587893557875\n",
      "Stochastic Gradient Descent(20761): loss=0.21294863440354195\n",
      "Stochastic Gradient Descent(20762): loss=6.829770557202894\n",
      "Stochastic Gradient Descent(20763): loss=0.014337228178573256\n",
      "Stochastic Gradient Descent(20764): loss=1.7970655777161597\n",
      "Stochastic Gradient Descent(20765): loss=1.5597994433500693\n",
      "Stochastic Gradient Descent(20766): loss=2.9301963262355883\n",
      "Stochastic Gradient Descent(20767): loss=45.71162104854969\n",
      "Stochastic Gradient Descent(20768): loss=22.323735132465693\n",
      "Stochastic Gradient Descent(20769): loss=0.34799131394229854\n",
      "Stochastic Gradient Descent(20770): loss=59.84594738416857\n",
      "Stochastic Gradient Descent(20771): loss=29.449505421444297\n",
      "Stochastic Gradient Descent(20772): loss=0.28733406433605435\n",
      "Stochastic Gradient Descent(20773): loss=9.57338069886383\n",
      "Stochastic Gradient Descent(20774): loss=2.635366916811141\n",
      "Stochastic Gradient Descent(20775): loss=0.11162013478435957\n",
      "Stochastic Gradient Descent(20776): loss=1.799885602540325\n",
      "Stochastic Gradient Descent(20777): loss=6.559238526562037\n",
      "Stochastic Gradient Descent(20778): loss=0.1538052608715259\n",
      "Stochastic Gradient Descent(20779): loss=1.344990600850075\n",
      "Stochastic Gradient Descent(20780): loss=0.6225995768090904\n",
      "Stochastic Gradient Descent(20781): loss=0.4351686726697727\n",
      "Stochastic Gradient Descent(20782): loss=36.671743682312545\n",
      "Stochastic Gradient Descent(20783): loss=0.11939451537610207\n",
      "Stochastic Gradient Descent(20784): loss=16.260135398155235\n",
      "Stochastic Gradient Descent(20785): loss=42.677045541413996\n",
      "Stochastic Gradient Descent(20786): loss=7.745311101979398\n",
      "Stochastic Gradient Descent(20787): loss=0.5275092055638425\n",
      "Stochastic Gradient Descent(20788): loss=4.683097710437575\n",
      "Stochastic Gradient Descent(20789): loss=0.030972050886874068\n",
      "Stochastic Gradient Descent(20790): loss=0.31702911842084985\n",
      "Stochastic Gradient Descent(20791): loss=10.66499447038941\n",
      "Stochastic Gradient Descent(20792): loss=0.15727945316718975\n",
      "Stochastic Gradient Descent(20793): loss=0.31228235541959976\n",
      "Stochastic Gradient Descent(20794): loss=0.0625778745663574\n",
      "Stochastic Gradient Descent(20795): loss=0.7725825686730733\n",
      "Stochastic Gradient Descent(20796): loss=23.455153337773723\n",
      "Stochastic Gradient Descent(20797): loss=0.46136776745523017\n",
      "Stochastic Gradient Descent(20798): loss=0.5374836205776778\n",
      "Stochastic Gradient Descent(20799): loss=1.2617018396881237\n",
      "Stochastic Gradient Descent(20800): loss=7.980487894710432e-05\n",
      "Stochastic Gradient Descent(20801): loss=0.9372748566358444\n",
      "Stochastic Gradient Descent(20802): loss=0.023197967290197772\n",
      "Stochastic Gradient Descent(20803): loss=0.5631996709327728\n",
      "Stochastic Gradient Descent(20804): loss=7.8195581039282125\n",
      "Stochastic Gradient Descent(20805): loss=0.5235414622749672\n",
      "Stochastic Gradient Descent(20806): loss=0.004632943443655967\n",
      "Stochastic Gradient Descent(20807): loss=0.44917365465848064\n",
      "Stochastic Gradient Descent(20808): loss=17.495060971421292\n",
      "Stochastic Gradient Descent(20809): loss=0.09913042100870072\n",
      "Stochastic Gradient Descent(20810): loss=0.0684400177097973\n",
      "Stochastic Gradient Descent(20811): loss=0.7241674794707971\n",
      "Stochastic Gradient Descent(20812): loss=0.9183139074907055\n",
      "Stochastic Gradient Descent(20813): loss=1.383641192953388\n",
      "Stochastic Gradient Descent(20814): loss=0.6760225082034961\n",
      "Stochastic Gradient Descent(20815): loss=1.1961967818500896\n",
      "Stochastic Gradient Descent(20816): loss=15.122594363675248\n",
      "Stochastic Gradient Descent(20817): loss=0.013355845839478226\n",
      "Stochastic Gradient Descent(20818): loss=2.732205035183772\n",
      "Stochastic Gradient Descent(20819): loss=9.334379266552382\n",
      "Stochastic Gradient Descent(20820): loss=1.8139345865669787\n",
      "Stochastic Gradient Descent(20821): loss=2.1690423787566484\n",
      "Stochastic Gradient Descent(20822): loss=5.669526118969348\n",
      "Stochastic Gradient Descent(20823): loss=9.19362522021674\n",
      "Stochastic Gradient Descent(20824): loss=4.413092474904954\n",
      "Stochastic Gradient Descent(20825): loss=9.590236823410159\n",
      "Stochastic Gradient Descent(20826): loss=0.23540965255043614\n",
      "Stochastic Gradient Descent(20827): loss=0.7465025432969\n",
      "Stochastic Gradient Descent(20828): loss=0.027335289056865202\n",
      "Stochastic Gradient Descent(20829): loss=1.6945348439864087\n",
      "Stochastic Gradient Descent(20830): loss=5.385008293041813\n",
      "Stochastic Gradient Descent(20831): loss=1.0743113080678355\n",
      "Stochastic Gradient Descent(20832): loss=0.06550592225014916\n",
      "Stochastic Gradient Descent(20833): loss=4.395164089502652\n",
      "Stochastic Gradient Descent(20834): loss=0.0026915336697110478\n",
      "Stochastic Gradient Descent(20835): loss=1.2537899445426186\n",
      "Stochastic Gradient Descent(20836): loss=2.0033504595565943\n",
      "Stochastic Gradient Descent(20837): loss=1.1981729763698206\n",
      "Stochastic Gradient Descent(20838): loss=2.2291626055934595\n",
      "Stochastic Gradient Descent(20839): loss=2.6745494842538498\n",
      "Stochastic Gradient Descent(20840): loss=0.36294858419803433\n",
      "Stochastic Gradient Descent(20841): loss=24.579163634279563\n",
      "Stochastic Gradient Descent(20842): loss=1.6260357771919625\n",
      "Stochastic Gradient Descent(20843): loss=0.01715893082023071\n",
      "Stochastic Gradient Descent(20844): loss=1.130234636075915\n",
      "Stochastic Gradient Descent(20845): loss=0.004170885929820071\n",
      "Stochastic Gradient Descent(20846): loss=0.7148917500435193\n",
      "Stochastic Gradient Descent(20847): loss=6.194996212423651\n",
      "Stochastic Gradient Descent(20848): loss=4.145210419511778\n",
      "Stochastic Gradient Descent(20849): loss=4.6241494677118835\n",
      "Stochastic Gradient Descent(20850): loss=7.403058560467655\n",
      "Stochastic Gradient Descent(20851): loss=8.96613498212983\n",
      "Stochastic Gradient Descent(20852): loss=3.4860587408543893\n",
      "Stochastic Gradient Descent(20853): loss=1.0996273339761395\n",
      "Stochastic Gradient Descent(20854): loss=6.524793771541491\n",
      "Stochastic Gradient Descent(20855): loss=0.22859058606657107\n",
      "Stochastic Gradient Descent(20856): loss=0.10126310389478103\n",
      "Stochastic Gradient Descent(20857): loss=0.0022850611045178728\n",
      "Stochastic Gradient Descent(20858): loss=2.109072997775716\n",
      "Stochastic Gradient Descent(20859): loss=3.860220695177289\n",
      "Stochastic Gradient Descent(20860): loss=3.103093480729451\n",
      "Stochastic Gradient Descent(20861): loss=16.96560394076827\n",
      "Stochastic Gradient Descent(20862): loss=7.93757635592721\n",
      "Stochastic Gradient Descent(20863): loss=0.1138903509111911\n",
      "Stochastic Gradient Descent(20864): loss=10.306370523012784\n",
      "Stochastic Gradient Descent(20865): loss=0.011805983943584659\n",
      "Stochastic Gradient Descent(20866): loss=0.2239614221534869\n",
      "Stochastic Gradient Descent(20867): loss=16.21248762858315\n",
      "Stochastic Gradient Descent(20868): loss=0.25372619853134015\n",
      "Stochastic Gradient Descent(20869): loss=3.1550347528178704\n",
      "Stochastic Gradient Descent(20870): loss=3.737030638056437\n",
      "Stochastic Gradient Descent(20871): loss=0.7329001400647457\n",
      "Stochastic Gradient Descent(20872): loss=1.704524811375175\n",
      "Stochastic Gradient Descent(20873): loss=0.038933532920915365\n",
      "Stochastic Gradient Descent(20874): loss=5.040040791353026\n",
      "Stochastic Gradient Descent(20875): loss=2.0218459271395948\n",
      "Stochastic Gradient Descent(20876): loss=0.00010928273532387221\n",
      "Stochastic Gradient Descent(20877): loss=10.199389803306369\n",
      "Stochastic Gradient Descent(20878): loss=0.06412019736012882\n",
      "Stochastic Gradient Descent(20879): loss=0.009057919948129411\n",
      "Stochastic Gradient Descent(20880): loss=15.193447016834696\n",
      "Stochastic Gradient Descent(20881): loss=11.631982450894402\n",
      "Stochastic Gradient Descent(20882): loss=3.7430619982275624\n",
      "Stochastic Gradient Descent(20883): loss=0.0683271233283913\n",
      "Stochastic Gradient Descent(20884): loss=0.2075455393181988\n",
      "Stochastic Gradient Descent(20885): loss=2.1320856666879604\n",
      "Stochastic Gradient Descent(20886): loss=0.26669582125607383\n",
      "Stochastic Gradient Descent(20887): loss=4.621707576825828\n",
      "Stochastic Gradient Descent(20888): loss=5.907210163424924\n",
      "Stochastic Gradient Descent(20889): loss=2.276229623827158\n",
      "Stochastic Gradient Descent(20890): loss=2.56668975569367\n",
      "Stochastic Gradient Descent(20891): loss=5.289084682391938\n",
      "Stochastic Gradient Descent(20892): loss=13.337006995770043\n",
      "Stochastic Gradient Descent(20893): loss=1.2151315664378068\n",
      "Stochastic Gradient Descent(20894): loss=0.11348190973576044\n",
      "Stochastic Gradient Descent(20895): loss=3.2318772410064707\n",
      "Stochastic Gradient Descent(20896): loss=8.253938381798921\n",
      "Stochastic Gradient Descent(20897): loss=0.25368764215769063\n",
      "Stochastic Gradient Descent(20898): loss=0.029047011139868854\n",
      "Stochastic Gradient Descent(20899): loss=2.3723227969894656\n",
      "Stochastic Gradient Descent(20900): loss=5.914240438722103\n",
      "Stochastic Gradient Descent(20901): loss=0.022410074974786042\n",
      "Stochastic Gradient Descent(20902): loss=0.5214544534482295\n",
      "Stochastic Gradient Descent(20903): loss=21.228881518863417\n",
      "Stochastic Gradient Descent(20904): loss=0.8610182901956334\n",
      "Stochastic Gradient Descent(20905): loss=10.91189270524921\n",
      "Stochastic Gradient Descent(20906): loss=2.202301265698256\n",
      "Stochastic Gradient Descent(20907): loss=6.3176228372969625\n",
      "Stochastic Gradient Descent(20908): loss=0.619193424570626\n",
      "Stochastic Gradient Descent(20909): loss=12.353612753637284\n",
      "Stochastic Gradient Descent(20910): loss=8.821104315217173\n",
      "Stochastic Gradient Descent(20911): loss=0.5099643427463211\n",
      "Stochastic Gradient Descent(20912): loss=3.1700938025320973\n",
      "Stochastic Gradient Descent(20913): loss=0.23219024717866477\n",
      "Stochastic Gradient Descent(20914): loss=0.5792153553108583\n",
      "Stochastic Gradient Descent(20915): loss=0.08714174156167613\n",
      "Stochastic Gradient Descent(20916): loss=9.302719659334416\n",
      "Stochastic Gradient Descent(20917): loss=0.0036648318388811337\n",
      "Stochastic Gradient Descent(20918): loss=2.2435716056873503\n",
      "Stochastic Gradient Descent(20919): loss=0.008521411321841503\n",
      "Stochastic Gradient Descent(20920): loss=0.02655156849014836\n",
      "Stochastic Gradient Descent(20921): loss=0.9187108549687901\n",
      "Stochastic Gradient Descent(20922): loss=9.421407039941796\n",
      "Stochastic Gradient Descent(20923): loss=4.9450215889577995\n",
      "Stochastic Gradient Descent(20924): loss=6.992119124612446\n",
      "Stochastic Gradient Descent(20925): loss=5.8437815058289574e-05\n",
      "Stochastic Gradient Descent(20926): loss=2.380925381598987\n",
      "Stochastic Gradient Descent(20927): loss=5.660717874569199\n",
      "Stochastic Gradient Descent(20928): loss=0.06690037557361134\n",
      "Stochastic Gradient Descent(20929): loss=9.861115319237243\n",
      "Stochastic Gradient Descent(20930): loss=5.935990885065135\n",
      "Stochastic Gradient Descent(20931): loss=1.0612735080659326\n",
      "Stochastic Gradient Descent(20932): loss=0.6803828818074953\n",
      "Stochastic Gradient Descent(20933): loss=0.05534676920814427\n",
      "Stochastic Gradient Descent(20934): loss=18.749554937559957\n",
      "Stochastic Gradient Descent(20935): loss=7.6840081182114925\n",
      "Stochastic Gradient Descent(20936): loss=1.5572499266259878\n",
      "Stochastic Gradient Descent(20937): loss=0.05086936472644828\n",
      "Stochastic Gradient Descent(20938): loss=0.38046493158443984\n",
      "Stochastic Gradient Descent(20939): loss=1.203158754153095\n",
      "Stochastic Gradient Descent(20940): loss=11.642255242301264\n",
      "Stochastic Gradient Descent(20941): loss=7.282415040668878e-06\n",
      "Stochastic Gradient Descent(20942): loss=7.790745857362813\n",
      "Stochastic Gradient Descent(20943): loss=15.448480784438738\n",
      "Stochastic Gradient Descent(20944): loss=2.7186206760969753\n",
      "Stochastic Gradient Descent(20945): loss=1.5518833581560578\n",
      "Stochastic Gradient Descent(20946): loss=10.778464300610812\n",
      "Stochastic Gradient Descent(20947): loss=6.851272304743197\n",
      "Stochastic Gradient Descent(20948): loss=2.597753802574577\n",
      "Stochastic Gradient Descent(20949): loss=0.10522990571303556\n",
      "Stochastic Gradient Descent(20950): loss=4.856272064867303\n",
      "Stochastic Gradient Descent(20951): loss=7.396364357865941\n",
      "Stochastic Gradient Descent(20952): loss=7.666794798770032\n",
      "Stochastic Gradient Descent(20953): loss=3.8853173403863996\n",
      "Stochastic Gradient Descent(20954): loss=9.459850717973765\n",
      "Stochastic Gradient Descent(20955): loss=2.678121343406087\n",
      "Stochastic Gradient Descent(20956): loss=14.827650906899681\n",
      "Stochastic Gradient Descent(20957): loss=0.0050162700388081075\n",
      "Stochastic Gradient Descent(20958): loss=2.8687278828624954\n",
      "Stochastic Gradient Descent(20959): loss=4.448465936798086\n",
      "Stochastic Gradient Descent(20960): loss=0.2508583814241363\n",
      "Stochastic Gradient Descent(20961): loss=0.10485573434201399\n",
      "Stochastic Gradient Descent(20962): loss=1.018200507001318\n",
      "Stochastic Gradient Descent(20963): loss=4.698793796529674\n",
      "Stochastic Gradient Descent(20964): loss=7.2762198401504055\n",
      "Stochastic Gradient Descent(20965): loss=4.232280464585993\n",
      "Stochastic Gradient Descent(20966): loss=5.66380379696379\n",
      "Stochastic Gradient Descent(20967): loss=0.27885321505546784\n",
      "Stochastic Gradient Descent(20968): loss=11.793040746514537\n",
      "Stochastic Gradient Descent(20969): loss=14.035465381053394\n",
      "Stochastic Gradient Descent(20970): loss=0.10386417981841883\n",
      "Stochastic Gradient Descent(20971): loss=1.6671260010716111\n",
      "Stochastic Gradient Descent(20972): loss=0.387719212706589\n",
      "Stochastic Gradient Descent(20973): loss=0.19956604394418279\n",
      "Stochastic Gradient Descent(20974): loss=3.8312293569590947\n",
      "Stochastic Gradient Descent(20975): loss=3.570679057249705\n",
      "Stochastic Gradient Descent(20976): loss=11.379936229512229\n",
      "Stochastic Gradient Descent(20977): loss=16.11284025132402\n",
      "Stochastic Gradient Descent(20978): loss=0.5646352742676516\n",
      "Stochastic Gradient Descent(20979): loss=1.3192634238795462\n",
      "Stochastic Gradient Descent(20980): loss=0.14313871749684837\n",
      "Stochastic Gradient Descent(20981): loss=1.7450385373965076\n",
      "Stochastic Gradient Descent(20982): loss=10.404432788930283\n",
      "Stochastic Gradient Descent(20983): loss=1.0593838110695804\n",
      "Stochastic Gradient Descent(20984): loss=5.605008715009143\n",
      "Stochastic Gradient Descent(20985): loss=0.821230096944232\n",
      "Stochastic Gradient Descent(20986): loss=5.999833507196684\n",
      "Stochastic Gradient Descent(20987): loss=15.72360682751301\n",
      "Stochastic Gradient Descent(20988): loss=1.6377473807357734\n",
      "Stochastic Gradient Descent(20989): loss=21.483164998908162\n",
      "Stochastic Gradient Descent(20990): loss=3.9027210570927497\n",
      "Stochastic Gradient Descent(20991): loss=7.134679780901049\n",
      "Stochastic Gradient Descent(20992): loss=1.0843509752871734\n",
      "Stochastic Gradient Descent(20993): loss=10.76464637217782\n",
      "Stochastic Gradient Descent(20994): loss=7.463283010761471\n",
      "Stochastic Gradient Descent(20995): loss=0.4046799968706596\n",
      "Stochastic Gradient Descent(20996): loss=2.8143778322068957\n",
      "Stochastic Gradient Descent(20997): loss=81.84695685868418\n",
      "Stochastic Gradient Descent(20998): loss=24.587368697698\n",
      "Stochastic Gradient Descent(20999): loss=8.247348827831301\n",
      "Stochastic Gradient Descent(21000): loss=59.393777449789916\n",
      "Stochastic Gradient Descent(21001): loss=19.770574166823163\n",
      "Stochastic Gradient Descent(21002): loss=0.9993811279910501\n",
      "Stochastic Gradient Descent(21003): loss=24.644945426046267\n",
      "Stochastic Gradient Descent(21004): loss=0.003356755606872827\n",
      "Stochastic Gradient Descent(21005): loss=0.03369012429492563\n",
      "Stochastic Gradient Descent(21006): loss=1.4076291289152316\n",
      "Stochastic Gradient Descent(21007): loss=4.367786783406637\n",
      "Stochastic Gradient Descent(21008): loss=0.10735441780331868\n",
      "Stochastic Gradient Descent(21009): loss=0.19049279100950772\n",
      "Stochastic Gradient Descent(21010): loss=1.5096623072406998\n",
      "Stochastic Gradient Descent(21011): loss=0.1160032253703972\n",
      "Stochastic Gradient Descent(21012): loss=15.785974407170864\n",
      "Stochastic Gradient Descent(21013): loss=0.7794195383484188\n",
      "Stochastic Gradient Descent(21014): loss=4.970016203207063\n",
      "Stochastic Gradient Descent(21015): loss=2.083931058020272\n",
      "Stochastic Gradient Descent(21016): loss=22.029723135281262\n",
      "Stochastic Gradient Descent(21017): loss=0.014373194336264555\n",
      "Stochastic Gradient Descent(21018): loss=1.7725589962596595\n",
      "Stochastic Gradient Descent(21019): loss=3.6788779446045705\n",
      "Stochastic Gradient Descent(21020): loss=4.131241832649977\n",
      "Stochastic Gradient Descent(21021): loss=0.1253382206241402\n",
      "Stochastic Gradient Descent(21022): loss=6.970106702824489\n",
      "Stochastic Gradient Descent(21023): loss=0.0019081287602507986\n",
      "Stochastic Gradient Descent(21024): loss=2.464650635698582\n",
      "Stochastic Gradient Descent(21025): loss=1.0623436113203004\n",
      "Stochastic Gradient Descent(21026): loss=2.1892478358970235\n",
      "Stochastic Gradient Descent(21027): loss=0.062069206807344283\n",
      "Stochastic Gradient Descent(21028): loss=0.13452528724799448\n",
      "Stochastic Gradient Descent(21029): loss=0.8594841250433638\n",
      "Stochastic Gradient Descent(21030): loss=2.6324310428323434\n",
      "Stochastic Gradient Descent(21031): loss=2.425225468257366\n",
      "Stochastic Gradient Descent(21032): loss=0.01581798446942198\n",
      "Stochastic Gradient Descent(21033): loss=17.508244920461557\n",
      "Stochastic Gradient Descent(21034): loss=1.0082277927170353\n",
      "Stochastic Gradient Descent(21035): loss=0.04366919101173921\n",
      "Stochastic Gradient Descent(21036): loss=3.3221537429053134\n",
      "Stochastic Gradient Descent(21037): loss=0.07137697192826274\n",
      "Stochastic Gradient Descent(21038): loss=8.659592091197814\n",
      "Stochastic Gradient Descent(21039): loss=19.583162301907866\n",
      "Stochastic Gradient Descent(21040): loss=2.8074557535121207\n",
      "Stochastic Gradient Descent(21041): loss=0.029793529013625315\n",
      "Stochastic Gradient Descent(21042): loss=7.538125947777154\n",
      "Stochastic Gradient Descent(21043): loss=21.46389678984757\n",
      "Stochastic Gradient Descent(21044): loss=9.814108550388303\n",
      "Stochastic Gradient Descent(21045): loss=0.5222213292902264\n",
      "Stochastic Gradient Descent(21046): loss=1.7378100465775983\n",
      "Stochastic Gradient Descent(21047): loss=0.648464587231803\n",
      "Stochastic Gradient Descent(21048): loss=0.10622450731264556\n",
      "Stochastic Gradient Descent(21049): loss=0.7090407612710625\n",
      "Stochastic Gradient Descent(21050): loss=0.4807316395771673\n",
      "Stochastic Gradient Descent(21051): loss=0.17428531773829462\n",
      "Stochastic Gradient Descent(21052): loss=9.39205766347172\n",
      "Stochastic Gradient Descent(21053): loss=0.0030144555526510374\n",
      "Stochastic Gradient Descent(21054): loss=0.19889885055977666\n",
      "Stochastic Gradient Descent(21055): loss=0.7937499347731395\n",
      "Stochastic Gradient Descent(21056): loss=7.154142084451213\n",
      "Stochastic Gradient Descent(21057): loss=0.41120765549633614\n",
      "Stochastic Gradient Descent(21058): loss=0.00013384843356579153\n",
      "Stochastic Gradient Descent(21059): loss=7.012155861113333\n",
      "Stochastic Gradient Descent(21060): loss=2.454812703504685\n",
      "Stochastic Gradient Descent(21061): loss=2.9505126831974575\n",
      "Stochastic Gradient Descent(21062): loss=0.0031775345875735225\n",
      "Stochastic Gradient Descent(21063): loss=0.10872942892329211\n",
      "Stochastic Gradient Descent(21064): loss=3.5305918546050084\n",
      "Stochastic Gradient Descent(21065): loss=0.5059898490501186\n",
      "Stochastic Gradient Descent(21066): loss=11.20486933166586\n",
      "Stochastic Gradient Descent(21067): loss=2.1903456988684784\n",
      "Stochastic Gradient Descent(21068): loss=1.8499348492164178\n",
      "Stochastic Gradient Descent(21069): loss=0.5696568094515684\n",
      "Stochastic Gradient Descent(21070): loss=2.2693097122322206\n",
      "Stochastic Gradient Descent(21071): loss=9.953107084514842\n",
      "Stochastic Gradient Descent(21072): loss=10.097484308996412\n",
      "Stochastic Gradient Descent(21073): loss=26.528960581877616\n",
      "Stochastic Gradient Descent(21074): loss=0.6675576537308077\n",
      "Stochastic Gradient Descent(21075): loss=3.91833359496677\n",
      "Stochastic Gradient Descent(21076): loss=1.5016744843875425\n",
      "Stochastic Gradient Descent(21077): loss=1.989341395564375\n",
      "Stochastic Gradient Descent(21078): loss=8.723709916789197\n",
      "Stochastic Gradient Descent(21079): loss=6.9782595485837575\n",
      "Stochastic Gradient Descent(21080): loss=8.815525227746841\n",
      "Stochastic Gradient Descent(21081): loss=9.50588913834904\n",
      "Stochastic Gradient Descent(21082): loss=12.377577880026259\n",
      "Stochastic Gradient Descent(21083): loss=0.48606427480153386\n",
      "Stochastic Gradient Descent(21084): loss=10.579748693522893\n",
      "Stochastic Gradient Descent(21085): loss=40.18094461488161\n",
      "Stochastic Gradient Descent(21086): loss=42.29443911641252\n",
      "Stochastic Gradient Descent(21087): loss=0.16850334769997982\n",
      "Stochastic Gradient Descent(21088): loss=0.17666092973280642\n",
      "Stochastic Gradient Descent(21089): loss=0.25487493715944926\n",
      "Stochastic Gradient Descent(21090): loss=37.13130623141744\n",
      "Stochastic Gradient Descent(21091): loss=1.030206766636949\n",
      "Stochastic Gradient Descent(21092): loss=2.4721574812291602\n",
      "Stochastic Gradient Descent(21093): loss=0.19256583891226328\n",
      "Stochastic Gradient Descent(21094): loss=5.85234410894113\n",
      "Stochastic Gradient Descent(21095): loss=8.106388530924226\n",
      "Stochastic Gradient Descent(21096): loss=9.532404486342717\n",
      "Stochastic Gradient Descent(21097): loss=0.016260269848848825\n",
      "Stochastic Gradient Descent(21098): loss=0.0199078201751802\n",
      "Stochastic Gradient Descent(21099): loss=8.687272191796042\n",
      "Stochastic Gradient Descent(21100): loss=4.083253695970327\n",
      "Stochastic Gradient Descent(21101): loss=1.117908764708034\n",
      "Stochastic Gradient Descent(21102): loss=6.568223021585134\n",
      "Stochastic Gradient Descent(21103): loss=0.6632066884361347\n",
      "Stochastic Gradient Descent(21104): loss=2.1531674938852756\n",
      "Stochastic Gradient Descent(21105): loss=2.3357560667177606\n",
      "Stochastic Gradient Descent(21106): loss=0.01977395028637572\n",
      "Stochastic Gradient Descent(21107): loss=7.157259422182745\n",
      "Stochastic Gradient Descent(21108): loss=1.8823375863445944\n",
      "Stochastic Gradient Descent(21109): loss=4.934952480077381\n",
      "Stochastic Gradient Descent(21110): loss=0.0634494074389024\n",
      "Stochastic Gradient Descent(21111): loss=0.2488660208320205\n",
      "Stochastic Gradient Descent(21112): loss=16.15971939826583\n",
      "Stochastic Gradient Descent(21113): loss=6.586734531127067\n",
      "Stochastic Gradient Descent(21114): loss=1.3068169974300388\n",
      "Stochastic Gradient Descent(21115): loss=2.611450389312699\n",
      "Stochastic Gradient Descent(21116): loss=3.2589572821849138\n",
      "Stochastic Gradient Descent(21117): loss=0.04865333327808382\n",
      "Stochastic Gradient Descent(21118): loss=0.8593043555305178\n",
      "Stochastic Gradient Descent(21119): loss=4.761570405713534\n",
      "Stochastic Gradient Descent(21120): loss=0.26779594345268914\n",
      "Stochastic Gradient Descent(21121): loss=29.26838661303421\n",
      "Stochastic Gradient Descent(21122): loss=1.5902153153513507\n",
      "Stochastic Gradient Descent(21123): loss=5.576492571923869\n",
      "Stochastic Gradient Descent(21124): loss=33.4378180891684\n",
      "Stochastic Gradient Descent(21125): loss=22.38783767288486\n",
      "Stochastic Gradient Descent(21126): loss=3.1359867306399667\n",
      "Stochastic Gradient Descent(21127): loss=1.3890302065068951\n",
      "Stochastic Gradient Descent(21128): loss=0.007022915439978777\n",
      "Stochastic Gradient Descent(21129): loss=1.131567288781415\n",
      "Stochastic Gradient Descent(21130): loss=5.511271980564571\n",
      "Stochastic Gradient Descent(21131): loss=0.3548539233312934\n",
      "Stochastic Gradient Descent(21132): loss=11.522396725178814\n",
      "Stochastic Gradient Descent(21133): loss=16.283219645773606\n",
      "Stochastic Gradient Descent(21134): loss=0.0029757184238375736\n",
      "Stochastic Gradient Descent(21135): loss=0.30960490016666165\n",
      "Stochastic Gradient Descent(21136): loss=8.55881366815806\n",
      "Stochastic Gradient Descent(21137): loss=24.916306784843\n",
      "Stochastic Gradient Descent(21138): loss=0.919146156242512\n",
      "Stochastic Gradient Descent(21139): loss=3.0948737989614696\n",
      "Stochastic Gradient Descent(21140): loss=5.623348591312713\n",
      "Stochastic Gradient Descent(21141): loss=3.2721942455525475\n",
      "Stochastic Gradient Descent(21142): loss=0.500461493109509\n",
      "Stochastic Gradient Descent(21143): loss=0.8125604545027897\n",
      "Stochastic Gradient Descent(21144): loss=2.715258039700513\n",
      "Stochastic Gradient Descent(21145): loss=0.332324022443351\n",
      "Stochastic Gradient Descent(21146): loss=0.38102675034988925\n",
      "Stochastic Gradient Descent(21147): loss=30.506666292864658\n",
      "Stochastic Gradient Descent(21148): loss=1.3432882681752512\n",
      "Stochastic Gradient Descent(21149): loss=1.1121734129580783\n",
      "Stochastic Gradient Descent(21150): loss=16.650896430159424\n",
      "Stochastic Gradient Descent(21151): loss=6.350963937798553\n",
      "Stochastic Gradient Descent(21152): loss=15.270548179236092\n",
      "Stochastic Gradient Descent(21153): loss=0.17911248317406364\n",
      "Stochastic Gradient Descent(21154): loss=0.03311907787715104\n",
      "Stochastic Gradient Descent(21155): loss=0.004910300730364378\n",
      "Stochastic Gradient Descent(21156): loss=3.7961686757982687\n",
      "Stochastic Gradient Descent(21157): loss=1.5322401789184183\n",
      "Stochastic Gradient Descent(21158): loss=16.608336162168005\n",
      "Stochastic Gradient Descent(21159): loss=2.271168706771332\n",
      "Stochastic Gradient Descent(21160): loss=2.949419574501575\n",
      "Stochastic Gradient Descent(21161): loss=6.388722702758694\n",
      "Stochastic Gradient Descent(21162): loss=0.2526809417103732\n",
      "Stochastic Gradient Descent(21163): loss=8.357915919856346\n",
      "Stochastic Gradient Descent(21164): loss=9.01765368760218\n",
      "Stochastic Gradient Descent(21165): loss=8.394928103879588\n",
      "Stochastic Gradient Descent(21166): loss=5.2211470580315575\n",
      "Stochastic Gradient Descent(21167): loss=0.6029382093251141\n",
      "Stochastic Gradient Descent(21168): loss=1.4705171664322296\n",
      "Stochastic Gradient Descent(21169): loss=3.180577747653765\n",
      "Stochastic Gradient Descent(21170): loss=0.0056494559195998575\n",
      "Stochastic Gradient Descent(21171): loss=0.1990835512140521\n",
      "Stochastic Gradient Descent(21172): loss=2.9026248751640127\n",
      "Stochastic Gradient Descent(21173): loss=0.16597804128326205\n",
      "Stochastic Gradient Descent(21174): loss=1.9699605011125658\n",
      "Stochastic Gradient Descent(21175): loss=13.218338895180784\n",
      "Stochastic Gradient Descent(21176): loss=0.11014032312671843\n",
      "Stochastic Gradient Descent(21177): loss=6.763850221933927\n",
      "Stochastic Gradient Descent(21178): loss=13.396242290949777\n",
      "Stochastic Gradient Descent(21179): loss=0.8039191545739224\n",
      "Stochastic Gradient Descent(21180): loss=2.947035853989138\n",
      "Stochastic Gradient Descent(21181): loss=3.2591667258996915\n",
      "Stochastic Gradient Descent(21182): loss=3.8819076688076084\n",
      "Stochastic Gradient Descent(21183): loss=21.42837881335991\n",
      "Stochastic Gradient Descent(21184): loss=40.31495070350869\n",
      "Stochastic Gradient Descent(21185): loss=4.170319480153083\n",
      "Stochastic Gradient Descent(21186): loss=17.563652147114468\n",
      "Stochastic Gradient Descent(21187): loss=0.00442497839040231\n",
      "Stochastic Gradient Descent(21188): loss=0.5533243473100634\n",
      "Stochastic Gradient Descent(21189): loss=0.39672120216307577\n",
      "Stochastic Gradient Descent(21190): loss=0.6081553795236292\n",
      "Stochastic Gradient Descent(21191): loss=2.6192213846886068\n",
      "Stochastic Gradient Descent(21192): loss=9.794992278052046\n",
      "Stochastic Gradient Descent(21193): loss=9.07110860138886\n",
      "Stochastic Gradient Descent(21194): loss=2.109347822517488\n",
      "Stochastic Gradient Descent(21195): loss=5.3893259658634305\n",
      "Stochastic Gradient Descent(21196): loss=3.865608829735983\n",
      "Stochastic Gradient Descent(21197): loss=3.1677617869807477\n",
      "Stochastic Gradient Descent(21198): loss=0.11077592501012219\n",
      "Stochastic Gradient Descent(21199): loss=1.1312755245053847\n",
      "Stochastic Gradient Descent(21200): loss=3.7176119000700654\n",
      "Stochastic Gradient Descent(21201): loss=0.36078590115316206\n",
      "Stochastic Gradient Descent(21202): loss=11.430323112068917\n",
      "Stochastic Gradient Descent(21203): loss=21.441726895195163\n",
      "Stochastic Gradient Descent(21204): loss=2.1251069903057602\n",
      "Stochastic Gradient Descent(21205): loss=0.35488359040401096\n",
      "Stochastic Gradient Descent(21206): loss=5.370116285797758\n",
      "Stochastic Gradient Descent(21207): loss=0.9746245217675477\n",
      "Stochastic Gradient Descent(21208): loss=2.6717088477982944\n",
      "Stochastic Gradient Descent(21209): loss=2.1098426064996714\n",
      "Stochastic Gradient Descent(21210): loss=2.7374065962460317\n",
      "Stochastic Gradient Descent(21211): loss=3.2079773370820104\n",
      "Stochastic Gradient Descent(21212): loss=0.4962501358124379\n",
      "Stochastic Gradient Descent(21213): loss=19.528491904808952\n",
      "Stochastic Gradient Descent(21214): loss=15.690021947650132\n",
      "Stochastic Gradient Descent(21215): loss=3.911091238284918\n",
      "Stochastic Gradient Descent(21216): loss=0.3730782153177447\n",
      "Stochastic Gradient Descent(21217): loss=1.2560661982669323\n",
      "Stochastic Gradient Descent(21218): loss=0.32962635612902524\n",
      "Stochastic Gradient Descent(21219): loss=0.8765958662305708\n",
      "Stochastic Gradient Descent(21220): loss=3.5536055074550217\n",
      "Stochastic Gradient Descent(21221): loss=5.511727919459402\n",
      "Stochastic Gradient Descent(21222): loss=2.38013720010398\n",
      "Stochastic Gradient Descent(21223): loss=21.161735360822046\n",
      "Stochastic Gradient Descent(21224): loss=0.13670189412000106\n",
      "Stochastic Gradient Descent(21225): loss=10.37574497292055\n",
      "Stochastic Gradient Descent(21226): loss=14.248601336874724\n",
      "Stochastic Gradient Descent(21227): loss=19.55983224797313\n",
      "Stochastic Gradient Descent(21228): loss=23.855452992433364\n",
      "Stochastic Gradient Descent(21229): loss=7.760142703355065\n",
      "Stochastic Gradient Descent(21230): loss=2.7638807134800776\n",
      "Stochastic Gradient Descent(21231): loss=0.1757054331114269\n",
      "Stochastic Gradient Descent(21232): loss=1.4004567559377523\n",
      "Stochastic Gradient Descent(21233): loss=2.4770775977035946\n",
      "Stochastic Gradient Descent(21234): loss=14.254532612695183\n",
      "Stochastic Gradient Descent(21235): loss=0.5880699025743025\n",
      "Stochastic Gradient Descent(21236): loss=4.370125083107003\n",
      "Stochastic Gradient Descent(21237): loss=9.482435059998155\n",
      "Stochastic Gradient Descent(21238): loss=3.7087173368198494\n",
      "Stochastic Gradient Descent(21239): loss=0.11033546159925507\n",
      "Stochastic Gradient Descent(21240): loss=0.0002126431102583845\n",
      "Stochastic Gradient Descent(21241): loss=0.708458951367075\n",
      "Stochastic Gradient Descent(21242): loss=1.1797462484833428\n",
      "Stochastic Gradient Descent(21243): loss=5.713811260214522\n",
      "Stochastic Gradient Descent(21244): loss=0.8401602624813516\n",
      "Stochastic Gradient Descent(21245): loss=2.1249543525047994\n",
      "Stochastic Gradient Descent(21246): loss=3.2202205875283605\n",
      "Stochastic Gradient Descent(21247): loss=16.1136236442905\n",
      "Stochastic Gradient Descent(21248): loss=12.127639111063315\n",
      "Stochastic Gradient Descent(21249): loss=0.02408621786779462\n",
      "Stochastic Gradient Descent(21250): loss=10.810722878647475\n",
      "Stochastic Gradient Descent(21251): loss=6.106015806088001\n",
      "Stochastic Gradient Descent(21252): loss=4.993308635154507\n",
      "Stochastic Gradient Descent(21253): loss=17.795627534311077\n",
      "Stochastic Gradient Descent(21254): loss=0.607627790807127\n",
      "Stochastic Gradient Descent(21255): loss=4.742316240982991\n",
      "Stochastic Gradient Descent(21256): loss=3.6263784044294387\n",
      "Stochastic Gradient Descent(21257): loss=1.7552722721133978\n",
      "Stochastic Gradient Descent(21258): loss=20.50130611393282\n",
      "Stochastic Gradient Descent(21259): loss=1.212828614850724\n",
      "Stochastic Gradient Descent(21260): loss=0.37869172917135535\n",
      "Stochastic Gradient Descent(21261): loss=2.7424659110472716\n",
      "Stochastic Gradient Descent(21262): loss=5.256078360587809\n",
      "Stochastic Gradient Descent(21263): loss=2.5525384937389153\n",
      "Stochastic Gradient Descent(21264): loss=7.016151085911377\n",
      "Stochastic Gradient Descent(21265): loss=1.481502646593995\n",
      "Stochastic Gradient Descent(21266): loss=1.3811400474980478\n",
      "Stochastic Gradient Descent(21267): loss=0.06964936443983094\n",
      "Stochastic Gradient Descent(21268): loss=16.00212859538938\n",
      "Stochastic Gradient Descent(21269): loss=60.28867594574011\n",
      "Stochastic Gradient Descent(21270): loss=0.0004668416363195505\n",
      "Stochastic Gradient Descent(21271): loss=3.3566546282076843\n",
      "Stochastic Gradient Descent(21272): loss=7.96398930929836\n",
      "Stochastic Gradient Descent(21273): loss=0.5317246135320021\n",
      "Stochastic Gradient Descent(21274): loss=0.3361280955151788\n",
      "Stochastic Gradient Descent(21275): loss=5.943512485615324\n",
      "Stochastic Gradient Descent(21276): loss=2.930757326178108\n",
      "Stochastic Gradient Descent(21277): loss=0.031706699213858325\n",
      "Stochastic Gradient Descent(21278): loss=0.0653483016485632\n",
      "Stochastic Gradient Descent(21279): loss=9.014913537884606\n",
      "Stochastic Gradient Descent(21280): loss=0.5201585401743779\n",
      "Stochastic Gradient Descent(21281): loss=0.23657329418911835\n",
      "Stochastic Gradient Descent(21282): loss=0.9493871817415706\n",
      "Stochastic Gradient Descent(21283): loss=5.282064628826576\n",
      "Stochastic Gradient Descent(21284): loss=1.6398351838471672\n",
      "Stochastic Gradient Descent(21285): loss=2.3735791016561505\n",
      "Stochastic Gradient Descent(21286): loss=12.374136214208177\n",
      "Stochastic Gradient Descent(21287): loss=2.625590376738905\n",
      "Stochastic Gradient Descent(21288): loss=4.221627821756635\n",
      "Stochastic Gradient Descent(21289): loss=4.6217383279882815\n",
      "Stochastic Gradient Descent(21290): loss=0.35486089539042687\n",
      "Stochastic Gradient Descent(21291): loss=6.498109640186818\n",
      "Stochastic Gradient Descent(21292): loss=15.509873034943844\n",
      "Stochastic Gradient Descent(21293): loss=3.65597488036187\n",
      "Stochastic Gradient Descent(21294): loss=1.6556450504861673\n",
      "Stochastic Gradient Descent(21295): loss=19.674194758569783\n",
      "Stochastic Gradient Descent(21296): loss=1.1138387883883294\n",
      "Stochastic Gradient Descent(21297): loss=6.27968278427609\n",
      "Stochastic Gradient Descent(21298): loss=10.259783652748645\n",
      "Stochastic Gradient Descent(21299): loss=2.939313031937579\n",
      "Stochastic Gradient Descent(21300): loss=1.4833042960150487\n",
      "Stochastic Gradient Descent(21301): loss=0.6991352795203007\n",
      "Stochastic Gradient Descent(21302): loss=0.053927443922900706\n",
      "Stochastic Gradient Descent(21303): loss=18.698467017506868\n",
      "Stochastic Gradient Descent(21304): loss=0.4259432746367015\n",
      "Stochastic Gradient Descent(21305): loss=11.86589143345276\n",
      "Stochastic Gradient Descent(21306): loss=15.31816614082458\n",
      "Stochastic Gradient Descent(21307): loss=1.1507918768257943\n",
      "Stochastic Gradient Descent(21308): loss=6.548582187657071\n",
      "Stochastic Gradient Descent(21309): loss=3.522981092130065\n",
      "Stochastic Gradient Descent(21310): loss=0.11161703962541446\n",
      "Stochastic Gradient Descent(21311): loss=5.521957825762387\n",
      "Stochastic Gradient Descent(21312): loss=0.5379094449217312\n",
      "Stochastic Gradient Descent(21313): loss=0.030943853852861934\n",
      "Stochastic Gradient Descent(21314): loss=2.100251491095311\n",
      "Stochastic Gradient Descent(21315): loss=8.112739475440605\n",
      "Stochastic Gradient Descent(21316): loss=7.132488531397726\n",
      "Stochastic Gradient Descent(21317): loss=2.39520487065311\n",
      "Stochastic Gradient Descent(21318): loss=3.086825117656345\n",
      "Stochastic Gradient Descent(21319): loss=1.8452634177938834\n",
      "Stochastic Gradient Descent(21320): loss=3.9877986828019236\n",
      "Stochastic Gradient Descent(21321): loss=15.940750395202302\n",
      "Stochastic Gradient Descent(21322): loss=0.0002958636474812455\n",
      "Stochastic Gradient Descent(21323): loss=33.627186866399946\n",
      "Stochastic Gradient Descent(21324): loss=1.975124381176304\n",
      "Stochastic Gradient Descent(21325): loss=0.012939806474061438\n",
      "Stochastic Gradient Descent(21326): loss=7.590965548828444\n",
      "Stochastic Gradient Descent(21327): loss=0.025227996178000194\n",
      "Stochastic Gradient Descent(21328): loss=0.12378814256505821\n",
      "Stochastic Gradient Descent(21329): loss=6.091712676925592\n",
      "Stochastic Gradient Descent(21330): loss=0.08558684765684448\n",
      "Stochastic Gradient Descent(21331): loss=14.683085644120437\n",
      "Stochastic Gradient Descent(21332): loss=0.31693516627934026\n",
      "Stochastic Gradient Descent(21333): loss=2.8688322300203772\n",
      "Stochastic Gradient Descent(21334): loss=2.8019321979764404\n",
      "Stochastic Gradient Descent(21335): loss=2.3003549563179306\n",
      "Stochastic Gradient Descent(21336): loss=5.483724396270782\n",
      "Stochastic Gradient Descent(21337): loss=0.8453921115797568\n",
      "Stochastic Gradient Descent(21338): loss=5.2439761838940955\n",
      "Stochastic Gradient Descent(21339): loss=3.827761748714378\n",
      "Stochastic Gradient Descent(21340): loss=0.00014392002715914398\n",
      "Stochastic Gradient Descent(21341): loss=33.46878238510854\n",
      "Stochastic Gradient Descent(21342): loss=0.24743642541114516\n",
      "Stochastic Gradient Descent(21343): loss=42.3330533682386\n",
      "Stochastic Gradient Descent(21344): loss=6.731368712569727\n",
      "Stochastic Gradient Descent(21345): loss=0.779539123292737\n",
      "Stochastic Gradient Descent(21346): loss=15.787176208352282\n",
      "Stochastic Gradient Descent(21347): loss=0.12571059537651297\n",
      "Stochastic Gradient Descent(21348): loss=1.0321402965048647\n",
      "Stochastic Gradient Descent(21349): loss=12.881150258788416\n",
      "Stochastic Gradient Descent(21350): loss=0.005171880769509002\n",
      "Stochastic Gradient Descent(21351): loss=0.25005262586759736\n",
      "Stochastic Gradient Descent(21352): loss=1.017822279362352\n",
      "Stochastic Gradient Descent(21353): loss=2.715909082590742\n",
      "Stochastic Gradient Descent(21354): loss=0.10000600463021579\n",
      "Stochastic Gradient Descent(21355): loss=3.3581409964090443\n",
      "Stochastic Gradient Descent(21356): loss=2.5893422854504418\n",
      "Stochastic Gradient Descent(21357): loss=0.2645926868993722\n",
      "Stochastic Gradient Descent(21358): loss=2.458451228738428\n",
      "Stochastic Gradient Descent(21359): loss=7.7349259894417886\n",
      "Stochastic Gradient Descent(21360): loss=9.595800386590446\n",
      "Stochastic Gradient Descent(21361): loss=16.363831149907153\n",
      "Stochastic Gradient Descent(21362): loss=0.0424183483943472\n",
      "Stochastic Gradient Descent(21363): loss=0.003914926954387709\n",
      "Stochastic Gradient Descent(21364): loss=0.18254798373585326\n",
      "Stochastic Gradient Descent(21365): loss=0.10645080703080556\n",
      "Stochastic Gradient Descent(21366): loss=2.3325146040202567\n",
      "Stochastic Gradient Descent(21367): loss=10.573077058235912\n",
      "Stochastic Gradient Descent(21368): loss=0.53844538814858\n",
      "Stochastic Gradient Descent(21369): loss=0.7784068476373354\n",
      "Stochastic Gradient Descent(21370): loss=0.051046247173372784\n",
      "Stochastic Gradient Descent(21371): loss=9.61443862822268\n",
      "Stochastic Gradient Descent(21372): loss=2.093620005593046\n",
      "Stochastic Gradient Descent(21373): loss=19.64946406722628\n",
      "Stochastic Gradient Descent(21374): loss=16.457248664927278\n",
      "Stochastic Gradient Descent(21375): loss=7.674772157971962\n",
      "Stochastic Gradient Descent(21376): loss=2.3138838468327325\n",
      "Stochastic Gradient Descent(21377): loss=4.945829174359519\n",
      "Stochastic Gradient Descent(21378): loss=1.437625482763667\n",
      "Stochastic Gradient Descent(21379): loss=4.28869480341971\n",
      "Stochastic Gradient Descent(21380): loss=0.7261793398954554\n",
      "Stochastic Gradient Descent(21381): loss=0.11823333997672535\n",
      "Stochastic Gradient Descent(21382): loss=11.289901860290279\n",
      "Stochastic Gradient Descent(21383): loss=11.618534924864035\n",
      "Stochastic Gradient Descent(21384): loss=3.4996831316993147\n",
      "Stochastic Gradient Descent(21385): loss=0.0033746678015629424\n",
      "Stochastic Gradient Descent(21386): loss=3.520089566569534\n",
      "Stochastic Gradient Descent(21387): loss=0.4746662330361238\n",
      "Stochastic Gradient Descent(21388): loss=2.3804310556934394\n",
      "Stochastic Gradient Descent(21389): loss=0.027729601087756614\n",
      "Stochastic Gradient Descent(21390): loss=3.46820893549454\n",
      "Stochastic Gradient Descent(21391): loss=2.2744018256835948e-05\n",
      "Stochastic Gradient Descent(21392): loss=0.26030336969280204\n",
      "Stochastic Gradient Descent(21393): loss=22.54294910759479\n",
      "Stochastic Gradient Descent(21394): loss=0.008195838603244339\n",
      "Stochastic Gradient Descent(21395): loss=0.0010505740151551507\n",
      "Stochastic Gradient Descent(21396): loss=0.3929283372180811\n",
      "Stochastic Gradient Descent(21397): loss=0.16063905221294122\n",
      "Stochastic Gradient Descent(21398): loss=6.805750486524983\n",
      "Stochastic Gradient Descent(21399): loss=0.9105939480503574\n",
      "Stochastic Gradient Descent(21400): loss=0.499899297034046\n",
      "Stochastic Gradient Descent(21401): loss=1.809281351625722\n",
      "Stochastic Gradient Descent(21402): loss=0.4443989061744209\n",
      "Stochastic Gradient Descent(21403): loss=0.25299546560320635\n",
      "Stochastic Gradient Descent(21404): loss=0.11831456650694164\n",
      "Stochastic Gradient Descent(21405): loss=18.172649352436213\n",
      "Stochastic Gradient Descent(21406): loss=0.14534934424922752\n",
      "Stochastic Gradient Descent(21407): loss=1.045124905878992\n",
      "Stochastic Gradient Descent(21408): loss=5.12710562280564\n",
      "Stochastic Gradient Descent(21409): loss=9.029438481927073\n",
      "Stochastic Gradient Descent(21410): loss=0.27723055371972155\n",
      "Stochastic Gradient Descent(21411): loss=3.0588984334988574\n",
      "Stochastic Gradient Descent(21412): loss=0.18399918642088017\n",
      "Stochastic Gradient Descent(21413): loss=0.5576503980293412\n",
      "Stochastic Gradient Descent(21414): loss=0.1913786050751772\n",
      "Stochastic Gradient Descent(21415): loss=2.0009459078962175\n",
      "Stochastic Gradient Descent(21416): loss=3.664357265094883\n",
      "Stochastic Gradient Descent(21417): loss=2.754883054252353\n",
      "Stochastic Gradient Descent(21418): loss=0.16028141663975584\n",
      "Stochastic Gradient Descent(21419): loss=1.43751977675447\n",
      "Stochastic Gradient Descent(21420): loss=0.052946174062049246\n",
      "Stochastic Gradient Descent(21421): loss=0.25606960022758607\n",
      "Stochastic Gradient Descent(21422): loss=0.23738231146017838\n",
      "Stochastic Gradient Descent(21423): loss=0.734823100871855\n",
      "Stochastic Gradient Descent(21424): loss=1.2943241364330307\n",
      "Stochastic Gradient Descent(21425): loss=0.26760280300129785\n",
      "Stochastic Gradient Descent(21426): loss=4.464111928361534\n",
      "Stochastic Gradient Descent(21427): loss=0.06160699505169762\n",
      "Stochastic Gradient Descent(21428): loss=3.209108629601079\n",
      "Stochastic Gradient Descent(21429): loss=8.475228229165861\n",
      "Stochastic Gradient Descent(21430): loss=1.696186060051577\n",
      "Stochastic Gradient Descent(21431): loss=3.09590435898581\n",
      "Stochastic Gradient Descent(21432): loss=0.7557577984384782\n",
      "Stochastic Gradient Descent(21433): loss=23.633086055249425\n",
      "Stochastic Gradient Descent(21434): loss=14.61434409444097\n",
      "Stochastic Gradient Descent(21435): loss=8.837707087428061\n",
      "Stochastic Gradient Descent(21436): loss=0.29488033062912544\n",
      "Stochastic Gradient Descent(21437): loss=2.1397759894893116\n",
      "Stochastic Gradient Descent(21438): loss=4.703702430498725\n",
      "Stochastic Gradient Descent(21439): loss=0.6775077055999791\n",
      "Stochastic Gradient Descent(21440): loss=3.4367511902669827\n",
      "Stochastic Gradient Descent(21441): loss=4.7553685540293475e-06\n",
      "Stochastic Gradient Descent(21442): loss=1.8791698844252216\n",
      "Stochastic Gradient Descent(21443): loss=0.4740221235759223\n",
      "Stochastic Gradient Descent(21444): loss=5.307179882128771\n",
      "Stochastic Gradient Descent(21445): loss=4.526860784718137\n",
      "Stochastic Gradient Descent(21446): loss=8.024584001621214\n",
      "Stochastic Gradient Descent(21447): loss=0.3412545586498127\n",
      "Stochastic Gradient Descent(21448): loss=6.16663479740497\n",
      "Stochastic Gradient Descent(21449): loss=7.150287565099437\n",
      "Stochastic Gradient Descent(21450): loss=3.694048097233192\n",
      "Stochastic Gradient Descent(21451): loss=1.5741751741133858\n",
      "Stochastic Gradient Descent(21452): loss=44.63844734883572\n",
      "Stochastic Gradient Descent(21453): loss=37.03599328640737\n",
      "Stochastic Gradient Descent(21454): loss=11.484711285885354\n",
      "Stochastic Gradient Descent(21455): loss=1.324300056867482\n",
      "Stochastic Gradient Descent(21456): loss=9.534078713764428\n",
      "Stochastic Gradient Descent(21457): loss=0.00910548899575727\n",
      "Stochastic Gradient Descent(21458): loss=33.10708010138708\n",
      "Stochastic Gradient Descent(21459): loss=59.21179244393059\n",
      "Stochastic Gradient Descent(21460): loss=9.31489486250576\n",
      "Stochastic Gradient Descent(21461): loss=0.538076360382149\n",
      "Stochastic Gradient Descent(21462): loss=1.0986451275914235\n",
      "Stochastic Gradient Descent(21463): loss=0.21528927960326555\n",
      "Stochastic Gradient Descent(21464): loss=1.3678861036066745\n",
      "Stochastic Gradient Descent(21465): loss=1.5696678602583458\n",
      "Stochastic Gradient Descent(21466): loss=0.18664145550678846\n",
      "Stochastic Gradient Descent(21467): loss=9.297632960733914\n",
      "Stochastic Gradient Descent(21468): loss=0.10715319464772223\n",
      "Stochastic Gradient Descent(21469): loss=3.9459411457151\n",
      "Stochastic Gradient Descent(21470): loss=0.9461056099831963\n",
      "Stochastic Gradient Descent(21471): loss=8.023098735070064\n",
      "Stochastic Gradient Descent(21472): loss=1.711323196217497\n",
      "Stochastic Gradient Descent(21473): loss=0.15601774369694796\n",
      "Stochastic Gradient Descent(21474): loss=0.31397354268941513\n",
      "Stochastic Gradient Descent(21475): loss=2.897175066571474\n",
      "Stochastic Gradient Descent(21476): loss=0.7353871821007085\n",
      "Stochastic Gradient Descent(21477): loss=1.4400091258716732\n",
      "Stochastic Gradient Descent(21478): loss=1.5958502795787208\n",
      "Stochastic Gradient Descent(21479): loss=8.772980729046433\n",
      "Stochastic Gradient Descent(21480): loss=10.47040156699467\n",
      "Stochastic Gradient Descent(21481): loss=3.226087838001979\n",
      "Stochastic Gradient Descent(21482): loss=1.57056311542693\n",
      "Stochastic Gradient Descent(21483): loss=0.3104574345479191\n",
      "Stochastic Gradient Descent(21484): loss=2.323653107643615\n",
      "Stochastic Gradient Descent(21485): loss=7.300770995395265\n",
      "Stochastic Gradient Descent(21486): loss=1.2856824890019805\n",
      "Stochastic Gradient Descent(21487): loss=6.515188838024864\n",
      "Stochastic Gradient Descent(21488): loss=5.408340170347615\n",
      "Stochastic Gradient Descent(21489): loss=1.6353431082766074\n",
      "Stochastic Gradient Descent(21490): loss=5.848098769445027\n",
      "Stochastic Gradient Descent(21491): loss=25.52659317582238\n",
      "Stochastic Gradient Descent(21492): loss=2.6928984472716486\n",
      "Stochastic Gradient Descent(21493): loss=0.6990266747336067\n",
      "Stochastic Gradient Descent(21494): loss=1.998378356153769\n",
      "Stochastic Gradient Descent(21495): loss=2.8155083822846216\n",
      "Stochastic Gradient Descent(21496): loss=13.44272391830641\n",
      "Stochastic Gradient Descent(21497): loss=1.12988119625572\n",
      "Stochastic Gradient Descent(21498): loss=4.3622217907573\n",
      "Stochastic Gradient Descent(21499): loss=3.414594869793364\n",
      "Stochastic Gradient Descent(21500): loss=2.3894786029534134\n",
      "Stochastic Gradient Descent(21501): loss=0.28952259248058715\n",
      "Stochastic Gradient Descent(21502): loss=0.7405099957601256\n",
      "Stochastic Gradient Descent(21503): loss=37.5340618226853\n",
      "Stochastic Gradient Descent(21504): loss=4.300434753914101\n",
      "Stochastic Gradient Descent(21505): loss=0.46383328234785176\n",
      "Stochastic Gradient Descent(21506): loss=5.384727303128028\n",
      "Stochastic Gradient Descent(21507): loss=9.878591254745881\n",
      "Stochastic Gradient Descent(21508): loss=7.4815337293281745\n",
      "Stochastic Gradient Descent(21509): loss=44.70445965603348\n",
      "Stochastic Gradient Descent(21510): loss=0.610294617869963\n",
      "Stochastic Gradient Descent(21511): loss=2.397962270319636\n",
      "Stochastic Gradient Descent(21512): loss=0.3109073047363645\n",
      "Stochastic Gradient Descent(21513): loss=14.954649581280346\n",
      "Stochastic Gradient Descent(21514): loss=0.6149652658885849\n",
      "Stochastic Gradient Descent(21515): loss=0.7195763115629402\n",
      "Stochastic Gradient Descent(21516): loss=6.659169085312395\n",
      "Stochastic Gradient Descent(21517): loss=0.31279652850763245\n",
      "Stochastic Gradient Descent(21518): loss=33.98798010969968\n",
      "Stochastic Gradient Descent(21519): loss=4.552313498555356\n",
      "Stochastic Gradient Descent(21520): loss=0.18194443755757556\n",
      "Stochastic Gradient Descent(21521): loss=11.162861829124992\n",
      "Stochastic Gradient Descent(21522): loss=2.9615907316979633\n",
      "Stochastic Gradient Descent(21523): loss=0.09930323620824845\n",
      "Stochastic Gradient Descent(21524): loss=13.3565993376021\n",
      "Stochastic Gradient Descent(21525): loss=0.21852233896525705\n",
      "Stochastic Gradient Descent(21526): loss=19.52719199991914\n",
      "Stochastic Gradient Descent(21527): loss=4.151146097007949\n",
      "Stochastic Gradient Descent(21528): loss=12.897655411438187\n",
      "Stochastic Gradient Descent(21529): loss=0.14819707769660612\n",
      "Stochastic Gradient Descent(21530): loss=1.3992915789977425\n",
      "Stochastic Gradient Descent(21531): loss=0.40815091137523224\n",
      "Stochastic Gradient Descent(21532): loss=2.410547682664773\n",
      "Stochastic Gradient Descent(21533): loss=1.7063297250853087\n",
      "Stochastic Gradient Descent(21534): loss=7.445186360826871\n",
      "Stochastic Gradient Descent(21535): loss=1.6215751679899566\n",
      "Stochastic Gradient Descent(21536): loss=21.049101514719407\n",
      "Stochastic Gradient Descent(21537): loss=6.0811338217169935\n",
      "Stochastic Gradient Descent(21538): loss=3.6499848915204396\n",
      "Stochastic Gradient Descent(21539): loss=0.18811448507510745\n",
      "Stochastic Gradient Descent(21540): loss=1.2074967461271617\n",
      "Stochastic Gradient Descent(21541): loss=0.74880019234771\n",
      "Stochastic Gradient Descent(21542): loss=5.281933114285067\n",
      "Stochastic Gradient Descent(21543): loss=1.6298904187770968\n",
      "Stochastic Gradient Descent(21544): loss=21.4979808622607\n",
      "Stochastic Gradient Descent(21545): loss=2.2199234244090693\n",
      "Stochastic Gradient Descent(21546): loss=15.868386569962928\n",
      "Stochastic Gradient Descent(21547): loss=3.774712418126252\n",
      "Stochastic Gradient Descent(21548): loss=5.714380222257842\n",
      "Stochastic Gradient Descent(21549): loss=1.0749065956304047\n",
      "Stochastic Gradient Descent(21550): loss=5.490234243961228\n",
      "Stochastic Gradient Descent(21551): loss=1.2338978825523954\n",
      "Stochastic Gradient Descent(21552): loss=15.405027855877002\n",
      "Stochastic Gradient Descent(21553): loss=9.672675423304577\n",
      "Stochastic Gradient Descent(21554): loss=8.259098918286863\n",
      "Stochastic Gradient Descent(21555): loss=0.04110310946080163\n",
      "Stochastic Gradient Descent(21556): loss=0.2783245388079978\n",
      "Stochastic Gradient Descent(21557): loss=7.300363413195208\n",
      "Stochastic Gradient Descent(21558): loss=0.061808971145421804\n",
      "Stochastic Gradient Descent(21559): loss=4.282668549839836\n",
      "Stochastic Gradient Descent(21560): loss=3.432844364530887\n",
      "Stochastic Gradient Descent(21561): loss=11.072250980963545\n",
      "Stochastic Gradient Descent(21562): loss=0.18844017218451306\n",
      "Stochastic Gradient Descent(21563): loss=0.08785959973611972\n",
      "Stochastic Gradient Descent(21564): loss=0.021959505695283956\n",
      "Stochastic Gradient Descent(21565): loss=1.638876423950797\n",
      "Stochastic Gradient Descent(21566): loss=0.07894308389560281\n",
      "Stochastic Gradient Descent(21567): loss=1.6219171387535942\n",
      "Stochastic Gradient Descent(21568): loss=3.33872752325464\n",
      "Stochastic Gradient Descent(21569): loss=1.1103516017420008\n",
      "Stochastic Gradient Descent(21570): loss=1.5749535117434919\n",
      "Stochastic Gradient Descent(21571): loss=0.691522286590847\n",
      "Stochastic Gradient Descent(21572): loss=0.993320288648112\n",
      "Stochastic Gradient Descent(21573): loss=6.31349956642853\n",
      "Stochastic Gradient Descent(21574): loss=6.344009682790903\n",
      "Stochastic Gradient Descent(21575): loss=15.593598536187693\n",
      "Stochastic Gradient Descent(21576): loss=13.998505544880322\n",
      "Stochastic Gradient Descent(21577): loss=13.981715609341645\n",
      "Stochastic Gradient Descent(21578): loss=0.00799602603767456\n",
      "Stochastic Gradient Descent(21579): loss=2.4236224087136753\n",
      "Stochastic Gradient Descent(21580): loss=0.025777865995406685\n",
      "Stochastic Gradient Descent(21581): loss=0.8833895370311403\n",
      "Stochastic Gradient Descent(21582): loss=0.5563770621086067\n",
      "Stochastic Gradient Descent(21583): loss=1.0862678570218829\n",
      "Stochastic Gradient Descent(21584): loss=0.4280893610787593\n",
      "Stochastic Gradient Descent(21585): loss=0.18476376286914128\n",
      "Stochastic Gradient Descent(21586): loss=17.05358818679635\n",
      "Stochastic Gradient Descent(21587): loss=1.6767657849132231\n",
      "Stochastic Gradient Descent(21588): loss=4.554645472437456\n",
      "Stochastic Gradient Descent(21589): loss=0.6142765328605666\n",
      "Stochastic Gradient Descent(21590): loss=0.9508384183050607\n",
      "Stochastic Gradient Descent(21591): loss=1.7009411683283246\n",
      "Stochastic Gradient Descent(21592): loss=0.9048548946942968\n",
      "Stochastic Gradient Descent(21593): loss=0.6726343034298773\n",
      "Stochastic Gradient Descent(21594): loss=1.5215114793497588\n",
      "Stochastic Gradient Descent(21595): loss=2.767929825427986\n",
      "Stochastic Gradient Descent(21596): loss=0.8163327497434878\n",
      "Stochastic Gradient Descent(21597): loss=0.00040258888401089097\n",
      "Stochastic Gradient Descent(21598): loss=0.0023597871536370795\n",
      "Stochastic Gradient Descent(21599): loss=13.809556316840384\n",
      "Stochastic Gradient Descent(21600): loss=2.7972801286278304\n",
      "Stochastic Gradient Descent(21601): loss=6.722487810810891\n",
      "Stochastic Gradient Descent(21602): loss=5.130242702197011\n",
      "Stochastic Gradient Descent(21603): loss=1.5716557372851852\n",
      "Stochastic Gradient Descent(21604): loss=9.946276993150793\n",
      "Stochastic Gradient Descent(21605): loss=17.72367033994353\n",
      "Stochastic Gradient Descent(21606): loss=37.243039744234686\n",
      "Stochastic Gradient Descent(21607): loss=3.3555166367091545\n",
      "Stochastic Gradient Descent(21608): loss=0.7898966764424117\n",
      "Stochastic Gradient Descent(21609): loss=1.465065553249\n",
      "Stochastic Gradient Descent(21610): loss=0.17090722987186913\n",
      "Stochastic Gradient Descent(21611): loss=4.303315519191846\n",
      "Stochastic Gradient Descent(21612): loss=3.2422805485306205\n",
      "Stochastic Gradient Descent(21613): loss=0.1412388220174721\n",
      "Stochastic Gradient Descent(21614): loss=1.1109407417753556\n",
      "Stochastic Gradient Descent(21615): loss=0.34505147215949\n",
      "Stochastic Gradient Descent(21616): loss=0.1436546948892732\n",
      "Stochastic Gradient Descent(21617): loss=0.0064754978182545485\n",
      "Stochastic Gradient Descent(21618): loss=13.61415602124877\n",
      "Stochastic Gradient Descent(21619): loss=0.001345626006966131\n",
      "Stochastic Gradient Descent(21620): loss=1.3261827229040013\n",
      "Stochastic Gradient Descent(21621): loss=1.898108414485764\n",
      "Stochastic Gradient Descent(21622): loss=4.332671225780201\n",
      "Stochastic Gradient Descent(21623): loss=33.794648641222444\n",
      "Stochastic Gradient Descent(21624): loss=0.00037939976644663336\n",
      "Stochastic Gradient Descent(21625): loss=18.622780995881993\n",
      "Stochastic Gradient Descent(21626): loss=1.2515218349505284\n",
      "Stochastic Gradient Descent(21627): loss=11.602557410886225\n",
      "Stochastic Gradient Descent(21628): loss=8.799227996328527\n",
      "Stochastic Gradient Descent(21629): loss=10.418980477272116\n",
      "Stochastic Gradient Descent(21630): loss=6.836427308975063\n",
      "Stochastic Gradient Descent(21631): loss=6.22804734496093\n",
      "Stochastic Gradient Descent(21632): loss=0.25476219089666785\n",
      "Stochastic Gradient Descent(21633): loss=0.14779789800567517\n",
      "Stochastic Gradient Descent(21634): loss=2.066842022171801\n",
      "Stochastic Gradient Descent(21635): loss=0.02370407584638004\n",
      "Stochastic Gradient Descent(21636): loss=0.14418252019285704\n",
      "Stochastic Gradient Descent(21637): loss=5.183467614290611\n",
      "Stochastic Gradient Descent(21638): loss=2.2348440421695153\n",
      "Stochastic Gradient Descent(21639): loss=3.9085764088460593\n",
      "Stochastic Gradient Descent(21640): loss=4.885491155884881\n",
      "Stochastic Gradient Descent(21641): loss=5.535677835796172\n",
      "Stochastic Gradient Descent(21642): loss=1.5463952245035322\n",
      "Stochastic Gradient Descent(21643): loss=7.862755868595944\n",
      "Stochastic Gradient Descent(21644): loss=3.3475549224863825\n",
      "Stochastic Gradient Descent(21645): loss=11.717429342844184\n",
      "Stochastic Gradient Descent(21646): loss=9.038159586106081\n",
      "Stochastic Gradient Descent(21647): loss=0.005876660199700637\n",
      "Stochastic Gradient Descent(21648): loss=39.588645825926406\n",
      "Stochastic Gradient Descent(21649): loss=13.326831318081299\n",
      "Stochastic Gradient Descent(21650): loss=1.625108818660733\n",
      "Stochastic Gradient Descent(21651): loss=4.992718806278749\n",
      "Stochastic Gradient Descent(21652): loss=5.895507064765269\n",
      "Stochastic Gradient Descent(21653): loss=0.02980611604974369\n",
      "Stochastic Gradient Descent(21654): loss=7.392807487129141\n",
      "Stochastic Gradient Descent(21655): loss=28.813741854749402\n",
      "Stochastic Gradient Descent(21656): loss=7.209707358151985e-05\n",
      "Stochastic Gradient Descent(21657): loss=2.413907299332221\n",
      "Stochastic Gradient Descent(21658): loss=6.226898651437476\n",
      "Stochastic Gradient Descent(21659): loss=10.171162344580964\n",
      "Stochastic Gradient Descent(21660): loss=3.821508923065595\n",
      "Stochastic Gradient Descent(21661): loss=1.4305569200722006\n",
      "Stochastic Gradient Descent(21662): loss=9.831911281367933\n",
      "Stochastic Gradient Descent(21663): loss=0.37634653102696886\n",
      "Stochastic Gradient Descent(21664): loss=2.5243487145695855\n",
      "Stochastic Gradient Descent(21665): loss=0.5180756960057059\n",
      "Stochastic Gradient Descent(21666): loss=0.025594607184812296\n",
      "Stochastic Gradient Descent(21667): loss=28.480562987033267\n",
      "Stochastic Gradient Descent(21668): loss=7.556599577163513\n",
      "Stochastic Gradient Descent(21669): loss=3.3139534187892763\n",
      "Stochastic Gradient Descent(21670): loss=0.331376876115875\n",
      "Stochastic Gradient Descent(21671): loss=0.31606252566802956\n",
      "Stochastic Gradient Descent(21672): loss=1.1230054284969861\n",
      "Stochastic Gradient Descent(21673): loss=0.07379988280838723\n",
      "Stochastic Gradient Descent(21674): loss=4.1509693360938105\n",
      "Stochastic Gradient Descent(21675): loss=4.910262197127389\n",
      "Stochastic Gradient Descent(21676): loss=0.000232401716810659\n",
      "Stochastic Gradient Descent(21677): loss=0.761766871530382\n",
      "Stochastic Gradient Descent(21678): loss=0.0057653030758425886\n",
      "Stochastic Gradient Descent(21679): loss=0.137576169522918\n",
      "Stochastic Gradient Descent(21680): loss=0.16801502651874117\n",
      "Stochastic Gradient Descent(21681): loss=0.33895856896404275\n",
      "Stochastic Gradient Descent(21682): loss=0.30025920880536633\n",
      "Stochastic Gradient Descent(21683): loss=4.381143393087887\n",
      "Stochastic Gradient Descent(21684): loss=0.48628854140095545\n",
      "Stochastic Gradient Descent(21685): loss=0.001058638803815878\n",
      "Stochastic Gradient Descent(21686): loss=6.269208555870965\n",
      "Stochastic Gradient Descent(21687): loss=0.06125867452627761\n",
      "Stochastic Gradient Descent(21688): loss=8.399443190124686\n",
      "Stochastic Gradient Descent(21689): loss=1.2945285582611599\n",
      "Stochastic Gradient Descent(21690): loss=6.5476903827845225\n",
      "Stochastic Gradient Descent(21691): loss=1.683638304573674\n",
      "Stochastic Gradient Descent(21692): loss=18.756399077261275\n",
      "Stochastic Gradient Descent(21693): loss=8.607284349163017\n",
      "Stochastic Gradient Descent(21694): loss=6.302155884301024\n",
      "Stochastic Gradient Descent(21695): loss=2.8497091559147663\n",
      "Stochastic Gradient Descent(21696): loss=1.5416470417629984\n",
      "Stochastic Gradient Descent(21697): loss=3.6829384087465487\n",
      "Stochastic Gradient Descent(21698): loss=0.7523429971639876\n",
      "Stochastic Gradient Descent(21699): loss=4.6873866678724525\n",
      "Stochastic Gradient Descent(21700): loss=35.38033796188173\n",
      "Stochastic Gradient Descent(21701): loss=28.52339948223818\n",
      "Stochastic Gradient Descent(21702): loss=0.04998323614657252\n",
      "Stochastic Gradient Descent(21703): loss=0.09058102809173513\n",
      "Stochastic Gradient Descent(21704): loss=13.254746764201753\n",
      "Stochastic Gradient Descent(21705): loss=5.337561610237085\n",
      "Stochastic Gradient Descent(21706): loss=169.13222468421097\n",
      "Stochastic Gradient Descent(21707): loss=57.63427434937677\n",
      "Stochastic Gradient Descent(21708): loss=83.40726507789466\n",
      "Stochastic Gradient Descent(21709): loss=5.50862407603219\n",
      "Stochastic Gradient Descent(21710): loss=13.195398047425707\n",
      "Stochastic Gradient Descent(21711): loss=0.4049035229993788\n",
      "Stochastic Gradient Descent(21712): loss=10.717345142073063\n",
      "Stochastic Gradient Descent(21713): loss=0.3338259840615559\n",
      "Stochastic Gradient Descent(21714): loss=8.822225298627357\n",
      "Stochastic Gradient Descent(21715): loss=0.2501569827839805\n",
      "Stochastic Gradient Descent(21716): loss=0.2628285705870668\n",
      "Stochastic Gradient Descent(21717): loss=2.5685963673744476\n",
      "Stochastic Gradient Descent(21718): loss=16.54726207830413\n",
      "Stochastic Gradient Descent(21719): loss=32.40280890193706\n",
      "Stochastic Gradient Descent(21720): loss=8.71034585890895\n",
      "Stochastic Gradient Descent(21721): loss=5.829514025372604\n",
      "Stochastic Gradient Descent(21722): loss=0.8912823322018888\n",
      "Stochastic Gradient Descent(21723): loss=0.6066961297127734\n",
      "Stochastic Gradient Descent(21724): loss=5.75547366078271\n",
      "Stochastic Gradient Descent(21725): loss=3.3463188779536956\n",
      "Stochastic Gradient Descent(21726): loss=2.0262370386111948\n",
      "Stochastic Gradient Descent(21727): loss=3.009018941422533\n",
      "Stochastic Gradient Descent(21728): loss=0.06996234789758804\n",
      "Stochastic Gradient Descent(21729): loss=0.795305237937391\n",
      "Stochastic Gradient Descent(21730): loss=5.295838437469807\n",
      "Stochastic Gradient Descent(21731): loss=1.898770511418931\n",
      "Stochastic Gradient Descent(21732): loss=2.0459628495852016\n",
      "Stochastic Gradient Descent(21733): loss=0.1442492921432622\n",
      "Stochastic Gradient Descent(21734): loss=0.05719895002009202\n",
      "Stochastic Gradient Descent(21735): loss=5.23190639433457\n",
      "Stochastic Gradient Descent(21736): loss=12.054620504856988\n",
      "Stochastic Gradient Descent(21737): loss=1.2281525845540848\n",
      "Stochastic Gradient Descent(21738): loss=1.6607949381948999\n",
      "Stochastic Gradient Descent(21739): loss=3.0416212477286164\n",
      "Stochastic Gradient Descent(21740): loss=0.008924720990114874\n",
      "Stochastic Gradient Descent(21741): loss=0.7160654394587502\n",
      "Stochastic Gradient Descent(21742): loss=1.9796033648683689\n",
      "Stochastic Gradient Descent(21743): loss=0.5331406326244754\n",
      "Stochastic Gradient Descent(21744): loss=2.4654178823818587\n",
      "Stochastic Gradient Descent(21745): loss=0.9092437075592126\n",
      "Stochastic Gradient Descent(21746): loss=0.18342623711381836\n",
      "Stochastic Gradient Descent(21747): loss=8.134116217879376\n",
      "Stochastic Gradient Descent(21748): loss=5.631559246037199\n",
      "Stochastic Gradient Descent(21749): loss=0.7678768902377643\n",
      "Stochastic Gradient Descent(21750): loss=0.027265488017579966\n",
      "Stochastic Gradient Descent(21751): loss=0.29191770015528473\n",
      "Stochastic Gradient Descent(21752): loss=0.2065348862013097\n",
      "Stochastic Gradient Descent(21753): loss=0.11594527268395502\n",
      "Stochastic Gradient Descent(21754): loss=0.047887387714837644\n",
      "Stochastic Gradient Descent(21755): loss=4.293627213434834\n",
      "Stochastic Gradient Descent(21756): loss=4.450894903452635\n",
      "Stochastic Gradient Descent(21757): loss=1.8402976683482932\n",
      "Stochastic Gradient Descent(21758): loss=13.354145608745679\n",
      "Stochastic Gradient Descent(21759): loss=0.6669916221912302\n",
      "Stochastic Gradient Descent(21760): loss=0.43605106111375713\n",
      "Stochastic Gradient Descent(21761): loss=2.0331617889985543\n",
      "Stochastic Gradient Descent(21762): loss=7.933717020136804\n",
      "Stochastic Gradient Descent(21763): loss=1.107171350127224\n",
      "Stochastic Gradient Descent(21764): loss=0.48993381563875227\n",
      "Stochastic Gradient Descent(21765): loss=8.583221553676458\n",
      "Stochastic Gradient Descent(21766): loss=1.1292025634185743\n",
      "Stochastic Gradient Descent(21767): loss=0.36953246991804795\n",
      "Stochastic Gradient Descent(21768): loss=2.009871589069278\n",
      "Stochastic Gradient Descent(21769): loss=7.890975705491868\n",
      "Stochastic Gradient Descent(21770): loss=0.19038336374344733\n",
      "Stochastic Gradient Descent(21771): loss=2.4748085096257486\n",
      "Stochastic Gradient Descent(21772): loss=0.9189441707729747\n",
      "Stochastic Gradient Descent(21773): loss=3.983043227744556\n",
      "Stochastic Gradient Descent(21774): loss=7.266324955400292\n",
      "Stochastic Gradient Descent(21775): loss=17.293087909811145\n",
      "Stochastic Gradient Descent(21776): loss=0.36159125061794944\n",
      "Stochastic Gradient Descent(21777): loss=0.21353943595113448\n",
      "Stochastic Gradient Descent(21778): loss=0.08108356358989675\n",
      "Stochastic Gradient Descent(21779): loss=21.879012311806417\n",
      "Stochastic Gradient Descent(21780): loss=24.002616484328158\n",
      "Stochastic Gradient Descent(21781): loss=0.09881176807731182\n",
      "Stochastic Gradient Descent(21782): loss=0.022434660144949815\n",
      "Stochastic Gradient Descent(21783): loss=0.7270891769788623\n",
      "Stochastic Gradient Descent(21784): loss=0.5550484248445612\n",
      "Stochastic Gradient Descent(21785): loss=15.687931503662364\n",
      "Stochastic Gradient Descent(21786): loss=10.079563583280036\n",
      "Stochastic Gradient Descent(21787): loss=0.01626778072803867\n",
      "Stochastic Gradient Descent(21788): loss=6.385127759465803\n",
      "Stochastic Gradient Descent(21789): loss=2.3660445008603443\n",
      "Stochastic Gradient Descent(21790): loss=0.5345542473911767\n",
      "Stochastic Gradient Descent(21791): loss=10.806744810002906\n",
      "Stochastic Gradient Descent(21792): loss=1.0972165429521779\n",
      "Stochastic Gradient Descent(21793): loss=0.566233346959283\n",
      "Stochastic Gradient Descent(21794): loss=1.6656027729617475\n",
      "Stochastic Gradient Descent(21795): loss=10.316346317477217\n",
      "Stochastic Gradient Descent(21796): loss=0.2747654829984728\n",
      "Stochastic Gradient Descent(21797): loss=2.3513299531289373\n",
      "Stochastic Gradient Descent(21798): loss=7.302713531690004\n",
      "Stochastic Gradient Descent(21799): loss=0.01324968327071046\n",
      "Stochastic Gradient Descent(21800): loss=1.1804313703667515\n",
      "Stochastic Gradient Descent(21801): loss=2.6106438257142175\n",
      "Stochastic Gradient Descent(21802): loss=2.8177614770694444\n",
      "Stochastic Gradient Descent(21803): loss=0.09325434803868231\n",
      "Stochastic Gradient Descent(21804): loss=8.313737664568574\n",
      "Stochastic Gradient Descent(21805): loss=3.207144426120016\n",
      "Stochastic Gradient Descent(21806): loss=1.4523151800633873\n",
      "Stochastic Gradient Descent(21807): loss=4.069726004650996\n",
      "Stochastic Gradient Descent(21808): loss=1.7354855512425669\n",
      "Stochastic Gradient Descent(21809): loss=0.04692564838113329\n",
      "Stochastic Gradient Descent(21810): loss=3.157584400802769\n",
      "Stochastic Gradient Descent(21811): loss=0.4257870511992487\n",
      "Stochastic Gradient Descent(21812): loss=4.27409950827047\n",
      "Stochastic Gradient Descent(21813): loss=0.11511741259186012\n",
      "Stochastic Gradient Descent(21814): loss=3.6491810326743837\n",
      "Stochastic Gradient Descent(21815): loss=14.016947811024062\n",
      "Stochastic Gradient Descent(21816): loss=0.021562965190533728\n",
      "Stochastic Gradient Descent(21817): loss=0.5336616634764012\n",
      "Stochastic Gradient Descent(21818): loss=1.3410101534767633\n",
      "Stochastic Gradient Descent(21819): loss=3.3466433901524915\n",
      "Stochastic Gradient Descent(21820): loss=22.828806833317827\n",
      "Stochastic Gradient Descent(21821): loss=1.457111459442936\n",
      "Stochastic Gradient Descent(21822): loss=15.174037995180319\n",
      "Stochastic Gradient Descent(21823): loss=4.456884414329458\n",
      "Stochastic Gradient Descent(21824): loss=0.0047345581753505265\n",
      "Stochastic Gradient Descent(21825): loss=2.671368309210936\n",
      "Stochastic Gradient Descent(21826): loss=3.7369317086093625\n",
      "Stochastic Gradient Descent(21827): loss=0.26121870842674183\n",
      "Stochastic Gradient Descent(21828): loss=14.660375002082636\n",
      "Stochastic Gradient Descent(21829): loss=3.086254408864375\n",
      "Stochastic Gradient Descent(21830): loss=0.24875720598016568\n",
      "Stochastic Gradient Descent(21831): loss=10.449651827075705\n",
      "Stochastic Gradient Descent(21832): loss=0.27404350866133825\n",
      "Stochastic Gradient Descent(21833): loss=13.464993012500729\n",
      "Stochastic Gradient Descent(21834): loss=3.9674386665734644\n",
      "Stochastic Gradient Descent(21835): loss=11.738736557440461\n",
      "Stochastic Gradient Descent(21836): loss=0.00730454628580692\n",
      "Stochastic Gradient Descent(21837): loss=15.764770696995523\n",
      "Stochastic Gradient Descent(21838): loss=2.2187117758056427\n",
      "Stochastic Gradient Descent(21839): loss=19.496558699672974\n",
      "Stochastic Gradient Descent(21840): loss=1.1206279328374993\n",
      "Stochastic Gradient Descent(21841): loss=24.482242656712987\n",
      "Stochastic Gradient Descent(21842): loss=0.004556560052933086\n",
      "Stochastic Gradient Descent(21843): loss=2.943305675189022\n",
      "Stochastic Gradient Descent(21844): loss=0.32885616462878037\n",
      "Stochastic Gradient Descent(21845): loss=0.25178992964120755\n",
      "Stochastic Gradient Descent(21846): loss=1.223483585201939\n",
      "Stochastic Gradient Descent(21847): loss=1.1833207774177572\n",
      "Stochastic Gradient Descent(21848): loss=28.38521085384892\n",
      "Stochastic Gradient Descent(21849): loss=0.34040691111597804\n",
      "Stochastic Gradient Descent(21850): loss=0.35389820894523627\n",
      "Stochastic Gradient Descent(21851): loss=4.388605866066602\n",
      "Stochastic Gradient Descent(21852): loss=8.992210579466146\n",
      "Stochastic Gradient Descent(21853): loss=0.040250545499618456\n",
      "Stochastic Gradient Descent(21854): loss=21.540939217094785\n",
      "Stochastic Gradient Descent(21855): loss=0.24956009047746214\n",
      "Stochastic Gradient Descent(21856): loss=2.316884305272104\n",
      "Stochastic Gradient Descent(21857): loss=0.15529065942693457\n",
      "Stochastic Gradient Descent(21858): loss=28.226325242869624\n",
      "Stochastic Gradient Descent(21859): loss=1.5364340138630272\n",
      "Stochastic Gradient Descent(21860): loss=0.04276142389400227\n",
      "Stochastic Gradient Descent(21861): loss=5.993148383468127\n",
      "Stochastic Gradient Descent(21862): loss=0.8191027479991054\n",
      "Stochastic Gradient Descent(21863): loss=17.564653592411346\n",
      "Stochastic Gradient Descent(21864): loss=0.6531638717284738\n",
      "Stochastic Gradient Descent(21865): loss=1.3207889093147727\n",
      "Stochastic Gradient Descent(21866): loss=0.42213823206059636\n",
      "Stochastic Gradient Descent(21867): loss=1.8589779676739366\n",
      "Stochastic Gradient Descent(21868): loss=4.655302279107524\n",
      "Stochastic Gradient Descent(21869): loss=0.6476699942677162\n",
      "Stochastic Gradient Descent(21870): loss=2.0434178584595215\n",
      "Stochastic Gradient Descent(21871): loss=6.349507805898782\n",
      "Stochastic Gradient Descent(21872): loss=0.8966731299924907\n",
      "Stochastic Gradient Descent(21873): loss=0.25813580765732547\n",
      "Stochastic Gradient Descent(21874): loss=1.5016680577970354\n",
      "Stochastic Gradient Descent(21875): loss=0.49066110584778166\n",
      "Stochastic Gradient Descent(21876): loss=0.1440518977619398\n",
      "Stochastic Gradient Descent(21877): loss=11.497153398927956\n",
      "Stochastic Gradient Descent(21878): loss=2.952340455698086\n",
      "Stochastic Gradient Descent(21879): loss=4.705102161328604\n",
      "Stochastic Gradient Descent(21880): loss=4.283615914178076\n",
      "Stochastic Gradient Descent(21881): loss=0.38592815727633095\n",
      "Stochastic Gradient Descent(21882): loss=0.07886560019214764\n",
      "Stochastic Gradient Descent(21883): loss=5.661113230932322\n",
      "Stochastic Gradient Descent(21884): loss=2.0628268803814436\n",
      "Stochastic Gradient Descent(21885): loss=2.7075574552033577\n",
      "Stochastic Gradient Descent(21886): loss=0.866953965879738\n",
      "Stochastic Gradient Descent(21887): loss=4.577261507862807\n",
      "Stochastic Gradient Descent(21888): loss=0.15312180198932043\n",
      "Stochastic Gradient Descent(21889): loss=2.150921958537006\n",
      "Stochastic Gradient Descent(21890): loss=16.202886215009748\n",
      "Stochastic Gradient Descent(21891): loss=8.1592877960858\n",
      "Stochastic Gradient Descent(21892): loss=1.291114264011835\n",
      "Stochastic Gradient Descent(21893): loss=0.028485546836392282\n",
      "Stochastic Gradient Descent(21894): loss=17.985800311081164\n",
      "Stochastic Gradient Descent(21895): loss=26.797988452947774\n",
      "Stochastic Gradient Descent(21896): loss=0.29128360136040143\n",
      "Stochastic Gradient Descent(21897): loss=5.306761013438674\n",
      "Stochastic Gradient Descent(21898): loss=15.776968417286037\n",
      "Stochastic Gradient Descent(21899): loss=0.4973383190304332\n",
      "Stochastic Gradient Descent(21900): loss=0.0011956796334536626\n",
      "Stochastic Gradient Descent(21901): loss=3.3540915116912555\n",
      "Stochastic Gradient Descent(21902): loss=0.7441417712075791\n",
      "Stochastic Gradient Descent(21903): loss=7.4324738025788655\n",
      "Stochastic Gradient Descent(21904): loss=0.6905652686671759\n",
      "Stochastic Gradient Descent(21905): loss=3.720909384241206\n",
      "Stochastic Gradient Descent(21906): loss=30.382203049594153\n",
      "Stochastic Gradient Descent(21907): loss=29.873823276309185\n",
      "Stochastic Gradient Descent(21908): loss=11.072447447331655\n",
      "Stochastic Gradient Descent(21909): loss=16.919212693784996\n",
      "Stochastic Gradient Descent(21910): loss=0.057450213169668335\n",
      "Stochastic Gradient Descent(21911): loss=0.9791915262252762\n",
      "Stochastic Gradient Descent(21912): loss=2.490347728905117e-07\n",
      "Stochastic Gradient Descent(21913): loss=6.0535059719816635\n",
      "Stochastic Gradient Descent(21914): loss=0.7378964616434732\n",
      "Stochastic Gradient Descent(21915): loss=6.60101272067061\n",
      "Stochastic Gradient Descent(21916): loss=0.06827721302851146\n",
      "Stochastic Gradient Descent(21917): loss=11.410392440288627\n",
      "Stochastic Gradient Descent(21918): loss=0.48332668367203746\n",
      "Stochastic Gradient Descent(21919): loss=7.521333510007705\n",
      "Stochastic Gradient Descent(21920): loss=0.8086427826987429\n",
      "Stochastic Gradient Descent(21921): loss=3.293015026466428\n",
      "Stochastic Gradient Descent(21922): loss=2.630359256587363\n",
      "Stochastic Gradient Descent(21923): loss=6.901034558347394\n",
      "Stochastic Gradient Descent(21924): loss=2.5649367887836467\n",
      "Stochastic Gradient Descent(21925): loss=0.1116449605252044\n",
      "Stochastic Gradient Descent(21926): loss=18.581982752859417\n",
      "Stochastic Gradient Descent(21927): loss=8.997311143677202\n",
      "Stochastic Gradient Descent(21928): loss=9.51573305510475\n",
      "Stochastic Gradient Descent(21929): loss=0.0032154112485141147\n",
      "Stochastic Gradient Descent(21930): loss=1.7925901591643842\n",
      "Stochastic Gradient Descent(21931): loss=5.204778037984489\n",
      "Stochastic Gradient Descent(21932): loss=3.201473482984316\n",
      "Stochastic Gradient Descent(21933): loss=2.2088060871161455\n",
      "Stochastic Gradient Descent(21934): loss=10.988807631208704\n",
      "Stochastic Gradient Descent(21935): loss=1.3836445639013564\n",
      "Stochastic Gradient Descent(21936): loss=1.7397626167721052\n",
      "Stochastic Gradient Descent(21937): loss=1.8066695148895884\n",
      "Stochastic Gradient Descent(21938): loss=1.3190908988733776\n",
      "Stochastic Gradient Descent(21939): loss=0.04895872302910394\n",
      "Stochastic Gradient Descent(21940): loss=5.124852668511057\n",
      "Stochastic Gradient Descent(21941): loss=4.301387644756606\n",
      "Stochastic Gradient Descent(21942): loss=1.935722843219539\n",
      "Stochastic Gradient Descent(21943): loss=0.23853594848411638\n",
      "Stochastic Gradient Descent(21944): loss=3.8552584250572894\n",
      "Stochastic Gradient Descent(21945): loss=3.7667250266545524\n",
      "Stochastic Gradient Descent(21946): loss=1.2304261956826557\n",
      "Stochastic Gradient Descent(21947): loss=8.418978387743506\n",
      "Stochastic Gradient Descent(21948): loss=20.524767088723003\n",
      "Stochastic Gradient Descent(21949): loss=3.550582706058437\n",
      "Stochastic Gradient Descent(21950): loss=0.5240712787923522\n",
      "Stochastic Gradient Descent(21951): loss=4.516948492886271\n",
      "Stochastic Gradient Descent(21952): loss=0.1364966206022872\n",
      "Stochastic Gradient Descent(21953): loss=3.9788518028940376\n",
      "Stochastic Gradient Descent(21954): loss=1.1146298444474256\n",
      "Stochastic Gradient Descent(21955): loss=0.17041026427554987\n",
      "Stochastic Gradient Descent(21956): loss=5.474577695878312\n",
      "Stochastic Gradient Descent(21957): loss=26.40972973703056\n",
      "Stochastic Gradient Descent(21958): loss=0.08533699962081012\n",
      "Stochastic Gradient Descent(21959): loss=1.7240466029145611\n",
      "Stochastic Gradient Descent(21960): loss=2.3093607495142137\n",
      "Stochastic Gradient Descent(21961): loss=12.801262354472328\n",
      "Stochastic Gradient Descent(21962): loss=3.310020379563046\n",
      "Stochastic Gradient Descent(21963): loss=0.6476211892231885\n",
      "Stochastic Gradient Descent(21964): loss=5.385674226996815\n",
      "Stochastic Gradient Descent(21965): loss=7.642820370644336\n",
      "Stochastic Gradient Descent(21966): loss=2.323531251926325\n",
      "Stochastic Gradient Descent(21967): loss=0.1717982711157655\n",
      "Stochastic Gradient Descent(21968): loss=0.1997362617426797\n",
      "Stochastic Gradient Descent(21969): loss=0.6951433648206278\n",
      "Stochastic Gradient Descent(21970): loss=16.25247786879377\n",
      "Stochastic Gradient Descent(21971): loss=4.07455269690614\n",
      "Stochastic Gradient Descent(21972): loss=0.757914694773416\n",
      "Stochastic Gradient Descent(21973): loss=6.0789602758849135\n",
      "Stochastic Gradient Descent(21974): loss=3.1199412230460153\n",
      "Stochastic Gradient Descent(21975): loss=4.7954717983077515\n",
      "Stochastic Gradient Descent(21976): loss=19.803082518729152\n",
      "Stochastic Gradient Descent(21977): loss=2.307676313224919\n",
      "Stochastic Gradient Descent(21978): loss=14.152842640517797\n",
      "Stochastic Gradient Descent(21979): loss=0.3838491500482411\n",
      "Stochastic Gradient Descent(21980): loss=5.7289649727763035\n",
      "Stochastic Gradient Descent(21981): loss=0.012009586389765129\n",
      "Stochastic Gradient Descent(21982): loss=14.659964993711885\n",
      "Stochastic Gradient Descent(21983): loss=6.021844466968023\n",
      "Stochastic Gradient Descent(21984): loss=1.2614772181814569\n",
      "Stochastic Gradient Descent(21985): loss=5.299370663326401\n",
      "Stochastic Gradient Descent(21986): loss=11.57800844632867\n",
      "Stochastic Gradient Descent(21987): loss=5.284760340569269\n",
      "Stochastic Gradient Descent(21988): loss=17.509456342018087\n",
      "Stochastic Gradient Descent(21989): loss=6.23729367736825\n",
      "Stochastic Gradient Descent(21990): loss=6.8862988322206045\n",
      "Stochastic Gradient Descent(21991): loss=6.607797999805095\n",
      "Stochastic Gradient Descent(21992): loss=3.982950326260241\n",
      "Stochastic Gradient Descent(21993): loss=0.6152523200045243\n",
      "Stochastic Gradient Descent(21994): loss=5.036468230054542\n",
      "Stochastic Gradient Descent(21995): loss=1.3029323806128612\n",
      "Stochastic Gradient Descent(21996): loss=0.26187638684093045\n",
      "Stochastic Gradient Descent(21997): loss=0.025842475564934693\n",
      "Stochastic Gradient Descent(21998): loss=5.5147514891263425\n",
      "Stochastic Gradient Descent(21999): loss=4.038878918230564\n",
      "Stochastic Gradient Descent(22000): loss=2.1152338828718795\n",
      "Stochastic Gradient Descent(22001): loss=12.498393645342869\n",
      "Stochastic Gradient Descent(22002): loss=0.7676273426643433\n",
      "Stochastic Gradient Descent(22003): loss=2.172777470973102\n",
      "Stochastic Gradient Descent(22004): loss=0.015841037520573545\n",
      "Stochastic Gradient Descent(22005): loss=0.00481526624116421\n",
      "Stochastic Gradient Descent(22006): loss=0.7732399773432117\n",
      "Stochastic Gradient Descent(22007): loss=7.348738708482558\n",
      "Stochastic Gradient Descent(22008): loss=1.393328488972323\n",
      "Stochastic Gradient Descent(22009): loss=0.009025467768979153\n",
      "Stochastic Gradient Descent(22010): loss=6.437997674564062\n",
      "Stochastic Gradient Descent(22011): loss=3.5193671660945154\n",
      "Stochastic Gradient Descent(22012): loss=26.900265490472343\n",
      "Stochastic Gradient Descent(22013): loss=2.827610166858331\n",
      "Stochastic Gradient Descent(22014): loss=11.657297876571079\n",
      "Stochastic Gradient Descent(22015): loss=0.5114286353391972\n",
      "Stochastic Gradient Descent(22016): loss=0.035803571324983095\n",
      "Stochastic Gradient Descent(22017): loss=0.00267999154973249\n",
      "Stochastic Gradient Descent(22018): loss=5.622042845853907\n",
      "Stochastic Gradient Descent(22019): loss=1.22250259596766\n",
      "Stochastic Gradient Descent(22020): loss=1.9614789760544107\n",
      "Stochastic Gradient Descent(22021): loss=4.808509926434181\n",
      "Stochastic Gradient Descent(22022): loss=0.3666282723052808\n",
      "Stochastic Gradient Descent(22023): loss=0.0006461468284490248\n",
      "Stochastic Gradient Descent(22024): loss=0.7741332349004987\n",
      "Stochastic Gradient Descent(22025): loss=1.0770347182086006\n",
      "Stochastic Gradient Descent(22026): loss=0.13758877592819238\n",
      "Stochastic Gradient Descent(22027): loss=0.060037052061796964\n",
      "Stochastic Gradient Descent(22028): loss=2.372527990502481\n",
      "Stochastic Gradient Descent(22029): loss=8.29656493702187\n",
      "Stochastic Gradient Descent(22030): loss=0.7378340495802903\n",
      "Stochastic Gradient Descent(22031): loss=0.46077524847885043\n",
      "Stochastic Gradient Descent(22032): loss=6.280823985464264\n",
      "Stochastic Gradient Descent(22033): loss=4.359183674224999\n",
      "Stochastic Gradient Descent(22034): loss=1.4528518484763666\n",
      "Stochastic Gradient Descent(22035): loss=4.238052968014811\n",
      "Stochastic Gradient Descent(22036): loss=0.1673947704445263\n",
      "Stochastic Gradient Descent(22037): loss=6.705119843810697\n",
      "Stochastic Gradient Descent(22038): loss=0.5991648782267506\n",
      "Stochastic Gradient Descent(22039): loss=0.5507864110733075\n",
      "Stochastic Gradient Descent(22040): loss=7.879885946031449\n",
      "Stochastic Gradient Descent(22041): loss=0.5561541444034296\n",
      "Stochastic Gradient Descent(22042): loss=0.05676691433912454\n",
      "Stochastic Gradient Descent(22043): loss=15.298366398415892\n",
      "Stochastic Gradient Descent(22044): loss=3.3258318751910787\n",
      "Stochastic Gradient Descent(22045): loss=0.9362436013817501\n",
      "Stochastic Gradient Descent(22046): loss=0.001808288252159369\n",
      "Stochastic Gradient Descent(22047): loss=0.004131182430726802\n",
      "Stochastic Gradient Descent(22048): loss=13.436283772051123\n",
      "Stochastic Gradient Descent(22049): loss=7.55939479959423\n",
      "Stochastic Gradient Descent(22050): loss=2.052630734067561\n",
      "Stochastic Gradient Descent(22051): loss=4.610668462452302\n",
      "Stochastic Gradient Descent(22052): loss=5.73421440140348\n",
      "Stochastic Gradient Descent(22053): loss=0.6161117767027261\n",
      "Stochastic Gradient Descent(22054): loss=33.712022849970026\n",
      "Stochastic Gradient Descent(22055): loss=4.300958018110701\n",
      "Stochastic Gradient Descent(22056): loss=5.095159645191337\n",
      "Stochastic Gradient Descent(22057): loss=2.2057272079607637\n",
      "Stochastic Gradient Descent(22058): loss=7.156318849376398\n",
      "Stochastic Gradient Descent(22059): loss=1.082768671745292\n",
      "Stochastic Gradient Descent(22060): loss=4.888335761463152\n",
      "Stochastic Gradient Descent(22061): loss=4.666859392694882\n",
      "Stochastic Gradient Descent(22062): loss=5.409685261518967\n",
      "Stochastic Gradient Descent(22063): loss=0.68855520977468\n",
      "Stochastic Gradient Descent(22064): loss=16.937950101796698\n",
      "Stochastic Gradient Descent(22065): loss=4.221430043839571\n",
      "Stochastic Gradient Descent(22066): loss=2.811124468647618\n",
      "Stochastic Gradient Descent(22067): loss=0.0006312080782410277\n",
      "Stochastic Gradient Descent(22068): loss=2.057530788773055\n",
      "Stochastic Gradient Descent(22069): loss=5.056295395766643\n",
      "Stochastic Gradient Descent(22070): loss=0.007021155328958632\n",
      "Stochastic Gradient Descent(22071): loss=7.061717321655405\n",
      "Stochastic Gradient Descent(22072): loss=3.826214390916484\n",
      "Stochastic Gradient Descent(22073): loss=4.309537236358506\n",
      "Stochastic Gradient Descent(22074): loss=13.449115324584913\n",
      "Stochastic Gradient Descent(22075): loss=17.649941518995032\n",
      "Stochastic Gradient Descent(22076): loss=0.06531538159363853\n",
      "Stochastic Gradient Descent(22077): loss=3.256587722794485\n",
      "Stochastic Gradient Descent(22078): loss=0.1963988030288047\n",
      "Stochastic Gradient Descent(22079): loss=3.7149284085229235\n",
      "Stochastic Gradient Descent(22080): loss=0.006809639914733301\n",
      "Stochastic Gradient Descent(22081): loss=0.4432892885493087\n",
      "Stochastic Gradient Descent(22082): loss=0.4642109087168115\n",
      "Stochastic Gradient Descent(22083): loss=11.353039948564614\n",
      "Stochastic Gradient Descent(22084): loss=0.973597848375313\n",
      "Stochastic Gradient Descent(22085): loss=2.011318476094004\n",
      "Stochastic Gradient Descent(22086): loss=2.542408192701875\n",
      "Stochastic Gradient Descent(22087): loss=0.6025453788030053\n",
      "Stochastic Gradient Descent(22088): loss=0.09398589529424967\n",
      "Stochastic Gradient Descent(22089): loss=7.041901696713435\n",
      "Stochastic Gradient Descent(22090): loss=4.303211742269133\n",
      "Stochastic Gradient Descent(22091): loss=0.17308670459258285\n",
      "Stochastic Gradient Descent(22092): loss=0.006591429976539738\n",
      "Stochastic Gradient Descent(22093): loss=14.666688959192133\n",
      "Stochastic Gradient Descent(22094): loss=55.74347950941666\n",
      "Stochastic Gradient Descent(22095): loss=12.274417840422537\n",
      "Stochastic Gradient Descent(22096): loss=3.1991501637733952\n",
      "Stochastic Gradient Descent(22097): loss=0.9302199268485927\n",
      "Stochastic Gradient Descent(22098): loss=0.5190054155641654\n",
      "Stochastic Gradient Descent(22099): loss=8.220111113068116\n",
      "Stochastic Gradient Descent(22100): loss=1.042654292991598\n",
      "Stochastic Gradient Descent(22101): loss=0.9006545834667461\n",
      "Stochastic Gradient Descent(22102): loss=0.6991154344712416\n",
      "Stochastic Gradient Descent(22103): loss=4.552758892632826\n",
      "Stochastic Gradient Descent(22104): loss=5.640673302691433\n",
      "Stochastic Gradient Descent(22105): loss=0.4202961766500791\n",
      "Stochastic Gradient Descent(22106): loss=9.582775272364662\n",
      "Stochastic Gradient Descent(22107): loss=0.05634783759929403\n",
      "Stochastic Gradient Descent(22108): loss=0.026362480830974096\n",
      "Stochastic Gradient Descent(22109): loss=5.020828820057164e-05\n",
      "Stochastic Gradient Descent(22110): loss=0.549098406894942\n",
      "Stochastic Gradient Descent(22111): loss=0.9377229625755085\n",
      "Stochastic Gradient Descent(22112): loss=3.139636467681408\n",
      "Stochastic Gradient Descent(22113): loss=2.2351159363678272\n",
      "Stochastic Gradient Descent(22114): loss=1.819748079484017\n",
      "Stochastic Gradient Descent(22115): loss=1.434590914442238\n",
      "Stochastic Gradient Descent(22116): loss=1.3426872668723417\n",
      "Stochastic Gradient Descent(22117): loss=0.0013177566374993766\n",
      "Stochastic Gradient Descent(22118): loss=5.599572944383275\n",
      "Stochastic Gradient Descent(22119): loss=0.38454569609910705\n",
      "Stochastic Gradient Descent(22120): loss=1.0779248963584849\n",
      "Stochastic Gradient Descent(22121): loss=0.03285512878073492\n",
      "Stochastic Gradient Descent(22122): loss=2.9664619581404246\n",
      "Stochastic Gradient Descent(22123): loss=0.8389658097154732\n",
      "Stochastic Gradient Descent(22124): loss=10.54481430513942\n",
      "Stochastic Gradient Descent(22125): loss=64.2701256512425\n",
      "Stochastic Gradient Descent(22126): loss=28.54114411651218\n",
      "Stochastic Gradient Descent(22127): loss=1.0676726999262907\n",
      "Stochastic Gradient Descent(22128): loss=0.26206915298057837\n",
      "Stochastic Gradient Descent(22129): loss=5.772340853183568\n",
      "Stochastic Gradient Descent(22130): loss=7.068529238799406\n",
      "Stochastic Gradient Descent(22131): loss=0.07106947698860165\n",
      "Stochastic Gradient Descent(22132): loss=2.77948743557944\n",
      "Stochastic Gradient Descent(22133): loss=0.5928853016379357\n",
      "Stochastic Gradient Descent(22134): loss=0.21799195569224306\n",
      "Stochastic Gradient Descent(22135): loss=7.242491987867582\n",
      "Stochastic Gradient Descent(22136): loss=2.115553501822736\n",
      "Stochastic Gradient Descent(22137): loss=0.9271719452365702\n",
      "Stochastic Gradient Descent(22138): loss=2.474370417451737\n",
      "Stochastic Gradient Descent(22139): loss=0.05808514309405035\n",
      "Stochastic Gradient Descent(22140): loss=2.027701816431827\n",
      "Stochastic Gradient Descent(22141): loss=0.16754724877350913\n",
      "Stochastic Gradient Descent(22142): loss=0.9927375983698375\n",
      "Stochastic Gradient Descent(22143): loss=2.394611311147754\n",
      "Stochastic Gradient Descent(22144): loss=0.44572653210897234\n",
      "Stochastic Gradient Descent(22145): loss=0.0009353440901807692\n",
      "Stochastic Gradient Descent(22146): loss=0.19839665168058596\n",
      "Stochastic Gradient Descent(22147): loss=11.018566250939895\n",
      "Stochastic Gradient Descent(22148): loss=0.5215918815577205\n",
      "Stochastic Gradient Descent(22149): loss=2.952683249445103\n",
      "Stochastic Gradient Descent(22150): loss=9.570524515892084\n",
      "Stochastic Gradient Descent(22151): loss=1.1188709938299206\n",
      "Stochastic Gradient Descent(22152): loss=0.3625314660093637\n",
      "Stochastic Gradient Descent(22153): loss=0.5475876466676789\n",
      "Stochastic Gradient Descent(22154): loss=10.91293203703722\n",
      "Stochastic Gradient Descent(22155): loss=15.578723038845315\n",
      "Stochastic Gradient Descent(22156): loss=0.31828727002439344\n",
      "Stochastic Gradient Descent(22157): loss=1.4800982736874644\n",
      "Stochastic Gradient Descent(22158): loss=0.0524977603802622\n",
      "Stochastic Gradient Descent(22159): loss=4.008350153311279\n",
      "Stochastic Gradient Descent(22160): loss=0.31614907571752815\n",
      "Stochastic Gradient Descent(22161): loss=12.097298136273363\n",
      "Stochastic Gradient Descent(22162): loss=2.3120907570816414\n",
      "Stochastic Gradient Descent(22163): loss=0.09840784860128189\n",
      "Stochastic Gradient Descent(22164): loss=0.03478302014289687\n",
      "Stochastic Gradient Descent(22165): loss=0.1713412475190195\n",
      "Stochastic Gradient Descent(22166): loss=0.9880172922187421\n",
      "Stochastic Gradient Descent(22167): loss=0.12648326156164638\n",
      "Stochastic Gradient Descent(22168): loss=3.658852483669635\n",
      "Stochastic Gradient Descent(22169): loss=11.042801764081092\n",
      "Stochastic Gradient Descent(22170): loss=3.716767006716369\n",
      "Stochastic Gradient Descent(22171): loss=0.0782020590672485\n",
      "Stochastic Gradient Descent(22172): loss=1.0470872228033536\n",
      "Stochastic Gradient Descent(22173): loss=22.317529471108777\n",
      "Stochastic Gradient Descent(22174): loss=0.24696471075426255\n",
      "Stochastic Gradient Descent(22175): loss=27.272641435918302\n",
      "Stochastic Gradient Descent(22176): loss=1.69178814078067\n",
      "Stochastic Gradient Descent(22177): loss=1.192216694588728\n",
      "Stochastic Gradient Descent(22178): loss=0.473444359922356\n",
      "Stochastic Gradient Descent(22179): loss=8.144783895087427\n",
      "Stochastic Gradient Descent(22180): loss=0.7023511813492335\n",
      "Stochastic Gradient Descent(22181): loss=0.20274979483895864\n",
      "Stochastic Gradient Descent(22182): loss=0.27912530826974447\n",
      "Stochastic Gradient Descent(22183): loss=0.6144275182083899\n",
      "Stochastic Gradient Descent(22184): loss=9.225543598396158\n",
      "Stochastic Gradient Descent(22185): loss=0.7596216752885853\n",
      "Stochastic Gradient Descent(22186): loss=4.257781485219244\n",
      "Stochastic Gradient Descent(22187): loss=21.637864847264378\n",
      "Stochastic Gradient Descent(22188): loss=0.011181401529549159\n",
      "Stochastic Gradient Descent(22189): loss=0.27130827619441406\n",
      "Stochastic Gradient Descent(22190): loss=9.163610828280966\n",
      "Stochastic Gradient Descent(22191): loss=0.17891402531336623\n",
      "Stochastic Gradient Descent(22192): loss=0.43792535595256865\n",
      "Stochastic Gradient Descent(22193): loss=1.4697402110616702\n",
      "Stochastic Gradient Descent(22194): loss=5.251834958308227\n",
      "Stochastic Gradient Descent(22195): loss=1.078328399299986\n",
      "Stochastic Gradient Descent(22196): loss=0.2248694495675622\n",
      "Stochastic Gradient Descent(22197): loss=0.45562347518339424\n",
      "Stochastic Gradient Descent(22198): loss=0.3292235782727705\n",
      "Stochastic Gradient Descent(22199): loss=0.13241244742263458\n",
      "Stochastic Gradient Descent(22200): loss=9.840342546133261\n",
      "Stochastic Gradient Descent(22201): loss=4.58560510765984\n",
      "Stochastic Gradient Descent(22202): loss=2.1102239118764685\n",
      "Stochastic Gradient Descent(22203): loss=0.0004540853391917148\n",
      "Stochastic Gradient Descent(22204): loss=12.897649320426494\n",
      "Stochastic Gradient Descent(22205): loss=3.6396756519164515\n",
      "Stochastic Gradient Descent(22206): loss=0.5388925398039548\n",
      "Stochastic Gradient Descent(22207): loss=1.2277646139714031\n",
      "Stochastic Gradient Descent(22208): loss=7.772550618067634\n",
      "Stochastic Gradient Descent(22209): loss=0.06358904024362529\n",
      "Stochastic Gradient Descent(22210): loss=0.10928336726391666\n",
      "Stochastic Gradient Descent(22211): loss=5.697576846584889\n",
      "Stochastic Gradient Descent(22212): loss=5.494512205673297\n",
      "Stochastic Gradient Descent(22213): loss=8.947945962415224\n",
      "Stochastic Gradient Descent(22214): loss=27.238721250535015\n",
      "Stochastic Gradient Descent(22215): loss=0.053756518444533774\n",
      "Stochastic Gradient Descent(22216): loss=21.812089382812925\n",
      "Stochastic Gradient Descent(22217): loss=12.161048007140868\n",
      "Stochastic Gradient Descent(22218): loss=0.10491323358265776\n",
      "Stochastic Gradient Descent(22219): loss=18.58703934762518\n",
      "Stochastic Gradient Descent(22220): loss=0.3946238021466436\n",
      "Stochastic Gradient Descent(22221): loss=6.751440214712467\n",
      "Stochastic Gradient Descent(22222): loss=3.564343477965172\n",
      "Stochastic Gradient Descent(22223): loss=1.8521854498378005\n",
      "Stochastic Gradient Descent(22224): loss=0.550086007758995\n",
      "Stochastic Gradient Descent(22225): loss=9.101891501817683\n",
      "Stochastic Gradient Descent(22226): loss=6.68305283173419\n",
      "Stochastic Gradient Descent(22227): loss=6.636452937114505\n",
      "Stochastic Gradient Descent(22228): loss=6.2994568874912895\n",
      "Stochastic Gradient Descent(22229): loss=18.54251831662684\n",
      "Stochastic Gradient Descent(22230): loss=0.11466644845653323\n",
      "Stochastic Gradient Descent(22231): loss=7.003701367265219\n",
      "Stochastic Gradient Descent(22232): loss=3.3212246173179993\n",
      "Stochastic Gradient Descent(22233): loss=0.07288420095343259\n",
      "Stochastic Gradient Descent(22234): loss=0.11934313422299252\n",
      "Stochastic Gradient Descent(22235): loss=0.42273052047838156\n",
      "Stochastic Gradient Descent(22236): loss=1.1421860695895665\n",
      "Stochastic Gradient Descent(22237): loss=1.6648655434916553\n",
      "Stochastic Gradient Descent(22238): loss=0.4122422285648133\n",
      "Stochastic Gradient Descent(22239): loss=18.850401924591296\n",
      "Stochastic Gradient Descent(22240): loss=1.3455381646427542\n",
      "Stochastic Gradient Descent(22241): loss=1.7550115932610615\n",
      "Stochastic Gradient Descent(22242): loss=0.23394552022990037\n",
      "Stochastic Gradient Descent(22243): loss=0.9584807884715457\n",
      "Stochastic Gradient Descent(22244): loss=1.7396870688428676\n",
      "Stochastic Gradient Descent(22245): loss=8.73747758878662\n",
      "Stochastic Gradient Descent(22246): loss=1.1597856402211777\n",
      "Stochastic Gradient Descent(22247): loss=0.01060009575619818\n",
      "Stochastic Gradient Descent(22248): loss=0.3801549324589518\n",
      "Stochastic Gradient Descent(22249): loss=0.009629018102584463\n",
      "Stochastic Gradient Descent(22250): loss=33.129512929613924\n",
      "Stochastic Gradient Descent(22251): loss=2.5519020945402087\n",
      "Stochastic Gradient Descent(22252): loss=12.005746665700308\n",
      "Stochastic Gradient Descent(22253): loss=0.7437685010572149\n",
      "Stochastic Gradient Descent(22254): loss=9.37099677447004\n",
      "Stochastic Gradient Descent(22255): loss=0.20687253784495185\n",
      "Stochastic Gradient Descent(22256): loss=7.905362251889671\n",
      "Stochastic Gradient Descent(22257): loss=3.2561750668942766\n",
      "Stochastic Gradient Descent(22258): loss=0.6366568319361723\n",
      "Stochastic Gradient Descent(22259): loss=5.848145712957037\n",
      "Stochastic Gradient Descent(22260): loss=0.0754402904951162\n",
      "Stochastic Gradient Descent(22261): loss=1.3472348431590135\n",
      "Stochastic Gradient Descent(22262): loss=0.034452141336927554\n",
      "Stochastic Gradient Descent(22263): loss=0.0006006126690333102\n",
      "Stochastic Gradient Descent(22264): loss=5.560090434720589\n",
      "Stochastic Gradient Descent(22265): loss=3.199072353645769\n",
      "Stochastic Gradient Descent(22266): loss=4.292642961630909\n",
      "Stochastic Gradient Descent(22267): loss=1.7871950340479925\n",
      "Stochastic Gradient Descent(22268): loss=2.407219888524542\n",
      "Stochastic Gradient Descent(22269): loss=0.9682237478324172\n",
      "Stochastic Gradient Descent(22270): loss=13.414192874999479\n",
      "Stochastic Gradient Descent(22271): loss=2.7877191862900657\n",
      "Stochastic Gradient Descent(22272): loss=0.20742578809173084\n",
      "Stochastic Gradient Descent(22273): loss=2.470521104256028\n",
      "Stochastic Gradient Descent(22274): loss=12.190104970759407\n",
      "Stochastic Gradient Descent(22275): loss=4.042311640637736\n",
      "Stochastic Gradient Descent(22276): loss=8.14556005041132\n",
      "Stochastic Gradient Descent(22277): loss=2.890236890816806\n",
      "Stochastic Gradient Descent(22278): loss=0.4010343502281534\n",
      "Stochastic Gradient Descent(22279): loss=7.191729853150453\n",
      "Stochastic Gradient Descent(22280): loss=0.07702404350339757\n",
      "Stochastic Gradient Descent(22281): loss=0.005178137773380187\n",
      "Stochastic Gradient Descent(22282): loss=2.616871476550267\n",
      "Stochastic Gradient Descent(22283): loss=10.79027580273083\n",
      "Stochastic Gradient Descent(22284): loss=1.642525709392233\n",
      "Stochastic Gradient Descent(22285): loss=0.12426369919865675\n",
      "Stochastic Gradient Descent(22286): loss=3.810597529674105\n",
      "Stochastic Gradient Descent(22287): loss=0.3411510150881137\n",
      "Stochastic Gradient Descent(22288): loss=2.5650640980088384\n",
      "Stochastic Gradient Descent(22289): loss=5.410728872298669\n",
      "Stochastic Gradient Descent(22290): loss=5.834210485357552\n",
      "Stochastic Gradient Descent(22291): loss=11.735429988523197\n",
      "Stochastic Gradient Descent(22292): loss=2.5421264457218555\n",
      "Stochastic Gradient Descent(22293): loss=0.3079417263161293\n",
      "Stochastic Gradient Descent(22294): loss=1.356611792961057\n",
      "Stochastic Gradient Descent(22295): loss=0.34083856445704275\n",
      "Stochastic Gradient Descent(22296): loss=0.4037750022239397\n",
      "Stochastic Gradient Descent(22297): loss=0.6884616804536934\n",
      "Stochastic Gradient Descent(22298): loss=0.1379890493851247\n",
      "Stochastic Gradient Descent(22299): loss=18.32610892362313\n",
      "Stochastic Gradient Descent(22300): loss=5.556091767218999\n",
      "Stochastic Gradient Descent(22301): loss=0.5018218799306344\n",
      "Stochastic Gradient Descent(22302): loss=1.4672495228342801\n",
      "Stochastic Gradient Descent(22303): loss=9.310591523373336\n",
      "Stochastic Gradient Descent(22304): loss=1.0455384136279804\n",
      "Stochastic Gradient Descent(22305): loss=0.006279550514173659\n",
      "Stochastic Gradient Descent(22306): loss=16.65955497237794\n",
      "Stochastic Gradient Descent(22307): loss=16.970982707562168\n",
      "Stochastic Gradient Descent(22308): loss=46.131835684040894\n",
      "Stochastic Gradient Descent(22309): loss=0.36431738271764835\n",
      "Stochastic Gradient Descent(22310): loss=1.026234679528704\n",
      "Stochastic Gradient Descent(22311): loss=4.605139993417732\n",
      "Stochastic Gradient Descent(22312): loss=7.754485646125202\n",
      "Stochastic Gradient Descent(22313): loss=0.3343348325001465\n",
      "Stochastic Gradient Descent(22314): loss=9.977346736309398\n",
      "Stochastic Gradient Descent(22315): loss=2.803851851476404\n",
      "Stochastic Gradient Descent(22316): loss=0.0028363063254378262\n",
      "Stochastic Gradient Descent(22317): loss=11.726251761193765\n",
      "Stochastic Gradient Descent(22318): loss=2.7182363267802723\n",
      "Stochastic Gradient Descent(22319): loss=2.1185959306899567\n",
      "Stochastic Gradient Descent(22320): loss=9.273740124188\n",
      "Stochastic Gradient Descent(22321): loss=0.12434687212733583\n",
      "Stochastic Gradient Descent(22322): loss=3.013505663744518\n",
      "Stochastic Gradient Descent(22323): loss=3.0285439453169354\n",
      "Stochastic Gradient Descent(22324): loss=6.246832131384953\n",
      "Stochastic Gradient Descent(22325): loss=4.922642915671679\n",
      "Stochastic Gradient Descent(22326): loss=9.060441212551012\n",
      "Stochastic Gradient Descent(22327): loss=0.3620202076532756\n",
      "Stochastic Gradient Descent(22328): loss=5.896471301833227\n",
      "Stochastic Gradient Descent(22329): loss=0.06619106087346614\n",
      "Stochastic Gradient Descent(22330): loss=0.8161427909635657\n",
      "Stochastic Gradient Descent(22331): loss=7.7996878157552265\n",
      "Stochastic Gradient Descent(22332): loss=0.010007022095490198\n",
      "Stochastic Gradient Descent(22333): loss=0.09305190563361951\n",
      "Stochastic Gradient Descent(22334): loss=3.3283673075510634\n",
      "Stochastic Gradient Descent(22335): loss=4.683845802361045\n",
      "Stochastic Gradient Descent(22336): loss=25.020987694778242\n",
      "Stochastic Gradient Descent(22337): loss=0.062063193491679595\n",
      "Stochastic Gradient Descent(22338): loss=0.5498024462232565\n",
      "Stochastic Gradient Descent(22339): loss=0.3360299623092574\n",
      "Stochastic Gradient Descent(22340): loss=8.115210354089811\n",
      "Stochastic Gradient Descent(22341): loss=8.786476487538891\n",
      "Stochastic Gradient Descent(22342): loss=1.9072592617874549\n",
      "Stochastic Gradient Descent(22343): loss=0.46016299249858117\n",
      "Stochastic Gradient Descent(22344): loss=0.5193348752368809\n",
      "Stochastic Gradient Descent(22345): loss=4.293405860791232\n",
      "Stochastic Gradient Descent(22346): loss=0.5838921241094118\n",
      "Stochastic Gradient Descent(22347): loss=0.34655491795874116\n",
      "Stochastic Gradient Descent(22348): loss=0.5319385202328163\n",
      "Stochastic Gradient Descent(22349): loss=0.5411966313160099\n",
      "Stochastic Gradient Descent(22350): loss=0.24169739097418966\n",
      "Stochastic Gradient Descent(22351): loss=15.148113212398377\n",
      "Stochastic Gradient Descent(22352): loss=8.878966936558216\n",
      "Stochastic Gradient Descent(22353): loss=3.4519021498267217\n",
      "Stochastic Gradient Descent(22354): loss=0.16380643332095945\n",
      "Stochastic Gradient Descent(22355): loss=7.605914564044133\n",
      "Stochastic Gradient Descent(22356): loss=8.451065625677568\n",
      "Stochastic Gradient Descent(22357): loss=2.3985817493145953\n",
      "Stochastic Gradient Descent(22358): loss=6.766939070693858\n",
      "Stochastic Gradient Descent(22359): loss=3.7885088811482066\n",
      "Stochastic Gradient Descent(22360): loss=4.9883899823469315\n",
      "Stochastic Gradient Descent(22361): loss=0.8611530141450972\n",
      "Stochastic Gradient Descent(22362): loss=3.4856164288125138\n",
      "Stochastic Gradient Descent(22363): loss=2.2178437364955106\n",
      "Stochastic Gradient Descent(22364): loss=2.0875967154235937\n",
      "Stochastic Gradient Descent(22365): loss=0.02350010609540524\n",
      "Stochastic Gradient Descent(22366): loss=23.07777146599778\n",
      "Stochastic Gradient Descent(22367): loss=0.0007999557724850275\n",
      "Stochastic Gradient Descent(22368): loss=10.798814842337789\n",
      "Stochastic Gradient Descent(22369): loss=0.8082296548844397\n",
      "Stochastic Gradient Descent(22370): loss=4.8965135447405865\n",
      "Stochastic Gradient Descent(22371): loss=0.35419725137072877\n",
      "Stochastic Gradient Descent(22372): loss=19.46969947479473\n",
      "Stochastic Gradient Descent(22373): loss=2.7069272720038295\n",
      "Stochastic Gradient Descent(22374): loss=0.5910148778835955\n",
      "Stochastic Gradient Descent(22375): loss=0.7045458776137787\n",
      "Stochastic Gradient Descent(22376): loss=1.989082537093336\n",
      "Stochastic Gradient Descent(22377): loss=5.781315679998338\n",
      "Stochastic Gradient Descent(22378): loss=0.06357037925875526\n",
      "Stochastic Gradient Descent(22379): loss=1.9224592937855984\n",
      "Stochastic Gradient Descent(22380): loss=0.8021888839218833\n",
      "Stochastic Gradient Descent(22381): loss=3.7068553418577976\n",
      "Stochastic Gradient Descent(22382): loss=2.850451281440023\n",
      "Stochastic Gradient Descent(22383): loss=16.564690600945333\n",
      "Stochastic Gradient Descent(22384): loss=5.386288697656989\n",
      "Stochastic Gradient Descent(22385): loss=4.263208574463055\n",
      "Stochastic Gradient Descent(22386): loss=0.3333306339673108\n",
      "Stochastic Gradient Descent(22387): loss=3.2872144057498804\n",
      "Stochastic Gradient Descent(22388): loss=5.43760108217709\n",
      "Stochastic Gradient Descent(22389): loss=2.0543197075580473\n",
      "Stochastic Gradient Descent(22390): loss=0.4661235499380218\n",
      "Stochastic Gradient Descent(22391): loss=2.071636503585353\n",
      "Stochastic Gradient Descent(22392): loss=0.5448186354998322\n",
      "Stochastic Gradient Descent(22393): loss=0.5910300286276639\n",
      "Stochastic Gradient Descent(22394): loss=0.04877479890431369\n",
      "Stochastic Gradient Descent(22395): loss=1.2803188852449139\n",
      "Stochastic Gradient Descent(22396): loss=7.876155346188508\n",
      "Stochastic Gradient Descent(22397): loss=0.1949141152666756\n",
      "Stochastic Gradient Descent(22398): loss=1.3063760680644696\n",
      "Stochastic Gradient Descent(22399): loss=0.014462574878558005\n",
      "Stochastic Gradient Descent(22400): loss=4.741710705003403\n",
      "Stochastic Gradient Descent(22401): loss=7.47872973500909\n",
      "Stochastic Gradient Descent(22402): loss=0.6598045537105472\n",
      "Stochastic Gradient Descent(22403): loss=22.047246511578148\n",
      "Stochastic Gradient Descent(22404): loss=2.8518707077454257\n",
      "Stochastic Gradient Descent(22405): loss=6.954270612466779\n",
      "Stochastic Gradient Descent(22406): loss=6.544156584557691\n",
      "Stochastic Gradient Descent(22407): loss=7.18714133833638\n",
      "Stochastic Gradient Descent(22408): loss=0.030065416032436457\n",
      "Stochastic Gradient Descent(22409): loss=5.592551518299306\n",
      "Stochastic Gradient Descent(22410): loss=8.784799829295887\n",
      "Stochastic Gradient Descent(22411): loss=2.669613124714974\n",
      "Stochastic Gradient Descent(22412): loss=25.02662836874601\n",
      "Stochastic Gradient Descent(22413): loss=5.620172095395638\n",
      "Stochastic Gradient Descent(22414): loss=0.021141078119911564\n",
      "Stochastic Gradient Descent(22415): loss=3.5956981727551383\n",
      "Stochastic Gradient Descent(22416): loss=0.1746996324386401\n",
      "Stochastic Gradient Descent(22417): loss=0.8229529693235593\n",
      "Stochastic Gradient Descent(22418): loss=1.7271062606844545\n",
      "Stochastic Gradient Descent(22419): loss=10.507351222133666\n",
      "Stochastic Gradient Descent(22420): loss=5.074463696919485\n",
      "Stochastic Gradient Descent(22421): loss=25.717569789438567\n",
      "Stochastic Gradient Descent(22422): loss=6.464378483487433\n",
      "Stochastic Gradient Descent(22423): loss=3.195375697519556\n",
      "Stochastic Gradient Descent(22424): loss=1.2099133806496898\n",
      "Stochastic Gradient Descent(22425): loss=0.1820411716176893\n",
      "Stochastic Gradient Descent(22426): loss=20.043397932384305\n",
      "Stochastic Gradient Descent(22427): loss=1.9331352779741426\n",
      "Stochastic Gradient Descent(22428): loss=0.23477635777047734\n",
      "Stochastic Gradient Descent(22429): loss=0.016474034201619556\n",
      "Stochastic Gradient Descent(22430): loss=0.043291156539682504\n",
      "Stochastic Gradient Descent(22431): loss=4.813242341700051\n",
      "Stochastic Gradient Descent(22432): loss=0.5469215604067861\n",
      "Stochastic Gradient Descent(22433): loss=15.13114776380357\n",
      "Stochastic Gradient Descent(22434): loss=4.2560289384770345\n",
      "Stochastic Gradient Descent(22435): loss=0.3847769340340467\n",
      "Stochastic Gradient Descent(22436): loss=1.4469118223970792\n",
      "Stochastic Gradient Descent(22437): loss=16.27911044991211\n",
      "Stochastic Gradient Descent(22438): loss=20.228801405503575\n",
      "Stochastic Gradient Descent(22439): loss=2.7504675976379263\n",
      "Stochastic Gradient Descent(22440): loss=2.969361288776011\n",
      "Stochastic Gradient Descent(22441): loss=0.11330841274717177\n",
      "Stochastic Gradient Descent(22442): loss=47.64351346256968\n",
      "Stochastic Gradient Descent(22443): loss=5.790936395076473\n",
      "Stochastic Gradient Descent(22444): loss=1.0283237270975387\n",
      "Stochastic Gradient Descent(22445): loss=2.9716537666474254\n",
      "Stochastic Gradient Descent(22446): loss=0.038620314156517614\n",
      "Stochastic Gradient Descent(22447): loss=4.137268827809613\n",
      "Stochastic Gradient Descent(22448): loss=33.96580423373415\n",
      "Stochastic Gradient Descent(22449): loss=0.5576818752580456\n",
      "Stochastic Gradient Descent(22450): loss=34.34208665483607\n",
      "Stochastic Gradient Descent(22451): loss=4.518093659454817\n",
      "Stochastic Gradient Descent(22452): loss=1.6930896394279378\n",
      "Stochastic Gradient Descent(22453): loss=1.6048147176302978\n",
      "Stochastic Gradient Descent(22454): loss=0.02515274104920133\n",
      "Stochastic Gradient Descent(22455): loss=4.399021105307074\n",
      "Stochastic Gradient Descent(22456): loss=21.683007203125474\n",
      "Stochastic Gradient Descent(22457): loss=6.648537756541797\n",
      "Stochastic Gradient Descent(22458): loss=0.3164753536312961\n",
      "Stochastic Gradient Descent(22459): loss=8.254758571031272e-05\n",
      "Stochastic Gradient Descent(22460): loss=0.5518564435134253\n",
      "Stochastic Gradient Descent(22461): loss=0.8946357383042501\n",
      "Stochastic Gradient Descent(22462): loss=3.3669862342185706\n",
      "Stochastic Gradient Descent(22463): loss=4.560011142651274\n",
      "Stochastic Gradient Descent(22464): loss=11.208785113287805\n",
      "Stochastic Gradient Descent(22465): loss=2.321979289319323\n",
      "Stochastic Gradient Descent(22466): loss=1.390732193508512\n",
      "Stochastic Gradient Descent(22467): loss=0.4525273959175195\n",
      "Stochastic Gradient Descent(22468): loss=28.31472686525808\n",
      "Stochastic Gradient Descent(22469): loss=2.2384139483143675\n",
      "Stochastic Gradient Descent(22470): loss=0.030989988781931516\n",
      "Stochastic Gradient Descent(22471): loss=6.374155034009051\n",
      "Stochastic Gradient Descent(22472): loss=9.040802776736882\n",
      "Stochastic Gradient Descent(22473): loss=0.023621182221075964\n",
      "Stochastic Gradient Descent(22474): loss=5.5421925262694325\n",
      "Stochastic Gradient Descent(22475): loss=23.700307400289358\n",
      "Stochastic Gradient Descent(22476): loss=1.0744707137053195\n",
      "Stochastic Gradient Descent(22477): loss=12.185841968041085\n",
      "Stochastic Gradient Descent(22478): loss=2.800053554988187\n",
      "Stochastic Gradient Descent(22479): loss=0.000123643015935858\n",
      "Stochastic Gradient Descent(22480): loss=9.717793404910726\n",
      "Stochastic Gradient Descent(22481): loss=12.933906974227579\n",
      "Stochastic Gradient Descent(22482): loss=3.737182464185973\n",
      "Stochastic Gradient Descent(22483): loss=1.190322367406046\n",
      "Stochastic Gradient Descent(22484): loss=0.6532249603636808\n",
      "Stochastic Gradient Descent(22485): loss=0.058490200579295194\n",
      "Stochastic Gradient Descent(22486): loss=3.62165632907214\n",
      "Stochastic Gradient Descent(22487): loss=0.6148685670270545\n",
      "Stochastic Gradient Descent(22488): loss=4.04379563449123\n",
      "Stochastic Gradient Descent(22489): loss=1.176326693577271\n",
      "Stochastic Gradient Descent(22490): loss=0.8552961266417671\n",
      "Stochastic Gradient Descent(22491): loss=8.130686044949728\n",
      "Stochastic Gradient Descent(22492): loss=2.995152551956008\n",
      "Stochastic Gradient Descent(22493): loss=0.23517071240621848\n",
      "Stochastic Gradient Descent(22494): loss=11.168555405838339\n",
      "Stochastic Gradient Descent(22495): loss=12.771818922732924\n",
      "Stochastic Gradient Descent(22496): loss=2.160977572442165\n",
      "Stochastic Gradient Descent(22497): loss=9.584339381134349\n",
      "Stochastic Gradient Descent(22498): loss=3.1327404132119336\n",
      "Stochastic Gradient Descent(22499): loss=0.03475107243280514\n",
      "Stochastic Gradient Descent(22500): loss=0.08099359010071548\n",
      "Stochastic Gradient Descent(22501): loss=28.62070999323967\n",
      "Stochastic Gradient Descent(22502): loss=0.9258504427332982\n",
      "Stochastic Gradient Descent(22503): loss=0.14388732772507287\n",
      "Stochastic Gradient Descent(22504): loss=0.7073402881611386\n",
      "Stochastic Gradient Descent(22505): loss=15.382470043950459\n",
      "Stochastic Gradient Descent(22506): loss=2.6794326636326513\n",
      "Stochastic Gradient Descent(22507): loss=1.1755840978320373\n",
      "Stochastic Gradient Descent(22508): loss=1.071554654589097\n",
      "Stochastic Gradient Descent(22509): loss=5.796968320761414\n",
      "Stochastic Gradient Descent(22510): loss=0.05829161413008454\n",
      "Stochastic Gradient Descent(22511): loss=32.25495100073726\n",
      "Stochastic Gradient Descent(22512): loss=0.43803493339413696\n",
      "Stochastic Gradient Descent(22513): loss=4.747651135734937\n",
      "Stochastic Gradient Descent(22514): loss=9.875791973530276\n",
      "Stochastic Gradient Descent(22515): loss=7.274016386310482\n",
      "Stochastic Gradient Descent(22516): loss=0.701203141510708\n",
      "Stochastic Gradient Descent(22517): loss=1.7767681352972593\n",
      "Stochastic Gradient Descent(22518): loss=0.8692483149506038\n",
      "Stochastic Gradient Descent(22519): loss=0.05219349862290993\n",
      "Stochastic Gradient Descent(22520): loss=0.21176241937658952\n",
      "Stochastic Gradient Descent(22521): loss=0.40328410416483723\n",
      "Stochastic Gradient Descent(22522): loss=0.2707461614863574\n",
      "Stochastic Gradient Descent(22523): loss=7.323977705905795\n",
      "Stochastic Gradient Descent(22524): loss=27.918138023884396\n",
      "Stochastic Gradient Descent(22525): loss=2.265961735152173\n",
      "Stochastic Gradient Descent(22526): loss=0.012258084838282293\n",
      "Stochastic Gradient Descent(22527): loss=1.9782061545051028\n",
      "Stochastic Gradient Descent(22528): loss=12.837589720302265\n",
      "Stochastic Gradient Descent(22529): loss=12.649957286710555\n",
      "Stochastic Gradient Descent(22530): loss=0.09808842299829026\n",
      "Stochastic Gradient Descent(22531): loss=49.286342160515645\n",
      "Stochastic Gradient Descent(22532): loss=16.837355287628554\n",
      "Stochastic Gradient Descent(22533): loss=1.0688060198830622\n",
      "Stochastic Gradient Descent(22534): loss=0.7527959789272336\n",
      "Stochastic Gradient Descent(22535): loss=0.4713462974446114\n",
      "Stochastic Gradient Descent(22536): loss=9.98944222426111\n",
      "Stochastic Gradient Descent(22537): loss=0.003511787501987329\n",
      "Stochastic Gradient Descent(22538): loss=0.027432710286409707\n",
      "Stochastic Gradient Descent(22539): loss=0.08701446432817381\n",
      "Stochastic Gradient Descent(22540): loss=26.57559198542427\n",
      "Stochastic Gradient Descent(22541): loss=0.9624210320494436\n",
      "Stochastic Gradient Descent(22542): loss=0.038615714685660496\n",
      "Stochastic Gradient Descent(22543): loss=7.3671183306878545\n",
      "Stochastic Gradient Descent(22544): loss=1.659845781127981\n",
      "Stochastic Gradient Descent(22545): loss=5.1557420406844985\n",
      "Stochastic Gradient Descent(22546): loss=6.674248670632004\n",
      "Stochastic Gradient Descent(22547): loss=0.03076867950129913\n",
      "Stochastic Gradient Descent(22548): loss=13.478514906259289\n",
      "Stochastic Gradient Descent(22549): loss=9.39111561139522\n",
      "Stochastic Gradient Descent(22550): loss=1.1802011063445035\n",
      "Stochastic Gradient Descent(22551): loss=5.909156956264955\n",
      "Stochastic Gradient Descent(22552): loss=5.897891013230959\n",
      "Stochastic Gradient Descent(22553): loss=23.695343419098542\n",
      "Stochastic Gradient Descent(22554): loss=1.0504305395428264\n",
      "Stochastic Gradient Descent(22555): loss=9.40779989309607\n",
      "Stochastic Gradient Descent(22556): loss=0.45489192796042804\n",
      "Stochastic Gradient Descent(22557): loss=4.695502672214727\n",
      "Stochastic Gradient Descent(22558): loss=0.10301430216366\n",
      "Stochastic Gradient Descent(22559): loss=1.323345071012506\n",
      "Stochastic Gradient Descent(22560): loss=2.271818048257622\n",
      "Stochastic Gradient Descent(22561): loss=1.5382322898108414\n",
      "Stochastic Gradient Descent(22562): loss=10.483677547414292\n",
      "Stochastic Gradient Descent(22563): loss=21.477765530933336\n",
      "Stochastic Gradient Descent(22564): loss=5.595551163455211\n",
      "Stochastic Gradient Descent(22565): loss=7.705686539881168\n",
      "Stochastic Gradient Descent(22566): loss=4.661015625958181\n",
      "Stochastic Gradient Descent(22567): loss=0.7038499822155683\n",
      "Stochastic Gradient Descent(22568): loss=6.613774975787783\n",
      "Stochastic Gradient Descent(22569): loss=1.831850074860345\n",
      "Stochastic Gradient Descent(22570): loss=3.8664456611865683\n",
      "Stochastic Gradient Descent(22571): loss=0.040716352964589295\n",
      "Stochastic Gradient Descent(22572): loss=2.020588252685956\n",
      "Stochastic Gradient Descent(22573): loss=10.49608934331959\n",
      "Stochastic Gradient Descent(22574): loss=0.034075341769043345\n",
      "Stochastic Gradient Descent(22575): loss=1.3336097088429968\n",
      "Stochastic Gradient Descent(22576): loss=0.17673326442472118\n",
      "Stochastic Gradient Descent(22577): loss=2.2831391123038816\n",
      "Stochastic Gradient Descent(22578): loss=2.3507789189521753\n",
      "Stochastic Gradient Descent(22579): loss=1.337440728118065\n",
      "Stochastic Gradient Descent(22580): loss=11.614102059792412\n",
      "Stochastic Gradient Descent(22581): loss=0.8509536031802566\n",
      "Stochastic Gradient Descent(22582): loss=0.003620179381503492\n",
      "Stochastic Gradient Descent(22583): loss=0.2405648088364129\n",
      "Stochastic Gradient Descent(22584): loss=1.745025327921488\n",
      "Stochastic Gradient Descent(22585): loss=11.631095754554375\n",
      "Stochastic Gradient Descent(22586): loss=2.9380398186278742\n",
      "Stochastic Gradient Descent(22587): loss=0.4389347499603933\n",
      "Stochastic Gradient Descent(22588): loss=0.004010568603477369\n",
      "Stochastic Gradient Descent(22589): loss=5.1126377508230885\n",
      "Stochastic Gradient Descent(22590): loss=1.0391313868031542\n",
      "Stochastic Gradient Descent(22591): loss=8.584317048441859\n",
      "Stochastic Gradient Descent(22592): loss=25.87677251361102\n",
      "Stochastic Gradient Descent(22593): loss=0.1445661145920371\n",
      "Stochastic Gradient Descent(22594): loss=0.6109945208484461\n",
      "Stochastic Gradient Descent(22595): loss=0.20643673580193486\n",
      "Stochastic Gradient Descent(22596): loss=1.4195416336237725\n",
      "Stochastic Gradient Descent(22597): loss=0.1171407610107577\n",
      "Stochastic Gradient Descent(22598): loss=8.611885693185322\n",
      "Stochastic Gradient Descent(22599): loss=0.6597614135352827\n",
      "Stochastic Gradient Descent(22600): loss=13.940731094823558\n",
      "Stochastic Gradient Descent(22601): loss=4.882253911093202\n",
      "Stochastic Gradient Descent(22602): loss=1.3724107499192835\n",
      "Stochastic Gradient Descent(22603): loss=14.527741561006685\n",
      "Stochastic Gradient Descent(22604): loss=1.061103775296683\n",
      "Stochastic Gradient Descent(22605): loss=1.434346830719175\n",
      "Stochastic Gradient Descent(22606): loss=2.2320131701507626\n",
      "Stochastic Gradient Descent(22607): loss=0.018356584715771698\n",
      "Stochastic Gradient Descent(22608): loss=0.17581170618180184\n",
      "Stochastic Gradient Descent(22609): loss=30.916421560710344\n",
      "Stochastic Gradient Descent(22610): loss=4.854934587601467\n",
      "Stochastic Gradient Descent(22611): loss=8.891154204734166\n",
      "Stochastic Gradient Descent(22612): loss=2.3057911072584423\n",
      "Stochastic Gradient Descent(22613): loss=0.13557771548199968\n",
      "Stochastic Gradient Descent(22614): loss=0.13650581213355664\n",
      "Stochastic Gradient Descent(22615): loss=9.574359675806717\n",
      "Stochastic Gradient Descent(22616): loss=2.998748052859769\n",
      "Stochastic Gradient Descent(22617): loss=3.020708788728518\n",
      "Stochastic Gradient Descent(22618): loss=8.744885776352811\n",
      "Stochastic Gradient Descent(22619): loss=0.016724377423334373\n",
      "Stochastic Gradient Descent(22620): loss=7.332753823244605\n",
      "Stochastic Gradient Descent(22621): loss=1.0180023290451286\n",
      "Stochastic Gradient Descent(22622): loss=1.5318767753868443\n",
      "Stochastic Gradient Descent(22623): loss=0.38218101282079225\n",
      "Stochastic Gradient Descent(22624): loss=10.259956063523763\n",
      "Stochastic Gradient Descent(22625): loss=3.4621579783369643\n",
      "Stochastic Gradient Descent(22626): loss=4.072798990870566\n",
      "Stochastic Gradient Descent(22627): loss=0.16718984121765065\n",
      "Stochastic Gradient Descent(22628): loss=1.9845447694988487\n",
      "Stochastic Gradient Descent(22629): loss=0.8750353705158455\n",
      "Stochastic Gradient Descent(22630): loss=0.963745218144833\n",
      "Stochastic Gradient Descent(22631): loss=5.3156657050398675\n",
      "Stochastic Gradient Descent(22632): loss=8.616200645032311\n",
      "Stochastic Gradient Descent(22633): loss=1.4888688376672987\n",
      "Stochastic Gradient Descent(22634): loss=0.01058232886710836\n",
      "Stochastic Gradient Descent(22635): loss=0.9794146170656343\n",
      "Stochastic Gradient Descent(22636): loss=0.7992032358587906\n",
      "Stochastic Gradient Descent(22637): loss=0.2622185499573354\n",
      "Stochastic Gradient Descent(22638): loss=3.6454627483696678\n",
      "Stochastic Gradient Descent(22639): loss=7.97873436057079\n",
      "Stochastic Gradient Descent(22640): loss=0.14693498519770767\n",
      "Stochastic Gradient Descent(22641): loss=15.289078402895825\n",
      "Stochastic Gradient Descent(22642): loss=18.262045907225577\n",
      "Stochastic Gradient Descent(22643): loss=4.885187766204964\n",
      "Stochastic Gradient Descent(22644): loss=0.047568277373937484\n",
      "Stochastic Gradient Descent(22645): loss=3.8965348739089567\n",
      "Stochastic Gradient Descent(22646): loss=0.15729903829155314\n",
      "Stochastic Gradient Descent(22647): loss=0.0365279236165478\n",
      "Stochastic Gradient Descent(22648): loss=14.50620585955541\n",
      "Stochastic Gradient Descent(22649): loss=1.8460775004718137\n",
      "Stochastic Gradient Descent(22650): loss=4.468683425071131\n",
      "Stochastic Gradient Descent(22651): loss=0.44188396647613576\n",
      "Stochastic Gradient Descent(22652): loss=0.2894701690296559\n",
      "Stochastic Gradient Descent(22653): loss=0.7270703839001459\n",
      "Stochastic Gradient Descent(22654): loss=0.6431326647156588\n",
      "Stochastic Gradient Descent(22655): loss=2.8393830599565204\n",
      "Stochastic Gradient Descent(22656): loss=0.778064524887218\n",
      "Stochastic Gradient Descent(22657): loss=1.6220736347607785\n",
      "Stochastic Gradient Descent(22658): loss=0.1730174076960666\n",
      "Stochastic Gradient Descent(22659): loss=1.3772136737945258\n",
      "Stochastic Gradient Descent(22660): loss=5.6564382686874115\n",
      "Stochastic Gradient Descent(22661): loss=0.7663957786569312\n",
      "Stochastic Gradient Descent(22662): loss=3.199897802260893\n",
      "Stochastic Gradient Descent(22663): loss=1.4276410206584929\n",
      "Stochastic Gradient Descent(22664): loss=15.207680109753966\n",
      "Stochastic Gradient Descent(22665): loss=3.0386863617499595\n",
      "Stochastic Gradient Descent(22666): loss=5.809221357201723\n",
      "Stochastic Gradient Descent(22667): loss=3.282944001479579\n",
      "Stochastic Gradient Descent(22668): loss=5.867855194651292\n",
      "Stochastic Gradient Descent(22669): loss=38.17286506120583\n",
      "Stochastic Gradient Descent(22670): loss=0.181860815868104\n",
      "Stochastic Gradient Descent(22671): loss=23.751043302261802\n",
      "Stochastic Gradient Descent(22672): loss=5.669013667214837\n",
      "Stochastic Gradient Descent(22673): loss=43.873203758530444\n",
      "Stochastic Gradient Descent(22674): loss=2.3329706753119144\n",
      "Stochastic Gradient Descent(22675): loss=1.4226975482364337\n",
      "Stochastic Gradient Descent(22676): loss=35.19498279783002\n",
      "Stochastic Gradient Descent(22677): loss=0.5623600703686926\n",
      "Stochastic Gradient Descent(22678): loss=0.01076973503093284\n",
      "Stochastic Gradient Descent(22679): loss=0.017710912581390317\n",
      "Stochastic Gradient Descent(22680): loss=0.5931837708460357\n",
      "Stochastic Gradient Descent(22681): loss=2.268800911782462\n",
      "Stochastic Gradient Descent(22682): loss=0.11848476584883048\n",
      "Stochastic Gradient Descent(22683): loss=0.8712743213919504\n",
      "Stochastic Gradient Descent(22684): loss=0.9831857342009992\n",
      "Stochastic Gradient Descent(22685): loss=5.779323318330586\n",
      "Stochastic Gradient Descent(22686): loss=8.712639332410905\n",
      "Stochastic Gradient Descent(22687): loss=11.77184141539131\n",
      "Stochastic Gradient Descent(22688): loss=0.5331828931720579\n",
      "Stochastic Gradient Descent(22689): loss=0.5335734304327286\n",
      "Stochastic Gradient Descent(22690): loss=23.464905102842355\n",
      "Stochastic Gradient Descent(22691): loss=4.335574820819014\n",
      "Stochastic Gradient Descent(22692): loss=0.323367911221428\n",
      "Stochastic Gradient Descent(22693): loss=0.7425320332615167\n",
      "Stochastic Gradient Descent(22694): loss=0.5961057655490374\n",
      "Stochastic Gradient Descent(22695): loss=3.6054931034983926\n",
      "Stochastic Gradient Descent(22696): loss=1.3650900965799322\n",
      "Stochastic Gradient Descent(22697): loss=8.26624696570768\n",
      "Stochastic Gradient Descent(22698): loss=6.607623105450555\n",
      "Stochastic Gradient Descent(22699): loss=11.889792700894704\n",
      "Stochastic Gradient Descent(22700): loss=13.102806126045149\n",
      "Stochastic Gradient Descent(22701): loss=0.28475539973510694\n",
      "Stochastic Gradient Descent(22702): loss=0.6824786730592919\n",
      "Stochastic Gradient Descent(22703): loss=2.7162277881467976\n",
      "Stochastic Gradient Descent(22704): loss=13.110880902955161\n",
      "Stochastic Gradient Descent(22705): loss=3.859796046795856\n",
      "Stochastic Gradient Descent(22706): loss=1.9398031970941367\n",
      "Stochastic Gradient Descent(22707): loss=5.605626435412517\n",
      "Stochastic Gradient Descent(22708): loss=16.263885250237365\n",
      "Stochastic Gradient Descent(22709): loss=0.7422934487927536\n",
      "Stochastic Gradient Descent(22710): loss=3.681000011661062\n",
      "Stochastic Gradient Descent(22711): loss=2.4362731997815383\n",
      "Stochastic Gradient Descent(22712): loss=8.203658848330376\n",
      "Stochastic Gradient Descent(22713): loss=25.87321245904828\n",
      "Stochastic Gradient Descent(22714): loss=0.7738528439438003\n",
      "Stochastic Gradient Descent(22715): loss=0.0004820893913910935\n",
      "Stochastic Gradient Descent(22716): loss=15.291117385998607\n",
      "Stochastic Gradient Descent(22717): loss=0.3431422982372774\n",
      "Stochastic Gradient Descent(22718): loss=0.6474840122390055\n",
      "Stochastic Gradient Descent(22719): loss=0.5748324850250446\n",
      "Stochastic Gradient Descent(22720): loss=0.14758310339109412\n",
      "Stochastic Gradient Descent(22721): loss=2.624826858001178\n",
      "Stochastic Gradient Descent(22722): loss=34.28512885310487\n",
      "Stochastic Gradient Descent(22723): loss=0.6511665389959831\n",
      "Stochastic Gradient Descent(22724): loss=4.684681819409032\n",
      "Stochastic Gradient Descent(22725): loss=2.7442902131913884\n",
      "Stochastic Gradient Descent(22726): loss=6.20380449644348\n",
      "Stochastic Gradient Descent(22727): loss=2.686687256910694\n",
      "Stochastic Gradient Descent(22728): loss=15.498053091391167\n",
      "Stochastic Gradient Descent(22729): loss=1.9315161383802364\n",
      "Stochastic Gradient Descent(22730): loss=2.16381003374368\n",
      "Stochastic Gradient Descent(22731): loss=1.406889188374169\n",
      "Stochastic Gradient Descent(22732): loss=0.059163255761998566\n",
      "Stochastic Gradient Descent(22733): loss=0.6625060314285673\n",
      "Stochastic Gradient Descent(22734): loss=23.56531568189739\n",
      "Stochastic Gradient Descent(22735): loss=0.4636156916754619\n",
      "Stochastic Gradient Descent(22736): loss=6.374645042882217\n",
      "Stochastic Gradient Descent(22737): loss=3.4262979663057225\n",
      "Stochastic Gradient Descent(22738): loss=1.7797829382230763\n",
      "Stochastic Gradient Descent(22739): loss=5.30327998723219\n",
      "Stochastic Gradient Descent(22740): loss=0.06450422170351987\n",
      "Stochastic Gradient Descent(22741): loss=0.9227184820165889\n",
      "Stochastic Gradient Descent(22742): loss=16.931309946188843\n",
      "Stochastic Gradient Descent(22743): loss=4.552137716658681\n",
      "Stochastic Gradient Descent(22744): loss=0.007670163637914633\n",
      "Stochastic Gradient Descent(22745): loss=9.412001725825451\n",
      "Stochastic Gradient Descent(22746): loss=0.8945196313658627\n",
      "Stochastic Gradient Descent(22747): loss=2.6140311296485135\n",
      "Stochastic Gradient Descent(22748): loss=0.10673034560144531\n",
      "Stochastic Gradient Descent(22749): loss=13.121095112498615\n",
      "Stochastic Gradient Descent(22750): loss=1.6730332304639204\n",
      "Stochastic Gradient Descent(22751): loss=0.8823439685948117\n",
      "Stochastic Gradient Descent(22752): loss=10.94822579691676\n",
      "Stochastic Gradient Descent(22753): loss=8.657271656830318\n",
      "Stochastic Gradient Descent(22754): loss=3.8460848261876004\n",
      "Stochastic Gradient Descent(22755): loss=7.138094844769716\n",
      "Stochastic Gradient Descent(22756): loss=2.849041493412596\n",
      "Stochastic Gradient Descent(22757): loss=0.24370791962783217\n",
      "Stochastic Gradient Descent(22758): loss=2.751672916129369\n",
      "Stochastic Gradient Descent(22759): loss=6.257190627574892\n",
      "Stochastic Gradient Descent(22760): loss=1.869836086495841\n",
      "Stochastic Gradient Descent(22761): loss=2.558572154475531\n",
      "Stochastic Gradient Descent(22762): loss=7.995457019317484\n",
      "Stochastic Gradient Descent(22763): loss=0.7763035402620302\n",
      "Stochastic Gradient Descent(22764): loss=0.07125494009227885\n",
      "Stochastic Gradient Descent(22765): loss=17.690572154235998\n",
      "Stochastic Gradient Descent(22766): loss=6.59549165567951\n",
      "Stochastic Gradient Descent(22767): loss=0.05733410185751077\n",
      "Stochastic Gradient Descent(22768): loss=10.855101890885615\n",
      "Stochastic Gradient Descent(22769): loss=0.03683334274834708\n",
      "Stochastic Gradient Descent(22770): loss=0.2112239603327305\n",
      "Stochastic Gradient Descent(22771): loss=0.24236237515979597\n",
      "Stochastic Gradient Descent(22772): loss=1.122040105683529\n",
      "Stochastic Gradient Descent(22773): loss=0.035944222273799634\n",
      "Stochastic Gradient Descent(22774): loss=0.9134683983350841\n",
      "Stochastic Gradient Descent(22775): loss=0.357339314176906\n",
      "Stochastic Gradient Descent(22776): loss=0.6243784023091807\n",
      "Stochastic Gradient Descent(22777): loss=4.299557230877444\n",
      "Stochastic Gradient Descent(22778): loss=0.042883750308323365\n",
      "Stochastic Gradient Descent(22779): loss=1.1043013380861884\n",
      "Stochastic Gradient Descent(22780): loss=8.503572568625275\n",
      "Stochastic Gradient Descent(22781): loss=0.25986426845027016\n",
      "Stochastic Gradient Descent(22782): loss=9.202975996943689\n",
      "Stochastic Gradient Descent(22783): loss=6.061354256545069\n",
      "Stochastic Gradient Descent(22784): loss=13.170151260265024\n",
      "Stochastic Gradient Descent(22785): loss=16.284545942117788\n",
      "Stochastic Gradient Descent(22786): loss=1.8558504560193598\n",
      "Stochastic Gradient Descent(22787): loss=0.07323492530442065\n",
      "Stochastic Gradient Descent(22788): loss=0.03624986781275872\n",
      "Stochastic Gradient Descent(22789): loss=14.614761250473425\n",
      "Stochastic Gradient Descent(22790): loss=5.786113821274887\n",
      "Stochastic Gradient Descent(22791): loss=4.696519909195986\n",
      "Stochastic Gradient Descent(22792): loss=0.021862731509214924\n",
      "Stochastic Gradient Descent(22793): loss=1.6595223942239352\n",
      "Stochastic Gradient Descent(22794): loss=1.7773358983107999\n",
      "Stochastic Gradient Descent(22795): loss=1.074445512343847\n",
      "Stochastic Gradient Descent(22796): loss=8.526673946431222\n",
      "Stochastic Gradient Descent(22797): loss=5.722153621741662\n",
      "Stochastic Gradient Descent(22798): loss=8.807368501653515\n",
      "Stochastic Gradient Descent(22799): loss=0.5945768839324526\n",
      "Stochastic Gradient Descent(22800): loss=0.31246209474656655\n",
      "Stochastic Gradient Descent(22801): loss=0.9472090798650806\n",
      "Stochastic Gradient Descent(22802): loss=1.946848502330108\n",
      "Stochastic Gradient Descent(22803): loss=2.6555483684987773\n",
      "Stochastic Gradient Descent(22804): loss=1.9266678046043344\n",
      "Stochastic Gradient Descent(22805): loss=1.1890537033753075\n",
      "Stochastic Gradient Descent(22806): loss=7.44662916824706\n",
      "Stochastic Gradient Descent(22807): loss=12.27410611537677\n",
      "Stochastic Gradient Descent(22808): loss=8.348689775880334\n",
      "Stochastic Gradient Descent(22809): loss=0.003595632026958127\n",
      "Stochastic Gradient Descent(22810): loss=2.6638767621569373\n",
      "Stochastic Gradient Descent(22811): loss=7.97496311552905\n",
      "Stochastic Gradient Descent(22812): loss=3.197683726590291\n",
      "Stochastic Gradient Descent(22813): loss=0.2671278091294532\n",
      "Stochastic Gradient Descent(22814): loss=0.06824272459198238\n",
      "Stochastic Gradient Descent(22815): loss=1.8700699082011394\n",
      "Stochastic Gradient Descent(22816): loss=3.4676235543672838\n",
      "Stochastic Gradient Descent(22817): loss=0.5740700267042717\n",
      "Stochastic Gradient Descent(22818): loss=1.6165283242387372\n",
      "Stochastic Gradient Descent(22819): loss=0.1192409632523066\n",
      "Stochastic Gradient Descent(22820): loss=1.7879170040329289\n",
      "Stochastic Gradient Descent(22821): loss=2.3269771745221983\n",
      "Stochastic Gradient Descent(22822): loss=0.2001010864520274\n",
      "Stochastic Gradient Descent(22823): loss=4.9679292520657725\n",
      "Stochastic Gradient Descent(22824): loss=1.0123909323686644\n",
      "Stochastic Gradient Descent(22825): loss=1.6613057025825881\n",
      "Stochastic Gradient Descent(22826): loss=4.9231666359310005\n",
      "Stochastic Gradient Descent(22827): loss=7.642854775408138\n",
      "Stochastic Gradient Descent(22828): loss=17.65999582023962\n",
      "Stochastic Gradient Descent(22829): loss=7.046984161902923\n",
      "Stochastic Gradient Descent(22830): loss=17.01299454924589\n",
      "Stochastic Gradient Descent(22831): loss=0.7765486647019452\n",
      "Stochastic Gradient Descent(22832): loss=11.970664863614715\n",
      "Stochastic Gradient Descent(22833): loss=18.00916016092879\n",
      "Stochastic Gradient Descent(22834): loss=0.8376518243122186\n",
      "Stochastic Gradient Descent(22835): loss=1.1145008494596562\n",
      "Stochastic Gradient Descent(22836): loss=0.1630680343059761\n",
      "Stochastic Gradient Descent(22837): loss=2.629413407194405\n",
      "Stochastic Gradient Descent(22838): loss=0.332504609115393\n",
      "Stochastic Gradient Descent(22839): loss=15.088190846791703\n",
      "Stochastic Gradient Descent(22840): loss=36.85986419532136\n",
      "Stochastic Gradient Descent(22841): loss=5.041015697149701\n",
      "Stochastic Gradient Descent(22842): loss=13.322166401448321\n",
      "Stochastic Gradient Descent(22843): loss=4.619961408560664\n",
      "Stochastic Gradient Descent(22844): loss=13.88768928351948\n",
      "Stochastic Gradient Descent(22845): loss=0.1464375288039816\n",
      "Stochastic Gradient Descent(22846): loss=7.326045967960952\n",
      "Stochastic Gradient Descent(22847): loss=0.4344714366574548\n",
      "Stochastic Gradient Descent(22848): loss=0.9791200197026989\n",
      "Stochastic Gradient Descent(22849): loss=7.673334180840956\n",
      "Stochastic Gradient Descent(22850): loss=0.34260272613275444\n",
      "Stochastic Gradient Descent(22851): loss=0.12837488950181947\n",
      "Stochastic Gradient Descent(22852): loss=2.0242165833055803\n",
      "Stochastic Gradient Descent(22853): loss=9.13378879406378\n",
      "Stochastic Gradient Descent(22854): loss=1.348851279795863\n",
      "Stochastic Gradient Descent(22855): loss=1.680825954096791\n",
      "Stochastic Gradient Descent(22856): loss=1.7506846249940717\n",
      "Stochastic Gradient Descent(22857): loss=5.801518768440061\n",
      "Stochastic Gradient Descent(22858): loss=0.13365253088642573\n",
      "Stochastic Gradient Descent(22859): loss=5.262575458924696\n",
      "Stochastic Gradient Descent(22860): loss=5.774206441095913\n",
      "Stochastic Gradient Descent(22861): loss=0.7758929740219473\n",
      "Stochastic Gradient Descent(22862): loss=1.5838878657762925\n",
      "Stochastic Gradient Descent(22863): loss=8.405114518691578\n",
      "Stochastic Gradient Descent(22864): loss=0.5401392784578788\n",
      "Stochastic Gradient Descent(22865): loss=24.643268024337434\n",
      "Stochastic Gradient Descent(22866): loss=0.9057502319426398\n",
      "Stochastic Gradient Descent(22867): loss=0.5217862703100186\n",
      "Stochastic Gradient Descent(22868): loss=17.035923683533003\n",
      "Stochastic Gradient Descent(22869): loss=2.1423799824170167\n",
      "Stochastic Gradient Descent(22870): loss=3.6308437842594916\n",
      "Stochastic Gradient Descent(22871): loss=12.031447687460016\n",
      "Stochastic Gradient Descent(22872): loss=33.26627882723671\n",
      "Stochastic Gradient Descent(22873): loss=0.4028114718598706\n",
      "Stochastic Gradient Descent(22874): loss=0.022660587355760212\n",
      "Stochastic Gradient Descent(22875): loss=3.9550841110625523\n",
      "Stochastic Gradient Descent(22876): loss=4.760409063299923\n",
      "Stochastic Gradient Descent(22877): loss=0.041478058532203446\n",
      "Stochastic Gradient Descent(22878): loss=0.03130413834982265\n",
      "Stochastic Gradient Descent(22879): loss=1.0031406440507822\n",
      "Stochastic Gradient Descent(22880): loss=18.62581040431809\n",
      "Stochastic Gradient Descent(22881): loss=0.44699090016281195\n",
      "Stochastic Gradient Descent(22882): loss=9.851672948390476\n",
      "Stochastic Gradient Descent(22883): loss=57.124861549529335\n",
      "Stochastic Gradient Descent(22884): loss=26.43414734778828\n",
      "Stochastic Gradient Descent(22885): loss=2.11403481641121\n",
      "Stochastic Gradient Descent(22886): loss=1.24097721620718\n",
      "Stochastic Gradient Descent(22887): loss=0.635387482795465\n",
      "Stochastic Gradient Descent(22888): loss=3.396428991720057\n",
      "Stochastic Gradient Descent(22889): loss=4.6055121879831535\n",
      "Stochastic Gradient Descent(22890): loss=0.28852562669778253\n",
      "Stochastic Gradient Descent(22891): loss=0.4636398519141782\n",
      "Stochastic Gradient Descent(22892): loss=8.526862414177064\n",
      "Stochastic Gradient Descent(22893): loss=15.995763229317204\n",
      "Stochastic Gradient Descent(22894): loss=0.9236239049269328\n",
      "Stochastic Gradient Descent(22895): loss=2.918909889502973\n",
      "Stochastic Gradient Descent(22896): loss=6.791717404097743\n",
      "Stochastic Gradient Descent(22897): loss=17.886768229577168\n",
      "Stochastic Gradient Descent(22898): loss=3.930455980948631\n",
      "Stochastic Gradient Descent(22899): loss=8.476515827318076\n",
      "Stochastic Gradient Descent(22900): loss=0.007095262368428865\n",
      "Stochastic Gradient Descent(22901): loss=0.022263305707562873\n",
      "Stochastic Gradient Descent(22902): loss=0.35817990212739537\n",
      "Stochastic Gradient Descent(22903): loss=3.3335876482179656\n",
      "Stochastic Gradient Descent(22904): loss=0.19838132421062343\n",
      "Stochastic Gradient Descent(22905): loss=9.877473642232983\n",
      "Stochastic Gradient Descent(22906): loss=11.943605110194605\n",
      "Stochastic Gradient Descent(22907): loss=0.03542219858979764\n",
      "Stochastic Gradient Descent(22908): loss=0.6911985159599504\n",
      "Stochastic Gradient Descent(22909): loss=1.1428915088642755\n",
      "Stochastic Gradient Descent(22910): loss=0.011053134138173853\n",
      "Stochastic Gradient Descent(22911): loss=2.2848904690086833\n",
      "Stochastic Gradient Descent(22912): loss=0.33617765245101405\n",
      "Stochastic Gradient Descent(22913): loss=15.561635464906084\n",
      "Stochastic Gradient Descent(22914): loss=0.8143409020626811\n",
      "Stochastic Gradient Descent(22915): loss=1.0558527479771946\n",
      "Stochastic Gradient Descent(22916): loss=0.05469077806815362\n",
      "Stochastic Gradient Descent(22917): loss=0.13860051160866835\n",
      "Stochastic Gradient Descent(22918): loss=1.2538011980889663\n",
      "Stochastic Gradient Descent(22919): loss=2.4058292036757876\n",
      "Stochastic Gradient Descent(22920): loss=0.8198421610791266\n",
      "Stochastic Gradient Descent(22921): loss=1.0475354395818144\n",
      "Stochastic Gradient Descent(22922): loss=6.110315206240454\n",
      "Stochastic Gradient Descent(22923): loss=0.27333599255367635\n",
      "Stochastic Gradient Descent(22924): loss=0.6406543299192363\n",
      "Stochastic Gradient Descent(22925): loss=0.000533774451701997\n",
      "Stochastic Gradient Descent(22926): loss=7.0184472598322145\n",
      "Stochastic Gradient Descent(22927): loss=1.5813108866812848\n",
      "Stochastic Gradient Descent(22928): loss=0.04445126964083087\n",
      "Stochastic Gradient Descent(22929): loss=0.32595667128234523\n",
      "Stochastic Gradient Descent(22930): loss=2.270499836953054\n",
      "Stochastic Gradient Descent(22931): loss=2.2995124703765413\n",
      "Stochastic Gradient Descent(22932): loss=4.492224225512895\n",
      "Stochastic Gradient Descent(22933): loss=0.20654008319547482\n",
      "Stochastic Gradient Descent(22934): loss=1.604513487639785\n",
      "Stochastic Gradient Descent(22935): loss=10.2769841974576\n",
      "Stochastic Gradient Descent(22936): loss=8.420482075452748\n",
      "Stochastic Gradient Descent(22937): loss=1.7756383088356054\n",
      "Stochastic Gradient Descent(22938): loss=17.2851007548497\n",
      "Stochastic Gradient Descent(22939): loss=11.34556618384284\n",
      "Stochastic Gradient Descent(22940): loss=7.510308563470815\n",
      "Stochastic Gradient Descent(22941): loss=13.347116804294444\n",
      "Stochastic Gradient Descent(22942): loss=4.914069474446137\n",
      "Stochastic Gradient Descent(22943): loss=17.410292637269958\n",
      "Stochastic Gradient Descent(22944): loss=0.6486926009249724\n",
      "Stochastic Gradient Descent(22945): loss=4.925918992404996\n",
      "Stochastic Gradient Descent(22946): loss=3.9022428383841903\n",
      "Stochastic Gradient Descent(22947): loss=9.3483119128774\n",
      "Stochastic Gradient Descent(22948): loss=14.26504510821038\n",
      "Stochastic Gradient Descent(22949): loss=11.677033808770197\n",
      "Stochastic Gradient Descent(22950): loss=10.97916283673754\n",
      "Stochastic Gradient Descent(22951): loss=2.13807938466033\n",
      "Stochastic Gradient Descent(22952): loss=0.4153380710986897\n",
      "Stochastic Gradient Descent(22953): loss=0.621006206422987\n",
      "Stochastic Gradient Descent(22954): loss=18.218723094103414\n",
      "Stochastic Gradient Descent(22955): loss=11.657229740230315\n",
      "Stochastic Gradient Descent(22956): loss=0.0769789644553514\n",
      "Stochastic Gradient Descent(22957): loss=0.07780563730991347\n",
      "Stochastic Gradient Descent(22958): loss=2.2174808135920685\n",
      "Stochastic Gradient Descent(22959): loss=15.325576155347466\n",
      "Stochastic Gradient Descent(22960): loss=7.689461625795176\n",
      "Stochastic Gradient Descent(22961): loss=13.737165833339322\n",
      "Stochastic Gradient Descent(22962): loss=9.185466356411565\n",
      "Stochastic Gradient Descent(22963): loss=2.267602845343369\n",
      "Stochastic Gradient Descent(22964): loss=5.01642459728814\n",
      "Stochastic Gradient Descent(22965): loss=3.2692706673126737\n",
      "Stochastic Gradient Descent(22966): loss=1.1822287257666326\n",
      "Stochastic Gradient Descent(22967): loss=0.001639227399605491\n",
      "Stochastic Gradient Descent(22968): loss=2.9172535462859175\n",
      "Stochastic Gradient Descent(22969): loss=1.4968559813137376\n",
      "Stochastic Gradient Descent(22970): loss=7.325014655898306\n",
      "Stochastic Gradient Descent(22971): loss=1.685062981066372\n",
      "Stochastic Gradient Descent(22972): loss=0.36467707971294194\n",
      "Stochastic Gradient Descent(22973): loss=7.832105718399551\n",
      "Stochastic Gradient Descent(22974): loss=6.524265119190852\n",
      "Stochastic Gradient Descent(22975): loss=6.41690894974769\n",
      "Stochastic Gradient Descent(22976): loss=0.6849471358577837\n",
      "Stochastic Gradient Descent(22977): loss=0.36363945218448235\n",
      "Stochastic Gradient Descent(22978): loss=1.8388841909367037\n",
      "Stochastic Gradient Descent(22979): loss=12.128093777450742\n",
      "Stochastic Gradient Descent(22980): loss=0.23440381195809976\n",
      "Stochastic Gradient Descent(22981): loss=0.03637768448657363\n",
      "Stochastic Gradient Descent(22982): loss=1.4459853760991124\n",
      "Stochastic Gradient Descent(22983): loss=3.8316101326822927\n",
      "Stochastic Gradient Descent(22984): loss=0.6415057568196016\n",
      "Stochastic Gradient Descent(22985): loss=0.30477593182084395\n",
      "Stochastic Gradient Descent(22986): loss=1.848124584739563\n",
      "Stochastic Gradient Descent(22987): loss=0.01812862329713534\n",
      "Stochastic Gradient Descent(22988): loss=3.1583311716293765\n",
      "Stochastic Gradient Descent(22989): loss=2.7856459782233305\n",
      "Stochastic Gradient Descent(22990): loss=0.3994434667357539\n",
      "Stochastic Gradient Descent(22991): loss=7.651449296328423\n",
      "Stochastic Gradient Descent(22992): loss=0.4326580212134127\n",
      "Stochastic Gradient Descent(22993): loss=6.062711058670798\n",
      "Stochastic Gradient Descent(22994): loss=6.17784449039349\n",
      "Stochastic Gradient Descent(22995): loss=7.448321337583101\n",
      "Stochastic Gradient Descent(22996): loss=2.833482576002831\n",
      "Stochastic Gradient Descent(22997): loss=6.9541048451900185\n",
      "Stochastic Gradient Descent(22998): loss=0.012093226529781849\n",
      "Stochastic Gradient Descent(22999): loss=0.22187544245539156\n",
      "Stochastic Gradient Descent(23000): loss=0.0704779452548618\n",
      "Stochastic Gradient Descent(23001): loss=0.14243305080087362\n",
      "Stochastic Gradient Descent(23002): loss=5.309600315595446\n",
      "Stochastic Gradient Descent(23003): loss=4.10861726486083\n",
      "Stochastic Gradient Descent(23004): loss=0.6718494945113019\n",
      "Stochastic Gradient Descent(23005): loss=33.28046022105691\n",
      "Stochastic Gradient Descent(23006): loss=0.9634964194433215\n",
      "Stochastic Gradient Descent(23007): loss=21.85485184709768\n",
      "Stochastic Gradient Descent(23008): loss=1.384822158382345\n",
      "Stochastic Gradient Descent(23009): loss=11.804618751204117\n",
      "Stochastic Gradient Descent(23010): loss=1.5809701145359698\n",
      "Stochastic Gradient Descent(23011): loss=4.760427495099281\n",
      "Stochastic Gradient Descent(23012): loss=2.2209153688410415\n",
      "Stochastic Gradient Descent(23013): loss=10.184297090471969\n",
      "Stochastic Gradient Descent(23014): loss=1.4173812990083616\n",
      "Stochastic Gradient Descent(23015): loss=2.5377855409865075\n",
      "Stochastic Gradient Descent(23016): loss=1.0020908372229387\n",
      "Stochastic Gradient Descent(23017): loss=0.23279827186777094\n",
      "Stochastic Gradient Descent(23018): loss=17.64550411100212\n",
      "Stochastic Gradient Descent(23019): loss=8.830001594340553\n",
      "Stochastic Gradient Descent(23020): loss=0.00259603788800299\n",
      "Stochastic Gradient Descent(23021): loss=1.7528160123622147\n",
      "Stochastic Gradient Descent(23022): loss=8.705245579268436\n",
      "Stochastic Gradient Descent(23023): loss=7.1579201100070815\n",
      "Stochastic Gradient Descent(23024): loss=13.123580854972024\n",
      "Stochastic Gradient Descent(23025): loss=60.17273080085249\n",
      "Stochastic Gradient Descent(23026): loss=0.6608677406434678\n",
      "Stochastic Gradient Descent(23027): loss=2.0986651631345654\n",
      "Stochastic Gradient Descent(23028): loss=3.077109849789239\n",
      "Stochastic Gradient Descent(23029): loss=33.12946665704048\n",
      "Stochastic Gradient Descent(23030): loss=3.509464616936324\n",
      "Stochastic Gradient Descent(23031): loss=1.1061790462938574\n",
      "Stochastic Gradient Descent(23032): loss=2.129744843385904\n",
      "Stochastic Gradient Descent(23033): loss=19.581699919370056\n",
      "Stochastic Gradient Descent(23034): loss=0.18681109377719832\n",
      "Stochastic Gradient Descent(23035): loss=2.5979846495883594\n",
      "Stochastic Gradient Descent(23036): loss=1.156302071256769\n",
      "Stochastic Gradient Descent(23037): loss=4.45746054166223\n",
      "Stochastic Gradient Descent(23038): loss=102.70726674301041\n",
      "Stochastic Gradient Descent(23039): loss=2.670086627653701\n",
      "Stochastic Gradient Descent(23040): loss=14.765122863751953\n",
      "Stochastic Gradient Descent(23041): loss=0.9174209507598312\n",
      "Stochastic Gradient Descent(23042): loss=7.804657899393382\n",
      "Stochastic Gradient Descent(23043): loss=0.07130185035492369\n",
      "Stochastic Gradient Descent(23044): loss=0.286102430797182\n",
      "Stochastic Gradient Descent(23045): loss=29.659386860606276\n",
      "Stochastic Gradient Descent(23046): loss=0.19688853383297322\n",
      "Stochastic Gradient Descent(23047): loss=12.334153650275189\n",
      "Stochastic Gradient Descent(23048): loss=26.591949156515515\n",
      "Stochastic Gradient Descent(23049): loss=9.224924628535735\n",
      "Stochastic Gradient Descent(23050): loss=4.530087832831512\n",
      "Stochastic Gradient Descent(23051): loss=25.934833180946015\n",
      "Stochastic Gradient Descent(23052): loss=3.2766238405880155\n",
      "Stochastic Gradient Descent(23053): loss=0.12731711203970153\n",
      "Stochastic Gradient Descent(23054): loss=21.042637845371242\n",
      "Stochastic Gradient Descent(23055): loss=6.2208580053365194\n",
      "Stochastic Gradient Descent(23056): loss=2.535061504853441\n",
      "Stochastic Gradient Descent(23057): loss=0.94210686155912\n",
      "Stochastic Gradient Descent(23058): loss=14.124099945515402\n",
      "Stochastic Gradient Descent(23059): loss=1.8371944005600074\n",
      "Stochastic Gradient Descent(23060): loss=2.995535535331623\n",
      "Stochastic Gradient Descent(23061): loss=0.7506084390173893\n",
      "Stochastic Gradient Descent(23062): loss=2.1671201995872154\n",
      "Stochastic Gradient Descent(23063): loss=6.931179397707608\n",
      "Stochastic Gradient Descent(23064): loss=0.22472518797007635\n",
      "Stochastic Gradient Descent(23065): loss=0.02645446496463188\n",
      "Stochastic Gradient Descent(23066): loss=5.27000649909898\n",
      "Stochastic Gradient Descent(23067): loss=1.0260456560569924\n",
      "Stochastic Gradient Descent(23068): loss=5.181355216288782\n",
      "Stochastic Gradient Descent(23069): loss=0.668717624493613\n",
      "Stochastic Gradient Descent(23070): loss=4.096045607809055\n",
      "Stochastic Gradient Descent(23071): loss=6.857569869235103\n",
      "Stochastic Gradient Descent(23072): loss=0.18307105031681042\n",
      "Stochastic Gradient Descent(23073): loss=5.0077162721545845\n",
      "Stochastic Gradient Descent(23074): loss=8.312870623749594\n",
      "Stochastic Gradient Descent(23075): loss=6.71039394973666\n",
      "Stochastic Gradient Descent(23076): loss=0.37084231548343\n",
      "Stochastic Gradient Descent(23077): loss=0.9165489914365154\n",
      "Stochastic Gradient Descent(23078): loss=1.2373758804250876\n",
      "Stochastic Gradient Descent(23079): loss=6.550909370615342\n",
      "Stochastic Gradient Descent(23080): loss=0.2522874618953075\n",
      "Stochastic Gradient Descent(23081): loss=5.448306135909709\n",
      "Stochastic Gradient Descent(23082): loss=3.2407784381529274\n",
      "Stochastic Gradient Descent(23083): loss=0.07177866315481063\n",
      "Stochastic Gradient Descent(23084): loss=42.98736601722463\n",
      "Stochastic Gradient Descent(23085): loss=1.7270695539093615\n",
      "Stochastic Gradient Descent(23086): loss=2.4328720614715835\n",
      "Stochastic Gradient Descent(23087): loss=0.26785880410566354\n",
      "Stochastic Gradient Descent(23088): loss=5.198077622211496\n",
      "Stochastic Gradient Descent(23089): loss=24.756351816855844\n",
      "Stochastic Gradient Descent(23090): loss=3.797566530007256\n",
      "Stochastic Gradient Descent(23091): loss=1.9541122195809248\n",
      "Stochastic Gradient Descent(23092): loss=0.10243084405403115\n",
      "Stochastic Gradient Descent(23093): loss=2.9669676478304288\n",
      "Stochastic Gradient Descent(23094): loss=1.0674849793426777\n",
      "Stochastic Gradient Descent(23095): loss=0.17836300254609141\n",
      "Stochastic Gradient Descent(23096): loss=11.800687794795477\n",
      "Stochastic Gradient Descent(23097): loss=6.03664985962028\n",
      "Stochastic Gradient Descent(23098): loss=6.241921232274707\n",
      "Stochastic Gradient Descent(23099): loss=0.5782959490931325\n",
      "Stochastic Gradient Descent(23100): loss=0.007840484175842924\n",
      "Stochastic Gradient Descent(23101): loss=4.142564028845492\n",
      "Stochastic Gradient Descent(23102): loss=0.9302853227494873\n",
      "Stochastic Gradient Descent(23103): loss=16.929764011066595\n",
      "Stochastic Gradient Descent(23104): loss=0.672736308808236\n",
      "Stochastic Gradient Descent(23105): loss=3.9482494732674382\n",
      "Stochastic Gradient Descent(23106): loss=24.42178210397649\n",
      "Stochastic Gradient Descent(23107): loss=11.000990766175429\n",
      "Stochastic Gradient Descent(23108): loss=8.556228519723641\n",
      "Stochastic Gradient Descent(23109): loss=0.05994768993653919\n",
      "Stochastic Gradient Descent(23110): loss=0.13830045881385042\n",
      "Stochastic Gradient Descent(23111): loss=1.8510490506385742\n",
      "Stochastic Gradient Descent(23112): loss=12.704983200548298\n",
      "Stochastic Gradient Descent(23113): loss=0.026456890498209547\n",
      "Stochastic Gradient Descent(23114): loss=0.1915092880698588\n",
      "Stochastic Gradient Descent(23115): loss=0.3180068778792211\n",
      "Stochastic Gradient Descent(23116): loss=2.1318690229087918\n",
      "Stochastic Gradient Descent(23117): loss=1.450274186810648\n",
      "Stochastic Gradient Descent(23118): loss=0.6822042792072379\n",
      "Stochastic Gradient Descent(23119): loss=1.8895129407530518\n",
      "Stochastic Gradient Descent(23120): loss=0.01156801777942556\n",
      "Stochastic Gradient Descent(23121): loss=2.78826610418522\n",
      "Stochastic Gradient Descent(23122): loss=3.255696783413551\n",
      "Stochastic Gradient Descent(23123): loss=0.12157798163196312\n",
      "Stochastic Gradient Descent(23124): loss=0.15054096557320204\n",
      "Stochastic Gradient Descent(23125): loss=3.6063548482876824\n",
      "Stochastic Gradient Descent(23126): loss=1.7497021230819556\n",
      "Stochastic Gradient Descent(23127): loss=0.826220895685602\n",
      "Stochastic Gradient Descent(23128): loss=6.065756648075149\n",
      "Stochastic Gradient Descent(23129): loss=0.5554079421678072\n",
      "Stochastic Gradient Descent(23130): loss=5.811003209932991\n",
      "Stochastic Gradient Descent(23131): loss=30.92322893610309\n",
      "Stochastic Gradient Descent(23132): loss=0.8850707915580251\n",
      "Stochastic Gradient Descent(23133): loss=6.782680988851688\n",
      "Stochastic Gradient Descent(23134): loss=29.388241488377385\n",
      "Stochastic Gradient Descent(23135): loss=3.6621429819924307\n",
      "Stochastic Gradient Descent(23136): loss=10.34041469824429\n",
      "Stochastic Gradient Descent(23137): loss=2.3348775412941265\n",
      "Stochastic Gradient Descent(23138): loss=1.038364539548047\n",
      "Stochastic Gradient Descent(23139): loss=9.605209529555873\n",
      "Stochastic Gradient Descent(23140): loss=3.7049170691360405\n",
      "Stochastic Gradient Descent(23141): loss=5.309587787635154\n",
      "Stochastic Gradient Descent(23142): loss=1.1873238373006951\n",
      "Stochastic Gradient Descent(23143): loss=1.6272185258870204\n",
      "Stochastic Gradient Descent(23144): loss=0.09794644746407062\n",
      "Stochastic Gradient Descent(23145): loss=9.754770976535612\n",
      "Stochastic Gradient Descent(23146): loss=10.862893902053639\n",
      "Stochastic Gradient Descent(23147): loss=1.5500818494071273\n",
      "Stochastic Gradient Descent(23148): loss=0.07331801628354469\n",
      "Stochastic Gradient Descent(23149): loss=1.7429478595165702\n",
      "Stochastic Gradient Descent(23150): loss=6.069810935621616\n",
      "Stochastic Gradient Descent(23151): loss=26.934407478139306\n",
      "Stochastic Gradient Descent(23152): loss=4.872587300741261\n",
      "Stochastic Gradient Descent(23153): loss=14.853704730589225\n",
      "Stochastic Gradient Descent(23154): loss=30.96601129905526\n",
      "Stochastic Gradient Descent(23155): loss=0.06602621060058184\n",
      "Stochastic Gradient Descent(23156): loss=17.754117814912345\n",
      "Stochastic Gradient Descent(23157): loss=1.3938838895714647\n",
      "Stochastic Gradient Descent(23158): loss=1.0507605273269558\n",
      "Stochastic Gradient Descent(23159): loss=2.067100291417018\n",
      "Stochastic Gradient Descent(23160): loss=0.37871447389653\n",
      "Stochastic Gradient Descent(23161): loss=2.716522758986271\n",
      "Stochastic Gradient Descent(23162): loss=10.406951623819582\n",
      "Stochastic Gradient Descent(23163): loss=1.0607788332597061\n",
      "Stochastic Gradient Descent(23164): loss=2.0279133844991883\n",
      "Stochastic Gradient Descent(23165): loss=2.589515614258831\n",
      "Stochastic Gradient Descent(23166): loss=14.593784585099385\n",
      "Stochastic Gradient Descent(23167): loss=5.858931563611086\n",
      "Stochastic Gradient Descent(23168): loss=4.031442879168676\n",
      "Stochastic Gradient Descent(23169): loss=14.302135044918998\n",
      "Stochastic Gradient Descent(23170): loss=3.66428254728768\n",
      "Stochastic Gradient Descent(23171): loss=2.687054742299872\n",
      "Stochastic Gradient Descent(23172): loss=4.6203273631319615\n",
      "Stochastic Gradient Descent(23173): loss=5.658907817680597\n",
      "Stochastic Gradient Descent(23174): loss=1.4012307674228937\n",
      "Stochastic Gradient Descent(23175): loss=1.42747225082329\n",
      "Stochastic Gradient Descent(23176): loss=0.03524821182048708\n",
      "Stochastic Gradient Descent(23177): loss=0.7638547831370176\n",
      "Stochastic Gradient Descent(23178): loss=21.99219377862779\n",
      "Stochastic Gradient Descent(23179): loss=0.29173729877020127\n",
      "Stochastic Gradient Descent(23180): loss=0.3298684838095657\n",
      "Stochastic Gradient Descent(23181): loss=2.2308490231179503\n",
      "Stochastic Gradient Descent(23182): loss=1.2616069471403992\n",
      "Stochastic Gradient Descent(23183): loss=0.11111779456745265\n",
      "Stochastic Gradient Descent(23184): loss=0.047141686671969524\n",
      "Stochastic Gradient Descent(23185): loss=4.168612043538412\n",
      "Stochastic Gradient Descent(23186): loss=1.8562562507064795\n",
      "Stochastic Gradient Descent(23187): loss=6.157592048724627\n",
      "Stochastic Gradient Descent(23188): loss=4.2808240012886705\n",
      "Stochastic Gradient Descent(23189): loss=0.655405314488387\n",
      "Stochastic Gradient Descent(23190): loss=1.205459290064623\n",
      "Stochastic Gradient Descent(23191): loss=14.656729450880434\n",
      "Stochastic Gradient Descent(23192): loss=0.26503813312070645\n",
      "Stochastic Gradient Descent(23193): loss=0.3708262458831979\n",
      "Stochastic Gradient Descent(23194): loss=1.291261381203956\n",
      "Stochastic Gradient Descent(23195): loss=0.30029263110467846\n",
      "Stochastic Gradient Descent(23196): loss=1.9390980364162598\n",
      "Stochastic Gradient Descent(23197): loss=12.265285963627058\n",
      "Stochastic Gradient Descent(23198): loss=3.6097734466414946\n",
      "Stochastic Gradient Descent(23199): loss=0.17098712771875468\n",
      "Stochastic Gradient Descent(23200): loss=19.10038520956956\n",
      "Stochastic Gradient Descent(23201): loss=2.897310621733217\n",
      "Stochastic Gradient Descent(23202): loss=3.046755836967575\n",
      "Stochastic Gradient Descent(23203): loss=0.01477284817379578\n",
      "Stochastic Gradient Descent(23204): loss=0.39113105493919437\n",
      "Stochastic Gradient Descent(23205): loss=0.2266639760444113\n",
      "Stochastic Gradient Descent(23206): loss=0.37614879758452113\n",
      "Stochastic Gradient Descent(23207): loss=2.3048348208471396\n",
      "Stochastic Gradient Descent(23208): loss=0.082648129249432\n",
      "Stochastic Gradient Descent(23209): loss=10.871084148900158\n",
      "Stochastic Gradient Descent(23210): loss=37.424178905882115\n",
      "Stochastic Gradient Descent(23211): loss=6.090520577564933\n",
      "Stochastic Gradient Descent(23212): loss=2.083767575314829\n",
      "Stochastic Gradient Descent(23213): loss=7.418377194310003\n",
      "Stochastic Gradient Descent(23214): loss=0.07182626829210707\n",
      "Stochastic Gradient Descent(23215): loss=26.428247929026885\n",
      "Stochastic Gradient Descent(23216): loss=1.815947296973287\n",
      "Stochastic Gradient Descent(23217): loss=2.374490167265232\n",
      "Stochastic Gradient Descent(23218): loss=8.23266195302402\n",
      "Stochastic Gradient Descent(23219): loss=8.503584587386065\n",
      "Stochastic Gradient Descent(23220): loss=2.8691827601107804\n",
      "Stochastic Gradient Descent(23221): loss=1.5884449906246543\n",
      "Stochastic Gradient Descent(23222): loss=0.0007819949730945455\n",
      "Stochastic Gradient Descent(23223): loss=0.39462452068552084\n",
      "Stochastic Gradient Descent(23224): loss=2.066590339721943\n",
      "Stochastic Gradient Descent(23225): loss=3.4217777740317254\n",
      "Stochastic Gradient Descent(23226): loss=1.0931100104002907\n",
      "Stochastic Gradient Descent(23227): loss=6.090906407814985\n",
      "Stochastic Gradient Descent(23228): loss=4.619307648287426\n",
      "Stochastic Gradient Descent(23229): loss=12.160101793690805\n",
      "Stochastic Gradient Descent(23230): loss=0.03663826623767733\n",
      "Stochastic Gradient Descent(23231): loss=4.703807703807241\n",
      "Stochastic Gradient Descent(23232): loss=3.3927612697015905\n",
      "Stochastic Gradient Descent(23233): loss=4.0017907335941825\n",
      "Stochastic Gradient Descent(23234): loss=4.5711626399378025\n",
      "Stochastic Gradient Descent(23235): loss=5.042317455374111\n",
      "Stochastic Gradient Descent(23236): loss=2.370489848435981\n",
      "Stochastic Gradient Descent(23237): loss=0.0014066134928449687\n",
      "Stochastic Gradient Descent(23238): loss=0.06583812991269916\n",
      "Stochastic Gradient Descent(23239): loss=0.8799229317125237\n",
      "Stochastic Gradient Descent(23240): loss=6.532161862937911\n",
      "Stochastic Gradient Descent(23241): loss=0.1469212524787207\n",
      "Stochastic Gradient Descent(23242): loss=1.216232935482427\n",
      "Stochastic Gradient Descent(23243): loss=0.0027390909224736587\n",
      "Stochastic Gradient Descent(23244): loss=4.084956880728056\n",
      "Stochastic Gradient Descent(23245): loss=6.8087150374641805\n",
      "Stochastic Gradient Descent(23246): loss=1.4249054736085431\n",
      "Stochastic Gradient Descent(23247): loss=0.022160544431365243\n",
      "Stochastic Gradient Descent(23248): loss=0.825942452654621\n",
      "Stochastic Gradient Descent(23249): loss=0.5541593548594514\n",
      "Stochastic Gradient Descent(23250): loss=12.279846741511824\n",
      "Stochastic Gradient Descent(23251): loss=0.33757319009737047\n",
      "Stochastic Gradient Descent(23252): loss=0.047921545052310985\n",
      "Stochastic Gradient Descent(23253): loss=0.0012367159128494824\n",
      "Stochastic Gradient Descent(23254): loss=2.425790671399934\n",
      "Stochastic Gradient Descent(23255): loss=4.3292263214861215\n",
      "Stochastic Gradient Descent(23256): loss=18.247929305742\n",
      "Stochastic Gradient Descent(23257): loss=9.425008796419561\n",
      "Stochastic Gradient Descent(23258): loss=2.2383758595277454\n",
      "Stochastic Gradient Descent(23259): loss=16.536097496194177\n",
      "Stochastic Gradient Descent(23260): loss=1.0771216880871604\n",
      "Stochastic Gradient Descent(23261): loss=0.09084364218255614\n",
      "Stochastic Gradient Descent(23262): loss=0.15464742959403527\n",
      "Stochastic Gradient Descent(23263): loss=0.3079101087358732\n",
      "Stochastic Gradient Descent(23264): loss=0.4235355981559353\n",
      "Stochastic Gradient Descent(23265): loss=2.2435663638533634\n",
      "Stochastic Gradient Descent(23266): loss=12.746531293590184\n",
      "Stochastic Gradient Descent(23267): loss=0.027289014728488204\n",
      "Stochastic Gradient Descent(23268): loss=0.15838836810801915\n",
      "Stochastic Gradient Descent(23269): loss=0.9406396530657994\n",
      "Stochastic Gradient Descent(23270): loss=8.246437551103488\n",
      "Stochastic Gradient Descent(23271): loss=0.2872449764346402\n",
      "Stochastic Gradient Descent(23272): loss=1.3170735422901343\n",
      "Stochastic Gradient Descent(23273): loss=5.3433320189214895\n",
      "Stochastic Gradient Descent(23274): loss=0.047351855407229804\n",
      "Stochastic Gradient Descent(23275): loss=2.4433228533965026\n",
      "Stochastic Gradient Descent(23276): loss=1.7756840697322118\n",
      "Stochastic Gradient Descent(23277): loss=0.0034202761905979813\n",
      "Stochastic Gradient Descent(23278): loss=8.102790496088863\n",
      "Stochastic Gradient Descent(23279): loss=6.763373452410545\n",
      "Stochastic Gradient Descent(23280): loss=7.051300041500614\n",
      "Stochastic Gradient Descent(23281): loss=11.647174795057579\n",
      "Stochastic Gradient Descent(23282): loss=7.933964923059463\n",
      "Stochastic Gradient Descent(23283): loss=4.770612754463251\n",
      "Stochastic Gradient Descent(23284): loss=8.1821833393935\n",
      "Stochastic Gradient Descent(23285): loss=4.853637606936774\n",
      "Stochastic Gradient Descent(23286): loss=0.1969832832624438\n",
      "Stochastic Gradient Descent(23287): loss=1.467910639267672\n",
      "Stochastic Gradient Descent(23288): loss=0.004953079370300225\n",
      "Stochastic Gradient Descent(23289): loss=0.30485842225496934\n",
      "Stochastic Gradient Descent(23290): loss=5.667561878249024\n",
      "Stochastic Gradient Descent(23291): loss=0.5557122875378401\n",
      "Stochastic Gradient Descent(23292): loss=3.410058853321146\n",
      "Stochastic Gradient Descent(23293): loss=24.014057210938017\n",
      "Stochastic Gradient Descent(23294): loss=0.064800597859403\n",
      "Stochastic Gradient Descent(23295): loss=3.246596292665836\n",
      "Stochastic Gradient Descent(23296): loss=17.25349549601722\n",
      "Stochastic Gradient Descent(23297): loss=10.570300052054941\n",
      "Stochastic Gradient Descent(23298): loss=0.139397698030846\n",
      "Stochastic Gradient Descent(23299): loss=41.13213044104225\n",
      "Stochastic Gradient Descent(23300): loss=9.911225981695166\n",
      "Stochastic Gradient Descent(23301): loss=0.024962687544563027\n",
      "Stochastic Gradient Descent(23302): loss=0.012311167476384088\n",
      "Stochastic Gradient Descent(23303): loss=15.73642301624532\n",
      "Stochastic Gradient Descent(23304): loss=3.9955348698527025\n",
      "Stochastic Gradient Descent(23305): loss=2.279533198683132\n",
      "Stochastic Gradient Descent(23306): loss=3.6241431856879855\n",
      "Stochastic Gradient Descent(23307): loss=32.98384160654092\n",
      "Stochastic Gradient Descent(23308): loss=7.361557811043047\n",
      "Stochastic Gradient Descent(23309): loss=4.8317818963208765\n",
      "Stochastic Gradient Descent(23310): loss=10.655719969928262\n",
      "Stochastic Gradient Descent(23311): loss=1.4891943389436288\n",
      "Stochastic Gradient Descent(23312): loss=0.0009227832758513735\n",
      "Stochastic Gradient Descent(23313): loss=0.4353447563714641\n",
      "Stochastic Gradient Descent(23314): loss=3.0584268174416516\n",
      "Stochastic Gradient Descent(23315): loss=3.940538235634957\n",
      "Stochastic Gradient Descent(23316): loss=3.802329555241062\n",
      "Stochastic Gradient Descent(23317): loss=9.393161597398256\n",
      "Stochastic Gradient Descent(23318): loss=14.554500707783967\n",
      "Stochastic Gradient Descent(23319): loss=0.7104711238526921\n",
      "Stochastic Gradient Descent(23320): loss=0.1826193935988503\n",
      "Stochastic Gradient Descent(23321): loss=3.06224444609864\n",
      "Stochastic Gradient Descent(23322): loss=0.20558342113887557\n",
      "Stochastic Gradient Descent(23323): loss=1.3866986481484358\n",
      "Stochastic Gradient Descent(23324): loss=3.2905266904111157\n",
      "Stochastic Gradient Descent(23325): loss=0.022917604512503784\n",
      "Stochastic Gradient Descent(23326): loss=7.414775356503187\n",
      "Stochastic Gradient Descent(23327): loss=1.991294657729738\n",
      "Stochastic Gradient Descent(23328): loss=8.732857026624048\n",
      "Stochastic Gradient Descent(23329): loss=0.0314248885643282\n",
      "Stochastic Gradient Descent(23330): loss=1.680057880867707\n",
      "Stochastic Gradient Descent(23331): loss=14.116368553868536\n",
      "Stochastic Gradient Descent(23332): loss=20.62286066285813\n",
      "Stochastic Gradient Descent(23333): loss=0.2961676147653361\n",
      "Stochastic Gradient Descent(23334): loss=2.321843399851942\n",
      "Stochastic Gradient Descent(23335): loss=5.535744030028574\n",
      "Stochastic Gradient Descent(23336): loss=0.5517958405261773\n",
      "Stochastic Gradient Descent(23337): loss=0.0016322224335621335\n",
      "Stochastic Gradient Descent(23338): loss=1.142039507857765\n",
      "Stochastic Gradient Descent(23339): loss=1.6801894096898629\n",
      "Stochastic Gradient Descent(23340): loss=0.046813966244393525\n",
      "Stochastic Gradient Descent(23341): loss=3.247514245856252\n",
      "Stochastic Gradient Descent(23342): loss=0.02506289148121382\n",
      "Stochastic Gradient Descent(23343): loss=0.19508392726477541\n",
      "Stochastic Gradient Descent(23344): loss=0.9971757890339185\n",
      "Stochastic Gradient Descent(23345): loss=0.31341831182529056\n",
      "Stochastic Gradient Descent(23346): loss=3.8020053824867883\n",
      "Stochastic Gradient Descent(23347): loss=2.6019339291593866\n",
      "Stochastic Gradient Descent(23348): loss=1.4689643330052091\n",
      "Stochastic Gradient Descent(23349): loss=3.4468644261556123\n",
      "Stochastic Gradient Descent(23350): loss=1.821834031056571\n",
      "Stochastic Gradient Descent(23351): loss=4.163985443894184\n",
      "Stochastic Gradient Descent(23352): loss=1.519445613062917\n",
      "Stochastic Gradient Descent(23353): loss=2.5676975548078573\n",
      "Stochastic Gradient Descent(23354): loss=6.338805318350599e-07\n",
      "Stochastic Gradient Descent(23355): loss=0.20351960257728363\n",
      "Stochastic Gradient Descent(23356): loss=1.931713045752857\n",
      "Stochastic Gradient Descent(23357): loss=7.0490880346231455\n",
      "Stochastic Gradient Descent(23358): loss=0.3356795849263584\n",
      "Stochastic Gradient Descent(23359): loss=0.12119066537094322\n",
      "Stochastic Gradient Descent(23360): loss=14.358421890477548\n",
      "Stochastic Gradient Descent(23361): loss=27.02824873537064\n",
      "Stochastic Gradient Descent(23362): loss=15.811080447476469\n",
      "Stochastic Gradient Descent(23363): loss=4.141987398412754\n",
      "Stochastic Gradient Descent(23364): loss=25.0655835721308\n",
      "Stochastic Gradient Descent(23365): loss=11.480158654528223\n",
      "Stochastic Gradient Descent(23366): loss=11.346204810474964\n",
      "Stochastic Gradient Descent(23367): loss=2.932557272584598\n",
      "Stochastic Gradient Descent(23368): loss=0.19706585388972556\n",
      "Stochastic Gradient Descent(23369): loss=0.35262997345630537\n",
      "Stochastic Gradient Descent(23370): loss=0.13410440576402988\n",
      "Stochastic Gradient Descent(23371): loss=3.3274875348776383\n",
      "Stochastic Gradient Descent(23372): loss=20.423138222629305\n",
      "Stochastic Gradient Descent(23373): loss=9.835627236150984\n",
      "Stochastic Gradient Descent(23374): loss=4.685449194743124\n",
      "Stochastic Gradient Descent(23375): loss=11.400946757229796\n",
      "Stochastic Gradient Descent(23376): loss=5.2107943807865595\n",
      "Stochastic Gradient Descent(23377): loss=0.43851967228386624\n",
      "Stochastic Gradient Descent(23378): loss=3.1946992050003287\n",
      "Stochastic Gradient Descent(23379): loss=37.69195939319871\n",
      "Stochastic Gradient Descent(23380): loss=2.7626652361460335\n",
      "Stochastic Gradient Descent(23381): loss=0.0699011776054481\n",
      "Stochastic Gradient Descent(23382): loss=2.1669535985032415\n",
      "Stochastic Gradient Descent(23383): loss=0.12204452294713367\n",
      "Stochastic Gradient Descent(23384): loss=2.331254249597664\n",
      "Stochastic Gradient Descent(23385): loss=0.26509431828679675\n",
      "Stochastic Gradient Descent(23386): loss=0.4493589510420285\n",
      "Stochastic Gradient Descent(23387): loss=3.6988843626378345\n",
      "Stochastic Gradient Descent(23388): loss=0.5316524017500054\n",
      "Stochastic Gradient Descent(23389): loss=2.384325223823871\n",
      "Stochastic Gradient Descent(23390): loss=0.15202377087222094\n",
      "Stochastic Gradient Descent(23391): loss=2.3211238594164363\n",
      "Stochastic Gradient Descent(23392): loss=1.8095609829396857\n",
      "Stochastic Gradient Descent(23393): loss=9.685547705911722\n",
      "Stochastic Gradient Descent(23394): loss=4.45408367535768\n",
      "Stochastic Gradient Descent(23395): loss=0.009767761786737609\n",
      "Stochastic Gradient Descent(23396): loss=0.5162596322452809\n",
      "Stochastic Gradient Descent(23397): loss=0.603851194991039\n",
      "Stochastic Gradient Descent(23398): loss=3.5634465312680335\n",
      "Stochastic Gradient Descent(23399): loss=0.048346639201369744\n",
      "Stochastic Gradient Descent(23400): loss=0.4053755406578867\n",
      "Stochastic Gradient Descent(23401): loss=4.770388008506983\n",
      "Stochastic Gradient Descent(23402): loss=11.213110038683268\n",
      "Stochastic Gradient Descent(23403): loss=13.419553843207249\n",
      "Stochastic Gradient Descent(23404): loss=8.418225786053526\n",
      "Stochastic Gradient Descent(23405): loss=0.17874941603917383\n",
      "Stochastic Gradient Descent(23406): loss=0.6786762892743385\n",
      "Stochastic Gradient Descent(23407): loss=2.125181166477146\n",
      "Stochastic Gradient Descent(23408): loss=2.075232370008905\n",
      "Stochastic Gradient Descent(23409): loss=8.578369576649758\n",
      "Stochastic Gradient Descent(23410): loss=0.06955280902782197\n",
      "Stochastic Gradient Descent(23411): loss=1.4333600087663956\n",
      "Stochastic Gradient Descent(23412): loss=0.9489874468973253\n",
      "Stochastic Gradient Descent(23413): loss=1.3617606850072035\n",
      "Stochastic Gradient Descent(23414): loss=0.10952367615212305\n",
      "Stochastic Gradient Descent(23415): loss=1.0051950895291946\n",
      "Stochastic Gradient Descent(23416): loss=7.60359402895873\n",
      "Stochastic Gradient Descent(23417): loss=16.321835841665703\n",
      "Stochastic Gradient Descent(23418): loss=3.88319336895825\n",
      "Stochastic Gradient Descent(23419): loss=0.3120998880606493\n",
      "Stochastic Gradient Descent(23420): loss=0.054973248729115264\n",
      "Stochastic Gradient Descent(23421): loss=35.68553486402766\n",
      "Stochastic Gradient Descent(23422): loss=0.005372892221367902\n",
      "Stochastic Gradient Descent(23423): loss=0.10511975282558862\n",
      "Stochastic Gradient Descent(23424): loss=9.230574270776263\n",
      "Stochastic Gradient Descent(23425): loss=3.1363934386387604\n",
      "Stochastic Gradient Descent(23426): loss=17.666625477941142\n",
      "Stochastic Gradient Descent(23427): loss=0.41675116431676773\n",
      "Stochastic Gradient Descent(23428): loss=8.492574939722068\n",
      "Stochastic Gradient Descent(23429): loss=13.568907808855633\n",
      "Stochastic Gradient Descent(23430): loss=20.05763764635374\n",
      "Stochastic Gradient Descent(23431): loss=5.743887750734331\n",
      "Stochastic Gradient Descent(23432): loss=0.12417887040945502\n",
      "Stochastic Gradient Descent(23433): loss=0.02675030047406141\n",
      "Stochastic Gradient Descent(23434): loss=2.649552107991226\n",
      "Stochastic Gradient Descent(23435): loss=0.277853485309361\n",
      "Stochastic Gradient Descent(23436): loss=15.827402697187301\n",
      "Stochastic Gradient Descent(23437): loss=14.027586185028964\n",
      "Stochastic Gradient Descent(23438): loss=0.31297268004255596\n",
      "Stochastic Gradient Descent(23439): loss=0.0059379372897715365\n",
      "Stochastic Gradient Descent(23440): loss=0.5548142954377845\n",
      "Stochastic Gradient Descent(23441): loss=2.0367411287665296\n",
      "Stochastic Gradient Descent(23442): loss=0.09010820670863978\n",
      "Stochastic Gradient Descent(23443): loss=1.7629681527849976\n",
      "Stochastic Gradient Descent(23444): loss=7.422238451271178\n",
      "Stochastic Gradient Descent(23445): loss=8.974870152488947\n",
      "Stochastic Gradient Descent(23446): loss=0.7494133763686776\n",
      "Stochastic Gradient Descent(23447): loss=4.740519644823141\n",
      "Stochastic Gradient Descent(23448): loss=0.09758628530935164\n",
      "Stochastic Gradient Descent(23449): loss=4.562927232928812\n",
      "Stochastic Gradient Descent(23450): loss=0.05574208570994775\n",
      "Stochastic Gradient Descent(23451): loss=2.4232098419769255\n",
      "Stochastic Gradient Descent(23452): loss=0.014342276430443826\n",
      "Stochastic Gradient Descent(23453): loss=1.070197699435282\n",
      "Stochastic Gradient Descent(23454): loss=0.3685285949996027\n",
      "Stochastic Gradient Descent(23455): loss=0.44001598361866423\n",
      "Stochastic Gradient Descent(23456): loss=0.12165287519101996\n",
      "Stochastic Gradient Descent(23457): loss=6.111193090515864\n",
      "Stochastic Gradient Descent(23458): loss=0.07344537418506823\n",
      "Stochastic Gradient Descent(23459): loss=1.062587851439812\n",
      "Stochastic Gradient Descent(23460): loss=0.5043327821764214\n",
      "Stochastic Gradient Descent(23461): loss=13.471176405608547\n",
      "Stochastic Gradient Descent(23462): loss=5.6983230978620005\n",
      "Stochastic Gradient Descent(23463): loss=0.09275341816727743\n",
      "Stochastic Gradient Descent(23464): loss=17.049587272438675\n",
      "Stochastic Gradient Descent(23465): loss=0.31695005467261456\n",
      "Stochastic Gradient Descent(23466): loss=2.048718963562135\n",
      "Stochastic Gradient Descent(23467): loss=17.22396104933492\n",
      "Stochastic Gradient Descent(23468): loss=0.11606564360783503\n",
      "Stochastic Gradient Descent(23469): loss=6.984679550631472\n",
      "Stochastic Gradient Descent(23470): loss=0.6482823127251979\n",
      "Stochastic Gradient Descent(23471): loss=0.08345145827621328\n",
      "Stochastic Gradient Descent(23472): loss=4.6207291742338406\n",
      "Stochastic Gradient Descent(23473): loss=0.7753928795880667\n",
      "Stochastic Gradient Descent(23474): loss=0.005874052593401374\n",
      "Stochastic Gradient Descent(23475): loss=0.3977730019986177\n",
      "Stochastic Gradient Descent(23476): loss=7.003504257081029\n",
      "Stochastic Gradient Descent(23477): loss=1.5546979115261903\n",
      "Stochastic Gradient Descent(23478): loss=2.462564646231706\n",
      "Stochastic Gradient Descent(23479): loss=0.0013835637857244054\n",
      "Stochastic Gradient Descent(23480): loss=0.1525905412605594\n",
      "Stochastic Gradient Descent(23481): loss=0.1651119511826637\n",
      "Stochastic Gradient Descent(23482): loss=16.097534932813076\n",
      "Stochastic Gradient Descent(23483): loss=2.418200565485221\n",
      "Stochastic Gradient Descent(23484): loss=2.9280336384004326\n",
      "Stochastic Gradient Descent(23485): loss=0.08027702027150077\n",
      "Stochastic Gradient Descent(23486): loss=3.0628238738977847\n",
      "Stochastic Gradient Descent(23487): loss=33.06962940171053\n",
      "Stochastic Gradient Descent(23488): loss=0.34839099545818725\n",
      "Stochastic Gradient Descent(23489): loss=0.798300778699978\n",
      "Stochastic Gradient Descent(23490): loss=2.907610162955484\n",
      "Stochastic Gradient Descent(23491): loss=5.38170129035049\n",
      "Stochastic Gradient Descent(23492): loss=9.066839915819665\n",
      "Stochastic Gradient Descent(23493): loss=0.8571534192416802\n",
      "Stochastic Gradient Descent(23494): loss=11.923132062505061\n",
      "Stochastic Gradient Descent(23495): loss=0.010888035923630135\n",
      "Stochastic Gradient Descent(23496): loss=2.139745895072143\n",
      "Stochastic Gradient Descent(23497): loss=0.006116291048658062\n",
      "Stochastic Gradient Descent(23498): loss=7.561559348647865\n",
      "Stochastic Gradient Descent(23499): loss=1.4880578902739654\n",
      "Stochastic Gradient Descent(23500): loss=3.876265037812198\n",
      "Stochastic Gradient Descent(23501): loss=1.7119136748272021\n",
      "Stochastic Gradient Descent(23502): loss=1.213820716507556\n",
      "Stochastic Gradient Descent(23503): loss=5.696409166632945\n",
      "Stochastic Gradient Descent(23504): loss=0.8743494779380881\n",
      "Stochastic Gradient Descent(23505): loss=12.452258460703687\n",
      "Stochastic Gradient Descent(23506): loss=0.8432520333092512\n",
      "Stochastic Gradient Descent(23507): loss=2.0885848678302352\n",
      "Stochastic Gradient Descent(23508): loss=0.8033768006941194\n",
      "Stochastic Gradient Descent(23509): loss=1.9549322711520238\n",
      "Stochastic Gradient Descent(23510): loss=5.145271140944132\n",
      "Stochastic Gradient Descent(23511): loss=0.014423390285407147\n",
      "Stochastic Gradient Descent(23512): loss=5.742211956865492\n",
      "Stochastic Gradient Descent(23513): loss=0.3044698323595773\n",
      "Stochastic Gradient Descent(23514): loss=8.320535490327284\n",
      "Stochastic Gradient Descent(23515): loss=1.8982996850359042\n",
      "Stochastic Gradient Descent(23516): loss=1.3236217847491865\n",
      "Stochastic Gradient Descent(23517): loss=1.3298856005516417\n",
      "Stochastic Gradient Descent(23518): loss=2.2660602883873375\n",
      "Stochastic Gradient Descent(23519): loss=22.40176448340334\n",
      "Stochastic Gradient Descent(23520): loss=13.694952177648739\n",
      "Stochastic Gradient Descent(23521): loss=1.0788525414949834\n",
      "Stochastic Gradient Descent(23522): loss=4.6739843470019435\n",
      "Stochastic Gradient Descent(23523): loss=0.1451906035647206\n",
      "Stochastic Gradient Descent(23524): loss=1.7550098334954334\n",
      "Stochastic Gradient Descent(23525): loss=32.545277021659125\n",
      "Stochastic Gradient Descent(23526): loss=2.9619875139354694\n",
      "Stochastic Gradient Descent(23527): loss=0.7187020296564915\n",
      "Stochastic Gradient Descent(23528): loss=0.30701029598049645\n",
      "Stochastic Gradient Descent(23529): loss=8.940625621135197\n",
      "Stochastic Gradient Descent(23530): loss=9.388795411931069\n",
      "Stochastic Gradient Descent(23531): loss=0.6576139420001151\n",
      "Stochastic Gradient Descent(23532): loss=0.4069735080473604\n",
      "Stochastic Gradient Descent(23533): loss=1.7871728937602671\n",
      "Stochastic Gradient Descent(23534): loss=2.8544937471114604\n",
      "Stochastic Gradient Descent(23535): loss=6.153512906906986\n",
      "Stochastic Gradient Descent(23536): loss=0.9461973651621445\n",
      "Stochastic Gradient Descent(23537): loss=0.1270982485355201\n",
      "Stochastic Gradient Descent(23538): loss=1.5955531296045302\n",
      "Stochastic Gradient Descent(23539): loss=0.012857550660217557\n",
      "Stochastic Gradient Descent(23540): loss=1.2086973829590881\n",
      "Stochastic Gradient Descent(23541): loss=0.17977270683821706\n",
      "Stochastic Gradient Descent(23542): loss=20.391517912639635\n",
      "Stochastic Gradient Descent(23543): loss=20.039439630321333\n",
      "Stochastic Gradient Descent(23544): loss=0.2837882840887489\n",
      "Stochastic Gradient Descent(23545): loss=1.0574577379636392\n",
      "Stochastic Gradient Descent(23546): loss=1.1047257550725638\n",
      "Stochastic Gradient Descent(23547): loss=1.4176844203493681\n",
      "Stochastic Gradient Descent(23548): loss=1.519017619430042\n",
      "Stochastic Gradient Descent(23549): loss=0.8108772430933315\n",
      "Stochastic Gradient Descent(23550): loss=1.072387110746655\n",
      "Stochastic Gradient Descent(23551): loss=30.930828753670728\n",
      "Stochastic Gradient Descent(23552): loss=5.25123931137974\n",
      "Stochastic Gradient Descent(23553): loss=8.246004967117898\n",
      "Stochastic Gradient Descent(23554): loss=1.684974581352305\n",
      "Stochastic Gradient Descent(23555): loss=0.00020816974408017634\n",
      "Stochastic Gradient Descent(23556): loss=6.350961087720436\n",
      "Stochastic Gradient Descent(23557): loss=0.11282853114425068\n",
      "Stochastic Gradient Descent(23558): loss=0.6721154739712543\n",
      "Stochastic Gradient Descent(23559): loss=2.6575399569837184\n",
      "Stochastic Gradient Descent(23560): loss=2.169927634133683\n",
      "Stochastic Gradient Descent(23561): loss=0.6795512671398678\n",
      "Stochastic Gradient Descent(23562): loss=6.031865766056845\n",
      "Stochastic Gradient Descent(23563): loss=0.11526314181285403\n",
      "Stochastic Gradient Descent(23564): loss=1.6766088465753457\n",
      "Stochastic Gradient Descent(23565): loss=3.442938634844936\n",
      "Stochastic Gradient Descent(23566): loss=0.5851298209826419\n",
      "Stochastic Gradient Descent(23567): loss=6.136306251244305\n",
      "Stochastic Gradient Descent(23568): loss=0.027749282810819356\n",
      "Stochastic Gradient Descent(23569): loss=1.2161911851960012\n",
      "Stochastic Gradient Descent(23570): loss=4.42164158732005\n",
      "Stochastic Gradient Descent(23571): loss=12.249774065407221\n",
      "Stochastic Gradient Descent(23572): loss=3.2661055951249773\n",
      "Stochastic Gradient Descent(23573): loss=6.712890669712848\n",
      "Stochastic Gradient Descent(23574): loss=6.446797205041327\n",
      "Stochastic Gradient Descent(23575): loss=0.5886298295766105\n",
      "Stochastic Gradient Descent(23576): loss=4.319369080944688\n",
      "Stochastic Gradient Descent(23577): loss=3.025319270868815\n",
      "Stochastic Gradient Descent(23578): loss=26.014735496600352\n",
      "Stochastic Gradient Descent(23579): loss=13.018298009891062\n",
      "Stochastic Gradient Descent(23580): loss=4.077276357966557\n",
      "Stochastic Gradient Descent(23581): loss=3.1258227037821573\n",
      "Stochastic Gradient Descent(23582): loss=0.13466058139147224\n",
      "Stochastic Gradient Descent(23583): loss=2.6216779364992635\n",
      "Stochastic Gradient Descent(23584): loss=0.0026830069507216415\n",
      "Stochastic Gradient Descent(23585): loss=1.2757276185521522\n",
      "Stochastic Gradient Descent(23586): loss=5.3522623728913645\n",
      "Stochastic Gradient Descent(23587): loss=3.0553554546853774\n",
      "Stochastic Gradient Descent(23588): loss=3.1157912675770127\n",
      "Stochastic Gradient Descent(23589): loss=0.02209399335305623\n",
      "Stochastic Gradient Descent(23590): loss=1.1045450644575465\n",
      "Stochastic Gradient Descent(23591): loss=0.7436465981050362\n",
      "Stochastic Gradient Descent(23592): loss=0.5191368596646254\n",
      "Stochastic Gradient Descent(23593): loss=0.9598894267497414\n",
      "Stochastic Gradient Descent(23594): loss=3.542719960665933\n",
      "Stochastic Gradient Descent(23595): loss=9.822072598341196\n",
      "Stochastic Gradient Descent(23596): loss=0.002934811178475613\n",
      "Stochastic Gradient Descent(23597): loss=0.2207650936637227\n",
      "Stochastic Gradient Descent(23598): loss=0.466395581165249\n",
      "Stochastic Gradient Descent(23599): loss=12.604767047970807\n",
      "Stochastic Gradient Descent(23600): loss=0.3154525330092\n",
      "Stochastic Gradient Descent(23601): loss=10.269938860835492\n",
      "Stochastic Gradient Descent(23602): loss=0.34312958507409125\n",
      "Stochastic Gradient Descent(23603): loss=1.6664900900956503\n",
      "Stochastic Gradient Descent(23604): loss=5.397550284577064\n",
      "Stochastic Gradient Descent(23605): loss=3.470882118745961\n",
      "Stochastic Gradient Descent(23606): loss=6.239879068673684\n",
      "Stochastic Gradient Descent(23607): loss=17.859022840472484\n",
      "Stochastic Gradient Descent(23608): loss=9.857815123912347\n",
      "Stochastic Gradient Descent(23609): loss=15.08012754011467\n",
      "Stochastic Gradient Descent(23610): loss=9.101026722427482\n",
      "Stochastic Gradient Descent(23611): loss=0.07778070457497196\n",
      "Stochastic Gradient Descent(23612): loss=6.892084260523814\n",
      "Stochastic Gradient Descent(23613): loss=6.48429805321911\n",
      "Stochastic Gradient Descent(23614): loss=11.45160529949231\n",
      "Stochastic Gradient Descent(23615): loss=3.692372664535676\n",
      "Stochastic Gradient Descent(23616): loss=7.347285295671255\n",
      "Stochastic Gradient Descent(23617): loss=6.61234489036297\n",
      "Stochastic Gradient Descent(23618): loss=0.700168997635528\n",
      "Stochastic Gradient Descent(23619): loss=5.987096323630055\n",
      "Stochastic Gradient Descent(23620): loss=0.9148585159649281\n",
      "Stochastic Gradient Descent(23621): loss=15.942631515181091\n",
      "Stochastic Gradient Descent(23622): loss=5.20862030145802\n",
      "Stochastic Gradient Descent(23623): loss=5.06103827629049\n",
      "Stochastic Gradient Descent(23624): loss=0.38371085703141933\n",
      "Stochastic Gradient Descent(23625): loss=0.15256383939169374\n",
      "Stochastic Gradient Descent(23626): loss=10.191966558609648\n",
      "Stochastic Gradient Descent(23627): loss=2.222105516031226\n",
      "Stochastic Gradient Descent(23628): loss=3.359867203874373\n",
      "Stochastic Gradient Descent(23629): loss=0.07201975282089838\n",
      "Stochastic Gradient Descent(23630): loss=15.899821701339848\n",
      "Stochastic Gradient Descent(23631): loss=12.476130263063922\n",
      "Stochastic Gradient Descent(23632): loss=24.852431472461497\n",
      "Stochastic Gradient Descent(23633): loss=0.008330620165805453\n",
      "Stochastic Gradient Descent(23634): loss=0.6012185661814958\n",
      "Stochastic Gradient Descent(23635): loss=32.528522207576586\n",
      "Stochastic Gradient Descent(23636): loss=3.834044871926542\n",
      "Stochastic Gradient Descent(23637): loss=0.5485244495668532\n",
      "Stochastic Gradient Descent(23638): loss=0.00011714664466749798\n",
      "Stochastic Gradient Descent(23639): loss=1.1128487121242485\n",
      "Stochastic Gradient Descent(23640): loss=0.2586183151196571\n",
      "Stochastic Gradient Descent(23641): loss=0.05493942607419579\n",
      "Stochastic Gradient Descent(23642): loss=0.24949967603644685\n",
      "Stochastic Gradient Descent(23643): loss=1.8152490834931194\n",
      "Stochastic Gradient Descent(23644): loss=6.278505435200928\n",
      "Stochastic Gradient Descent(23645): loss=1.0674478991664438\n",
      "Stochastic Gradient Descent(23646): loss=0.1601486709081055\n",
      "Stochastic Gradient Descent(23647): loss=21.41043350797681\n",
      "Stochastic Gradient Descent(23648): loss=7.725710942060833\n",
      "Stochastic Gradient Descent(23649): loss=1.2932220482138705\n",
      "Stochastic Gradient Descent(23650): loss=0.11958891083759163\n",
      "Stochastic Gradient Descent(23651): loss=10.148178701492853\n",
      "Stochastic Gradient Descent(23652): loss=2.122631350247231\n",
      "Stochastic Gradient Descent(23653): loss=4.2657511735525055\n",
      "Stochastic Gradient Descent(23654): loss=1.524808448394802\n",
      "Stochastic Gradient Descent(23655): loss=7.737886431270272\n",
      "Stochastic Gradient Descent(23656): loss=10.208396755297983\n",
      "Stochastic Gradient Descent(23657): loss=13.531612288550837\n",
      "Stochastic Gradient Descent(23658): loss=3.3591074731565267\n",
      "Stochastic Gradient Descent(23659): loss=0.006247909313175636\n",
      "Stochastic Gradient Descent(23660): loss=9.821591184006635\n",
      "Stochastic Gradient Descent(23661): loss=5.871481411781141\n",
      "Stochastic Gradient Descent(23662): loss=4.119039189479865\n",
      "Stochastic Gradient Descent(23663): loss=9.052779826163768\n",
      "Stochastic Gradient Descent(23664): loss=37.29081881490685\n",
      "Stochastic Gradient Descent(23665): loss=6.084360730634455\n",
      "Stochastic Gradient Descent(23666): loss=3.0810258914296687\n",
      "Stochastic Gradient Descent(23667): loss=5.281774287874488\n",
      "Stochastic Gradient Descent(23668): loss=2.3713496993178813\n",
      "Stochastic Gradient Descent(23669): loss=0.18600049851668746\n",
      "Stochastic Gradient Descent(23670): loss=2.574028040121579\n",
      "Stochastic Gradient Descent(23671): loss=4.043173362924391\n",
      "Stochastic Gradient Descent(23672): loss=1.8065152733702003\n",
      "Stochastic Gradient Descent(23673): loss=15.054917152831816\n",
      "Stochastic Gradient Descent(23674): loss=1.3108490753645927\n",
      "Stochastic Gradient Descent(23675): loss=0.4067931603169812\n",
      "Stochastic Gradient Descent(23676): loss=18.150500677437353\n",
      "Stochastic Gradient Descent(23677): loss=4.2236055093584035\n",
      "Stochastic Gradient Descent(23678): loss=6.629943367408938\n",
      "Stochastic Gradient Descent(23679): loss=11.530800183338176\n",
      "Stochastic Gradient Descent(23680): loss=1.3095569333424995\n",
      "Stochastic Gradient Descent(23681): loss=10.927951032080367\n",
      "Stochastic Gradient Descent(23682): loss=51.87559975820249\n",
      "Stochastic Gradient Descent(23683): loss=2.591377770981474\n",
      "Stochastic Gradient Descent(23684): loss=0.6322393738546926\n",
      "Stochastic Gradient Descent(23685): loss=1.6997317820389293\n",
      "Stochastic Gradient Descent(23686): loss=3.8325808843837046\n",
      "Stochastic Gradient Descent(23687): loss=0.3086877613106117\n",
      "Stochastic Gradient Descent(23688): loss=14.246216855902771\n",
      "Stochastic Gradient Descent(23689): loss=0.6015782913590051\n",
      "Stochastic Gradient Descent(23690): loss=0.3782039629123216\n",
      "Stochastic Gradient Descent(23691): loss=0.07420670180710483\n",
      "Stochastic Gradient Descent(23692): loss=0.6804356239207974\n",
      "Stochastic Gradient Descent(23693): loss=6.581628457886501\n",
      "Stochastic Gradient Descent(23694): loss=4.112750585589513\n",
      "Stochastic Gradient Descent(23695): loss=0.45485351810789676\n",
      "Stochastic Gradient Descent(23696): loss=0.39332255915106284\n",
      "Stochastic Gradient Descent(23697): loss=9.695363093688066\n",
      "Stochastic Gradient Descent(23698): loss=9.22917339218173\n",
      "Stochastic Gradient Descent(23699): loss=0.8732476688667925\n",
      "Stochastic Gradient Descent(23700): loss=7.904497462020309\n",
      "Stochastic Gradient Descent(23701): loss=0.02196758924475481\n",
      "Stochastic Gradient Descent(23702): loss=5.444088260085658\n",
      "Stochastic Gradient Descent(23703): loss=11.148993933148493\n",
      "Stochastic Gradient Descent(23704): loss=2.008030087464086\n",
      "Stochastic Gradient Descent(23705): loss=0.12667729163596333\n",
      "Stochastic Gradient Descent(23706): loss=0.0022170001387848363\n",
      "Stochastic Gradient Descent(23707): loss=0.9711472443812992\n",
      "Stochastic Gradient Descent(23708): loss=13.362887436056575\n",
      "Stochastic Gradient Descent(23709): loss=0.32499841618800707\n",
      "Stochastic Gradient Descent(23710): loss=0.003907186332138807\n",
      "Stochastic Gradient Descent(23711): loss=0.8716488025312017\n",
      "Stochastic Gradient Descent(23712): loss=1.626504606678406\n",
      "Stochastic Gradient Descent(23713): loss=2.546303381427709\n",
      "Stochastic Gradient Descent(23714): loss=4.495188849199434\n",
      "Stochastic Gradient Descent(23715): loss=6.711493809813895\n",
      "Stochastic Gradient Descent(23716): loss=5.56958221287682\n",
      "Stochastic Gradient Descent(23717): loss=0.003615208477919527\n",
      "Stochastic Gradient Descent(23718): loss=2.7267634336179554\n",
      "Stochastic Gradient Descent(23719): loss=2.2992629488772565\n",
      "Stochastic Gradient Descent(23720): loss=11.450111266392994\n",
      "Stochastic Gradient Descent(23721): loss=3.5188422295913555\n",
      "Stochastic Gradient Descent(23722): loss=0.09927609998106261\n",
      "Stochastic Gradient Descent(23723): loss=3.515961306396686\n",
      "Stochastic Gradient Descent(23724): loss=2.0970912077083668\n",
      "Stochastic Gradient Descent(23725): loss=7.560276235883583\n",
      "Stochastic Gradient Descent(23726): loss=2.0856818202315375\n",
      "Stochastic Gradient Descent(23727): loss=0.006012195595068863\n",
      "Stochastic Gradient Descent(23728): loss=18.89770309012597\n",
      "Stochastic Gradient Descent(23729): loss=1.628230210076285\n",
      "Stochastic Gradient Descent(23730): loss=6.843726819600275\n",
      "Stochastic Gradient Descent(23731): loss=6.862452425294847\n",
      "Stochastic Gradient Descent(23732): loss=1.5961221023788479\n",
      "Stochastic Gradient Descent(23733): loss=0.43789115885728463\n",
      "Stochastic Gradient Descent(23734): loss=0.6787285826723422\n",
      "Stochastic Gradient Descent(23735): loss=3.280892050781758\n",
      "Stochastic Gradient Descent(23736): loss=2.1187887932903253\n",
      "Stochastic Gradient Descent(23737): loss=2.804472437904752\n",
      "Stochastic Gradient Descent(23738): loss=9.687106408711847\n",
      "Stochastic Gradient Descent(23739): loss=0.2560790233447361\n",
      "Stochastic Gradient Descent(23740): loss=2.567384132320546\n",
      "Stochastic Gradient Descent(23741): loss=0.0581883321282115\n",
      "Stochastic Gradient Descent(23742): loss=0.12000250228885365\n",
      "Stochastic Gradient Descent(23743): loss=8.18986439509966\n",
      "Stochastic Gradient Descent(23744): loss=4.624291609365236\n",
      "Stochastic Gradient Descent(23745): loss=2.010432084006401\n",
      "Stochastic Gradient Descent(23746): loss=0.8140012986107124\n",
      "Stochastic Gradient Descent(23747): loss=8.183398599068765\n",
      "Stochastic Gradient Descent(23748): loss=2.8614667404332974\n",
      "Stochastic Gradient Descent(23749): loss=12.200293476521214\n",
      "Stochastic Gradient Descent(23750): loss=6.100927629586124\n",
      "Stochastic Gradient Descent(23751): loss=3.3250534656949453\n",
      "Stochastic Gradient Descent(23752): loss=19.897928054413867\n",
      "Stochastic Gradient Descent(23753): loss=2.216854793376361\n",
      "Stochastic Gradient Descent(23754): loss=9.57674755001348\n",
      "Stochastic Gradient Descent(23755): loss=0.21059559194659913\n",
      "Stochastic Gradient Descent(23756): loss=7.587663362589554\n",
      "Stochastic Gradient Descent(23757): loss=1.3587516835171136\n",
      "Stochastic Gradient Descent(23758): loss=26.20776146437997\n",
      "Stochastic Gradient Descent(23759): loss=0.2587760878321925\n",
      "Stochastic Gradient Descent(23760): loss=14.003424454886424\n",
      "Stochastic Gradient Descent(23761): loss=0.4023320263771957\n",
      "Stochastic Gradient Descent(23762): loss=12.992431055803436\n",
      "Stochastic Gradient Descent(23763): loss=0.4064525874270417\n",
      "Stochastic Gradient Descent(23764): loss=3.961250926840278\n",
      "Stochastic Gradient Descent(23765): loss=11.465843558301628\n",
      "Stochastic Gradient Descent(23766): loss=3.0634780623766122\n",
      "Stochastic Gradient Descent(23767): loss=0.7804801661372304\n",
      "Stochastic Gradient Descent(23768): loss=0.1389596816378068\n",
      "Stochastic Gradient Descent(23769): loss=5.191617858478468\n",
      "Stochastic Gradient Descent(23770): loss=5.91117108696319\n",
      "Stochastic Gradient Descent(23771): loss=0.08280843193814734\n",
      "Stochastic Gradient Descent(23772): loss=0.20141985202467094\n",
      "Stochastic Gradient Descent(23773): loss=0.00045328525462438803\n",
      "Stochastic Gradient Descent(23774): loss=0.42572001075432647\n",
      "Stochastic Gradient Descent(23775): loss=1.8487768474803004\n",
      "Stochastic Gradient Descent(23776): loss=3.337249997982008\n",
      "Stochastic Gradient Descent(23777): loss=21.475105103110206\n",
      "Stochastic Gradient Descent(23778): loss=3.465056820362334\n",
      "Stochastic Gradient Descent(23779): loss=14.386550835678698\n",
      "Stochastic Gradient Descent(23780): loss=4.754629634069904\n",
      "Stochastic Gradient Descent(23781): loss=0.6191944298749144\n",
      "Stochastic Gradient Descent(23782): loss=3.252666654316494\n",
      "Stochastic Gradient Descent(23783): loss=16.78132394012546\n",
      "Stochastic Gradient Descent(23784): loss=0.08851181310995329\n",
      "Stochastic Gradient Descent(23785): loss=4.192921535480156\n",
      "Stochastic Gradient Descent(23786): loss=1.989062432577947\n",
      "Stochastic Gradient Descent(23787): loss=30.724256436839447\n",
      "Stochastic Gradient Descent(23788): loss=5.4254238009201785\n",
      "Stochastic Gradient Descent(23789): loss=0.024262435835159853\n",
      "Stochastic Gradient Descent(23790): loss=2.3529990160303145\n",
      "Stochastic Gradient Descent(23791): loss=0.014745135182236\n",
      "Stochastic Gradient Descent(23792): loss=0.19760149200151705\n",
      "Stochastic Gradient Descent(23793): loss=0.498048586380172\n",
      "Stochastic Gradient Descent(23794): loss=3.902898607665815\n",
      "Stochastic Gradient Descent(23795): loss=2.9812944812316013\n",
      "Stochastic Gradient Descent(23796): loss=0.8485492144230555\n",
      "Stochastic Gradient Descent(23797): loss=4.5484868652857315\n",
      "Stochastic Gradient Descent(23798): loss=9.88093453510293\n",
      "Stochastic Gradient Descent(23799): loss=6.155050037875314\n",
      "Stochastic Gradient Descent(23800): loss=15.767517334970808\n",
      "Stochastic Gradient Descent(23801): loss=2.4374110728525666\n",
      "Stochastic Gradient Descent(23802): loss=1.1552232792929735\n",
      "Stochastic Gradient Descent(23803): loss=0.8566255835985473\n",
      "Stochastic Gradient Descent(23804): loss=6.193079221753201\n",
      "Stochastic Gradient Descent(23805): loss=5.726872000266199\n",
      "Stochastic Gradient Descent(23806): loss=14.587122265213225\n",
      "Stochastic Gradient Descent(23807): loss=10.587347380280555\n",
      "Stochastic Gradient Descent(23808): loss=4.278063606635163\n",
      "Stochastic Gradient Descent(23809): loss=1.568921767722255\n",
      "Stochastic Gradient Descent(23810): loss=0.025510435167434682\n",
      "Stochastic Gradient Descent(23811): loss=15.568879001758296\n",
      "Stochastic Gradient Descent(23812): loss=0.12382969218302832\n",
      "Stochastic Gradient Descent(23813): loss=8.822120284154552\n",
      "Stochastic Gradient Descent(23814): loss=2.0431750956547545\n",
      "Stochastic Gradient Descent(23815): loss=1.1489476351986758\n",
      "Stochastic Gradient Descent(23816): loss=0.11488698493187703\n",
      "Stochastic Gradient Descent(23817): loss=9.642847272930887\n",
      "Stochastic Gradient Descent(23818): loss=4.495685590272138\n",
      "Stochastic Gradient Descent(23819): loss=2.268472701846041\n",
      "Stochastic Gradient Descent(23820): loss=14.563936065656133\n",
      "Stochastic Gradient Descent(23821): loss=3.2926999899364437\n",
      "Stochastic Gradient Descent(23822): loss=3.199567601929153\n",
      "Stochastic Gradient Descent(23823): loss=6.473681580935102\n",
      "Stochastic Gradient Descent(23824): loss=0.64546606391513\n",
      "Stochastic Gradient Descent(23825): loss=5.412544935754154\n",
      "Stochastic Gradient Descent(23826): loss=3.7646466386816586\n",
      "Stochastic Gradient Descent(23827): loss=7.77082520567412\n",
      "Stochastic Gradient Descent(23828): loss=0.6920077745957972\n",
      "Stochastic Gradient Descent(23829): loss=3.1763553369127497\n",
      "Stochastic Gradient Descent(23830): loss=0.7922072211212358\n",
      "Stochastic Gradient Descent(23831): loss=0.09260201096832389\n",
      "Stochastic Gradient Descent(23832): loss=2.391620511497945\n",
      "Stochastic Gradient Descent(23833): loss=16.998924095453578\n",
      "Stochastic Gradient Descent(23834): loss=0.9404152797524012\n",
      "Stochastic Gradient Descent(23835): loss=19.275294793611536\n",
      "Stochastic Gradient Descent(23836): loss=19.594711974600667\n",
      "Stochastic Gradient Descent(23837): loss=20.722334917727025\n",
      "Stochastic Gradient Descent(23838): loss=0.03338665727703013\n",
      "Stochastic Gradient Descent(23839): loss=0.029175261811664768\n",
      "Stochastic Gradient Descent(23840): loss=1.5024236308574936\n",
      "Stochastic Gradient Descent(23841): loss=0.7610784824997497\n",
      "Stochastic Gradient Descent(23842): loss=14.931614454780561\n",
      "Stochastic Gradient Descent(23843): loss=6.105530029588293\n",
      "Stochastic Gradient Descent(23844): loss=0.17449515318198908\n",
      "Stochastic Gradient Descent(23845): loss=0.8552649576328535\n",
      "Stochastic Gradient Descent(23846): loss=5.004133945922275\n",
      "Stochastic Gradient Descent(23847): loss=0.005517384049571822\n",
      "Stochastic Gradient Descent(23848): loss=0.16737567200129053\n",
      "Stochastic Gradient Descent(23849): loss=3.723571156131302\n",
      "Stochastic Gradient Descent(23850): loss=1.4440188443126132\n",
      "Stochastic Gradient Descent(23851): loss=14.185246328791958\n",
      "Stochastic Gradient Descent(23852): loss=9.6571474146752\n",
      "Stochastic Gradient Descent(23853): loss=1.9218742970276257\n",
      "Stochastic Gradient Descent(23854): loss=9.439702850530951\n",
      "Stochastic Gradient Descent(23855): loss=9.386370817202959\n",
      "Stochastic Gradient Descent(23856): loss=26.105145296229423\n",
      "Stochastic Gradient Descent(23857): loss=0.8220258891030862\n",
      "Stochastic Gradient Descent(23858): loss=6.495778896356066\n",
      "Stochastic Gradient Descent(23859): loss=10.214750665932861\n",
      "Stochastic Gradient Descent(23860): loss=7.159010211617996\n",
      "Stochastic Gradient Descent(23861): loss=2.3669332512536823\n",
      "Stochastic Gradient Descent(23862): loss=0.8165753783246773\n",
      "Stochastic Gradient Descent(23863): loss=28.34797804022396\n",
      "Stochastic Gradient Descent(23864): loss=4.624753368811419\n",
      "Stochastic Gradient Descent(23865): loss=0.12798229494288094\n",
      "Stochastic Gradient Descent(23866): loss=3.085072038954652\n",
      "Stochastic Gradient Descent(23867): loss=19.426098574221758\n",
      "Stochastic Gradient Descent(23868): loss=1.208419661689193\n",
      "Stochastic Gradient Descent(23869): loss=1.6327140730249097\n",
      "Stochastic Gradient Descent(23870): loss=0.4466389081599182\n",
      "Stochastic Gradient Descent(23871): loss=7.848994531405859\n",
      "Stochastic Gradient Descent(23872): loss=9.186314226729065\n",
      "Stochastic Gradient Descent(23873): loss=0.00014219086424830123\n",
      "Stochastic Gradient Descent(23874): loss=0.8558042481410155\n",
      "Stochastic Gradient Descent(23875): loss=0.8289786066472774\n",
      "Stochastic Gradient Descent(23876): loss=1.1315467468479494\n",
      "Stochastic Gradient Descent(23877): loss=6.651436004224166\n",
      "Stochastic Gradient Descent(23878): loss=0.9189255564521728\n",
      "Stochastic Gradient Descent(23879): loss=0.04217311926733871\n",
      "Stochastic Gradient Descent(23880): loss=14.47566322060479\n",
      "Stochastic Gradient Descent(23881): loss=9.402676326170543\n",
      "Stochastic Gradient Descent(23882): loss=0.7628218625338615\n",
      "Stochastic Gradient Descent(23883): loss=7.814798323320765\n",
      "Stochastic Gradient Descent(23884): loss=4.2750737362885065\n",
      "Stochastic Gradient Descent(23885): loss=7.301118337334956\n",
      "Stochastic Gradient Descent(23886): loss=9.940115555683503\n",
      "Stochastic Gradient Descent(23887): loss=0.5225646969168418\n",
      "Stochastic Gradient Descent(23888): loss=1.1335788659878725\n",
      "Stochastic Gradient Descent(23889): loss=30.39887924560236\n",
      "Stochastic Gradient Descent(23890): loss=0.1599728358444531\n",
      "Stochastic Gradient Descent(23891): loss=6.8655604788033395\n",
      "Stochastic Gradient Descent(23892): loss=6.584197743565849\n",
      "Stochastic Gradient Descent(23893): loss=1.888995656936498\n",
      "Stochastic Gradient Descent(23894): loss=1.09292211113568\n",
      "Stochastic Gradient Descent(23895): loss=4.431748894936388\n",
      "Stochastic Gradient Descent(23896): loss=7.749000827882691\n",
      "Stochastic Gradient Descent(23897): loss=5.3029041453535815\n",
      "Stochastic Gradient Descent(23898): loss=0.0005257722779534413\n",
      "Stochastic Gradient Descent(23899): loss=24.78544932534327\n",
      "Stochastic Gradient Descent(23900): loss=1.2261559367411814\n",
      "Stochastic Gradient Descent(23901): loss=7.7283884522793755\n",
      "Stochastic Gradient Descent(23902): loss=2.611228646143332\n",
      "Stochastic Gradient Descent(23903): loss=5.647457385075964\n",
      "Stochastic Gradient Descent(23904): loss=0.7309771739056067\n",
      "Stochastic Gradient Descent(23905): loss=2.7637589247883145\n",
      "Stochastic Gradient Descent(23906): loss=3.746791266726277\n",
      "Stochastic Gradient Descent(23907): loss=2.8245989916105114\n",
      "Stochastic Gradient Descent(23908): loss=2.1678358407003113\n",
      "Stochastic Gradient Descent(23909): loss=10.3911345848188\n",
      "Stochastic Gradient Descent(23910): loss=1.062149328420767\n",
      "Stochastic Gradient Descent(23911): loss=0.7165745079514032\n",
      "Stochastic Gradient Descent(23912): loss=0.1536157235516052\n",
      "Stochastic Gradient Descent(23913): loss=0.4441049301367785\n",
      "Stochastic Gradient Descent(23914): loss=10.568211277563243\n",
      "Stochastic Gradient Descent(23915): loss=48.544298088563174\n",
      "Stochastic Gradient Descent(23916): loss=6.891626928655526\n",
      "Stochastic Gradient Descent(23917): loss=23.54725725538729\n",
      "Stochastic Gradient Descent(23918): loss=50.78763678133286\n",
      "Stochastic Gradient Descent(23919): loss=0.18298739599438157\n",
      "Stochastic Gradient Descent(23920): loss=1.3761201292928744\n",
      "Stochastic Gradient Descent(23921): loss=0.003273517691826161\n",
      "Stochastic Gradient Descent(23922): loss=0.6914940633164882\n",
      "Stochastic Gradient Descent(23923): loss=5.478820509772764\n",
      "Stochastic Gradient Descent(23924): loss=0.8158933778047467\n",
      "Stochastic Gradient Descent(23925): loss=30.870921084120866\n",
      "Stochastic Gradient Descent(23926): loss=5.229557784503541\n",
      "Stochastic Gradient Descent(23927): loss=0.22173112350970958\n",
      "Stochastic Gradient Descent(23928): loss=1.5607574435195748\n",
      "Stochastic Gradient Descent(23929): loss=4.550284001457772\n",
      "Stochastic Gradient Descent(23930): loss=0.12121719483371003\n",
      "Stochastic Gradient Descent(23931): loss=1.5223256097360571\n",
      "Stochastic Gradient Descent(23932): loss=3.3907879119173145\n",
      "Stochastic Gradient Descent(23933): loss=0.38017285396324446\n",
      "Stochastic Gradient Descent(23934): loss=2.846685065232228\n",
      "Stochastic Gradient Descent(23935): loss=19.8839334117811\n",
      "Stochastic Gradient Descent(23936): loss=0.7201082258983247\n",
      "Stochastic Gradient Descent(23937): loss=3.1077796295334776\n",
      "Stochastic Gradient Descent(23938): loss=1.215984018361257\n",
      "Stochastic Gradient Descent(23939): loss=0.10224150076021396\n",
      "Stochastic Gradient Descent(23940): loss=1.9252543451663664\n",
      "Stochastic Gradient Descent(23941): loss=2.9966386251315034\n",
      "Stochastic Gradient Descent(23942): loss=0.7568371071557856\n",
      "Stochastic Gradient Descent(23943): loss=1.6374367992458594\n",
      "Stochastic Gradient Descent(23944): loss=16.706959657200183\n",
      "Stochastic Gradient Descent(23945): loss=15.549086603027146\n",
      "Stochastic Gradient Descent(23946): loss=0.2951666274722609\n",
      "Stochastic Gradient Descent(23947): loss=23.93293630256153\n",
      "Stochastic Gradient Descent(23948): loss=6.476865565116488\n",
      "Stochastic Gradient Descent(23949): loss=3.636454467029645\n",
      "Stochastic Gradient Descent(23950): loss=11.235806973881168\n",
      "Stochastic Gradient Descent(23951): loss=12.273835104422153\n",
      "Stochastic Gradient Descent(23952): loss=4.9898504772143975\n",
      "Stochastic Gradient Descent(23953): loss=7.413169549267056\n",
      "Stochastic Gradient Descent(23954): loss=3.258431921872855\n",
      "Stochastic Gradient Descent(23955): loss=0.8496561993001776\n",
      "Stochastic Gradient Descent(23956): loss=0.14026442228314967\n",
      "Stochastic Gradient Descent(23957): loss=3.4816723039728874\n",
      "Stochastic Gradient Descent(23958): loss=0.6797562682306699\n",
      "Stochastic Gradient Descent(23959): loss=2.1984275242248863\n",
      "Stochastic Gradient Descent(23960): loss=0.17807026902415915\n",
      "Stochastic Gradient Descent(23961): loss=23.018767843207446\n",
      "Stochastic Gradient Descent(23962): loss=3.5819435670061273\n",
      "Stochastic Gradient Descent(23963): loss=0.06263609580604798\n",
      "Stochastic Gradient Descent(23964): loss=11.345603547668773\n",
      "Stochastic Gradient Descent(23965): loss=1.7178537249654708\n",
      "Stochastic Gradient Descent(23966): loss=1.6409513171236507\n",
      "Stochastic Gradient Descent(23967): loss=6.036389270292528\n",
      "Stochastic Gradient Descent(23968): loss=0.6903134222551948\n",
      "Stochastic Gradient Descent(23969): loss=4.225206899072845\n",
      "Stochastic Gradient Descent(23970): loss=7.654393967446026\n",
      "Stochastic Gradient Descent(23971): loss=11.497505172331437\n",
      "Stochastic Gradient Descent(23972): loss=4.5872529885866715\n",
      "Stochastic Gradient Descent(23973): loss=1.1033854797910343\n",
      "Stochastic Gradient Descent(23974): loss=3.1131030309435395\n",
      "Stochastic Gradient Descent(23975): loss=0.051235651637891115\n",
      "Stochastic Gradient Descent(23976): loss=5.545940643492368\n",
      "Stochastic Gradient Descent(23977): loss=0.4226105407577536\n",
      "Stochastic Gradient Descent(23978): loss=2.2062650913482336\n",
      "Stochastic Gradient Descent(23979): loss=4.744691495451212e-07\n",
      "Stochastic Gradient Descent(23980): loss=0.29710239364604046\n",
      "Stochastic Gradient Descent(23981): loss=0.0008837959418157729\n",
      "Stochastic Gradient Descent(23982): loss=1.1265259949363593\n",
      "Stochastic Gradient Descent(23983): loss=0.1425881832062804\n",
      "Stochastic Gradient Descent(23984): loss=0.3440413424321437\n",
      "Stochastic Gradient Descent(23985): loss=0.39571705932167606\n",
      "Stochastic Gradient Descent(23986): loss=4.636486747548174\n",
      "Stochastic Gradient Descent(23987): loss=1.2033857658200615\n",
      "Stochastic Gradient Descent(23988): loss=0.25663855283309484\n",
      "Stochastic Gradient Descent(23989): loss=0.7845666803403142\n",
      "Stochastic Gradient Descent(23990): loss=4.7709105799911375\n",
      "Stochastic Gradient Descent(23991): loss=32.039269633323585\n",
      "Stochastic Gradient Descent(23992): loss=0.4747038623713521\n",
      "Stochastic Gradient Descent(23993): loss=3.689203962795668\n",
      "Stochastic Gradient Descent(23994): loss=0.005113448016301995\n",
      "Stochastic Gradient Descent(23995): loss=0.8868679878379522\n",
      "Stochastic Gradient Descent(23996): loss=1.5515354006420297\n",
      "Stochastic Gradient Descent(23997): loss=9.65752181358664\n",
      "Stochastic Gradient Descent(23998): loss=7.074047534622809\n",
      "Stochastic Gradient Descent(23999): loss=0.18160736558912802\n",
      "Stochastic Gradient Descent(24000): loss=0.36333265373019846\n",
      "Stochastic Gradient Descent(24001): loss=3.8457332637959865\n",
      "Stochastic Gradient Descent(24002): loss=0.8036469656565954\n",
      "Stochastic Gradient Descent(24003): loss=1.2706551836118603\n",
      "Stochastic Gradient Descent(24004): loss=9.567282197119638\n",
      "Stochastic Gradient Descent(24005): loss=0.866915315406415\n",
      "Stochastic Gradient Descent(24006): loss=0.7955217066634337\n",
      "Stochastic Gradient Descent(24007): loss=0.38032948212402523\n",
      "Stochastic Gradient Descent(24008): loss=4.778068718630333\n",
      "Stochastic Gradient Descent(24009): loss=6.411130955514825\n",
      "Stochastic Gradient Descent(24010): loss=7.735799422171131\n",
      "Stochastic Gradient Descent(24011): loss=1.3060693293468737\n",
      "Stochastic Gradient Descent(24012): loss=3.024506731320666\n",
      "Stochastic Gradient Descent(24013): loss=1.024502959154639\n",
      "Stochastic Gradient Descent(24014): loss=0.0048235125254954825\n",
      "Stochastic Gradient Descent(24015): loss=4.9173385941954\n",
      "Stochastic Gradient Descent(24016): loss=3.2213168376395465\n",
      "Stochastic Gradient Descent(24017): loss=0.646017366222393\n",
      "Stochastic Gradient Descent(24018): loss=0.7117175664935097\n",
      "Stochastic Gradient Descent(24019): loss=4.189407828977672\n",
      "Stochastic Gradient Descent(24020): loss=2.8152433975020674\n",
      "Stochastic Gradient Descent(24021): loss=0.4585567619908361\n",
      "Stochastic Gradient Descent(24022): loss=2.3974564701034184\n",
      "Stochastic Gradient Descent(24023): loss=0.023555844670249605\n",
      "Stochastic Gradient Descent(24024): loss=9.21592797192988\n",
      "Stochastic Gradient Descent(24025): loss=13.586423772320135\n",
      "Stochastic Gradient Descent(24026): loss=0.8492287983496736\n",
      "Stochastic Gradient Descent(24027): loss=0.23238610989823885\n",
      "Stochastic Gradient Descent(24028): loss=11.710264544007403\n",
      "Stochastic Gradient Descent(24029): loss=0.8384067805248976\n",
      "Stochastic Gradient Descent(24030): loss=3.553027432934484\n",
      "Stochastic Gradient Descent(24031): loss=0.3172705110866749\n",
      "Stochastic Gradient Descent(24032): loss=1.2079167118237608\n",
      "Stochastic Gradient Descent(24033): loss=13.472323792216356\n",
      "Stochastic Gradient Descent(24034): loss=7.157326693798584\n",
      "Stochastic Gradient Descent(24035): loss=2.9979824717091397\n",
      "Stochastic Gradient Descent(24036): loss=1.8626966800351554\n",
      "Stochastic Gradient Descent(24037): loss=1.065728669192183\n",
      "Stochastic Gradient Descent(24038): loss=6.490967037409692\n",
      "Stochastic Gradient Descent(24039): loss=4.577111712101457\n",
      "Stochastic Gradient Descent(24040): loss=0.42302110261044995\n",
      "Stochastic Gradient Descent(24041): loss=0.011444525364368676\n",
      "Stochastic Gradient Descent(24042): loss=0.03969759277025444\n",
      "Stochastic Gradient Descent(24043): loss=2.0211323840022044\n",
      "Stochastic Gradient Descent(24044): loss=0.4143988111724406\n",
      "Stochastic Gradient Descent(24045): loss=0.55115255006256\n",
      "Stochastic Gradient Descent(24046): loss=1.1437056416341431\n",
      "Stochastic Gradient Descent(24047): loss=1.6414090275036664\n",
      "Stochastic Gradient Descent(24048): loss=5.611459858941512\n",
      "Stochastic Gradient Descent(24049): loss=0.31803406779062554\n",
      "Stochastic Gradient Descent(24050): loss=0.034643385773688785\n",
      "Stochastic Gradient Descent(24051): loss=0.8083412289562766\n",
      "Stochastic Gradient Descent(24052): loss=6.980830910697184\n",
      "Stochastic Gradient Descent(24053): loss=19.662074624801136\n",
      "Stochastic Gradient Descent(24054): loss=6.869732406010744\n",
      "Stochastic Gradient Descent(24055): loss=10.160740049146412\n",
      "Stochastic Gradient Descent(24056): loss=13.768562612857716\n",
      "Stochastic Gradient Descent(24057): loss=8.276018898824782\n",
      "Stochastic Gradient Descent(24058): loss=3.9404709165026945\n",
      "Stochastic Gradient Descent(24059): loss=0.6640522604140465\n",
      "Stochastic Gradient Descent(24060): loss=19.5027016910254\n",
      "Stochastic Gradient Descent(24061): loss=6.886169742665665\n",
      "Stochastic Gradient Descent(24062): loss=2.8388376447072505\n",
      "Stochastic Gradient Descent(24063): loss=5.688029969638798\n",
      "Stochastic Gradient Descent(24064): loss=30.532477352746817\n",
      "Stochastic Gradient Descent(24065): loss=8.362725567443622\n",
      "Stochastic Gradient Descent(24066): loss=0.2117535947651968\n",
      "Stochastic Gradient Descent(24067): loss=0.0965139352685581\n",
      "Stochastic Gradient Descent(24068): loss=2.19408682352044\n",
      "Stochastic Gradient Descent(24069): loss=6.4693198813179045\n",
      "Stochastic Gradient Descent(24070): loss=1.19277398174747\n",
      "Stochastic Gradient Descent(24071): loss=5.583864759221329\n",
      "Stochastic Gradient Descent(24072): loss=0.7625302058959549\n",
      "Stochastic Gradient Descent(24073): loss=3.539729620274577\n",
      "Stochastic Gradient Descent(24074): loss=0.0035718817556795106\n",
      "Stochastic Gradient Descent(24075): loss=1.1941345131915384\n",
      "Stochastic Gradient Descent(24076): loss=4.9971563351598665\n",
      "Stochastic Gradient Descent(24077): loss=3.1153793888431225\n",
      "Stochastic Gradient Descent(24078): loss=1.7469103230897836\n",
      "Stochastic Gradient Descent(24079): loss=31.595407726418465\n",
      "Stochastic Gradient Descent(24080): loss=14.713512294254155\n",
      "Stochastic Gradient Descent(24081): loss=0.04362144232673437\n",
      "Stochastic Gradient Descent(24082): loss=2.4260931013184908\n",
      "Stochastic Gradient Descent(24083): loss=28.76191902884656\n",
      "Stochastic Gradient Descent(24084): loss=5.0931806193918385\n",
      "Stochastic Gradient Descent(24085): loss=1.957403562340486\n",
      "Stochastic Gradient Descent(24086): loss=1.0059253722025427\n",
      "Stochastic Gradient Descent(24087): loss=2.8389578822034967\n",
      "Stochastic Gradient Descent(24088): loss=0.6973674637745925\n",
      "Stochastic Gradient Descent(24089): loss=5.435783714213218\n",
      "Stochastic Gradient Descent(24090): loss=5.480953481804479\n",
      "Stochastic Gradient Descent(24091): loss=8.339905624821979\n",
      "Stochastic Gradient Descent(24092): loss=0.020878329411594326\n",
      "Stochastic Gradient Descent(24093): loss=1.5792305307779326\n",
      "Stochastic Gradient Descent(24094): loss=1.3277045127131353\n",
      "Stochastic Gradient Descent(24095): loss=8.222782893415912\n",
      "Stochastic Gradient Descent(24096): loss=2.455858267840561\n",
      "Stochastic Gradient Descent(24097): loss=0.11502640689477896\n",
      "Stochastic Gradient Descent(24098): loss=4.460584433167546\n",
      "Stochastic Gradient Descent(24099): loss=8.54379703024383\n",
      "Stochastic Gradient Descent(24100): loss=14.977774374440266\n",
      "Stochastic Gradient Descent(24101): loss=0.025064035713891097\n",
      "Stochastic Gradient Descent(24102): loss=0.07374632980659987\n",
      "Stochastic Gradient Descent(24103): loss=2.43129345573093\n",
      "Stochastic Gradient Descent(24104): loss=0.2788766078364706\n",
      "Stochastic Gradient Descent(24105): loss=0.29214453991647277\n",
      "Stochastic Gradient Descent(24106): loss=1.7250026878301532\n",
      "Stochastic Gradient Descent(24107): loss=4.714089943037902\n",
      "Stochastic Gradient Descent(24108): loss=3.8120911492543756\n",
      "Stochastic Gradient Descent(24109): loss=1.7138250177964023\n",
      "Stochastic Gradient Descent(24110): loss=0.9291743042448366\n",
      "Stochastic Gradient Descent(24111): loss=4.183488097842124\n",
      "Stochastic Gradient Descent(24112): loss=16.883138307602774\n",
      "Stochastic Gradient Descent(24113): loss=1.0335613280906781\n",
      "Stochastic Gradient Descent(24114): loss=5.385238569230666\n",
      "Stochastic Gradient Descent(24115): loss=9.19677564909656\n",
      "Stochastic Gradient Descent(24116): loss=2.7637915731774876\n",
      "Stochastic Gradient Descent(24117): loss=2.1058120817049413\n",
      "Stochastic Gradient Descent(24118): loss=1.8193744496702584\n",
      "Stochastic Gradient Descent(24119): loss=1.06226643691907\n",
      "Stochastic Gradient Descent(24120): loss=0.5149240359431378\n",
      "Stochastic Gradient Descent(24121): loss=4.478928541308908\n",
      "Stochastic Gradient Descent(24122): loss=4.9502360352316055\n",
      "Stochastic Gradient Descent(24123): loss=0.6449227630558112\n",
      "Stochastic Gradient Descent(24124): loss=0.2692243126593579\n",
      "Stochastic Gradient Descent(24125): loss=0.39876338941862555\n",
      "Stochastic Gradient Descent(24126): loss=0.3041528724136868\n",
      "Stochastic Gradient Descent(24127): loss=3.041640521473667\n",
      "Stochastic Gradient Descent(24128): loss=5.116527374207212\n",
      "Stochastic Gradient Descent(24129): loss=1.1618081280816852\n",
      "Stochastic Gradient Descent(24130): loss=0.9713382824601895\n",
      "Stochastic Gradient Descent(24131): loss=17.056812300594608\n",
      "Stochastic Gradient Descent(24132): loss=0.0710959935851371\n",
      "Stochastic Gradient Descent(24133): loss=3.940455591424857\n",
      "Stochastic Gradient Descent(24134): loss=0.4999648678778751\n",
      "Stochastic Gradient Descent(24135): loss=15.55744094198687\n",
      "Stochastic Gradient Descent(24136): loss=0.4813326231806691\n",
      "Stochastic Gradient Descent(24137): loss=0.09988402492461841\n",
      "Stochastic Gradient Descent(24138): loss=1.1630210201955267\n",
      "Stochastic Gradient Descent(24139): loss=0.8302130635427687\n",
      "Stochastic Gradient Descent(24140): loss=1.8089999614912151\n",
      "Stochastic Gradient Descent(24141): loss=4.705281323270891\n",
      "Stochastic Gradient Descent(24142): loss=2.0420467689784583\n",
      "Stochastic Gradient Descent(24143): loss=0.25433244328992066\n",
      "Stochastic Gradient Descent(24144): loss=0.00039113444644661526\n",
      "Stochastic Gradient Descent(24145): loss=0.32429079617918655\n",
      "Stochastic Gradient Descent(24146): loss=0.480652355874221\n",
      "Stochastic Gradient Descent(24147): loss=2.464748040475021\n",
      "Stochastic Gradient Descent(24148): loss=3.262009213318502\n",
      "Stochastic Gradient Descent(24149): loss=12.083687503870415\n",
      "Stochastic Gradient Descent(24150): loss=8.734379368055878\n",
      "Stochastic Gradient Descent(24151): loss=0.17962932886646174\n",
      "Stochastic Gradient Descent(24152): loss=19.607589576556425\n",
      "Stochastic Gradient Descent(24153): loss=0.23372403192073074\n",
      "Stochastic Gradient Descent(24154): loss=4.968676284092461\n",
      "Stochastic Gradient Descent(24155): loss=0.640480500946681\n",
      "Stochastic Gradient Descent(24156): loss=7.419165483670972\n",
      "Stochastic Gradient Descent(24157): loss=3.1889334365584743\n",
      "Stochastic Gradient Descent(24158): loss=14.917453654647051\n",
      "Stochastic Gradient Descent(24159): loss=3.301527626342059\n",
      "Stochastic Gradient Descent(24160): loss=0.04682792909201748\n",
      "Stochastic Gradient Descent(24161): loss=0.42513121343056876\n",
      "Stochastic Gradient Descent(24162): loss=2.459646555927377\n",
      "Stochastic Gradient Descent(24163): loss=73.8861630821039\n",
      "Stochastic Gradient Descent(24164): loss=146.45518560147093\n",
      "Stochastic Gradient Descent(24165): loss=34.537014758557596\n",
      "Stochastic Gradient Descent(24166): loss=16.222502689277633\n",
      "Stochastic Gradient Descent(24167): loss=4.033314330104436\n",
      "Stochastic Gradient Descent(24168): loss=12.658345799922344\n",
      "Stochastic Gradient Descent(24169): loss=4.834058084028103\n",
      "Stochastic Gradient Descent(24170): loss=3.1661459103215206\n",
      "Stochastic Gradient Descent(24171): loss=6.228738059021699\n",
      "Stochastic Gradient Descent(24172): loss=18.95125314071036\n",
      "Stochastic Gradient Descent(24173): loss=43.97097687990829\n",
      "Stochastic Gradient Descent(24174): loss=11.121341804996762\n",
      "Stochastic Gradient Descent(24175): loss=16.670034764495096\n",
      "Stochastic Gradient Descent(24176): loss=0.9474907473352742\n",
      "Stochastic Gradient Descent(24177): loss=20.38420449679439\n",
      "Stochastic Gradient Descent(24178): loss=0.00392193647163642\n",
      "Stochastic Gradient Descent(24179): loss=0.3782563419785786\n",
      "Stochastic Gradient Descent(24180): loss=0.840263569338163\n",
      "Stochastic Gradient Descent(24181): loss=0.5620665643899542\n",
      "Stochastic Gradient Descent(24182): loss=7.368782675382068\n",
      "Stochastic Gradient Descent(24183): loss=0.42659998999800186\n",
      "Stochastic Gradient Descent(24184): loss=0.4663341268815776\n",
      "Stochastic Gradient Descent(24185): loss=0.5433839769652891\n",
      "Stochastic Gradient Descent(24186): loss=7.345296138597907\n",
      "Stochastic Gradient Descent(24187): loss=1.1948195281241611\n",
      "Stochastic Gradient Descent(24188): loss=8.375916931741196\n",
      "Stochastic Gradient Descent(24189): loss=2.258320005664297\n",
      "Stochastic Gradient Descent(24190): loss=0.024594319942328257\n",
      "Stochastic Gradient Descent(24191): loss=0.004527987111515539\n",
      "Stochastic Gradient Descent(24192): loss=1.6607609080798353\n",
      "Stochastic Gradient Descent(24193): loss=6.879347227009527\n",
      "Stochastic Gradient Descent(24194): loss=0.02399046651353494\n",
      "Stochastic Gradient Descent(24195): loss=20.651741371206025\n",
      "Stochastic Gradient Descent(24196): loss=2.1417534418876336\n",
      "Stochastic Gradient Descent(24197): loss=16.265354423233312\n",
      "Stochastic Gradient Descent(24198): loss=4.107875491675277\n",
      "Stochastic Gradient Descent(24199): loss=0.9943884164916437\n",
      "Stochastic Gradient Descent(24200): loss=0.17158944689062744\n",
      "Stochastic Gradient Descent(24201): loss=7.248253713107059\n",
      "Stochastic Gradient Descent(24202): loss=0.6825969834842323\n",
      "Stochastic Gradient Descent(24203): loss=1.5603521779889735\n",
      "Stochastic Gradient Descent(24204): loss=0.0050896004721096896\n",
      "Stochastic Gradient Descent(24205): loss=0.14549266311731204\n",
      "Stochastic Gradient Descent(24206): loss=1.344300283620038\n",
      "Stochastic Gradient Descent(24207): loss=0.006355049333934389\n",
      "Stochastic Gradient Descent(24208): loss=0.08988142857589218\n",
      "Stochastic Gradient Descent(24209): loss=0.9059739479441792\n",
      "Stochastic Gradient Descent(24210): loss=16.27244544335106\n",
      "Stochastic Gradient Descent(24211): loss=3.4983209322465045\n",
      "Stochastic Gradient Descent(24212): loss=1.0652495655739136\n",
      "Stochastic Gradient Descent(24213): loss=7.899497605745039\n",
      "Stochastic Gradient Descent(24214): loss=1.521396671684115\n",
      "Stochastic Gradient Descent(24215): loss=0.021814406552555416\n",
      "Stochastic Gradient Descent(24216): loss=2.046509160568345\n",
      "Stochastic Gradient Descent(24217): loss=2.36943795571593\n",
      "Stochastic Gradient Descent(24218): loss=0.0071176892769652545\n",
      "Stochastic Gradient Descent(24219): loss=0.9736457874268237\n",
      "Stochastic Gradient Descent(24220): loss=0.38557243034813693\n",
      "Stochastic Gradient Descent(24221): loss=3.9631226608565195\n",
      "Stochastic Gradient Descent(24222): loss=5.9796186477248465\n",
      "Stochastic Gradient Descent(24223): loss=14.284905490482524\n",
      "Stochastic Gradient Descent(24224): loss=6.800464217115706\n",
      "Stochastic Gradient Descent(24225): loss=5.954071826008748\n",
      "Stochastic Gradient Descent(24226): loss=1.387179267953702\n",
      "Stochastic Gradient Descent(24227): loss=2.628446566215899\n",
      "Stochastic Gradient Descent(24228): loss=4.089066061507625\n",
      "Stochastic Gradient Descent(24229): loss=0.1274998749316468\n",
      "Stochastic Gradient Descent(24230): loss=2.4424973411925652\n",
      "Stochastic Gradient Descent(24231): loss=8.115635940549936\n",
      "Stochastic Gradient Descent(24232): loss=23.46133261799059\n",
      "Stochastic Gradient Descent(24233): loss=2.108642238600848\n",
      "Stochastic Gradient Descent(24234): loss=0.32869070820694396\n",
      "Stochastic Gradient Descent(24235): loss=12.46483087367345\n",
      "Stochastic Gradient Descent(24236): loss=11.38538982583537\n",
      "Stochastic Gradient Descent(24237): loss=0.26072325328749185\n",
      "Stochastic Gradient Descent(24238): loss=0.16089264348915444\n",
      "Stochastic Gradient Descent(24239): loss=3.2409889626533115\n",
      "Stochastic Gradient Descent(24240): loss=0.15006944579176817\n",
      "Stochastic Gradient Descent(24241): loss=32.1625842774838\n",
      "Stochastic Gradient Descent(24242): loss=0.9950330352363891\n",
      "Stochastic Gradient Descent(24243): loss=3.3651746137613516\n",
      "Stochastic Gradient Descent(24244): loss=17.62875305719265\n",
      "Stochastic Gradient Descent(24245): loss=5.545917775589048\n",
      "Stochastic Gradient Descent(24246): loss=0.5952523804221836\n",
      "Stochastic Gradient Descent(24247): loss=12.295669619671932\n",
      "Stochastic Gradient Descent(24248): loss=1.2355189045785497\n",
      "Stochastic Gradient Descent(24249): loss=0.23856083319287286\n",
      "Stochastic Gradient Descent(24250): loss=0.4684827847578649\n",
      "Stochastic Gradient Descent(24251): loss=1.122966369007089\n",
      "Stochastic Gradient Descent(24252): loss=0.004763723433358966\n",
      "Stochastic Gradient Descent(24253): loss=0.3858750947345201\n",
      "Stochastic Gradient Descent(24254): loss=5.286515129735492\n",
      "Stochastic Gradient Descent(24255): loss=0.5074581235235772\n",
      "Stochastic Gradient Descent(24256): loss=0.9421328399643785\n",
      "Stochastic Gradient Descent(24257): loss=1.9423938979365756\n",
      "Stochastic Gradient Descent(24258): loss=0.14073184003894373\n",
      "Stochastic Gradient Descent(24259): loss=0.023934820562560622\n",
      "Stochastic Gradient Descent(24260): loss=0.0010075811416353276\n",
      "Stochastic Gradient Descent(24261): loss=0.005762950721722141\n",
      "Stochastic Gradient Descent(24262): loss=2.6965888625084027\n",
      "Stochastic Gradient Descent(24263): loss=0.017057498155584452\n",
      "Stochastic Gradient Descent(24264): loss=4.265194175435587e-05\n",
      "Stochastic Gradient Descent(24265): loss=0.006282671051734829\n",
      "Stochastic Gradient Descent(24266): loss=0.3031222929098444\n",
      "Stochastic Gradient Descent(24267): loss=9.876004668432078\n",
      "Stochastic Gradient Descent(24268): loss=0.030018705235549028\n",
      "Stochastic Gradient Descent(24269): loss=1.7860328505549758\n",
      "Stochastic Gradient Descent(24270): loss=4.774376854015288\n",
      "Stochastic Gradient Descent(24271): loss=0.3635225643811151\n",
      "Stochastic Gradient Descent(24272): loss=2.373238860703964\n",
      "Stochastic Gradient Descent(24273): loss=2.224221559223895\n",
      "Stochastic Gradient Descent(24274): loss=11.922106431736566\n",
      "Stochastic Gradient Descent(24275): loss=5.896784648262358\n",
      "Stochastic Gradient Descent(24276): loss=0.008284053636270999\n",
      "Stochastic Gradient Descent(24277): loss=0.0813770672096729\n",
      "Stochastic Gradient Descent(24278): loss=2.8004217666916715\n",
      "Stochastic Gradient Descent(24279): loss=0.654573727540695\n",
      "Stochastic Gradient Descent(24280): loss=1.6707842213941517\n",
      "Stochastic Gradient Descent(24281): loss=1.9936414177058353\n",
      "Stochastic Gradient Descent(24282): loss=2.74961263101376\n",
      "Stochastic Gradient Descent(24283): loss=1.6273322334827036\n",
      "Stochastic Gradient Descent(24284): loss=1.326930482894428\n",
      "Stochastic Gradient Descent(24285): loss=0.00010315233475025046\n",
      "Stochastic Gradient Descent(24286): loss=0.34900168107133445\n",
      "Stochastic Gradient Descent(24287): loss=3.991031175630352\n",
      "Stochastic Gradient Descent(24288): loss=1.501307671994452\n",
      "Stochastic Gradient Descent(24289): loss=0.24850076140021787\n",
      "Stochastic Gradient Descent(24290): loss=4.517259621124675\n",
      "Stochastic Gradient Descent(24291): loss=1.948121282803711\n",
      "Stochastic Gradient Descent(24292): loss=24.552201598192283\n",
      "Stochastic Gradient Descent(24293): loss=7.275607404742765\n",
      "Stochastic Gradient Descent(24294): loss=11.872983894571885\n",
      "Stochastic Gradient Descent(24295): loss=2.886150062904739\n",
      "Stochastic Gradient Descent(24296): loss=0.6470509944150499\n",
      "Stochastic Gradient Descent(24297): loss=3.3035005589712223\n",
      "Stochastic Gradient Descent(24298): loss=1.4315646474838162\n",
      "Stochastic Gradient Descent(24299): loss=0.629046463515755\n",
      "Stochastic Gradient Descent(24300): loss=13.143382363476023\n",
      "Stochastic Gradient Descent(24301): loss=1.6624221251940534\n",
      "Stochastic Gradient Descent(24302): loss=0.009607755656312385\n",
      "Stochastic Gradient Descent(24303): loss=9.52759747525404\n",
      "Stochastic Gradient Descent(24304): loss=0.23623449652000342\n",
      "Stochastic Gradient Descent(24305): loss=2.0098478712302152\n",
      "Stochastic Gradient Descent(24306): loss=2.501090566466757\n",
      "Stochastic Gradient Descent(24307): loss=0.06289451269191605\n",
      "Stochastic Gradient Descent(24308): loss=1.187600307027827\n",
      "Stochastic Gradient Descent(24309): loss=0.3003927649159778\n",
      "Stochastic Gradient Descent(24310): loss=0.776614455617538\n",
      "Stochastic Gradient Descent(24311): loss=3.7341370228739423\n",
      "Stochastic Gradient Descent(24312): loss=0.09536675257840334\n",
      "Stochastic Gradient Descent(24313): loss=18.941414726190178\n",
      "Stochastic Gradient Descent(24314): loss=1.2086449606294527\n",
      "Stochastic Gradient Descent(24315): loss=0.2459163631830922\n",
      "Stochastic Gradient Descent(24316): loss=1.3731620251273096\n",
      "Stochastic Gradient Descent(24317): loss=10.067661747131693\n",
      "Stochastic Gradient Descent(24318): loss=5.748193270745605\n",
      "Stochastic Gradient Descent(24319): loss=2.0659634688043145\n",
      "Stochastic Gradient Descent(24320): loss=0.9632066484487632\n",
      "Stochastic Gradient Descent(24321): loss=0.4170004169491573\n",
      "Stochastic Gradient Descent(24322): loss=0.08263361105811179\n",
      "Stochastic Gradient Descent(24323): loss=0.10234726072947632\n",
      "Stochastic Gradient Descent(24324): loss=0.6067650296150034\n",
      "Stochastic Gradient Descent(24325): loss=9.568737570237934\n",
      "Stochastic Gradient Descent(24326): loss=1.905175223280007\n",
      "Stochastic Gradient Descent(24327): loss=16.45246990126361\n",
      "Stochastic Gradient Descent(24328): loss=1.2752452750577887\n",
      "Stochastic Gradient Descent(24329): loss=5.953169860918953\n",
      "Stochastic Gradient Descent(24330): loss=1.7536687432749778\n",
      "Stochastic Gradient Descent(24331): loss=5.420151637329122\n",
      "Stochastic Gradient Descent(24332): loss=0.4911341403219903\n",
      "Stochastic Gradient Descent(24333): loss=0.27046030050269165\n",
      "Stochastic Gradient Descent(24334): loss=2.3568681732147683\n",
      "Stochastic Gradient Descent(24335): loss=2.31372089277861\n",
      "Stochastic Gradient Descent(24336): loss=0.7788336822161988\n",
      "Stochastic Gradient Descent(24337): loss=2.89447488363931\n",
      "Stochastic Gradient Descent(24338): loss=4.812676216672754\n",
      "Stochastic Gradient Descent(24339): loss=0.29418438032718125\n",
      "Stochastic Gradient Descent(24340): loss=0.03608810014108338\n",
      "Stochastic Gradient Descent(24341): loss=1.3270459540233859\n",
      "Stochastic Gradient Descent(24342): loss=6.041927790731992\n",
      "Stochastic Gradient Descent(24343): loss=14.417346750370683\n",
      "Stochastic Gradient Descent(24344): loss=18.31621238786223\n",
      "Stochastic Gradient Descent(24345): loss=2.747046431427826\n",
      "Stochastic Gradient Descent(24346): loss=11.597413260062076\n",
      "Stochastic Gradient Descent(24347): loss=0.4494122605902777\n",
      "Stochastic Gradient Descent(24348): loss=1.85006689724962\n",
      "Stochastic Gradient Descent(24349): loss=0.783985494260929\n",
      "Stochastic Gradient Descent(24350): loss=0.1174096450829652\n",
      "Stochastic Gradient Descent(24351): loss=5.674113448191202\n",
      "Stochastic Gradient Descent(24352): loss=1.7319349350730118\n",
      "Stochastic Gradient Descent(24353): loss=11.362156734771961\n",
      "Stochastic Gradient Descent(24354): loss=1.21740103528369\n",
      "Stochastic Gradient Descent(24355): loss=0.026862492992476932\n",
      "Stochastic Gradient Descent(24356): loss=1.2662129645420306\n",
      "Stochastic Gradient Descent(24357): loss=0.2636485860128698\n",
      "Stochastic Gradient Descent(24358): loss=0.8891507688887467\n",
      "Stochastic Gradient Descent(24359): loss=1.26154850779086\n",
      "Stochastic Gradient Descent(24360): loss=2.5770925742258886\n",
      "Stochastic Gradient Descent(24361): loss=28.88979335625827\n",
      "Stochastic Gradient Descent(24362): loss=2.9389924309479527\n",
      "Stochastic Gradient Descent(24363): loss=2.832702526999746\n",
      "Stochastic Gradient Descent(24364): loss=0.3223807621812312\n",
      "Stochastic Gradient Descent(24365): loss=0.10329017500102294\n",
      "Stochastic Gradient Descent(24366): loss=0.13412299017063792\n",
      "Stochastic Gradient Descent(24367): loss=0.45747215344624426\n",
      "Stochastic Gradient Descent(24368): loss=0.33722710474716683\n",
      "Stochastic Gradient Descent(24369): loss=5.02224356796417\n",
      "Stochastic Gradient Descent(24370): loss=2.3327285434423923\n",
      "Stochastic Gradient Descent(24371): loss=0.11918435385760283\n",
      "Stochastic Gradient Descent(24372): loss=0.0941167310321425\n",
      "Stochastic Gradient Descent(24373): loss=0.92424479302551\n",
      "Stochastic Gradient Descent(24374): loss=1.0969406987641224\n",
      "Stochastic Gradient Descent(24375): loss=7.425631398458552\n",
      "Stochastic Gradient Descent(24376): loss=1.8351968920242874\n",
      "Stochastic Gradient Descent(24377): loss=0.02056702145457914\n",
      "Stochastic Gradient Descent(24378): loss=1.46571222153323\n",
      "Stochastic Gradient Descent(24379): loss=1.2503557989249627\n",
      "Stochastic Gradient Descent(24380): loss=7.908722689573108\n",
      "Stochastic Gradient Descent(24381): loss=5.734860569691953\n",
      "Stochastic Gradient Descent(24382): loss=0.6039183487364852\n",
      "Stochastic Gradient Descent(24383): loss=1.700561205382489\n",
      "Stochastic Gradient Descent(24384): loss=11.602827549498747\n",
      "Stochastic Gradient Descent(24385): loss=0.1612767307956275\n",
      "Stochastic Gradient Descent(24386): loss=0.024314646189528916\n",
      "Stochastic Gradient Descent(24387): loss=14.063839525458393\n",
      "Stochastic Gradient Descent(24388): loss=6.484470262897799\n",
      "Stochastic Gradient Descent(24389): loss=0.6918443164695554\n",
      "Stochastic Gradient Descent(24390): loss=0.00011885044163976328\n",
      "Stochastic Gradient Descent(24391): loss=3.3250880088982258\n",
      "Stochastic Gradient Descent(24392): loss=23.53086550029905\n",
      "Stochastic Gradient Descent(24393): loss=0.018791973341809296\n",
      "Stochastic Gradient Descent(24394): loss=1.8757914742043114\n",
      "Stochastic Gradient Descent(24395): loss=2.211560205996867e-05\n",
      "Stochastic Gradient Descent(24396): loss=1.1816453536745057\n",
      "Stochastic Gradient Descent(24397): loss=3.453335899388218\n",
      "Stochastic Gradient Descent(24398): loss=6.929615342155713\n",
      "Stochastic Gradient Descent(24399): loss=2.6278934141217674\n",
      "Stochastic Gradient Descent(24400): loss=0.040870892546378976\n",
      "Stochastic Gradient Descent(24401): loss=7.20110442807036\n",
      "Stochastic Gradient Descent(24402): loss=1.0423827895572495\n",
      "Stochastic Gradient Descent(24403): loss=0.6111454290807057\n",
      "Stochastic Gradient Descent(24404): loss=9.928704735435693\n",
      "Stochastic Gradient Descent(24405): loss=40.73064075773394\n",
      "Stochastic Gradient Descent(24406): loss=0.402501522407829\n",
      "Stochastic Gradient Descent(24407): loss=0.6876473442625508\n",
      "Stochastic Gradient Descent(24408): loss=3.8491971823536004\n",
      "Stochastic Gradient Descent(24409): loss=10.142637182988972\n",
      "Stochastic Gradient Descent(24410): loss=0.02081355918835859\n",
      "Stochastic Gradient Descent(24411): loss=3.360350578851095\n",
      "Stochastic Gradient Descent(24412): loss=1.140533722366324\n",
      "Stochastic Gradient Descent(24413): loss=14.306449216329593\n",
      "Stochastic Gradient Descent(24414): loss=1.8759180681090761\n",
      "Stochastic Gradient Descent(24415): loss=0.28767899999422286\n",
      "Stochastic Gradient Descent(24416): loss=0.9175405936420112\n",
      "Stochastic Gradient Descent(24417): loss=1.6309825818734505\n",
      "Stochastic Gradient Descent(24418): loss=0.12661751003880153\n",
      "Stochastic Gradient Descent(24419): loss=3.5966588378038025\n",
      "Stochastic Gradient Descent(24420): loss=1.7113628151519944\n",
      "Stochastic Gradient Descent(24421): loss=0.7404534727897611\n",
      "Stochastic Gradient Descent(24422): loss=9.638551288696833\n",
      "Stochastic Gradient Descent(24423): loss=0.9649011268738403\n",
      "Stochastic Gradient Descent(24424): loss=6.908695013337333\n",
      "Stochastic Gradient Descent(24425): loss=7.388716566963681\n",
      "Stochastic Gradient Descent(24426): loss=0.004372574698691489\n",
      "Stochastic Gradient Descent(24427): loss=0.5985214154641756\n",
      "Stochastic Gradient Descent(24428): loss=2.513739706814652\n",
      "Stochastic Gradient Descent(24429): loss=1.6576132651943782\n",
      "Stochastic Gradient Descent(24430): loss=2.4272185075810486\n",
      "Stochastic Gradient Descent(24431): loss=1.9050580718454901\n",
      "Stochastic Gradient Descent(24432): loss=0.23992037255727738\n",
      "Stochastic Gradient Descent(24433): loss=5.390417790330071\n",
      "Stochastic Gradient Descent(24434): loss=0.10497077898134574\n",
      "Stochastic Gradient Descent(24435): loss=6.909743621564984\n",
      "Stochastic Gradient Descent(24436): loss=0.1678285084833674\n",
      "Stochastic Gradient Descent(24437): loss=0.4142327620432563\n",
      "Stochastic Gradient Descent(24438): loss=0.06575536703038529\n",
      "Stochastic Gradient Descent(24439): loss=0.23326090751520812\n",
      "Stochastic Gradient Descent(24440): loss=0.828460867315101\n",
      "Stochastic Gradient Descent(24441): loss=0.012146882979335326\n",
      "Stochastic Gradient Descent(24442): loss=4.1204702943292135\n",
      "Stochastic Gradient Descent(24443): loss=0.11646812984000528\n",
      "Stochastic Gradient Descent(24444): loss=11.350356463666435\n",
      "Stochastic Gradient Descent(24445): loss=7.522377401361978\n",
      "Stochastic Gradient Descent(24446): loss=0.7118974881821268\n",
      "Stochastic Gradient Descent(24447): loss=5.649255777999004\n",
      "Stochastic Gradient Descent(24448): loss=10.175106259040442\n",
      "Stochastic Gradient Descent(24449): loss=16.5655940361742\n",
      "Stochastic Gradient Descent(24450): loss=9.584234320505585\n",
      "Stochastic Gradient Descent(24451): loss=1.3025487705262295\n",
      "Stochastic Gradient Descent(24452): loss=0.004224651959313439\n",
      "Stochastic Gradient Descent(24453): loss=0.015110635237323141\n",
      "Stochastic Gradient Descent(24454): loss=5.524814335770829\n",
      "Stochastic Gradient Descent(24455): loss=8.57628881356345\n",
      "Stochastic Gradient Descent(24456): loss=12.3877074694332\n",
      "Stochastic Gradient Descent(24457): loss=0.2839441246479618\n",
      "Stochastic Gradient Descent(24458): loss=5.762695327005164\n",
      "Stochastic Gradient Descent(24459): loss=3.713277768269613\n",
      "Stochastic Gradient Descent(24460): loss=1.1931913457380596\n",
      "Stochastic Gradient Descent(24461): loss=4.279772466683002\n",
      "Stochastic Gradient Descent(24462): loss=2.3293770228350974\n",
      "Stochastic Gradient Descent(24463): loss=10.915792178920336\n",
      "Stochastic Gradient Descent(24464): loss=1.2253879513257204\n",
      "Stochastic Gradient Descent(24465): loss=3.662557681494997\n",
      "Stochastic Gradient Descent(24466): loss=10.032569121086492\n",
      "Stochastic Gradient Descent(24467): loss=9.314272000911927\n",
      "Stochastic Gradient Descent(24468): loss=6.133501046929596\n",
      "Stochastic Gradient Descent(24469): loss=1.2454588897982466\n",
      "Stochastic Gradient Descent(24470): loss=7.712607851879527\n",
      "Stochastic Gradient Descent(24471): loss=0.3959715973969894\n",
      "Stochastic Gradient Descent(24472): loss=28.844796430163104\n",
      "Stochastic Gradient Descent(24473): loss=0.092190970726097\n",
      "Stochastic Gradient Descent(24474): loss=3.8708748229398022\n",
      "Stochastic Gradient Descent(24475): loss=17.964232613518174\n",
      "Stochastic Gradient Descent(24476): loss=1.154160799499854\n",
      "Stochastic Gradient Descent(24477): loss=0.3222839336523259\n",
      "Stochastic Gradient Descent(24478): loss=6.337082106847888\n",
      "Stochastic Gradient Descent(24479): loss=0.12570657329493995\n",
      "Stochastic Gradient Descent(24480): loss=0.07619556773956382\n",
      "Stochastic Gradient Descent(24481): loss=4.562953908737134\n",
      "Stochastic Gradient Descent(24482): loss=2.2660109251116145\n",
      "Stochastic Gradient Descent(24483): loss=0.14132829926690388\n",
      "Stochastic Gradient Descent(24484): loss=12.198113180984187\n",
      "Stochastic Gradient Descent(24485): loss=0.044548549873034124\n",
      "Stochastic Gradient Descent(24486): loss=25.829128608765437\n",
      "Stochastic Gradient Descent(24487): loss=1.0062170194869637\n",
      "Stochastic Gradient Descent(24488): loss=5.62958564994013\n",
      "Stochastic Gradient Descent(24489): loss=2.6764422457713817\n",
      "Stochastic Gradient Descent(24490): loss=1.6174848058910154\n",
      "Stochastic Gradient Descent(24491): loss=5.976191357266589\n",
      "Stochastic Gradient Descent(24492): loss=0.012266427652236348\n",
      "Stochastic Gradient Descent(24493): loss=8.904657509710438\n",
      "Stochastic Gradient Descent(24494): loss=6.683663046330121\n",
      "Stochastic Gradient Descent(24495): loss=1.6740580401237664\n",
      "Stochastic Gradient Descent(24496): loss=8.804974864074335\n",
      "Stochastic Gradient Descent(24497): loss=0.07964706748732589\n",
      "Stochastic Gradient Descent(24498): loss=19.023256794959526\n",
      "Stochastic Gradient Descent(24499): loss=0.005294373256852797\n",
      "Stochastic Gradient Descent(24500): loss=0.2756404136176636\n",
      "Stochastic Gradient Descent(24501): loss=4.017812811720623\n",
      "Stochastic Gradient Descent(24502): loss=20.972126511343617\n",
      "Stochastic Gradient Descent(24503): loss=3.25345748437953\n",
      "Stochastic Gradient Descent(24504): loss=6.781424555334855\n",
      "Stochastic Gradient Descent(24505): loss=5.081129801814672\n",
      "Stochastic Gradient Descent(24506): loss=0.3222656269368664\n",
      "Stochastic Gradient Descent(24507): loss=0.5736777226506065\n",
      "Stochastic Gradient Descent(24508): loss=0.963390021965227\n",
      "Stochastic Gradient Descent(24509): loss=0.07627515627554071\n",
      "Stochastic Gradient Descent(24510): loss=2.905753925126297\n",
      "Stochastic Gradient Descent(24511): loss=8.212668458122666\n",
      "Stochastic Gradient Descent(24512): loss=28.59330492236896\n",
      "Stochastic Gradient Descent(24513): loss=13.828871585103947\n",
      "Stochastic Gradient Descent(24514): loss=1.4225114735969746\n",
      "Stochastic Gradient Descent(24515): loss=1.7392099255184683\n",
      "Stochastic Gradient Descent(24516): loss=7.44770690103087\n",
      "Stochastic Gradient Descent(24517): loss=16.490209375478358\n",
      "Stochastic Gradient Descent(24518): loss=10.233397218823377\n",
      "Stochastic Gradient Descent(24519): loss=8.26293216889278\n",
      "Stochastic Gradient Descent(24520): loss=0.9683325931148932\n",
      "Stochastic Gradient Descent(24521): loss=5.9860427696015694\n",
      "Stochastic Gradient Descent(24522): loss=1.2506165914794316\n",
      "Stochastic Gradient Descent(24523): loss=2.1260483507339116\n",
      "Stochastic Gradient Descent(24524): loss=1.1647089806645587\n",
      "Stochastic Gradient Descent(24525): loss=18.531265784348363\n",
      "Stochastic Gradient Descent(24526): loss=4.879955885473154\n",
      "Stochastic Gradient Descent(24527): loss=2.010730405578731\n",
      "Stochastic Gradient Descent(24528): loss=9.3324790973882\n",
      "Stochastic Gradient Descent(24529): loss=0.07533591587707028\n",
      "Stochastic Gradient Descent(24530): loss=9.527181973722403\n",
      "Stochastic Gradient Descent(24531): loss=0.0011447716220284226\n",
      "Stochastic Gradient Descent(24532): loss=2.2219953027638577\n",
      "Stochastic Gradient Descent(24533): loss=0.9055198841030372\n",
      "Stochastic Gradient Descent(24534): loss=0.18717637147764002\n",
      "Stochastic Gradient Descent(24535): loss=8.646249823788278\n",
      "Stochastic Gradient Descent(24536): loss=0.010849138331626618\n",
      "Stochastic Gradient Descent(24537): loss=1.0128936979473797\n",
      "Stochastic Gradient Descent(24538): loss=0.009471261286554294\n",
      "Stochastic Gradient Descent(24539): loss=1.1350911840625952\n",
      "Stochastic Gradient Descent(24540): loss=9.765273422982851\n",
      "Stochastic Gradient Descent(24541): loss=2.128313091674984\n",
      "Stochastic Gradient Descent(24542): loss=3.907555059154909\n",
      "Stochastic Gradient Descent(24543): loss=0.12069562781811867\n",
      "Stochastic Gradient Descent(24544): loss=0.13521105667544533\n",
      "Stochastic Gradient Descent(24545): loss=0.6537858707754763\n",
      "Stochastic Gradient Descent(24546): loss=1.422767890932764\n",
      "Stochastic Gradient Descent(24547): loss=0.6191285552501686\n",
      "Stochastic Gradient Descent(24548): loss=2.591736886804962\n",
      "Stochastic Gradient Descent(24549): loss=0.870656695601655\n",
      "Stochastic Gradient Descent(24550): loss=0.00039867391802833076\n",
      "Stochastic Gradient Descent(24551): loss=0.32045766242138785\n",
      "Stochastic Gradient Descent(24552): loss=0.05675928978496449\n",
      "Stochastic Gradient Descent(24553): loss=0.2985822511984401\n",
      "Stochastic Gradient Descent(24554): loss=3.052484365988883\n",
      "Stochastic Gradient Descent(24555): loss=0.029948701833658033\n",
      "Stochastic Gradient Descent(24556): loss=8.167867244813275\n",
      "Stochastic Gradient Descent(24557): loss=1.147802337378834\n",
      "Stochastic Gradient Descent(24558): loss=3.8472927081089403\n",
      "Stochastic Gradient Descent(24559): loss=2.1064740059665987\n",
      "Stochastic Gradient Descent(24560): loss=16.634381762481123\n",
      "Stochastic Gradient Descent(24561): loss=1.8093942585987803\n",
      "Stochastic Gradient Descent(24562): loss=13.320635034542601\n",
      "Stochastic Gradient Descent(24563): loss=0.14641856691535257\n",
      "Stochastic Gradient Descent(24564): loss=0.10637334916834756\n",
      "Stochastic Gradient Descent(24565): loss=0.9237960521417554\n",
      "Stochastic Gradient Descent(24566): loss=1.1999693565366887\n",
      "Stochastic Gradient Descent(24567): loss=0.037076353289389914\n",
      "Stochastic Gradient Descent(24568): loss=1.2013602680947415\n",
      "Stochastic Gradient Descent(24569): loss=2.769665499018304\n",
      "Stochastic Gradient Descent(24570): loss=0.003257240859423289\n",
      "Stochastic Gradient Descent(24571): loss=0.0005333750169145197\n",
      "Stochastic Gradient Descent(24572): loss=0.1911349712788509\n",
      "Stochastic Gradient Descent(24573): loss=0.04728378386568352\n",
      "Stochastic Gradient Descent(24574): loss=0.8727282821804636\n",
      "Stochastic Gradient Descent(24575): loss=64.71075204251903\n",
      "Stochastic Gradient Descent(24576): loss=37.30403635754203\n",
      "Stochastic Gradient Descent(24577): loss=2.0593316249238214\n",
      "Stochastic Gradient Descent(24578): loss=29.473710327812316\n",
      "Stochastic Gradient Descent(24579): loss=0.034936995486974286\n",
      "Stochastic Gradient Descent(24580): loss=12.663696222674485\n",
      "Stochastic Gradient Descent(24581): loss=29.149730944348278\n",
      "Stochastic Gradient Descent(24582): loss=0.0040374607924327155\n",
      "Stochastic Gradient Descent(24583): loss=0.06544699518660888\n",
      "Stochastic Gradient Descent(24584): loss=5.972851880527063\n",
      "Stochastic Gradient Descent(24585): loss=2.7962046256851303\n",
      "Stochastic Gradient Descent(24586): loss=1.6086223712104206\n",
      "Stochastic Gradient Descent(24587): loss=1.6221648385696243\n",
      "Stochastic Gradient Descent(24588): loss=5.994950320317457\n",
      "Stochastic Gradient Descent(24589): loss=4.181679096624242\n",
      "Stochastic Gradient Descent(24590): loss=2.8696458354720336\n",
      "Stochastic Gradient Descent(24591): loss=5.256112409825258\n",
      "Stochastic Gradient Descent(24592): loss=0.398004423858235\n",
      "Stochastic Gradient Descent(24593): loss=1.0677862398029945\n",
      "Stochastic Gradient Descent(24594): loss=34.58433042928813\n",
      "Stochastic Gradient Descent(24595): loss=9.051899416355331\n",
      "Stochastic Gradient Descent(24596): loss=9.94929970463333\n",
      "Stochastic Gradient Descent(24597): loss=10.359739177939634\n",
      "Stochastic Gradient Descent(24598): loss=8.07653825161283\n",
      "Stochastic Gradient Descent(24599): loss=9.095471886662814\n",
      "Stochastic Gradient Descent(24600): loss=0.031915132436818275\n",
      "Stochastic Gradient Descent(24601): loss=4.825871238899864\n",
      "Stochastic Gradient Descent(24602): loss=0.45271655384068943\n",
      "Stochastic Gradient Descent(24603): loss=0.37103425489604175\n",
      "Stochastic Gradient Descent(24604): loss=1.2449497681121844\n",
      "Stochastic Gradient Descent(24605): loss=0.039296572677139534\n",
      "Stochastic Gradient Descent(24606): loss=8.593369552117057\n",
      "Stochastic Gradient Descent(24607): loss=18.330160165801995\n",
      "Stochastic Gradient Descent(24608): loss=0.0011037239637504575\n",
      "Stochastic Gradient Descent(24609): loss=1.13577913952372\n",
      "Stochastic Gradient Descent(24610): loss=0.0031811470186754874\n",
      "Stochastic Gradient Descent(24611): loss=7.067870222622008\n",
      "Stochastic Gradient Descent(24612): loss=0.0002506942639132421\n",
      "Stochastic Gradient Descent(24613): loss=20.019384297680038\n",
      "Stochastic Gradient Descent(24614): loss=1.3682560671174195\n",
      "Stochastic Gradient Descent(24615): loss=0.406767287238888\n",
      "Stochastic Gradient Descent(24616): loss=3.9803486029247885\n",
      "Stochastic Gradient Descent(24617): loss=0.8655151482967232\n",
      "Stochastic Gradient Descent(24618): loss=0.008615772219729357\n",
      "Stochastic Gradient Descent(24619): loss=0.008666565128133089\n",
      "Stochastic Gradient Descent(24620): loss=3.344687081606056\n",
      "Stochastic Gradient Descent(24621): loss=0.11211357265648865\n",
      "Stochastic Gradient Descent(24622): loss=0.5468863782474089\n",
      "Stochastic Gradient Descent(24623): loss=6.6226889572698155\n",
      "Stochastic Gradient Descent(24624): loss=1.3718318806170076\n",
      "Stochastic Gradient Descent(24625): loss=1.6674133202201271\n",
      "Stochastic Gradient Descent(24626): loss=7.031406915994373\n",
      "Stochastic Gradient Descent(24627): loss=3.585920223610074\n",
      "Stochastic Gradient Descent(24628): loss=1.1880714548640743\n",
      "Stochastic Gradient Descent(24629): loss=3.2336432442499032\n",
      "Stochastic Gradient Descent(24630): loss=0.42739305839047353\n",
      "Stochastic Gradient Descent(24631): loss=7.137550112241832\n",
      "Stochastic Gradient Descent(24632): loss=0.025819713458879227\n",
      "Stochastic Gradient Descent(24633): loss=0.3722735838762507\n",
      "Stochastic Gradient Descent(24634): loss=4.493147547830538\n",
      "Stochastic Gradient Descent(24635): loss=2.376840705303516\n",
      "Stochastic Gradient Descent(24636): loss=4.152992444861646\n",
      "Stochastic Gradient Descent(24637): loss=0.8081109005423266\n",
      "Stochastic Gradient Descent(24638): loss=5.527985210289033\n",
      "Stochastic Gradient Descent(24639): loss=2.0502897153239306\n",
      "Stochastic Gradient Descent(24640): loss=0.6372977421969052\n",
      "Stochastic Gradient Descent(24641): loss=4.127364806673392\n",
      "Stochastic Gradient Descent(24642): loss=2.414484881267879\n",
      "Stochastic Gradient Descent(24643): loss=3.9068296450865527\n",
      "Stochastic Gradient Descent(24644): loss=1.0493676487415247\n",
      "Stochastic Gradient Descent(24645): loss=0.13235414941982504\n",
      "Stochastic Gradient Descent(24646): loss=0.7144132340559819\n",
      "Stochastic Gradient Descent(24647): loss=2.1463464016389007\n",
      "Stochastic Gradient Descent(24648): loss=10.078369618516364\n",
      "Stochastic Gradient Descent(24649): loss=5.44021450687597\n",
      "Stochastic Gradient Descent(24650): loss=0.45595482008941346\n",
      "Stochastic Gradient Descent(24651): loss=2.050700936147457\n",
      "Stochastic Gradient Descent(24652): loss=11.884795097193171\n",
      "Stochastic Gradient Descent(24653): loss=7.0034842920024944\n",
      "Stochastic Gradient Descent(24654): loss=0.1576205571118728\n",
      "Stochastic Gradient Descent(24655): loss=1.536849956522197\n",
      "Stochastic Gradient Descent(24656): loss=1.5647884790339215\n",
      "Stochastic Gradient Descent(24657): loss=1.575780714100971\n",
      "Stochastic Gradient Descent(24658): loss=2.275848728977202\n",
      "Stochastic Gradient Descent(24659): loss=4.778205649928529\n",
      "Stochastic Gradient Descent(24660): loss=12.914760914780567\n",
      "Stochastic Gradient Descent(24661): loss=0.21948686742942364\n",
      "Stochastic Gradient Descent(24662): loss=0.7830101478054744\n",
      "Stochastic Gradient Descent(24663): loss=0.30310589537540134\n",
      "Stochastic Gradient Descent(24664): loss=0.23055984948535133\n",
      "Stochastic Gradient Descent(24665): loss=3.4345780524190133\n",
      "Stochastic Gradient Descent(24666): loss=3.4982419954333084\n",
      "Stochastic Gradient Descent(24667): loss=2.2478921691440985\n",
      "Stochastic Gradient Descent(24668): loss=9.267077590360623\n",
      "Stochastic Gradient Descent(24669): loss=21.768368285975882\n",
      "Stochastic Gradient Descent(24670): loss=3.341938693393136\n",
      "Stochastic Gradient Descent(24671): loss=5.449328065694361\n",
      "Stochastic Gradient Descent(24672): loss=2.1372829402724887\n",
      "Stochastic Gradient Descent(24673): loss=3.3470997881470335\n",
      "Stochastic Gradient Descent(24674): loss=4.528119454893337\n",
      "Stochastic Gradient Descent(24675): loss=3.4196426889174405\n",
      "Stochastic Gradient Descent(24676): loss=0.6685190928055\n",
      "Stochastic Gradient Descent(24677): loss=0.06832334174089408\n",
      "Stochastic Gradient Descent(24678): loss=0.13274428990875692\n",
      "Stochastic Gradient Descent(24679): loss=2.272173731757604\n",
      "Stochastic Gradient Descent(24680): loss=0.029767330588962027\n",
      "Stochastic Gradient Descent(24681): loss=0.13222796620360291\n",
      "Stochastic Gradient Descent(24682): loss=0.191913464839527\n",
      "Stochastic Gradient Descent(24683): loss=1.593455037392705\n",
      "Stochastic Gradient Descent(24684): loss=3.5521121703484386\n",
      "Stochastic Gradient Descent(24685): loss=1.0159954356566017\n",
      "Stochastic Gradient Descent(24686): loss=0.19804803008294922\n",
      "Stochastic Gradient Descent(24687): loss=9.39049543626044\n",
      "Stochastic Gradient Descent(24688): loss=2.4031161803737175\n",
      "Stochastic Gradient Descent(24689): loss=7.5790611572907585\n",
      "Stochastic Gradient Descent(24690): loss=0.7282373579825498\n",
      "Stochastic Gradient Descent(24691): loss=14.584286148034428\n",
      "Stochastic Gradient Descent(24692): loss=1.5168702327927077\n",
      "Stochastic Gradient Descent(24693): loss=18.911504034292197\n",
      "Stochastic Gradient Descent(24694): loss=1.9042162702277154\n",
      "Stochastic Gradient Descent(24695): loss=1.0316358608195628\n",
      "Stochastic Gradient Descent(24696): loss=0.011254298389580251\n",
      "Stochastic Gradient Descent(24697): loss=0.5865270793358843\n",
      "Stochastic Gradient Descent(24698): loss=0.11752278427637218\n",
      "Stochastic Gradient Descent(24699): loss=6.3728940043398055\n",
      "Stochastic Gradient Descent(24700): loss=0.0020072696136082305\n",
      "Stochastic Gradient Descent(24701): loss=1.263203557880569\n",
      "Stochastic Gradient Descent(24702): loss=1.4785939356610358\n",
      "Stochastic Gradient Descent(24703): loss=2.120622256650258\n",
      "Stochastic Gradient Descent(24704): loss=17.078822211319043\n",
      "Stochastic Gradient Descent(24705): loss=1.1490963421214446\n",
      "Stochastic Gradient Descent(24706): loss=12.847521256534819\n",
      "Stochastic Gradient Descent(24707): loss=4.592466862948757\n",
      "Stochastic Gradient Descent(24708): loss=0.8066362972610537\n",
      "Stochastic Gradient Descent(24709): loss=3.663704193760701\n",
      "Stochastic Gradient Descent(24710): loss=8.877872235460353\n",
      "Stochastic Gradient Descent(24711): loss=1.9883721850627254\n",
      "Stochastic Gradient Descent(24712): loss=2.9281936815899967\n",
      "Stochastic Gradient Descent(24713): loss=1.4641085024445073\n",
      "Stochastic Gradient Descent(24714): loss=4.218928054002399\n",
      "Stochastic Gradient Descent(24715): loss=6.2577883644474275\n",
      "Stochastic Gradient Descent(24716): loss=0.044058522593276306\n",
      "Stochastic Gradient Descent(24717): loss=1.078564981553405\n",
      "Stochastic Gradient Descent(24718): loss=0.13968809120578293\n",
      "Stochastic Gradient Descent(24719): loss=8.797847724598197\n",
      "Stochastic Gradient Descent(24720): loss=1.5956375749520701\n",
      "Stochastic Gradient Descent(24721): loss=0.23387834226148602\n",
      "Stochastic Gradient Descent(24722): loss=2.6017445483884654\n",
      "Stochastic Gradient Descent(24723): loss=6.849582179966974\n",
      "Stochastic Gradient Descent(24724): loss=0.8042734220452945\n",
      "Stochastic Gradient Descent(24725): loss=0.8023182005549764\n",
      "Stochastic Gradient Descent(24726): loss=5.401902248362367\n",
      "Stochastic Gradient Descent(24727): loss=0.055597776965413806\n",
      "Stochastic Gradient Descent(24728): loss=5.281349370318796\n",
      "Stochastic Gradient Descent(24729): loss=22.725718088843543\n",
      "Stochastic Gradient Descent(24730): loss=8.332966932166121\n",
      "Stochastic Gradient Descent(24731): loss=23.36184509643014\n",
      "Stochastic Gradient Descent(24732): loss=5.603387743801648\n",
      "Stochastic Gradient Descent(24733): loss=0.24825897581096554\n",
      "Stochastic Gradient Descent(24734): loss=0.023912448082513693\n",
      "Stochastic Gradient Descent(24735): loss=5.860152666762105\n",
      "Stochastic Gradient Descent(24736): loss=0.17488356469423086\n",
      "Stochastic Gradient Descent(24737): loss=0.08822094356737609\n",
      "Stochastic Gradient Descent(24738): loss=0.003390036006806619\n",
      "Stochastic Gradient Descent(24739): loss=1.252649368509562\n",
      "Stochastic Gradient Descent(24740): loss=2.660525198874514\n",
      "Stochastic Gradient Descent(24741): loss=5.038513537084169\n",
      "Stochastic Gradient Descent(24742): loss=0.22064076642563007\n",
      "Stochastic Gradient Descent(24743): loss=0.24766125142587264\n",
      "Stochastic Gradient Descent(24744): loss=1.0408827957197\n",
      "Stochastic Gradient Descent(24745): loss=0.37654113343534473\n",
      "Stochastic Gradient Descent(24746): loss=1.1099388413033744\n",
      "Stochastic Gradient Descent(24747): loss=4.547132191311628\n",
      "Stochastic Gradient Descent(24748): loss=11.839849486046331\n",
      "Stochastic Gradient Descent(24749): loss=23.324616549989905\n",
      "Stochastic Gradient Descent(24750): loss=8.687477157606038\n",
      "Stochastic Gradient Descent(24751): loss=10.525843607413872\n",
      "Stochastic Gradient Descent(24752): loss=4.8159201900711155\n",
      "Stochastic Gradient Descent(24753): loss=2.3666516192920146\n",
      "Stochastic Gradient Descent(24754): loss=2.1262615917766423\n",
      "Stochastic Gradient Descent(24755): loss=0.2382550226966387\n",
      "Stochastic Gradient Descent(24756): loss=0.45302169821923005\n",
      "Stochastic Gradient Descent(24757): loss=9.984928949718782\n",
      "Stochastic Gradient Descent(24758): loss=11.791644192186887\n",
      "Stochastic Gradient Descent(24759): loss=3.210647878028217\n",
      "Stochastic Gradient Descent(24760): loss=0.42150094107833014\n",
      "Stochastic Gradient Descent(24761): loss=1.1926291455930063\n",
      "Stochastic Gradient Descent(24762): loss=36.25497887054613\n",
      "Stochastic Gradient Descent(24763): loss=3.453551179491372\n",
      "Stochastic Gradient Descent(24764): loss=15.697314338962714\n",
      "Stochastic Gradient Descent(24765): loss=0.00019384894788202048\n",
      "Stochastic Gradient Descent(24766): loss=0.0026905728627887164\n",
      "Stochastic Gradient Descent(24767): loss=0.5888842339454377\n",
      "Stochastic Gradient Descent(24768): loss=2.5516926578789434\n",
      "Stochastic Gradient Descent(24769): loss=5.666514538632243\n",
      "Stochastic Gradient Descent(24770): loss=2.1335493295543904\n",
      "Stochastic Gradient Descent(24771): loss=40.68879613773463\n",
      "Stochastic Gradient Descent(24772): loss=11.535038350815823\n",
      "Stochastic Gradient Descent(24773): loss=10.52405479676084\n",
      "Stochastic Gradient Descent(24774): loss=9.126196852299907\n",
      "Stochastic Gradient Descent(24775): loss=0.7755603474019575\n",
      "Stochastic Gradient Descent(24776): loss=0.538704940289938\n",
      "Stochastic Gradient Descent(24777): loss=1.8311651834440599\n",
      "Stochastic Gradient Descent(24778): loss=3.7444563896888154\n",
      "Stochastic Gradient Descent(24779): loss=23.624534060259798\n",
      "Stochastic Gradient Descent(24780): loss=10.480717144675406\n",
      "Stochastic Gradient Descent(24781): loss=0.013943908660129007\n",
      "Stochastic Gradient Descent(24782): loss=0.8085240819984667\n",
      "Stochastic Gradient Descent(24783): loss=2.6656799628266707\n",
      "Stochastic Gradient Descent(24784): loss=4.606681996508006\n",
      "Stochastic Gradient Descent(24785): loss=6.930884923959828\n",
      "Stochastic Gradient Descent(24786): loss=1.7117779908983788\n",
      "Stochastic Gradient Descent(24787): loss=1.8939984691748293\n",
      "Stochastic Gradient Descent(24788): loss=5.731186590185368\n",
      "Stochastic Gradient Descent(24789): loss=29.975349767775707\n",
      "Stochastic Gradient Descent(24790): loss=5.633217884967982\n",
      "Stochastic Gradient Descent(24791): loss=8.6147085603528\n",
      "Stochastic Gradient Descent(24792): loss=18.6546008833352\n",
      "Stochastic Gradient Descent(24793): loss=2.500045001673985\n",
      "Stochastic Gradient Descent(24794): loss=6.1726024651144185\n",
      "Stochastic Gradient Descent(24795): loss=12.989109536836573\n",
      "Stochastic Gradient Descent(24796): loss=0.8543619043209165\n",
      "Stochastic Gradient Descent(24797): loss=2.147996981689689\n",
      "Stochastic Gradient Descent(24798): loss=22.70004086304863\n",
      "Stochastic Gradient Descent(24799): loss=3.1028183155307554\n",
      "Stochastic Gradient Descent(24800): loss=3.58026008560585\n",
      "Stochastic Gradient Descent(24801): loss=3.0050636634917165\n",
      "Stochastic Gradient Descent(24802): loss=1.9318564425376106\n",
      "Stochastic Gradient Descent(24803): loss=1.9042153043983843\n",
      "Stochastic Gradient Descent(24804): loss=0.7889242196536355\n",
      "Stochastic Gradient Descent(24805): loss=3.3999606502835595\n",
      "Stochastic Gradient Descent(24806): loss=15.126169376220597\n",
      "Stochastic Gradient Descent(24807): loss=0.7383379928285387\n",
      "Stochastic Gradient Descent(24808): loss=0.009047093018085078\n",
      "Stochastic Gradient Descent(24809): loss=12.947084035620831\n",
      "Stochastic Gradient Descent(24810): loss=12.163829452404023\n",
      "Stochastic Gradient Descent(24811): loss=0.26502081051933646\n",
      "Stochastic Gradient Descent(24812): loss=16.92040596762973\n",
      "Stochastic Gradient Descent(24813): loss=5.110694863068427\n",
      "Stochastic Gradient Descent(24814): loss=4.161732502158299\n",
      "Stochastic Gradient Descent(24815): loss=4.478028130916985\n",
      "Stochastic Gradient Descent(24816): loss=0.4345762756387085\n",
      "Stochastic Gradient Descent(24817): loss=2.757817518165411\n",
      "Stochastic Gradient Descent(24818): loss=0.21903175598801666\n",
      "Stochastic Gradient Descent(24819): loss=0.3799040582783197\n",
      "Stochastic Gradient Descent(24820): loss=0.023602989096267166\n",
      "Stochastic Gradient Descent(24821): loss=0.0019971977168080513\n",
      "Stochastic Gradient Descent(24822): loss=1.3106949503555756\n",
      "Stochastic Gradient Descent(24823): loss=6.785403269415426\n",
      "Stochastic Gradient Descent(24824): loss=0.9211637301508396\n",
      "Stochastic Gradient Descent(24825): loss=0.43897884789532815\n",
      "Stochastic Gradient Descent(24826): loss=0.22524499699848463\n",
      "Stochastic Gradient Descent(24827): loss=3.4127406214157543\n",
      "Stochastic Gradient Descent(24828): loss=4.169785692461433\n",
      "Stochastic Gradient Descent(24829): loss=0.21262907812831836\n",
      "Stochastic Gradient Descent(24830): loss=2.279399417809917\n",
      "Stochastic Gradient Descent(24831): loss=2.866415560635192\n",
      "Stochastic Gradient Descent(24832): loss=23.20914076743203\n",
      "Stochastic Gradient Descent(24833): loss=10.03713943653587\n",
      "Stochastic Gradient Descent(24834): loss=10.250731768570333\n",
      "Stochastic Gradient Descent(24835): loss=0.24280709869346476\n",
      "Stochastic Gradient Descent(24836): loss=3.7050668341229844\n",
      "Stochastic Gradient Descent(24837): loss=36.55972201433787\n",
      "Stochastic Gradient Descent(24838): loss=9.283060352065709\n",
      "Stochastic Gradient Descent(24839): loss=4.362870684477603\n",
      "Stochastic Gradient Descent(24840): loss=10.238640879545065\n",
      "Stochastic Gradient Descent(24841): loss=0.11428743086238263\n",
      "Stochastic Gradient Descent(24842): loss=4.048441305062706\n",
      "Stochastic Gradient Descent(24843): loss=0.24325458626903906\n",
      "Stochastic Gradient Descent(24844): loss=8.51214434052085\n",
      "Stochastic Gradient Descent(24845): loss=5.501755894002097\n",
      "Stochastic Gradient Descent(24846): loss=1.0851512264894032\n",
      "Stochastic Gradient Descent(24847): loss=2.9480090543971738\n",
      "Stochastic Gradient Descent(24848): loss=0.14235826967567158\n",
      "Stochastic Gradient Descent(24849): loss=11.572075609532048\n",
      "Stochastic Gradient Descent(24850): loss=57.313879108504636\n",
      "Stochastic Gradient Descent(24851): loss=17.742901074892433\n",
      "Stochastic Gradient Descent(24852): loss=0.13419166825687628\n",
      "Stochastic Gradient Descent(24853): loss=0.7434825545412771\n",
      "Stochastic Gradient Descent(24854): loss=4.935792902665574\n",
      "Stochastic Gradient Descent(24855): loss=7.187443853713041\n",
      "Stochastic Gradient Descent(24856): loss=5.006033782161603\n",
      "Stochastic Gradient Descent(24857): loss=29.91761104542137\n",
      "Stochastic Gradient Descent(24858): loss=5.720530112247008\n",
      "Stochastic Gradient Descent(24859): loss=2.7802104034937263\n",
      "Stochastic Gradient Descent(24860): loss=0.5794074766631048\n",
      "Stochastic Gradient Descent(24861): loss=0.15583808476752925\n",
      "Stochastic Gradient Descent(24862): loss=3.648224440973866\n",
      "Stochastic Gradient Descent(24863): loss=0.9345572064082776\n",
      "Stochastic Gradient Descent(24864): loss=6.789609339516371\n",
      "Stochastic Gradient Descent(24865): loss=3.8668782075856147\n",
      "Stochastic Gradient Descent(24866): loss=3.0293806659912152\n",
      "Stochastic Gradient Descent(24867): loss=1.730867579840092\n",
      "Stochastic Gradient Descent(24868): loss=1.9118173769656115\n",
      "Stochastic Gradient Descent(24869): loss=2.8986810097581133\n",
      "Stochastic Gradient Descent(24870): loss=11.159384700741688\n",
      "Stochastic Gradient Descent(24871): loss=8.328130609809946\n",
      "Stochastic Gradient Descent(24872): loss=6.756869970113831\n",
      "Stochastic Gradient Descent(24873): loss=1.1830719350468009\n",
      "Stochastic Gradient Descent(24874): loss=8.969138297302425\n",
      "Stochastic Gradient Descent(24875): loss=0.9645907350761789\n",
      "Stochastic Gradient Descent(24876): loss=1.7107593340170855\n",
      "Stochastic Gradient Descent(24877): loss=0.3483510251730327\n",
      "Stochastic Gradient Descent(24878): loss=0.049269389526092264\n",
      "Stochastic Gradient Descent(24879): loss=6.416904752040191\n",
      "Stochastic Gradient Descent(24880): loss=4.1632998268859644\n",
      "Stochastic Gradient Descent(24881): loss=3.757996122373789\n",
      "Stochastic Gradient Descent(24882): loss=0.0924396033431296\n",
      "Stochastic Gradient Descent(24883): loss=18.249946848484196\n",
      "Stochastic Gradient Descent(24884): loss=1.655672789868405e-05\n",
      "Stochastic Gradient Descent(24885): loss=3.797494672189513\n",
      "Stochastic Gradient Descent(24886): loss=1.0052316522397438\n",
      "Stochastic Gradient Descent(24887): loss=0.2873971355287873\n",
      "Stochastic Gradient Descent(24888): loss=7.0023303749046\n",
      "Stochastic Gradient Descent(24889): loss=0.000522555218647105\n",
      "Stochastic Gradient Descent(24890): loss=0.5446204749720506\n",
      "Stochastic Gradient Descent(24891): loss=5.600561172221567\n",
      "Stochastic Gradient Descent(24892): loss=5.238658323790806\n",
      "Stochastic Gradient Descent(24893): loss=4.320396346591279\n",
      "Stochastic Gradient Descent(24894): loss=9.663985867986753\n",
      "Stochastic Gradient Descent(24895): loss=0.42419910102852004\n",
      "Stochastic Gradient Descent(24896): loss=0.6520273538801703\n",
      "Stochastic Gradient Descent(24897): loss=1.124225610317469\n",
      "Stochastic Gradient Descent(24898): loss=2.326128878620417\n",
      "Stochastic Gradient Descent(24899): loss=2.9961310090142863\n",
      "Stochastic Gradient Descent(24900): loss=0.03438971115496479\n",
      "Stochastic Gradient Descent(24901): loss=2.4639444862640354\n",
      "Stochastic Gradient Descent(24902): loss=14.716320114356856\n",
      "Stochastic Gradient Descent(24903): loss=4.271748802101329\n",
      "Stochastic Gradient Descent(24904): loss=2.7141779364230647\n",
      "Stochastic Gradient Descent(24905): loss=12.668307317993591\n",
      "Stochastic Gradient Descent(24906): loss=8.692565707011546\n",
      "Stochastic Gradient Descent(24907): loss=6.023039330574135\n",
      "Stochastic Gradient Descent(24908): loss=1.3095998466035623\n",
      "Stochastic Gradient Descent(24909): loss=2.325124963751742\n",
      "Stochastic Gradient Descent(24910): loss=3.0637710421377324\n",
      "Stochastic Gradient Descent(24911): loss=0.0021833403808455905\n",
      "Stochastic Gradient Descent(24912): loss=10.594977027454586\n",
      "Stochastic Gradient Descent(24913): loss=0.6528421316650259\n",
      "Stochastic Gradient Descent(24914): loss=7.135295544296099\n",
      "Stochastic Gradient Descent(24915): loss=1.6563053250541766\n",
      "Stochastic Gradient Descent(24916): loss=0.24214651015954916\n",
      "Stochastic Gradient Descent(24917): loss=3.1235970339487453\n",
      "Stochastic Gradient Descent(24918): loss=0.7743399673717923\n",
      "Stochastic Gradient Descent(24919): loss=0.002321498792836533\n",
      "Stochastic Gradient Descent(24920): loss=14.120679101131218\n",
      "Stochastic Gradient Descent(24921): loss=0.8794332337121903\n",
      "Stochastic Gradient Descent(24922): loss=0.34917686847404583\n",
      "Stochastic Gradient Descent(24923): loss=0.02716477904523509\n",
      "Stochastic Gradient Descent(24924): loss=0.3701951650461405\n",
      "Stochastic Gradient Descent(24925): loss=6.089717125108503\n",
      "Stochastic Gradient Descent(24926): loss=13.621544627320624\n",
      "Stochastic Gradient Descent(24927): loss=23.464387589475127\n",
      "Stochastic Gradient Descent(24928): loss=1.0791242432243575\n",
      "Stochastic Gradient Descent(24929): loss=5.612163146199766\n",
      "Stochastic Gradient Descent(24930): loss=0.872720206167348\n",
      "Stochastic Gradient Descent(24931): loss=0.25695719820462226\n",
      "Stochastic Gradient Descent(24932): loss=8.414839646148655\n",
      "Stochastic Gradient Descent(24933): loss=1.6975760988769584\n",
      "Stochastic Gradient Descent(24934): loss=22.362507710896335\n",
      "Stochastic Gradient Descent(24935): loss=3.197022435099577\n",
      "Stochastic Gradient Descent(24936): loss=0.6971257848403837\n",
      "Stochastic Gradient Descent(24937): loss=0.7082838768058948\n",
      "Stochastic Gradient Descent(24938): loss=2.9872392103589367\n",
      "Stochastic Gradient Descent(24939): loss=5.750624248562928\n",
      "Stochastic Gradient Descent(24940): loss=5.340075892854261\n",
      "Stochastic Gradient Descent(24941): loss=5.179608486255774\n",
      "Stochastic Gradient Descent(24942): loss=7.686478347737738\n",
      "Stochastic Gradient Descent(24943): loss=17.607792520438682\n",
      "Stochastic Gradient Descent(24944): loss=2.864720962380243\n",
      "Stochastic Gradient Descent(24945): loss=0.013291262123470067\n",
      "Stochastic Gradient Descent(24946): loss=7.422528731872924\n",
      "Stochastic Gradient Descent(24947): loss=1.0193116544542222\n",
      "Stochastic Gradient Descent(24948): loss=14.683233007419448\n",
      "Stochastic Gradient Descent(24949): loss=0.09223255867716339\n",
      "Stochastic Gradient Descent(24950): loss=0.7101883501659007\n",
      "Stochastic Gradient Descent(24951): loss=6.9386127847649295\n",
      "Stochastic Gradient Descent(24952): loss=5.572596994188236\n",
      "Stochastic Gradient Descent(24953): loss=2.186785339011721\n",
      "Stochastic Gradient Descent(24954): loss=4.9177789901224\n",
      "Stochastic Gradient Descent(24955): loss=8.82093702151486\n",
      "Stochastic Gradient Descent(24956): loss=0.11262764367096674\n",
      "Stochastic Gradient Descent(24957): loss=13.515234513281882\n",
      "Stochastic Gradient Descent(24958): loss=1.4418473611651987\n",
      "Stochastic Gradient Descent(24959): loss=0.10870934284090185\n",
      "Stochastic Gradient Descent(24960): loss=6.32180887018202\n",
      "Stochastic Gradient Descent(24961): loss=0.23897297829936698\n",
      "Stochastic Gradient Descent(24962): loss=0.01926683841309553\n",
      "Stochastic Gradient Descent(24963): loss=0.6035835517309316\n",
      "Stochastic Gradient Descent(24964): loss=1.4468166600596535\n",
      "Stochastic Gradient Descent(24965): loss=0.2288628542181254\n",
      "Stochastic Gradient Descent(24966): loss=0.0031696401847166765\n",
      "Stochastic Gradient Descent(24967): loss=0.12806908576212314\n",
      "Stochastic Gradient Descent(24968): loss=3.851096826340289\n",
      "Stochastic Gradient Descent(24969): loss=7.299664285181144\n",
      "Stochastic Gradient Descent(24970): loss=0.000918420366761859\n",
      "Stochastic Gradient Descent(24971): loss=0.725054224386003\n",
      "Stochastic Gradient Descent(24972): loss=0.14529826858721967\n",
      "Stochastic Gradient Descent(24973): loss=0.1424127334753715\n",
      "Stochastic Gradient Descent(24974): loss=3.837108139324799\n",
      "Stochastic Gradient Descent(24975): loss=1.916661438835123\n",
      "Stochastic Gradient Descent(24976): loss=0.002116358082542852\n",
      "Stochastic Gradient Descent(24977): loss=1.122487409903084\n",
      "Stochastic Gradient Descent(24978): loss=0.2523747445573252\n",
      "Stochastic Gradient Descent(24979): loss=5.604197380627079\n",
      "Stochastic Gradient Descent(24980): loss=2.130610059354547\n",
      "Stochastic Gradient Descent(24981): loss=0.9803092740802932\n",
      "Stochastic Gradient Descent(24982): loss=3.4332060797467214\n",
      "Stochastic Gradient Descent(24983): loss=17.602042711995942\n",
      "Stochastic Gradient Descent(24984): loss=0.07258573617024876\n",
      "Stochastic Gradient Descent(24985): loss=2.454501701745248\n",
      "Stochastic Gradient Descent(24986): loss=1.1794293907469395\n",
      "Stochastic Gradient Descent(24987): loss=6.388499473987107\n",
      "Stochastic Gradient Descent(24988): loss=1.5247895417332658\n",
      "Stochastic Gradient Descent(24989): loss=6.521313594380014e-05\n",
      "Stochastic Gradient Descent(24990): loss=2.2873309559725885\n",
      "Stochastic Gradient Descent(24991): loss=1.6189587683672115\n",
      "Stochastic Gradient Descent(24992): loss=3.3576312396258765\n",
      "Stochastic Gradient Descent(24993): loss=7.050584146234625\n",
      "Stochastic Gradient Descent(24994): loss=7.91234087335564\n",
      "Stochastic Gradient Descent(24995): loss=0.8945820324120543\n",
      "Stochastic Gradient Descent(24996): loss=2.0483206899838797\n",
      "Stochastic Gradient Descent(24997): loss=7.891943419636952\n",
      "Stochastic Gradient Descent(24998): loss=3.712728099548145\n",
      "Stochastic Gradient Descent(24999): loss=3.6301903965124165\n",
      "Stochastic Gradient Descent(25000): loss=0.7674602016885319\n",
      "Stochastic Gradient Descent(25001): loss=1.8654821060640954\n",
      "Stochastic Gradient Descent(25002): loss=7.048302023538737\n",
      "Stochastic Gradient Descent(25003): loss=0.05722198345936013\n",
      "Stochastic Gradient Descent(25004): loss=10.409491708905017\n",
      "Stochastic Gradient Descent(25005): loss=1.6236537085885194\n",
      "Stochastic Gradient Descent(25006): loss=2.687362998684421\n",
      "Stochastic Gradient Descent(25007): loss=1.4729829177793319\n",
      "Stochastic Gradient Descent(25008): loss=0.11254437135200966\n",
      "Stochastic Gradient Descent(25009): loss=0.4339219849302915\n",
      "Stochastic Gradient Descent(25010): loss=0.23679826048753908\n",
      "Stochastic Gradient Descent(25011): loss=1.0615694120557158\n",
      "Stochastic Gradient Descent(25012): loss=8.477180053181415\n",
      "Stochastic Gradient Descent(25013): loss=0.23296785500580475\n",
      "Stochastic Gradient Descent(25014): loss=10.699398339916188\n",
      "Stochastic Gradient Descent(25015): loss=1.1369064305569645\n",
      "Stochastic Gradient Descent(25016): loss=42.31091103066482\n",
      "Stochastic Gradient Descent(25017): loss=1.0667007115345437\n",
      "Stochastic Gradient Descent(25018): loss=25.203985531540575\n",
      "Stochastic Gradient Descent(25019): loss=2.338866200122157\n",
      "Stochastic Gradient Descent(25020): loss=12.749358499155532\n",
      "Stochastic Gradient Descent(25021): loss=0.43231056093228504\n",
      "Stochastic Gradient Descent(25022): loss=0.012512859413312191\n",
      "Stochastic Gradient Descent(25023): loss=0.4576287630949294\n",
      "Stochastic Gradient Descent(25024): loss=29.654114783534066\n",
      "Stochastic Gradient Descent(25025): loss=19.44516064813571\n",
      "Stochastic Gradient Descent(25026): loss=18.559886906874183\n",
      "Stochastic Gradient Descent(25027): loss=3.152749079421692\n",
      "Stochastic Gradient Descent(25028): loss=0.010499934758965827\n",
      "Stochastic Gradient Descent(25029): loss=1.9832067910279354\n",
      "Stochastic Gradient Descent(25030): loss=0.10876214161845915\n",
      "Stochastic Gradient Descent(25031): loss=1.4819794175173615\n",
      "Stochastic Gradient Descent(25032): loss=2.1102454776360475\n",
      "Stochastic Gradient Descent(25033): loss=0.022136466968859903\n",
      "Stochastic Gradient Descent(25034): loss=10.79733476086623\n",
      "Stochastic Gradient Descent(25035): loss=5.086798343336619\n",
      "Stochastic Gradient Descent(25036): loss=19.271963314037876\n",
      "Stochastic Gradient Descent(25037): loss=0.0013675184592245255\n",
      "Stochastic Gradient Descent(25038): loss=1.598321178437018\n",
      "Stochastic Gradient Descent(25039): loss=6.955551694390429\n",
      "Stochastic Gradient Descent(25040): loss=5.792233071075616\n",
      "Stochastic Gradient Descent(25041): loss=0.05336096075784818\n",
      "Stochastic Gradient Descent(25042): loss=0.041683037525231066\n",
      "Stochastic Gradient Descent(25043): loss=4.254310195595171\n",
      "Stochastic Gradient Descent(25044): loss=7.401610607757201\n",
      "Stochastic Gradient Descent(25045): loss=14.171336945645436\n",
      "Stochastic Gradient Descent(25046): loss=3.072421930098623\n",
      "Stochastic Gradient Descent(25047): loss=2.0560222142717497\n",
      "Stochastic Gradient Descent(25048): loss=2.8577062780506917\n",
      "Stochastic Gradient Descent(25049): loss=0.010704782202837219\n",
      "Stochastic Gradient Descent(25050): loss=2.2279173019703156\n",
      "Stochastic Gradient Descent(25051): loss=27.876621218105694\n",
      "Stochastic Gradient Descent(25052): loss=1.150496285235662\n",
      "Stochastic Gradient Descent(25053): loss=2.505975699795791\n",
      "Stochastic Gradient Descent(25054): loss=1.7343533889340028\n",
      "Stochastic Gradient Descent(25055): loss=4.5179604939570694\n",
      "Stochastic Gradient Descent(25056): loss=1.4651207476644228\n",
      "Stochastic Gradient Descent(25057): loss=15.056155829224164\n",
      "Stochastic Gradient Descent(25058): loss=3.5456097002196323\n",
      "Stochastic Gradient Descent(25059): loss=25.037864579205724\n",
      "Stochastic Gradient Descent(25060): loss=16.68307261298948\n",
      "Stochastic Gradient Descent(25061): loss=0.057386688638084736\n",
      "Stochastic Gradient Descent(25062): loss=2.0183045277580556\n",
      "Stochastic Gradient Descent(25063): loss=80.29660433308597\n",
      "Stochastic Gradient Descent(25064): loss=9.939130988136181\n",
      "Stochastic Gradient Descent(25065): loss=0.05362948789319383\n",
      "Stochastic Gradient Descent(25066): loss=0.24105215865749097\n",
      "Stochastic Gradient Descent(25067): loss=1.9285055458482019\n",
      "Stochastic Gradient Descent(25068): loss=2.878125581275755\n",
      "Stochastic Gradient Descent(25069): loss=10.00798247491879\n",
      "Stochastic Gradient Descent(25070): loss=9.910886170351446\n",
      "Stochastic Gradient Descent(25071): loss=0.05305747447632074\n",
      "Stochastic Gradient Descent(25072): loss=128.9670612434391\n",
      "Stochastic Gradient Descent(25073): loss=26.32843387584753\n",
      "Stochastic Gradient Descent(25074): loss=12.26387493523877\n",
      "Stochastic Gradient Descent(25075): loss=182.74945145810509\n",
      "Stochastic Gradient Descent(25076): loss=7.5349322089514725\n",
      "Stochastic Gradient Descent(25077): loss=4.99674885636295\n",
      "Stochastic Gradient Descent(25078): loss=1.8903007837054389\n",
      "Stochastic Gradient Descent(25079): loss=4.670156942254094\n",
      "Stochastic Gradient Descent(25080): loss=109.7419311866918\n",
      "Stochastic Gradient Descent(25081): loss=0.07883394449124762\n",
      "Stochastic Gradient Descent(25082): loss=0.04340945793610457\n",
      "Stochastic Gradient Descent(25083): loss=3.180521321208098\n",
      "Stochastic Gradient Descent(25084): loss=15.817283971856133\n",
      "Stochastic Gradient Descent(25085): loss=14.278445979664342\n",
      "Stochastic Gradient Descent(25086): loss=0.04818135562773832\n",
      "Stochastic Gradient Descent(25087): loss=3.1051056871355156\n",
      "Stochastic Gradient Descent(25088): loss=22.280565543661762\n",
      "Stochastic Gradient Descent(25089): loss=5.141696501768746\n",
      "Stochastic Gradient Descent(25090): loss=14.823161120324896\n",
      "Stochastic Gradient Descent(25091): loss=4.49907844068066\n",
      "Stochastic Gradient Descent(25092): loss=13.00302437277545\n",
      "Stochastic Gradient Descent(25093): loss=0.7261682623321342\n",
      "Stochastic Gradient Descent(25094): loss=9.795245991395234\n",
      "Stochastic Gradient Descent(25095): loss=1.9451079652346428\n",
      "Stochastic Gradient Descent(25096): loss=9.89141572193287\n",
      "Stochastic Gradient Descent(25097): loss=0.22025145818829028\n",
      "Stochastic Gradient Descent(25098): loss=1.1503124673523517\n",
      "Stochastic Gradient Descent(25099): loss=10.324907811707948\n",
      "Stochastic Gradient Descent(25100): loss=0.7103219872483447\n",
      "Stochastic Gradient Descent(25101): loss=8.361614325785466\n",
      "Stochastic Gradient Descent(25102): loss=7.752023850394027\n",
      "Stochastic Gradient Descent(25103): loss=2.002652127031846\n",
      "Stochastic Gradient Descent(25104): loss=3.907226952284999\n",
      "Stochastic Gradient Descent(25105): loss=0.01995788228436353\n",
      "Stochastic Gradient Descent(25106): loss=0.5513848359062391\n",
      "Stochastic Gradient Descent(25107): loss=3.943491153934992\n",
      "Stochastic Gradient Descent(25108): loss=2.9141529852306327\n",
      "Stochastic Gradient Descent(25109): loss=1.066025029216561\n",
      "Stochastic Gradient Descent(25110): loss=18.558130943264082\n",
      "Stochastic Gradient Descent(25111): loss=1.4854933137350002\n",
      "Stochastic Gradient Descent(25112): loss=11.50857977878936\n",
      "Stochastic Gradient Descent(25113): loss=1.3700760958548865\n",
      "Stochastic Gradient Descent(25114): loss=1.4084946503116813\n",
      "Stochastic Gradient Descent(25115): loss=0.027119707872336026\n",
      "Stochastic Gradient Descent(25116): loss=6.556501746137989\n",
      "Stochastic Gradient Descent(25117): loss=3.363284201966915\n",
      "Stochastic Gradient Descent(25118): loss=8.230029814767368\n",
      "Stochastic Gradient Descent(25119): loss=0.7704350665598855\n",
      "Stochastic Gradient Descent(25120): loss=4.601116312627156\n",
      "Stochastic Gradient Descent(25121): loss=0.25317859098163886\n",
      "Stochastic Gradient Descent(25122): loss=0.21747634677509417\n",
      "Stochastic Gradient Descent(25123): loss=0.46105131499587476\n",
      "Stochastic Gradient Descent(25124): loss=0.35336712819967564\n",
      "Stochastic Gradient Descent(25125): loss=0.057201719835701104\n",
      "Stochastic Gradient Descent(25126): loss=0.2179997194733682\n",
      "Stochastic Gradient Descent(25127): loss=2.529250405489761\n",
      "Stochastic Gradient Descent(25128): loss=7.855221306606141\n",
      "Stochastic Gradient Descent(25129): loss=0.8024593124530299\n",
      "Stochastic Gradient Descent(25130): loss=0.9821607179361352\n",
      "Stochastic Gradient Descent(25131): loss=10.834873568258928\n",
      "Stochastic Gradient Descent(25132): loss=0.4005584487675463\n",
      "Stochastic Gradient Descent(25133): loss=1.2396490103199913\n",
      "Stochastic Gradient Descent(25134): loss=2.7649759606595814\n",
      "Stochastic Gradient Descent(25135): loss=1.1520617447332069\n",
      "Stochastic Gradient Descent(25136): loss=15.95758823487434\n",
      "Stochastic Gradient Descent(25137): loss=28.792284960566413\n",
      "Stochastic Gradient Descent(25138): loss=10.279844562082994\n",
      "Stochastic Gradient Descent(25139): loss=0.15579754002107582\n",
      "Stochastic Gradient Descent(25140): loss=45.4777771667579\n",
      "Stochastic Gradient Descent(25141): loss=4.464440710460782\n",
      "Stochastic Gradient Descent(25142): loss=0.019338558247115684\n",
      "Stochastic Gradient Descent(25143): loss=21.2443911983302\n",
      "Stochastic Gradient Descent(25144): loss=7.793515598142069\n",
      "Stochastic Gradient Descent(25145): loss=25.500531967214602\n",
      "Stochastic Gradient Descent(25146): loss=0.6801828338378201\n",
      "Stochastic Gradient Descent(25147): loss=3.8290625357904218\n",
      "Stochastic Gradient Descent(25148): loss=3.0417368791281074\n",
      "Stochastic Gradient Descent(25149): loss=5.8137238319914015\n",
      "Stochastic Gradient Descent(25150): loss=4.785821492751577\n",
      "Stochastic Gradient Descent(25151): loss=7.452216514315174\n",
      "Stochastic Gradient Descent(25152): loss=1.5634845282197225\n",
      "Stochastic Gradient Descent(25153): loss=0.9539646489534689\n",
      "Stochastic Gradient Descent(25154): loss=0.48440590099047265\n",
      "Stochastic Gradient Descent(25155): loss=3.9047814699367063\n",
      "Stochastic Gradient Descent(25156): loss=0.004842170966807457\n",
      "Stochastic Gradient Descent(25157): loss=8.305002390657313\n",
      "Stochastic Gradient Descent(25158): loss=4.229103354891742\n",
      "Stochastic Gradient Descent(25159): loss=0.005475819366724174\n",
      "Stochastic Gradient Descent(25160): loss=8.486419404944549\n",
      "Stochastic Gradient Descent(25161): loss=2.2532969694750857\n",
      "Stochastic Gradient Descent(25162): loss=1.8237097173686372\n",
      "Stochastic Gradient Descent(25163): loss=23.276882604543857\n",
      "Stochastic Gradient Descent(25164): loss=1.3840995473379418\n",
      "Stochastic Gradient Descent(25165): loss=7.767250918579006\n",
      "Stochastic Gradient Descent(25166): loss=1.9264516864986934\n",
      "Stochastic Gradient Descent(25167): loss=0.3028008225389418\n",
      "Stochastic Gradient Descent(25168): loss=0.38118764384358483\n",
      "Stochastic Gradient Descent(25169): loss=1.2361202311726078\n",
      "Stochastic Gradient Descent(25170): loss=0.06948722805573428\n",
      "Stochastic Gradient Descent(25171): loss=13.09446856246758\n",
      "Stochastic Gradient Descent(25172): loss=6.831348037670073\n",
      "Stochastic Gradient Descent(25173): loss=18.87473958753125\n",
      "Stochastic Gradient Descent(25174): loss=4.839723233876734\n",
      "Stochastic Gradient Descent(25175): loss=0.29725277452109194\n",
      "Stochastic Gradient Descent(25176): loss=0.047612974191164247\n",
      "Stochastic Gradient Descent(25177): loss=2.079727056291347\n",
      "Stochastic Gradient Descent(25178): loss=1.5425129628376288\n",
      "Stochastic Gradient Descent(25179): loss=10.782452865253981\n",
      "Stochastic Gradient Descent(25180): loss=0.4955466531465685\n",
      "Stochastic Gradient Descent(25181): loss=5.48904935628189\n",
      "Stochastic Gradient Descent(25182): loss=7.169773708961949\n",
      "Stochastic Gradient Descent(25183): loss=3.065604880731785\n",
      "Stochastic Gradient Descent(25184): loss=4.208688255801182\n",
      "Stochastic Gradient Descent(25185): loss=15.71690263663607\n",
      "Stochastic Gradient Descent(25186): loss=37.6906089974421\n",
      "Stochastic Gradient Descent(25187): loss=2.5158681396185516\n",
      "Stochastic Gradient Descent(25188): loss=1.2600836930340689\n",
      "Stochastic Gradient Descent(25189): loss=3.5782919157380833\n",
      "Stochastic Gradient Descent(25190): loss=0.04440297031833245\n",
      "Stochastic Gradient Descent(25191): loss=0.0008966131897607043\n",
      "Stochastic Gradient Descent(25192): loss=3.9484787787364644\n",
      "Stochastic Gradient Descent(25193): loss=0.016697301319931225\n",
      "Stochastic Gradient Descent(25194): loss=0.36351859740396975\n",
      "Stochastic Gradient Descent(25195): loss=1.3278888457682954\n",
      "Stochastic Gradient Descent(25196): loss=0.19311171346923675\n",
      "Stochastic Gradient Descent(25197): loss=6.249019108458046\n",
      "Stochastic Gradient Descent(25198): loss=1.4335648356135169\n",
      "Stochastic Gradient Descent(25199): loss=0.15685474558498777\n",
      "Stochastic Gradient Descent(25200): loss=1.4042556374573978\n",
      "Stochastic Gradient Descent(25201): loss=0.7215961472442028\n",
      "Stochastic Gradient Descent(25202): loss=0.8485377315091505\n",
      "Stochastic Gradient Descent(25203): loss=14.613700476981732\n",
      "Stochastic Gradient Descent(25204): loss=1.93409643627537\n",
      "Stochastic Gradient Descent(25205): loss=1.03603144207073\n",
      "Stochastic Gradient Descent(25206): loss=5.252364217147798\n",
      "Stochastic Gradient Descent(25207): loss=1.7906315965020907\n",
      "Stochastic Gradient Descent(25208): loss=0.0004827029524987507\n",
      "Stochastic Gradient Descent(25209): loss=8.388846050289152\n",
      "Stochastic Gradient Descent(25210): loss=0.012176700518192514\n",
      "Stochastic Gradient Descent(25211): loss=0.4079761265708464\n",
      "Stochastic Gradient Descent(25212): loss=0.20005684275112007\n",
      "Stochastic Gradient Descent(25213): loss=13.126118518564672\n",
      "Stochastic Gradient Descent(25214): loss=30.037544579410785\n",
      "Stochastic Gradient Descent(25215): loss=0.07443159446757701\n",
      "Stochastic Gradient Descent(25216): loss=3.7850747716574977\n",
      "Stochastic Gradient Descent(25217): loss=0.22580060730292295\n",
      "Stochastic Gradient Descent(25218): loss=0.1675129501726164\n",
      "Stochastic Gradient Descent(25219): loss=7.225022802536552\n",
      "Stochastic Gradient Descent(25220): loss=0.3067309835372839\n",
      "Stochastic Gradient Descent(25221): loss=0.19220731470909427\n",
      "Stochastic Gradient Descent(25222): loss=10.470978668390092\n",
      "Stochastic Gradient Descent(25223): loss=0.05913410856943084\n",
      "Stochastic Gradient Descent(25224): loss=0.1923163809148427\n",
      "Stochastic Gradient Descent(25225): loss=0.03932937224010602\n",
      "Stochastic Gradient Descent(25226): loss=0.5938553351336899\n",
      "Stochastic Gradient Descent(25227): loss=0.723853702065363\n",
      "Stochastic Gradient Descent(25228): loss=1.8827105386595266\n",
      "Stochastic Gradient Descent(25229): loss=0.6832732191285956\n",
      "Stochastic Gradient Descent(25230): loss=0.24397985114144824\n",
      "Stochastic Gradient Descent(25231): loss=1.7963058432963164\n",
      "Stochastic Gradient Descent(25232): loss=0.15985375233976315\n",
      "Stochastic Gradient Descent(25233): loss=1.0139329091620308\n",
      "Stochastic Gradient Descent(25234): loss=0.7945468451554045\n",
      "Stochastic Gradient Descent(25235): loss=0.29790409358165076\n",
      "Stochastic Gradient Descent(25236): loss=14.056557074573208\n",
      "Stochastic Gradient Descent(25237): loss=9.144941294736405\n",
      "Stochastic Gradient Descent(25238): loss=1.3722438210159549\n",
      "Stochastic Gradient Descent(25239): loss=0.013905602236869806\n",
      "Stochastic Gradient Descent(25240): loss=5.160463968210631\n",
      "Stochastic Gradient Descent(25241): loss=0.22817281281233556\n",
      "Stochastic Gradient Descent(25242): loss=2.4616108237045253\n",
      "Stochastic Gradient Descent(25243): loss=0.1372635303095252\n",
      "Stochastic Gradient Descent(25244): loss=0.16546695929608068\n",
      "Stochastic Gradient Descent(25245): loss=10.927032631309348\n",
      "Stochastic Gradient Descent(25246): loss=0.030648441126784044\n",
      "Stochastic Gradient Descent(25247): loss=1.0109184876852988\n",
      "Stochastic Gradient Descent(25248): loss=2.097579052918335\n",
      "Stochastic Gradient Descent(25249): loss=3.3621151790909405\n",
      "Stochastic Gradient Descent(25250): loss=1.4627297554676566\n",
      "Stochastic Gradient Descent(25251): loss=12.938252852554143\n",
      "Stochastic Gradient Descent(25252): loss=3.246722951936708\n",
      "Stochastic Gradient Descent(25253): loss=10.316729401555827\n",
      "Stochastic Gradient Descent(25254): loss=3.0862373190701455\n",
      "Stochastic Gradient Descent(25255): loss=1.089785538099148\n",
      "Stochastic Gradient Descent(25256): loss=2.2418872120728803\n",
      "Stochastic Gradient Descent(25257): loss=1.8850730948394216\n",
      "Stochastic Gradient Descent(25258): loss=23.36682672849013\n",
      "Stochastic Gradient Descent(25259): loss=0.5045481121119714\n",
      "Stochastic Gradient Descent(25260): loss=0.8893392003835323\n",
      "Stochastic Gradient Descent(25261): loss=0.03229854711554929\n",
      "Stochastic Gradient Descent(25262): loss=4.09808960104374\n",
      "Stochastic Gradient Descent(25263): loss=0.6642812528042068\n",
      "Stochastic Gradient Descent(25264): loss=1.6570750706924788\n",
      "Stochastic Gradient Descent(25265): loss=0.16336461718164239\n",
      "Stochastic Gradient Descent(25266): loss=0.22458672494394102\n",
      "Stochastic Gradient Descent(25267): loss=0.6647771057925431\n",
      "Stochastic Gradient Descent(25268): loss=0.6661589003246724\n",
      "Stochastic Gradient Descent(25269): loss=7.072188901238105\n",
      "Stochastic Gradient Descent(25270): loss=1.9576246271763966\n",
      "Stochastic Gradient Descent(25271): loss=8.359960530349957\n",
      "Stochastic Gradient Descent(25272): loss=9.457602275379166\n",
      "Stochastic Gradient Descent(25273): loss=5.799981726860764\n",
      "Stochastic Gradient Descent(25274): loss=0.07558875286531273\n",
      "Stochastic Gradient Descent(25275): loss=0.9817896151206454\n",
      "Stochastic Gradient Descent(25276): loss=2.6094052701814854\n",
      "Stochastic Gradient Descent(25277): loss=3.457613842633641\n",
      "Stochastic Gradient Descent(25278): loss=0.22041586443220862\n",
      "Stochastic Gradient Descent(25279): loss=3.534019594556182\n",
      "Stochastic Gradient Descent(25280): loss=0.42524032255904093\n",
      "Stochastic Gradient Descent(25281): loss=0.8593459774733964\n",
      "Stochastic Gradient Descent(25282): loss=25.07927781531825\n",
      "Stochastic Gradient Descent(25283): loss=0.577561948713156\n",
      "Stochastic Gradient Descent(25284): loss=3.485185548850471\n",
      "Stochastic Gradient Descent(25285): loss=0.7011845113129014\n",
      "Stochastic Gradient Descent(25286): loss=1.8388286711417396\n",
      "Stochastic Gradient Descent(25287): loss=1.3863864235177517\n",
      "Stochastic Gradient Descent(25288): loss=0.042062133326063615\n",
      "Stochastic Gradient Descent(25289): loss=0.0665234264920855\n",
      "Stochastic Gradient Descent(25290): loss=0.003045893239797587\n",
      "Stochastic Gradient Descent(25291): loss=1.0756994203460146\n",
      "Stochastic Gradient Descent(25292): loss=1.281524202831583\n",
      "Stochastic Gradient Descent(25293): loss=13.591540952211105\n",
      "Stochastic Gradient Descent(25294): loss=6.563483621511234\n",
      "Stochastic Gradient Descent(25295): loss=0.9292507584585169\n",
      "Stochastic Gradient Descent(25296): loss=16.739196848777436\n",
      "Stochastic Gradient Descent(25297): loss=10.89679633179726\n",
      "Stochastic Gradient Descent(25298): loss=0.00044161994809537096\n",
      "Stochastic Gradient Descent(25299): loss=0.5039446755125699\n",
      "Stochastic Gradient Descent(25300): loss=0.0040493970046381455\n",
      "Stochastic Gradient Descent(25301): loss=4.104148090254042\n",
      "Stochastic Gradient Descent(25302): loss=1.7335193332226115\n",
      "Stochastic Gradient Descent(25303): loss=0.794980606097175\n",
      "Stochastic Gradient Descent(25304): loss=12.378161826981932\n",
      "Stochastic Gradient Descent(25305): loss=13.457137164072485\n",
      "Stochastic Gradient Descent(25306): loss=2.498308171943675\n",
      "Stochastic Gradient Descent(25307): loss=13.207097713516182\n",
      "Stochastic Gradient Descent(25308): loss=9.834311354202026\n",
      "Stochastic Gradient Descent(25309): loss=1.663126325876973\n",
      "Stochastic Gradient Descent(25310): loss=0.48034046080364523\n",
      "Stochastic Gradient Descent(25311): loss=2.0524541178704996\n",
      "Stochastic Gradient Descent(25312): loss=1.4704696801125443\n",
      "Stochastic Gradient Descent(25313): loss=0.1972352065723612\n",
      "Stochastic Gradient Descent(25314): loss=13.030354127044687\n",
      "Stochastic Gradient Descent(25315): loss=0.0850102579724351\n",
      "Stochastic Gradient Descent(25316): loss=0.8937137788444018\n",
      "Stochastic Gradient Descent(25317): loss=9.386593091887713\n",
      "Stochastic Gradient Descent(25318): loss=7.114489569703457\n",
      "Stochastic Gradient Descent(25319): loss=1.373924138554964\n",
      "Stochastic Gradient Descent(25320): loss=0.7311474653181758\n",
      "Stochastic Gradient Descent(25321): loss=0.06866173440324036\n",
      "Stochastic Gradient Descent(25322): loss=15.160116561388126\n",
      "Stochastic Gradient Descent(25323): loss=12.714448533633552\n",
      "Stochastic Gradient Descent(25324): loss=15.563386944035997\n",
      "Stochastic Gradient Descent(25325): loss=0.04137104078799103\n",
      "Stochastic Gradient Descent(25326): loss=0.9686478223987499\n",
      "Stochastic Gradient Descent(25327): loss=9.89975053855556\n",
      "Stochastic Gradient Descent(25328): loss=5.428724975154384\n",
      "Stochastic Gradient Descent(25329): loss=8.375588821147746\n",
      "Stochastic Gradient Descent(25330): loss=0.3751394136888956\n",
      "Stochastic Gradient Descent(25331): loss=37.09582547164097\n",
      "Stochastic Gradient Descent(25332): loss=6.963237325219632\n",
      "Stochastic Gradient Descent(25333): loss=6.417181430015178\n",
      "Stochastic Gradient Descent(25334): loss=0.4503440951046523\n",
      "Stochastic Gradient Descent(25335): loss=0.0015911249102462954\n",
      "Stochastic Gradient Descent(25336): loss=3.229293346390184\n",
      "Stochastic Gradient Descent(25337): loss=1.537679937077694\n",
      "Stochastic Gradient Descent(25338): loss=0.1548716634459352\n",
      "Stochastic Gradient Descent(25339): loss=1.185772809425642\n",
      "Stochastic Gradient Descent(25340): loss=15.77018784602565\n",
      "Stochastic Gradient Descent(25341): loss=9.127963430808913\n",
      "Stochastic Gradient Descent(25342): loss=4.381778401149797\n",
      "Stochastic Gradient Descent(25343): loss=5.32681584916727\n",
      "Stochastic Gradient Descent(25344): loss=22.262811606748333\n",
      "Stochastic Gradient Descent(25345): loss=0.08325344608105828\n",
      "Stochastic Gradient Descent(25346): loss=0.13793182145557642\n",
      "Stochastic Gradient Descent(25347): loss=14.023521539715944\n",
      "Stochastic Gradient Descent(25348): loss=1.2619278255723676e-05\n",
      "Stochastic Gradient Descent(25349): loss=2.246012582645912\n",
      "Stochastic Gradient Descent(25350): loss=2.9086540461933956\n",
      "Stochastic Gradient Descent(25351): loss=3.0088290901270898\n",
      "Stochastic Gradient Descent(25352): loss=5.3167287207574345\n",
      "Stochastic Gradient Descent(25353): loss=0.30409228608912015\n",
      "Stochastic Gradient Descent(25354): loss=0.00885530468449653\n",
      "Stochastic Gradient Descent(25355): loss=1.4530974184041217\n",
      "Stochastic Gradient Descent(25356): loss=10.376591919938814\n",
      "Stochastic Gradient Descent(25357): loss=1.3543008712969582\n",
      "Stochastic Gradient Descent(25358): loss=11.112052566543579\n",
      "Stochastic Gradient Descent(25359): loss=1.3400898628664697\n",
      "Stochastic Gradient Descent(25360): loss=5.34739960843335\n",
      "Stochastic Gradient Descent(25361): loss=12.43056753735112\n",
      "Stochastic Gradient Descent(25362): loss=7.39721595260335\n",
      "Stochastic Gradient Descent(25363): loss=0.0017710177148438854\n",
      "Stochastic Gradient Descent(25364): loss=19.816246255027107\n",
      "Stochastic Gradient Descent(25365): loss=6.534679641180007\n",
      "Stochastic Gradient Descent(25366): loss=1.3653357305580918\n",
      "Stochastic Gradient Descent(25367): loss=0.3694535271115216\n",
      "Stochastic Gradient Descent(25368): loss=5.253459667768672\n",
      "Stochastic Gradient Descent(25369): loss=8.616701205854664\n",
      "Stochastic Gradient Descent(25370): loss=4.397067154109299\n",
      "Stochastic Gradient Descent(25371): loss=1.4924943999843585\n",
      "Stochastic Gradient Descent(25372): loss=0.37242727168128653\n",
      "Stochastic Gradient Descent(25373): loss=2.695641543453525\n",
      "Stochastic Gradient Descent(25374): loss=9.136185386625925\n",
      "Stochastic Gradient Descent(25375): loss=7.816424374896679\n",
      "Stochastic Gradient Descent(25376): loss=4.181764494618401\n",
      "Stochastic Gradient Descent(25377): loss=20.333738015624643\n",
      "Stochastic Gradient Descent(25378): loss=17.756608685238113\n",
      "Stochastic Gradient Descent(25379): loss=0.25992092425278934\n",
      "Stochastic Gradient Descent(25380): loss=7.950478762369038\n",
      "Stochastic Gradient Descent(25381): loss=5.4522771203087546\n",
      "Stochastic Gradient Descent(25382): loss=0.6635627633173012\n",
      "Stochastic Gradient Descent(25383): loss=3.1418651416349515\n",
      "Stochastic Gradient Descent(25384): loss=0.4505967786737197\n",
      "Stochastic Gradient Descent(25385): loss=4.735646886356602\n",
      "Stochastic Gradient Descent(25386): loss=0.2778213917176263\n",
      "Stochastic Gradient Descent(25387): loss=0.5525553118891458\n",
      "Stochastic Gradient Descent(25388): loss=3.0531648579014643\n",
      "Stochastic Gradient Descent(25389): loss=3.4838329790446805\n",
      "Stochastic Gradient Descent(25390): loss=0.2779582454528451\n",
      "Stochastic Gradient Descent(25391): loss=1.30657152402553\n",
      "Stochastic Gradient Descent(25392): loss=9.75656266920105\n",
      "Stochastic Gradient Descent(25393): loss=2.6686914881764783\n",
      "Stochastic Gradient Descent(25394): loss=1.3642799818103992\n",
      "Stochastic Gradient Descent(25395): loss=0.3842891870937305\n",
      "Stochastic Gradient Descent(25396): loss=6.728344017629643\n",
      "Stochastic Gradient Descent(25397): loss=0.15643955437893672\n",
      "Stochastic Gradient Descent(25398): loss=12.490152282649213\n",
      "Stochastic Gradient Descent(25399): loss=0.29539369064531573\n",
      "Stochastic Gradient Descent(25400): loss=0.010946929335911144\n",
      "Stochastic Gradient Descent(25401): loss=1.891330748760076\n",
      "Stochastic Gradient Descent(25402): loss=0.22597595314614974\n",
      "Stochastic Gradient Descent(25403): loss=0.06381993904568896\n",
      "Stochastic Gradient Descent(25404): loss=2.3609635773475297\n",
      "Stochastic Gradient Descent(25405): loss=24.798519985881217\n",
      "Stochastic Gradient Descent(25406): loss=3.140682408690569\n",
      "Stochastic Gradient Descent(25407): loss=2.2525261729912156\n",
      "Stochastic Gradient Descent(25408): loss=16.403638803382368\n",
      "Stochastic Gradient Descent(25409): loss=14.052215883374215\n",
      "Stochastic Gradient Descent(25410): loss=0.4724651882822941\n",
      "Stochastic Gradient Descent(25411): loss=2.6186186242019613\n",
      "Stochastic Gradient Descent(25412): loss=13.453692065974424\n",
      "Stochastic Gradient Descent(25413): loss=8.114408787004919\n",
      "Stochastic Gradient Descent(25414): loss=0.8662042053440754\n",
      "Stochastic Gradient Descent(25415): loss=0.013436319415186046\n",
      "Stochastic Gradient Descent(25416): loss=1.3831949428432941\n",
      "Stochastic Gradient Descent(25417): loss=2.062450761537288\n",
      "Stochastic Gradient Descent(25418): loss=2.1436365809097424\n",
      "Stochastic Gradient Descent(25419): loss=21.287420783330507\n",
      "Stochastic Gradient Descent(25420): loss=10.542271730085364\n",
      "Stochastic Gradient Descent(25421): loss=17.539204378570616\n",
      "Stochastic Gradient Descent(25422): loss=13.288060360724387\n",
      "Stochastic Gradient Descent(25423): loss=1.0551260586843327\n",
      "Stochastic Gradient Descent(25424): loss=0.36799138019347233\n",
      "Stochastic Gradient Descent(25425): loss=14.049571172075247\n",
      "Stochastic Gradient Descent(25426): loss=0.3002148221127772\n",
      "Stochastic Gradient Descent(25427): loss=0.4578642095028836\n",
      "Stochastic Gradient Descent(25428): loss=0.3168028179340919\n",
      "Stochastic Gradient Descent(25429): loss=18.842377540784334\n",
      "Stochastic Gradient Descent(25430): loss=2.264273615824536\n",
      "Stochastic Gradient Descent(25431): loss=5.063202957620068\n",
      "Stochastic Gradient Descent(25432): loss=0.8333523043404966\n",
      "Stochastic Gradient Descent(25433): loss=0.44324905200176723\n",
      "Stochastic Gradient Descent(25434): loss=1.0766602905282818\n",
      "Stochastic Gradient Descent(25435): loss=2.9861406656848266\n",
      "Stochastic Gradient Descent(25436): loss=18.155694610619104\n",
      "Stochastic Gradient Descent(25437): loss=10.18777431762691\n",
      "Stochastic Gradient Descent(25438): loss=10.984934203267384\n",
      "Stochastic Gradient Descent(25439): loss=10.373401718602864\n",
      "Stochastic Gradient Descent(25440): loss=1.386718007939991\n",
      "Stochastic Gradient Descent(25441): loss=11.374549002584919\n",
      "Stochastic Gradient Descent(25442): loss=1.7886598276667822\n",
      "Stochastic Gradient Descent(25443): loss=0.5368368301250185\n",
      "Stochastic Gradient Descent(25444): loss=0.36703735660721304\n",
      "Stochastic Gradient Descent(25445): loss=14.002468384460169\n",
      "Stochastic Gradient Descent(25446): loss=28.39153030216804\n",
      "Stochastic Gradient Descent(25447): loss=2.0093032763569627\n",
      "Stochastic Gradient Descent(25448): loss=11.808865314413909\n",
      "Stochastic Gradient Descent(25449): loss=0.5796130876055264\n",
      "Stochastic Gradient Descent(25450): loss=13.020845908072529\n",
      "Stochastic Gradient Descent(25451): loss=1.5073545144255203\n",
      "Stochastic Gradient Descent(25452): loss=0.7158123966930402\n",
      "Stochastic Gradient Descent(25453): loss=0.6778349707998946\n",
      "Stochastic Gradient Descent(25454): loss=0.16696845234410432\n",
      "Stochastic Gradient Descent(25455): loss=0.8592181633840964\n",
      "Stochastic Gradient Descent(25456): loss=2.628844356966065\n",
      "Stochastic Gradient Descent(25457): loss=12.107798678437314\n",
      "Stochastic Gradient Descent(25458): loss=3.456035319527749\n",
      "Stochastic Gradient Descent(25459): loss=3.389344496939026\n",
      "Stochastic Gradient Descent(25460): loss=4.2880500292955395\n",
      "Stochastic Gradient Descent(25461): loss=12.234260215297725\n",
      "Stochastic Gradient Descent(25462): loss=0.03581463089695108\n",
      "Stochastic Gradient Descent(25463): loss=1.8673344852941092\n",
      "Stochastic Gradient Descent(25464): loss=12.181377996881945\n",
      "Stochastic Gradient Descent(25465): loss=1.5707253219641821\n",
      "Stochastic Gradient Descent(25466): loss=1.5436913015172655\n",
      "Stochastic Gradient Descent(25467): loss=0.11012788761628686\n",
      "Stochastic Gradient Descent(25468): loss=0.6430864648745434\n",
      "Stochastic Gradient Descent(25469): loss=0.22264947212672856\n",
      "Stochastic Gradient Descent(25470): loss=0.08931312612854049\n",
      "Stochastic Gradient Descent(25471): loss=0.4320548749608903\n",
      "Stochastic Gradient Descent(25472): loss=7.132284803233635\n",
      "Stochastic Gradient Descent(25473): loss=14.681488664370061\n",
      "Stochastic Gradient Descent(25474): loss=3.1058463574502\n",
      "Stochastic Gradient Descent(25475): loss=7.6614236028730875\n",
      "Stochastic Gradient Descent(25476): loss=0.07023962433696378\n",
      "Stochastic Gradient Descent(25477): loss=0.187610303443822\n",
      "Stochastic Gradient Descent(25478): loss=0.011581249438620076\n",
      "Stochastic Gradient Descent(25479): loss=2.22503585432337\n",
      "Stochastic Gradient Descent(25480): loss=3.2524911268607557\n",
      "Stochastic Gradient Descent(25481): loss=2.7665310385427424\n",
      "Stochastic Gradient Descent(25482): loss=0.0004974768393543641\n",
      "Stochastic Gradient Descent(25483): loss=17.50910867131741\n",
      "Stochastic Gradient Descent(25484): loss=17.04383276273355\n",
      "Stochastic Gradient Descent(25485): loss=0.08994848499172094\n",
      "Stochastic Gradient Descent(25486): loss=0.9422253015412575\n",
      "Stochastic Gradient Descent(25487): loss=0.011906475183501426\n",
      "Stochastic Gradient Descent(25488): loss=0.1752844622484593\n",
      "Stochastic Gradient Descent(25489): loss=7.498075037712824\n",
      "Stochastic Gradient Descent(25490): loss=0.1834215505965812\n",
      "Stochastic Gradient Descent(25491): loss=0.42663342858674025\n",
      "Stochastic Gradient Descent(25492): loss=3.768895899622978\n",
      "Stochastic Gradient Descent(25493): loss=3.1366128790291854\n",
      "Stochastic Gradient Descent(25494): loss=2.4969574433602006\n",
      "Stochastic Gradient Descent(25495): loss=0.0008838754727564466\n",
      "Stochastic Gradient Descent(25496): loss=16.395440081973554\n",
      "Stochastic Gradient Descent(25497): loss=1.2035797536600281\n",
      "Stochastic Gradient Descent(25498): loss=7.188995725765537\n",
      "Stochastic Gradient Descent(25499): loss=0.23541635528395174\n",
      "Stochastic Gradient Descent(25500): loss=6.148017559410071\n",
      "Stochastic Gradient Descent(25501): loss=0.39257543991117133\n",
      "Stochastic Gradient Descent(25502): loss=3.7402997328950747\n",
      "Stochastic Gradient Descent(25503): loss=0.2700050601754123\n",
      "Stochastic Gradient Descent(25504): loss=0.007946637101723575\n",
      "Stochastic Gradient Descent(25505): loss=2.57921370463993\n",
      "Stochastic Gradient Descent(25506): loss=1.101981130658158\n",
      "Stochastic Gradient Descent(25507): loss=3.940457085547461\n",
      "Stochastic Gradient Descent(25508): loss=13.410210186562805\n",
      "Stochastic Gradient Descent(25509): loss=6.7335929606797915\n",
      "Stochastic Gradient Descent(25510): loss=2.8179446881296473\n",
      "Stochastic Gradient Descent(25511): loss=17.86807647982965\n",
      "Stochastic Gradient Descent(25512): loss=0.20791576630626474\n",
      "Stochastic Gradient Descent(25513): loss=6.225492823704035\n",
      "Stochastic Gradient Descent(25514): loss=0.8477520309087024\n",
      "Stochastic Gradient Descent(25515): loss=14.816789820339002\n",
      "Stochastic Gradient Descent(25516): loss=4.10220344066604\n",
      "Stochastic Gradient Descent(25517): loss=12.727047271386935\n",
      "Stochastic Gradient Descent(25518): loss=9.574923565858901\n",
      "Stochastic Gradient Descent(25519): loss=2.912034378267977\n",
      "Stochastic Gradient Descent(25520): loss=1.1698614655320616\n",
      "Stochastic Gradient Descent(25521): loss=2.188497350028396\n",
      "Stochastic Gradient Descent(25522): loss=1.025494976290902\n",
      "Stochastic Gradient Descent(25523): loss=2.435805659692983\n",
      "Stochastic Gradient Descent(25524): loss=2.6051323683578467\n",
      "Stochastic Gradient Descent(25525): loss=0.724059980090563\n",
      "Stochastic Gradient Descent(25526): loss=13.393529596316412\n",
      "Stochastic Gradient Descent(25527): loss=23.646933776318612\n",
      "Stochastic Gradient Descent(25528): loss=1.026667664288105\n",
      "Stochastic Gradient Descent(25529): loss=0.09612400515201003\n",
      "Stochastic Gradient Descent(25530): loss=34.28978132493603\n",
      "Stochastic Gradient Descent(25531): loss=6.321944439248711\n",
      "Stochastic Gradient Descent(25532): loss=1.8297914484615938\n",
      "Stochastic Gradient Descent(25533): loss=2.332308034821222\n",
      "Stochastic Gradient Descent(25534): loss=0.7021897462171725\n",
      "Stochastic Gradient Descent(25535): loss=2.62664852716053\n",
      "Stochastic Gradient Descent(25536): loss=0.029538571229638027\n",
      "Stochastic Gradient Descent(25537): loss=0.4419072716649346\n",
      "Stochastic Gradient Descent(25538): loss=0.16702121834153913\n",
      "Stochastic Gradient Descent(25539): loss=17.54414721278544\n",
      "Stochastic Gradient Descent(25540): loss=1.6342284214840312\n",
      "Stochastic Gradient Descent(25541): loss=5.342378212201551\n",
      "Stochastic Gradient Descent(25542): loss=2.6979908513036914\n",
      "Stochastic Gradient Descent(25543): loss=21.404834226403537\n",
      "Stochastic Gradient Descent(25544): loss=0.5109985361456301\n",
      "Stochastic Gradient Descent(25545): loss=4.1706998962210005\n",
      "Stochastic Gradient Descent(25546): loss=0.018923078624687784\n",
      "Stochastic Gradient Descent(25547): loss=0.005738424657666236\n",
      "Stochastic Gradient Descent(25548): loss=11.800449534865063\n",
      "Stochastic Gradient Descent(25549): loss=0.04151294092929116\n",
      "Stochastic Gradient Descent(25550): loss=0.29143583032586445\n",
      "Stochastic Gradient Descent(25551): loss=64.35598271059173\n",
      "Stochastic Gradient Descent(25552): loss=289.6539759165445\n",
      "Stochastic Gradient Descent(25553): loss=156.60627462309844\n",
      "Stochastic Gradient Descent(25554): loss=37.413100853679516\n",
      "Stochastic Gradient Descent(25555): loss=120.35161772821415\n",
      "Stochastic Gradient Descent(25556): loss=17.80681436200006\n",
      "Stochastic Gradient Descent(25557): loss=16.262534884607536\n",
      "Stochastic Gradient Descent(25558): loss=12.099439204195239\n",
      "Stochastic Gradient Descent(25559): loss=12.090596086453177\n",
      "Stochastic Gradient Descent(25560): loss=32.653483735375346\n",
      "Stochastic Gradient Descent(25561): loss=194.8356852814802\n",
      "Stochastic Gradient Descent(25562): loss=11.371260127641834\n",
      "Stochastic Gradient Descent(25563): loss=0.3821711059692403\n",
      "Stochastic Gradient Descent(25564): loss=0.1362756461843537\n",
      "Stochastic Gradient Descent(25565): loss=4.652976175574375\n",
      "Stochastic Gradient Descent(25566): loss=4.205530108413059\n",
      "Stochastic Gradient Descent(25567): loss=0.128727799398844\n",
      "Stochastic Gradient Descent(25568): loss=2.3225057818334496\n",
      "Stochastic Gradient Descent(25569): loss=1.3061362064882291\n",
      "Stochastic Gradient Descent(25570): loss=1.7034034560686682\n",
      "Stochastic Gradient Descent(25571): loss=1.3196869110845735\n",
      "Stochastic Gradient Descent(25572): loss=3.5110078052325053\n",
      "Stochastic Gradient Descent(25573): loss=27.255720831182565\n",
      "Stochastic Gradient Descent(25574): loss=0.4282596010635101\n",
      "Stochastic Gradient Descent(25575): loss=5.995006038717875\n",
      "Stochastic Gradient Descent(25576): loss=4.57114344804748\n",
      "Stochastic Gradient Descent(25577): loss=18.039395341889264\n",
      "Stochastic Gradient Descent(25578): loss=2.5598776673101584\n",
      "Stochastic Gradient Descent(25579): loss=1.8191784109801215\n",
      "Stochastic Gradient Descent(25580): loss=35.032931210394366\n",
      "Stochastic Gradient Descent(25581): loss=1.130276381712526\n",
      "Stochastic Gradient Descent(25582): loss=1.2801214156277296\n",
      "Stochastic Gradient Descent(25583): loss=2.1153765705920624\n",
      "Stochastic Gradient Descent(25584): loss=17.789604871762926\n",
      "Stochastic Gradient Descent(25585): loss=0.23296323482368653\n",
      "Stochastic Gradient Descent(25586): loss=2.6717085764912594\n",
      "Stochastic Gradient Descent(25587): loss=0.015808188846660003\n",
      "Stochastic Gradient Descent(25588): loss=10.941344172525731\n",
      "Stochastic Gradient Descent(25589): loss=0.5854919759536096\n",
      "Stochastic Gradient Descent(25590): loss=7.606581100483828\n",
      "Stochastic Gradient Descent(25591): loss=1.4039415164466456\n",
      "Stochastic Gradient Descent(25592): loss=0.2039898174286939\n",
      "Stochastic Gradient Descent(25593): loss=9.21255518396032\n",
      "Stochastic Gradient Descent(25594): loss=7.341084972950696\n",
      "Stochastic Gradient Descent(25595): loss=0.991291251392915\n",
      "Stochastic Gradient Descent(25596): loss=9.015084239451692\n",
      "Stochastic Gradient Descent(25597): loss=4.388847482807456\n",
      "Stochastic Gradient Descent(25598): loss=0.04738779142233045\n",
      "Stochastic Gradient Descent(25599): loss=1.9828842627403547\n",
      "Stochastic Gradient Descent(25600): loss=10.362007702083025\n",
      "Stochastic Gradient Descent(25601): loss=1.548490947890811\n",
      "Stochastic Gradient Descent(25602): loss=0.660423329655819\n",
      "Stochastic Gradient Descent(25603): loss=14.403772777311806\n",
      "Stochastic Gradient Descent(25604): loss=0.004758956393198863\n",
      "Stochastic Gradient Descent(25605): loss=2.9195292620424156\n",
      "Stochastic Gradient Descent(25606): loss=4.029485030200369\n",
      "Stochastic Gradient Descent(25607): loss=0.16307386747425148\n",
      "Stochastic Gradient Descent(25608): loss=1.2156963122287734\n",
      "Stochastic Gradient Descent(25609): loss=5.408506323485962\n",
      "Stochastic Gradient Descent(25610): loss=17.14746417988933\n",
      "Stochastic Gradient Descent(25611): loss=0.883728540175212\n",
      "Stochastic Gradient Descent(25612): loss=0.7632196437544804\n",
      "Stochastic Gradient Descent(25613): loss=3.31142534107747\n",
      "Stochastic Gradient Descent(25614): loss=18.54717073600327\n",
      "Stochastic Gradient Descent(25615): loss=5.535584774304793\n",
      "Stochastic Gradient Descent(25616): loss=0.28776199778094813\n",
      "Stochastic Gradient Descent(25617): loss=0.6185671122811458\n",
      "Stochastic Gradient Descent(25618): loss=5.958692260570668\n",
      "Stochastic Gradient Descent(25619): loss=2.310147663816444\n",
      "Stochastic Gradient Descent(25620): loss=4.407437044658522\n",
      "Stochastic Gradient Descent(25621): loss=3.8626832499817985\n",
      "Stochastic Gradient Descent(25622): loss=8.482099612190511\n",
      "Stochastic Gradient Descent(25623): loss=0.8107391902963984\n",
      "Stochastic Gradient Descent(25624): loss=2.341097353731497\n",
      "Stochastic Gradient Descent(25625): loss=20.428769021525596\n",
      "Stochastic Gradient Descent(25626): loss=7.508461513216301\n",
      "Stochastic Gradient Descent(25627): loss=1.1913581422009372\n",
      "Stochastic Gradient Descent(25628): loss=0.001175062400468791\n",
      "Stochastic Gradient Descent(25629): loss=4.913381678836834\n",
      "Stochastic Gradient Descent(25630): loss=2.728803440453496\n",
      "Stochastic Gradient Descent(25631): loss=6.488923583514569\n",
      "Stochastic Gradient Descent(25632): loss=0.44645726331746277\n",
      "Stochastic Gradient Descent(25633): loss=0.09997474325204238\n",
      "Stochastic Gradient Descent(25634): loss=5.259613387496671\n",
      "Stochastic Gradient Descent(25635): loss=1.3903503997684261\n",
      "Stochastic Gradient Descent(25636): loss=1.1170029598608948\n",
      "Stochastic Gradient Descent(25637): loss=1.703720994717323\n",
      "Stochastic Gradient Descent(25638): loss=6.071800333747433\n",
      "Stochastic Gradient Descent(25639): loss=12.274041425522128\n",
      "Stochastic Gradient Descent(25640): loss=7.212654317774438\n",
      "Stochastic Gradient Descent(25641): loss=0.5033901652563563\n",
      "Stochastic Gradient Descent(25642): loss=21.509091983293356\n",
      "Stochastic Gradient Descent(25643): loss=38.662669368526274\n",
      "Stochastic Gradient Descent(25644): loss=0.038525541094254004\n",
      "Stochastic Gradient Descent(25645): loss=0.30817846487213807\n",
      "Stochastic Gradient Descent(25646): loss=2.2298727083211647e-13\n",
      "Stochastic Gradient Descent(25647): loss=1.4048792469411173\n",
      "Stochastic Gradient Descent(25648): loss=0.31636276116270423\n",
      "Stochastic Gradient Descent(25649): loss=1.8805252237260257\n",
      "Stochastic Gradient Descent(25650): loss=0.7729638725636117\n",
      "Stochastic Gradient Descent(25651): loss=6.295470341609765\n",
      "Stochastic Gradient Descent(25652): loss=0.4106747265347622\n",
      "Stochastic Gradient Descent(25653): loss=7.9407368444882644\n",
      "Stochastic Gradient Descent(25654): loss=3.254255836863861\n",
      "Stochastic Gradient Descent(25655): loss=0.3980537717700562\n",
      "Stochastic Gradient Descent(25656): loss=4.884598842369278\n",
      "Stochastic Gradient Descent(25657): loss=15.053524803189314\n",
      "Stochastic Gradient Descent(25658): loss=11.350494109241367\n",
      "Stochastic Gradient Descent(25659): loss=1.5268953286020055\n",
      "Stochastic Gradient Descent(25660): loss=3.9568482044838764\n",
      "Stochastic Gradient Descent(25661): loss=9.912255819189134\n",
      "Stochastic Gradient Descent(25662): loss=0.1982469641146206\n",
      "Stochastic Gradient Descent(25663): loss=0.42240250600986007\n",
      "Stochastic Gradient Descent(25664): loss=0.2926837514014955\n",
      "Stochastic Gradient Descent(25665): loss=4.46538238008523\n",
      "Stochastic Gradient Descent(25666): loss=15.667444083345677\n",
      "Stochastic Gradient Descent(25667): loss=0.38315637286194104\n",
      "Stochastic Gradient Descent(25668): loss=0.8942193688481483\n",
      "Stochastic Gradient Descent(25669): loss=0.17967997910356825\n",
      "Stochastic Gradient Descent(25670): loss=3.529800736846565\n",
      "Stochastic Gradient Descent(25671): loss=6.601313562492414\n",
      "Stochastic Gradient Descent(25672): loss=1.6015853541266367\n",
      "Stochastic Gradient Descent(25673): loss=2.859923701658681\n",
      "Stochastic Gradient Descent(25674): loss=20.14193593430413\n",
      "Stochastic Gradient Descent(25675): loss=1.7029780357041109\n",
      "Stochastic Gradient Descent(25676): loss=41.971689943765256\n",
      "Stochastic Gradient Descent(25677): loss=0.5431682275057961\n",
      "Stochastic Gradient Descent(25678): loss=2.3887320050175807\n",
      "Stochastic Gradient Descent(25679): loss=6.14396335710674\n",
      "Stochastic Gradient Descent(25680): loss=15.682635032450607\n",
      "Stochastic Gradient Descent(25681): loss=1.135812289624206\n",
      "Stochastic Gradient Descent(25682): loss=0.321916250030793\n",
      "Stochastic Gradient Descent(25683): loss=47.088716068729134\n",
      "Stochastic Gradient Descent(25684): loss=0.2231801086554094\n",
      "Stochastic Gradient Descent(25685): loss=7.249745111031977\n",
      "Stochastic Gradient Descent(25686): loss=0.14884083277703206\n",
      "Stochastic Gradient Descent(25687): loss=4.8142452989566795\n",
      "Stochastic Gradient Descent(25688): loss=3.8574588274358863\n",
      "Stochastic Gradient Descent(25689): loss=3.0093997863508113\n",
      "Stochastic Gradient Descent(25690): loss=4.1344904032163035\n",
      "Stochastic Gradient Descent(25691): loss=18.372277989985097\n",
      "Stochastic Gradient Descent(25692): loss=7.3376640060107805\n",
      "Stochastic Gradient Descent(25693): loss=1.1458866654480073\n",
      "Stochastic Gradient Descent(25694): loss=0.10633791459939824\n",
      "Stochastic Gradient Descent(25695): loss=0.7129503905722043\n",
      "Stochastic Gradient Descent(25696): loss=0.39573700935191586\n",
      "Stochastic Gradient Descent(25697): loss=1.3763926445515982\n",
      "Stochastic Gradient Descent(25698): loss=3.4836920207365716\n",
      "Stochastic Gradient Descent(25699): loss=0.3477496910744966\n",
      "Stochastic Gradient Descent(25700): loss=1.3009796544001677\n",
      "Stochastic Gradient Descent(25701): loss=4.21121768948576\n",
      "Stochastic Gradient Descent(25702): loss=5.374293663711629\n",
      "Stochastic Gradient Descent(25703): loss=11.690837672721981\n",
      "Stochastic Gradient Descent(25704): loss=0.1411028967965569\n",
      "Stochastic Gradient Descent(25705): loss=6.006215791237484\n",
      "Stochastic Gradient Descent(25706): loss=6.785855241456759\n",
      "Stochastic Gradient Descent(25707): loss=0.32375103769826996\n",
      "Stochastic Gradient Descent(25708): loss=9.693597769309656\n",
      "Stochastic Gradient Descent(25709): loss=0.17386649944163277\n",
      "Stochastic Gradient Descent(25710): loss=0.07077399926842755\n",
      "Stochastic Gradient Descent(25711): loss=4.630742560776555\n",
      "Stochastic Gradient Descent(25712): loss=0.794979702622828\n",
      "Stochastic Gradient Descent(25713): loss=17.5921088126086\n",
      "Stochastic Gradient Descent(25714): loss=2.4749689760550875\n",
      "Stochastic Gradient Descent(25715): loss=11.956859450839605\n",
      "Stochastic Gradient Descent(25716): loss=3.7638344222028084\n",
      "Stochastic Gradient Descent(25717): loss=0.6241497876869901\n",
      "Stochastic Gradient Descent(25718): loss=4.315689338708713\n",
      "Stochastic Gradient Descent(25719): loss=6.043907634149389\n",
      "Stochastic Gradient Descent(25720): loss=36.66237395951425\n",
      "Stochastic Gradient Descent(25721): loss=1.2492457061010336\n",
      "Stochastic Gradient Descent(25722): loss=2.438619187269248\n",
      "Stochastic Gradient Descent(25723): loss=3.7224234737047204\n",
      "Stochastic Gradient Descent(25724): loss=20.432478135477194\n",
      "Stochastic Gradient Descent(25725): loss=0.3589810112523489\n",
      "Stochastic Gradient Descent(25726): loss=9.735419539000771\n",
      "Stochastic Gradient Descent(25727): loss=3.611072254522025\n",
      "Stochastic Gradient Descent(25728): loss=1.0562106445023303\n",
      "Stochastic Gradient Descent(25729): loss=0.6513873744288312\n",
      "Stochastic Gradient Descent(25730): loss=1.0335765759211575\n",
      "Stochastic Gradient Descent(25731): loss=0.8092287977027615\n",
      "Stochastic Gradient Descent(25732): loss=0.00038308076091869394\n",
      "Stochastic Gradient Descent(25733): loss=0.41967520169753736\n",
      "Stochastic Gradient Descent(25734): loss=1.0794604498434757\n",
      "Stochastic Gradient Descent(25735): loss=2.3819065941319284\n",
      "Stochastic Gradient Descent(25736): loss=0.27811145058482895\n",
      "Stochastic Gradient Descent(25737): loss=2.0633357314110574\n",
      "Stochastic Gradient Descent(25738): loss=2.9527044176886825\n",
      "Stochastic Gradient Descent(25739): loss=1.1018558236852083\n",
      "Stochastic Gradient Descent(25740): loss=0.5016846314093161\n",
      "Stochastic Gradient Descent(25741): loss=2.98631931875922\n",
      "Stochastic Gradient Descent(25742): loss=5.7564957941775345\n",
      "Stochastic Gradient Descent(25743): loss=12.88750366627091\n",
      "Stochastic Gradient Descent(25744): loss=17.81017736353044\n",
      "Stochastic Gradient Descent(25745): loss=27.517306723549616\n",
      "Stochastic Gradient Descent(25746): loss=4.688119020219869\n",
      "Stochastic Gradient Descent(25747): loss=0.5644600275059142\n",
      "Stochastic Gradient Descent(25748): loss=0.16557067316112223\n",
      "Stochastic Gradient Descent(25749): loss=4.7408406500275975\n",
      "Stochastic Gradient Descent(25750): loss=2.445167356938319\n",
      "Stochastic Gradient Descent(25751): loss=4.98955081329864\n",
      "Stochastic Gradient Descent(25752): loss=0.9144403332173303\n",
      "Stochastic Gradient Descent(25753): loss=13.470412630922173\n",
      "Stochastic Gradient Descent(25754): loss=0.09661981645007389\n",
      "Stochastic Gradient Descent(25755): loss=1.344557054897367\n",
      "Stochastic Gradient Descent(25756): loss=8.003591645475865\n",
      "Stochastic Gradient Descent(25757): loss=0.7965253882350576\n",
      "Stochastic Gradient Descent(25758): loss=35.45281440417672\n",
      "Stochastic Gradient Descent(25759): loss=3.9428093413445393\n",
      "Stochastic Gradient Descent(25760): loss=0.16923375468099303\n",
      "Stochastic Gradient Descent(25761): loss=2.306972856257629\n",
      "Stochastic Gradient Descent(25762): loss=0.1614799174927189\n",
      "Stochastic Gradient Descent(25763): loss=2.126274222487637\n",
      "Stochastic Gradient Descent(25764): loss=0.2845354782853801\n",
      "Stochastic Gradient Descent(25765): loss=1.065144126062494\n",
      "Stochastic Gradient Descent(25766): loss=0.0045660917277323045\n",
      "Stochastic Gradient Descent(25767): loss=2.902992791402663\n",
      "Stochastic Gradient Descent(25768): loss=2.929924885430656\n",
      "Stochastic Gradient Descent(25769): loss=12.325910072241744\n",
      "Stochastic Gradient Descent(25770): loss=43.01812041531575\n",
      "Stochastic Gradient Descent(25771): loss=10.022772141761227\n",
      "Stochastic Gradient Descent(25772): loss=1.9622439405782046\n",
      "Stochastic Gradient Descent(25773): loss=2.5784214520202244\n",
      "Stochastic Gradient Descent(25774): loss=1.2763008206101745\n",
      "Stochastic Gradient Descent(25775): loss=20.79344460173321\n",
      "Stochastic Gradient Descent(25776): loss=3.115455401679021\n",
      "Stochastic Gradient Descent(25777): loss=0.11277319411239872\n",
      "Stochastic Gradient Descent(25778): loss=0.2868366908864686\n",
      "Stochastic Gradient Descent(25779): loss=6.230770555183241\n",
      "Stochastic Gradient Descent(25780): loss=9.043090784196425\n",
      "Stochastic Gradient Descent(25781): loss=2.0612708494828893\n",
      "Stochastic Gradient Descent(25782): loss=0.028518954031429997\n",
      "Stochastic Gradient Descent(25783): loss=10.989287079553934\n",
      "Stochastic Gradient Descent(25784): loss=4.636026373634356\n",
      "Stochastic Gradient Descent(25785): loss=0.6777208418121101\n",
      "Stochastic Gradient Descent(25786): loss=2.1317438357445218\n",
      "Stochastic Gradient Descent(25787): loss=7.285856230402977\n",
      "Stochastic Gradient Descent(25788): loss=1.1916188274800898\n",
      "Stochastic Gradient Descent(25789): loss=14.832093938848294\n",
      "Stochastic Gradient Descent(25790): loss=5.493961796670494\n",
      "Stochastic Gradient Descent(25791): loss=0.45280572351472215\n",
      "Stochastic Gradient Descent(25792): loss=0.09693436330849067\n",
      "Stochastic Gradient Descent(25793): loss=0.42169905009139996\n",
      "Stochastic Gradient Descent(25794): loss=1.3613649227711802\n",
      "Stochastic Gradient Descent(25795): loss=0.029166185682097327\n",
      "Stochastic Gradient Descent(25796): loss=0.23102961167782105\n",
      "Stochastic Gradient Descent(25797): loss=0.5379755847807052\n",
      "Stochastic Gradient Descent(25798): loss=0.0007763108419716021\n",
      "Stochastic Gradient Descent(25799): loss=0.44888313119174544\n",
      "Stochastic Gradient Descent(25800): loss=11.338210322522809\n",
      "Stochastic Gradient Descent(25801): loss=0.0004119245036496503\n",
      "Stochastic Gradient Descent(25802): loss=8.683577301726048\n",
      "Stochastic Gradient Descent(25803): loss=2.744232440747142\n",
      "Stochastic Gradient Descent(25804): loss=3.13207988740048\n",
      "Stochastic Gradient Descent(25805): loss=0.14599214712734354\n",
      "Stochastic Gradient Descent(25806): loss=0.39437456132159726\n",
      "Stochastic Gradient Descent(25807): loss=0.06426492784778408\n",
      "Stochastic Gradient Descent(25808): loss=0.4730166179641706\n",
      "Stochastic Gradient Descent(25809): loss=0.058446734015977345\n",
      "Stochastic Gradient Descent(25810): loss=0.4022601302474948\n",
      "Stochastic Gradient Descent(25811): loss=0.002538539070936264\n",
      "Stochastic Gradient Descent(25812): loss=0.09024086512184407\n",
      "Stochastic Gradient Descent(25813): loss=4.154460300894807\n",
      "Stochastic Gradient Descent(25814): loss=0.0591240996968103\n",
      "Stochastic Gradient Descent(25815): loss=0.5010203435428315\n",
      "Stochastic Gradient Descent(25816): loss=10.410045602103905\n",
      "Stochastic Gradient Descent(25817): loss=0.34807996412061853\n",
      "Stochastic Gradient Descent(25818): loss=1.3874906839290122\n",
      "Stochastic Gradient Descent(25819): loss=1.0731792737049297\n",
      "Stochastic Gradient Descent(25820): loss=1.0877481202333756\n",
      "Stochastic Gradient Descent(25821): loss=3.867782942577017\n",
      "Stochastic Gradient Descent(25822): loss=0.0030418602918579345\n",
      "Stochastic Gradient Descent(25823): loss=5.982694099799256\n",
      "Stochastic Gradient Descent(25824): loss=6.729961150815687\n",
      "Stochastic Gradient Descent(25825): loss=18.14246258521637\n",
      "Stochastic Gradient Descent(25826): loss=3.914299802679849\n",
      "Stochastic Gradient Descent(25827): loss=0.0335304952823107\n",
      "Stochastic Gradient Descent(25828): loss=8.042281018406696\n",
      "Stochastic Gradient Descent(25829): loss=1.0483887739967719\n",
      "Stochastic Gradient Descent(25830): loss=0.46125210851834697\n",
      "Stochastic Gradient Descent(25831): loss=9.664044688694034\n",
      "Stochastic Gradient Descent(25832): loss=1.2868366609810293\n",
      "Stochastic Gradient Descent(25833): loss=0.7152941935155266\n",
      "Stochastic Gradient Descent(25834): loss=4.734171347943262\n",
      "Stochastic Gradient Descent(25835): loss=4.8188064638909545\n",
      "Stochastic Gradient Descent(25836): loss=0.16693605949706486\n",
      "Stochastic Gradient Descent(25837): loss=1.9647491494908558\n",
      "Stochastic Gradient Descent(25838): loss=0.06597997573483155\n",
      "Stochastic Gradient Descent(25839): loss=9.410099426985578\n",
      "Stochastic Gradient Descent(25840): loss=4.544254605041632\n",
      "Stochastic Gradient Descent(25841): loss=1.1065250260775623\n",
      "Stochastic Gradient Descent(25842): loss=5.01090906656222\n",
      "Stochastic Gradient Descent(25843): loss=0.008685922524775399\n",
      "Stochastic Gradient Descent(25844): loss=6.18711822342503\n",
      "Stochastic Gradient Descent(25845): loss=9.355455589761872\n",
      "Stochastic Gradient Descent(25846): loss=1.3655123030113094\n",
      "Stochastic Gradient Descent(25847): loss=0.8148982289918287\n",
      "Stochastic Gradient Descent(25848): loss=0.6848618388309112\n",
      "Stochastic Gradient Descent(25849): loss=6.694121247138421\n",
      "Stochastic Gradient Descent(25850): loss=0.08168171772437502\n",
      "Stochastic Gradient Descent(25851): loss=0.07549606774703554\n",
      "Stochastic Gradient Descent(25852): loss=2.376181630758664\n",
      "Stochastic Gradient Descent(25853): loss=19.131118624962877\n",
      "Stochastic Gradient Descent(25854): loss=1.696507712284865\n",
      "Stochastic Gradient Descent(25855): loss=57.0341261976177\n",
      "Stochastic Gradient Descent(25856): loss=0.3663896462597896\n",
      "Stochastic Gradient Descent(25857): loss=0.2953150256251041\n",
      "Stochastic Gradient Descent(25858): loss=0.005104020554021175\n",
      "Stochastic Gradient Descent(25859): loss=1.0203342452962405\n",
      "Stochastic Gradient Descent(25860): loss=2.665364523035178\n",
      "Stochastic Gradient Descent(25861): loss=1.630535973659489\n",
      "Stochastic Gradient Descent(25862): loss=12.32094310808107\n",
      "Stochastic Gradient Descent(25863): loss=6.676554527831163\n",
      "Stochastic Gradient Descent(25864): loss=9.081607700263127\n",
      "Stochastic Gradient Descent(25865): loss=0.6584221166681645\n",
      "Stochastic Gradient Descent(25866): loss=9.093037342095563\n",
      "Stochastic Gradient Descent(25867): loss=2.7737123556783194\n",
      "Stochastic Gradient Descent(25868): loss=0.5653566081200615\n",
      "Stochastic Gradient Descent(25869): loss=0.5095914496901509\n",
      "Stochastic Gradient Descent(25870): loss=4.069846646599126\n",
      "Stochastic Gradient Descent(25871): loss=2.4047575552146507\n",
      "Stochastic Gradient Descent(25872): loss=0.6158810910922174\n",
      "Stochastic Gradient Descent(25873): loss=1.6733610429947439\n",
      "Stochastic Gradient Descent(25874): loss=5.292604438377215\n",
      "Stochastic Gradient Descent(25875): loss=16.431716329489667\n",
      "Stochastic Gradient Descent(25876): loss=18.24192994088997\n",
      "Stochastic Gradient Descent(25877): loss=3.26542905441928\n",
      "Stochastic Gradient Descent(25878): loss=2.6707275970510835\n",
      "Stochastic Gradient Descent(25879): loss=5.300732639403277\n",
      "Stochastic Gradient Descent(25880): loss=0.08202781945283015\n",
      "Stochastic Gradient Descent(25881): loss=0.1926584894198329\n",
      "Stochastic Gradient Descent(25882): loss=0.1589917236568688\n",
      "Stochastic Gradient Descent(25883): loss=11.823087605971962\n",
      "Stochastic Gradient Descent(25884): loss=0.010845116449255924\n",
      "Stochastic Gradient Descent(25885): loss=3.260307437881397\n",
      "Stochastic Gradient Descent(25886): loss=0.8544451084353588\n",
      "Stochastic Gradient Descent(25887): loss=1.6695320540503664\n",
      "Stochastic Gradient Descent(25888): loss=0.7315663545537286\n",
      "Stochastic Gradient Descent(25889): loss=4.4902175352878055\n",
      "Stochastic Gradient Descent(25890): loss=5.569744360595388\n",
      "Stochastic Gradient Descent(25891): loss=0.034046184949688765\n",
      "Stochastic Gradient Descent(25892): loss=6.638621313482424\n",
      "Stochastic Gradient Descent(25893): loss=1.5606253185496362\n",
      "Stochastic Gradient Descent(25894): loss=2.466587526440817\n",
      "Stochastic Gradient Descent(25895): loss=15.52823066115214\n",
      "Stochastic Gradient Descent(25896): loss=19.37322241708047\n",
      "Stochastic Gradient Descent(25897): loss=0.5790396180880946\n",
      "Stochastic Gradient Descent(25898): loss=2.1194041258686966\n",
      "Stochastic Gradient Descent(25899): loss=0.6364304771729842\n",
      "Stochastic Gradient Descent(25900): loss=0.2630533936222903\n",
      "Stochastic Gradient Descent(25901): loss=1.7282082460697432\n",
      "Stochastic Gradient Descent(25902): loss=6.046294863395328\n",
      "Stochastic Gradient Descent(25903): loss=1.6828589468182873\n",
      "Stochastic Gradient Descent(25904): loss=0.7021943641701966\n",
      "Stochastic Gradient Descent(25905): loss=0.21631643473429174\n",
      "Stochastic Gradient Descent(25906): loss=15.506279903729768\n",
      "Stochastic Gradient Descent(25907): loss=8.049292315008417\n",
      "Stochastic Gradient Descent(25908): loss=1.1145489356618938\n",
      "Stochastic Gradient Descent(25909): loss=2.6525706714848005\n",
      "Stochastic Gradient Descent(25910): loss=0.6341805083013999\n",
      "Stochastic Gradient Descent(25911): loss=1.980297544280429\n",
      "Stochastic Gradient Descent(25912): loss=0.39998311105622064\n",
      "Stochastic Gradient Descent(25913): loss=6.229834347790276\n",
      "Stochastic Gradient Descent(25914): loss=3.792920414310615\n",
      "Stochastic Gradient Descent(25915): loss=0.06827222870812033\n",
      "Stochastic Gradient Descent(25916): loss=2.2177673966142657\n",
      "Stochastic Gradient Descent(25917): loss=9.152192861745451\n",
      "Stochastic Gradient Descent(25918): loss=1.0636177670098264\n",
      "Stochastic Gradient Descent(25919): loss=13.841142372823365\n",
      "Stochastic Gradient Descent(25920): loss=4.464049909883131\n",
      "Stochastic Gradient Descent(25921): loss=32.69725687883544\n",
      "Stochastic Gradient Descent(25922): loss=3.7683363079955683\n",
      "Stochastic Gradient Descent(25923): loss=0.7584268145702405\n",
      "Stochastic Gradient Descent(25924): loss=0.17538955194376077\n",
      "Stochastic Gradient Descent(25925): loss=7.128237780189519\n",
      "Stochastic Gradient Descent(25926): loss=1.6441741148382316\n",
      "Stochastic Gradient Descent(25927): loss=0.1135000368240878\n",
      "Stochastic Gradient Descent(25928): loss=1.5875943207004322\n",
      "Stochastic Gradient Descent(25929): loss=6.349343638928237\n",
      "Stochastic Gradient Descent(25930): loss=5.07941987199431\n",
      "Stochastic Gradient Descent(25931): loss=0.7737281084077052\n",
      "Stochastic Gradient Descent(25932): loss=1.0403855750274942\n",
      "Stochastic Gradient Descent(25933): loss=10.212657200438121\n",
      "Stochastic Gradient Descent(25934): loss=0.5222317760797793\n",
      "Stochastic Gradient Descent(25935): loss=0.31537893181448845\n",
      "Stochastic Gradient Descent(25936): loss=4.7649970538996005\n",
      "Stochastic Gradient Descent(25937): loss=2.7678370742192353\n",
      "Stochastic Gradient Descent(25938): loss=5.864712714046501\n",
      "Stochastic Gradient Descent(25939): loss=0.3767307316505557\n",
      "Stochastic Gradient Descent(25940): loss=9.550210695366433\n",
      "Stochastic Gradient Descent(25941): loss=2.7278264890365707\n",
      "Stochastic Gradient Descent(25942): loss=0.375716602498338\n",
      "Stochastic Gradient Descent(25943): loss=4.680358000802625\n",
      "Stochastic Gradient Descent(25944): loss=7.48477363377422\n",
      "Stochastic Gradient Descent(25945): loss=4.978515048156446\n",
      "Stochastic Gradient Descent(25946): loss=1.012862558090562e-05\n",
      "Stochastic Gradient Descent(25947): loss=0.5659518355917774\n",
      "Stochastic Gradient Descent(25948): loss=0.15804472867241026\n",
      "Stochastic Gradient Descent(25949): loss=6.034978643916846\n",
      "Stochastic Gradient Descent(25950): loss=0.5527420890616693\n",
      "Stochastic Gradient Descent(25951): loss=1.2353986068156684\n",
      "Stochastic Gradient Descent(25952): loss=1.7349997881698673\n",
      "Stochastic Gradient Descent(25953): loss=0.2856390942518098\n",
      "Stochastic Gradient Descent(25954): loss=0.0021796572743732934\n",
      "Stochastic Gradient Descent(25955): loss=0.004556823304509492\n",
      "Stochastic Gradient Descent(25956): loss=0.7279784843067431\n",
      "Stochastic Gradient Descent(25957): loss=5.679863492439898\n",
      "Stochastic Gradient Descent(25958): loss=0.0015818256465287939\n",
      "Stochastic Gradient Descent(25959): loss=2.398096951919071\n",
      "Stochastic Gradient Descent(25960): loss=3.534537641709471\n",
      "Stochastic Gradient Descent(25961): loss=3.905478711055897\n",
      "Stochastic Gradient Descent(25962): loss=9.446237156789236\n",
      "Stochastic Gradient Descent(25963): loss=0.07964977451751283\n",
      "Stochastic Gradient Descent(25964): loss=80.1741461369591\n",
      "Stochastic Gradient Descent(25965): loss=0.3248592461129896\n",
      "Stochastic Gradient Descent(25966): loss=156.99415389708412\n",
      "Stochastic Gradient Descent(25967): loss=91.23465426787826\n",
      "Stochastic Gradient Descent(25968): loss=0.908894894018462\n",
      "Stochastic Gradient Descent(25969): loss=14.428503254041408\n",
      "Stochastic Gradient Descent(25970): loss=0.04774053158959385\n",
      "Stochastic Gradient Descent(25971): loss=0.873013459722533\n",
      "Stochastic Gradient Descent(25972): loss=2.5625121978617043\n",
      "Stochastic Gradient Descent(25973): loss=11.17580741173339\n",
      "Stochastic Gradient Descent(25974): loss=0.3562097986609957\n",
      "Stochastic Gradient Descent(25975): loss=5.177730446410175\n",
      "Stochastic Gradient Descent(25976): loss=0.699416936150528\n",
      "Stochastic Gradient Descent(25977): loss=0.01239709760491146\n",
      "Stochastic Gradient Descent(25978): loss=5.01899556800896\n",
      "Stochastic Gradient Descent(25979): loss=1.4199412768687665\n",
      "Stochastic Gradient Descent(25980): loss=0.03275406158571869\n",
      "Stochastic Gradient Descent(25981): loss=5.633996731441279\n",
      "Stochastic Gradient Descent(25982): loss=0.07085922402686039\n",
      "Stochastic Gradient Descent(25983): loss=4.526881464733384\n",
      "Stochastic Gradient Descent(25984): loss=0.04426807461288654\n",
      "Stochastic Gradient Descent(25985): loss=7.097447262631156\n",
      "Stochastic Gradient Descent(25986): loss=0.3629657844652012\n",
      "Stochastic Gradient Descent(25987): loss=0.11800444472882371\n",
      "Stochastic Gradient Descent(25988): loss=0.20163973335795898\n",
      "Stochastic Gradient Descent(25989): loss=0.5894955812254673\n",
      "Stochastic Gradient Descent(25990): loss=0.8213314485565689\n",
      "Stochastic Gradient Descent(25991): loss=0.5356945129688007\n",
      "Stochastic Gradient Descent(25992): loss=2.61624168069685\n",
      "Stochastic Gradient Descent(25993): loss=1.6308828342482844\n",
      "Stochastic Gradient Descent(25994): loss=0.4240874020443953\n",
      "Stochastic Gradient Descent(25995): loss=0.14771102245984033\n",
      "Stochastic Gradient Descent(25996): loss=0.06416435315561206\n",
      "Stochastic Gradient Descent(25997): loss=0.4740125381820824\n",
      "Stochastic Gradient Descent(25998): loss=9.26374049815577\n",
      "Stochastic Gradient Descent(25999): loss=2.044241805133213\n",
      "Stochastic Gradient Descent(26000): loss=7.000322146974577\n",
      "Stochastic Gradient Descent(26001): loss=5.251596029756408\n",
      "Stochastic Gradient Descent(26002): loss=9.401093611251925\n",
      "Stochastic Gradient Descent(26003): loss=2.206114197805459\n",
      "Stochastic Gradient Descent(26004): loss=12.441618742497052\n",
      "Stochastic Gradient Descent(26005): loss=1.7787533967567042\n",
      "Stochastic Gradient Descent(26006): loss=0.23272513686518248\n",
      "Stochastic Gradient Descent(26007): loss=1.1068667879163139\n",
      "Stochastic Gradient Descent(26008): loss=1.4818858503540129\n",
      "Stochastic Gradient Descent(26009): loss=7.412072341751944\n",
      "Stochastic Gradient Descent(26010): loss=0.3464703763689419\n",
      "Stochastic Gradient Descent(26011): loss=57.02424446641013\n",
      "Stochastic Gradient Descent(26012): loss=5.410850424449431\n",
      "Stochastic Gradient Descent(26013): loss=6.151201665896605\n",
      "Stochastic Gradient Descent(26014): loss=0.0023242866150583108\n",
      "Stochastic Gradient Descent(26015): loss=0.7888692855841128\n",
      "Stochastic Gradient Descent(26016): loss=3.814069798342017\n",
      "Stochastic Gradient Descent(26017): loss=0.5482523364216669\n",
      "Stochastic Gradient Descent(26018): loss=0.4683454289769025\n",
      "Stochastic Gradient Descent(26019): loss=22.383678519827715\n",
      "Stochastic Gradient Descent(26020): loss=4.174902338735161\n",
      "Stochastic Gradient Descent(26021): loss=10.133765335397204\n",
      "Stochastic Gradient Descent(26022): loss=7.150964668247089e-05\n",
      "Stochastic Gradient Descent(26023): loss=0.6285005372109644\n",
      "Stochastic Gradient Descent(26024): loss=1.772548443819615\n",
      "Stochastic Gradient Descent(26025): loss=1.3824705764027365\n",
      "Stochastic Gradient Descent(26026): loss=0.14242469016741813\n",
      "Stochastic Gradient Descent(26027): loss=21.019206691659058\n",
      "Stochastic Gradient Descent(26028): loss=0.12688568114732257\n",
      "Stochastic Gradient Descent(26029): loss=9.058752788054402\n",
      "Stochastic Gradient Descent(26030): loss=4.279771056367747\n",
      "Stochastic Gradient Descent(26031): loss=3.37248525838431\n",
      "Stochastic Gradient Descent(26032): loss=0.07594151464565761\n",
      "Stochastic Gradient Descent(26033): loss=0.202427933783189\n",
      "Stochastic Gradient Descent(26034): loss=7.006985434374158\n",
      "Stochastic Gradient Descent(26035): loss=5.7324798532986065\n",
      "Stochastic Gradient Descent(26036): loss=8.12055994513847\n",
      "Stochastic Gradient Descent(26037): loss=3.710760996789754\n",
      "Stochastic Gradient Descent(26038): loss=0.5518228081263659\n",
      "Stochastic Gradient Descent(26039): loss=0.7683721307199737\n",
      "Stochastic Gradient Descent(26040): loss=0.026667663883782618\n",
      "Stochastic Gradient Descent(26041): loss=1.885921673995793\n",
      "Stochastic Gradient Descent(26042): loss=2.2736201972356977\n",
      "Stochastic Gradient Descent(26043): loss=10.06891515437356\n",
      "Stochastic Gradient Descent(26044): loss=15.075602362321813\n",
      "Stochastic Gradient Descent(26045): loss=4.055614026134628\n",
      "Stochastic Gradient Descent(26046): loss=1.6386412776185317\n",
      "Stochastic Gradient Descent(26047): loss=0.006028267331186302\n",
      "Stochastic Gradient Descent(26048): loss=0.08599298310056822\n",
      "Stochastic Gradient Descent(26049): loss=0.14912306370464776\n",
      "Stochastic Gradient Descent(26050): loss=2.7625954512374813\n",
      "Stochastic Gradient Descent(26051): loss=6.409481241033696\n",
      "Stochastic Gradient Descent(26052): loss=0.32174693927453946\n",
      "Stochastic Gradient Descent(26053): loss=17.712569654724497\n",
      "Stochastic Gradient Descent(26054): loss=0.09949670550425857\n",
      "Stochastic Gradient Descent(26055): loss=21.214894657396382\n",
      "Stochastic Gradient Descent(26056): loss=0.8439837675782145\n",
      "Stochastic Gradient Descent(26057): loss=2.5958770258993193\n",
      "Stochastic Gradient Descent(26058): loss=1.520860591203956\n",
      "Stochastic Gradient Descent(26059): loss=0.38510964153382227\n",
      "Stochastic Gradient Descent(26060): loss=15.638361705756914\n",
      "Stochastic Gradient Descent(26061): loss=0.8754379436694281\n",
      "Stochastic Gradient Descent(26062): loss=0.3025478562061006\n",
      "Stochastic Gradient Descent(26063): loss=0.4210165412261136\n",
      "Stochastic Gradient Descent(26064): loss=40.297311461882536\n",
      "Stochastic Gradient Descent(26065): loss=0.003783228802366787\n",
      "Stochastic Gradient Descent(26066): loss=3.9872019192100345\n",
      "Stochastic Gradient Descent(26067): loss=4.525353183237286\n",
      "Stochastic Gradient Descent(26068): loss=5.990554646420198\n",
      "Stochastic Gradient Descent(26069): loss=2.7375655303038053\n",
      "Stochastic Gradient Descent(26070): loss=17.02447320282681\n",
      "Stochastic Gradient Descent(26071): loss=0.17350886727981107\n",
      "Stochastic Gradient Descent(26072): loss=0.1958757650994906\n",
      "Stochastic Gradient Descent(26073): loss=3.541734486407526\n",
      "Stochastic Gradient Descent(26074): loss=5.411735018625363\n",
      "Stochastic Gradient Descent(26075): loss=6.060259375039965\n",
      "Stochastic Gradient Descent(26076): loss=9.82812933692878\n",
      "Stochastic Gradient Descent(26077): loss=1.417843100895109\n",
      "Stochastic Gradient Descent(26078): loss=7.591018571187147\n",
      "Stochastic Gradient Descent(26079): loss=0.26038426218400357\n",
      "Stochastic Gradient Descent(26080): loss=0.15381681967551927\n",
      "Stochastic Gradient Descent(26081): loss=7.372811815583693\n",
      "Stochastic Gradient Descent(26082): loss=0.7318445847529246\n",
      "Stochastic Gradient Descent(26083): loss=0.055034691813400684\n",
      "Stochastic Gradient Descent(26084): loss=5.892103898729933\n",
      "Stochastic Gradient Descent(26085): loss=0.0016177972352986055\n",
      "Stochastic Gradient Descent(26086): loss=1.005405846194483\n",
      "Stochastic Gradient Descent(26087): loss=6.123698582582779\n",
      "Stochastic Gradient Descent(26088): loss=0.8039880392804059\n",
      "Stochastic Gradient Descent(26089): loss=1.4198268957876914\n",
      "Stochastic Gradient Descent(26090): loss=10.578292095391099\n",
      "Stochastic Gradient Descent(26091): loss=0.5757535450637417\n",
      "Stochastic Gradient Descent(26092): loss=0.8048828038816264\n",
      "Stochastic Gradient Descent(26093): loss=0.7556005546378786\n",
      "Stochastic Gradient Descent(26094): loss=12.613279939052106\n",
      "Stochastic Gradient Descent(26095): loss=3.6903148359634343\n",
      "Stochastic Gradient Descent(26096): loss=2.4286071555081987\n",
      "Stochastic Gradient Descent(26097): loss=2.179308738389151\n",
      "Stochastic Gradient Descent(26098): loss=10.984080010204622\n",
      "Stochastic Gradient Descent(26099): loss=1.9631618412001792\n",
      "Stochastic Gradient Descent(26100): loss=0.20040544057483944\n",
      "Stochastic Gradient Descent(26101): loss=1.4747201059946458\n",
      "Stochastic Gradient Descent(26102): loss=6.209281442610901\n",
      "Stochastic Gradient Descent(26103): loss=11.015404039018131\n",
      "Stochastic Gradient Descent(26104): loss=1.1153975248091899\n",
      "Stochastic Gradient Descent(26105): loss=11.885128774201656\n",
      "Stochastic Gradient Descent(26106): loss=9.895643270850552\n",
      "Stochastic Gradient Descent(26107): loss=15.534820270015025\n",
      "Stochastic Gradient Descent(26108): loss=14.79176908556278\n",
      "Stochastic Gradient Descent(26109): loss=0.003644750076758428\n",
      "Stochastic Gradient Descent(26110): loss=1.0374307407465406\n",
      "Stochastic Gradient Descent(26111): loss=6.194191264232633\n",
      "Stochastic Gradient Descent(26112): loss=0.00518262245450963\n",
      "Stochastic Gradient Descent(26113): loss=2.272436219528985\n",
      "Stochastic Gradient Descent(26114): loss=3.1821732892925136\n",
      "Stochastic Gradient Descent(26115): loss=4.165761612382589\n",
      "Stochastic Gradient Descent(26116): loss=1.1466482774592213\n",
      "Stochastic Gradient Descent(26117): loss=4.831519072618107\n",
      "Stochastic Gradient Descent(26118): loss=3.3977581026673036\n",
      "Stochastic Gradient Descent(26119): loss=0.038481455745647396\n",
      "Stochastic Gradient Descent(26120): loss=51.5360692857007\n",
      "Stochastic Gradient Descent(26121): loss=0.2214025625056571\n",
      "Stochastic Gradient Descent(26122): loss=23.507429151971223\n",
      "Stochastic Gradient Descent(26123): loss=1.2341537076712163\n",
      "Stochastic Gradient Descent(26124): loss=0.3599839007069195\n",
      "Stochastic Gradient Descent(26125): loss=0.2804049791889785\n",
      "Stochastic Gradient Descent(26126): loss=6.852664078778556\n",
      "Stochastic Gradient Descent(26127): loss=2.922733104215596\n",
      "Stochastic Gradient Descent(26128): loss=54.91930606474991\n",
      "Stochastic Gradient Descent(26129): loss=0.879934011260651\n",
      "Stochastic Gradient Descent(26130): loss=5.208835215047021\n",
      "Stochastic Gradient Descent(26131): loss=0.052300070545145405\n",
      "Stochastic Gradient Descent(26132): loss=5.341163115183796\n",
      "Stochastic Gradient Descent(26133): loss=3.4224490534516674\n",
      "Stochastic Gradient Descent(26134): loss=2.5099197720541455\n",
      "Stochastic Gradient Descent(26135): loss=6.8298351911269135\n",
      "Stochastic Gradient Descent(26136): loss=8.41971247073102\n",
      "Stochastic Gradient Descent(26137): loss=0.28390102653532956\n",
      "Stochastic Gradient Descent(26138): loss=2.200212236526145\n",
      "Stochastic Gradient Descent(26139): loss=5.839980656956715\n",
      "Stochastic Gradient Descent(26140): loss=4.123739594262026\n",
      "Stochastic Gradient Descent(26141): loss=0.3180212960071039\n",
      "Stochastic Gradient Descent(26142): loss=8.391410920842944\n",
      "Stochastic Gradient Descent(26143): loss=5.42331241136928\n",
      "Stochastic Gradient Descent(26144): loss=14.044266521042653\n",
      "Stochastic Gradient Descent(26145): loss=0.9205366212099206\n",
      "Stochastic Gradient Descent(26146): loss=6.380759618072189\n",
      "Stochastic Gradient Descent(26147): loss=1.7435364578498829\n",
      "Stochastic Gradient Descent(26148): loss=6.840568916192806\n",
      "Stochastic Gradient Descent(26149): loss=1.8682725326062963\n",
      "Stochastic Gradient Descent(26150): loss=29.605428157100203\n",
      "Stochastic Gradient Descent(26151): loss=3.944221273815166\n",
      "Stochastic Gradient Descent(26152): loss=0.28331241634865445\n",
      "Stochastic Gradient Descent(26153): loss=16.325314089246017\n",
      "Stochastic Gradient Descent(26154): loss=1.5923390761778644\n",
      "Stochastic Gradient Descent(26155): loss=1.4933938180597155\n",
      "Stochastic Gradient Descent(26156): loss=20.784431192932075\n",
      "Stochastic Gradient Descent(26157): loss=0.01494908715837707\n",
      "Stochastic Gradient Descent(26158): loss=4.1726431647572975\n",
      "Stochastic Gradient Descent(26159): loss=0.05440006819764165\n",
      "Stochastic Gradient Descent(26160): loss=0.35702705887760566\n",
      "Stochastic Gradient Descent(26161): loss=9.545405784626215\n",
      "Stochastic Gradient Descent(26162): loss=3.0329229654510232\n",
      "Stochastic Gradient Descent(26163): loss=19.63572310392424\n",
      "Stochastic Gradient Descent(26164): loss=1.3350264007108938\n",
      "Stochastic Gradient Descent(26165): loss=4.888245440533008\n",
      "Stochastic Gradient Descent(26166): loss=19.81414782177414\n",
      "Stochastic Gradient Descent(26167): loss=7.091199570487777\n",
      "Stochastic Gradient Descent(26168): loss=0.8572933187029651\n",
      "Stochastic Gradient Descent(26169): loss=7.8108883616839515\n",
      "Stochastic Gradient Descent(26170): loss=6.265978399648451\n",
      "Stochastic Gradient Descent(26171): loss=1.5904395813056937\n",
      "Stochastic Gradient Descent(26172): loss=0.17640246447082508\n",
      "Stochastic Gradient Descent(26173): loss=14.412787205737347\n",
      "Stochastic Gradient Descent(26174): loss=2.6033060431085184\n",
      "Stochastic Gradient Descent(26175): loss=5.948984751558941\n",
      "Stochastic Gradient Descent(26176): loss=4.698117293654318\n",
      "Stochastic Gradient Descent(26177): loss=14.864180325383048\n",
      "Stochastic Gradient Descent(26178): loss=6.725312452536347\n",
      "Stochastic Gradient Descent(26179): loss=1.0377554636099535\n",
      "Stochastic Gradient Descent(26180): loss=0.09910781854012171\n",
      "Stochastic Gradient Descent(26181): loss=1.430241921092144\n",
      "Stochastic Gradient Descent(26182): loss=9.693495344506829\n",
      "Stochastic Gradient Descent(26183): loss=0.11518229352153406\n",
      "Stochastic Gradient Descent(26184): loss=6.854789710813189\n",
      "Stochastic Gradient Descent(26185): loss=10.493316022538586\n",
      "Stochastic Gradient Descent(26186): loss=8.877238343161396\n",
      "Stochastic Gradient Descent(26187): loss=2.6324356007262018\n",
      "Stochastic Gradient Descent(26188): loss=0.5566036482296057\n",
      "Stochastic Gradient Descent(26189): loss=3.044072115342727\n",
      "Stochastic Gradient Descent(26190): loss=0.7355162869800099\n",
      "Stochastic Gradient Descent(26191): loss=18.168951060444265\n",
      "Stochastic Gradient Descent(26192): loss=1.1719911967091312\n",
      "Stochastic Gradient Descent(26193): loss=3.361135533319229\n",
      "Stochastic Gradient Descent(26194): loss=6.1249176817644\n",
      "Stochastic Gradient Descent(26195): loss=0.6373664868914816\n",
      "Stochastic Gradient Descent(26196): loss=0.316848590980378\n",
      "Stochastic Gradient Descent(26197): loss=1.6032506922111385\n",
      "Stochastic Gradient Descent(26198): loss=0.5580803149660922\n",
      "Stochastic Gradient Descent(26199): loss=12.39322560466512\n",
      "Stochastic Gradient Descent(26200): loss=0.20905861438965273\n",
      "Stochastic Gradient Descent(26201): loss=42.5520302262929\n",
      "Stochastic Gradient Descent(26202): loss=6.013705651496443\n",
      "Stochastic Gradient Descent(26203): loss=0.8348390471072333\n",
      "Stochastic Gradient Descent(26204): loss=0.21369452197379987\n",
      "Stochastic Gradient Descent(26205): loss=0.20577402723628443\n",
      "Stochastic Gradient Descent(26206): loss=14.335818024616843\n",
      "Stochastic Gradient Descent(26207): loss=3.208106304929428\n",
      "Stochastic Gradient Descent(26208): loss=3.7409300364076112\n",
      "Stochastic Gradient Descent(26209): loss=7.899843946414963\n",
      "Stochastic Gradient Descent(26210): loss=0.13299767329043147\n",
      "Stochastic Gradient Descent(26211): loss=19.57086093465829\n",
      "Stochastic Gradient Descent(26212): loss=18.50995318816268\n",
      "Stochastic Gradient Descent(26213): loss=7.253840465893653\n",
      "Stochastic Gradient Descent(26214): loss=0.4256013819470279\n",
      "Stochastic Gradient Descent(26215): loss=62.31079195384558\n",
      "Stochastic Gradient Descent(26216): loss=55.74756471686765\n",
      "Stochastic Gradient Descent(26217): loss=5.674882138980163\n",
      "Stochastic Gradient Descent(26218): loss=40.81357475081036\n",
      "Stochastic Gradient Descent(26219): loss=3.0338963866678137\n",
      "Stochastic Gradient Descent(26220): loss=2.419087426945359\n",
      "Stochastic Gradient Descent(26221): loss=0.8547625698315138\n",
      "Stochastic Gradient Descent(26222): loss=0.8218050627743736\n",
      "Stochastic Gradient Descent(26223): loss=0.21563009274166417\n",
      "Stochastic Gradient Descent(26224): loss=0.0031401890704847797\n",
      "Stochastic Gradient Descent(26225): loss=1.6319121883947931\n",
      "Stochastic Gradient Descent(26226): loss=0.0005572411747887321\n",
      "Stochastic Gradient Descent(26227): loss=2.3322314888696933\n",
      "Stochastic Gradient Descent(26228): loss=0.012843943030923606\n",
      "Stochastic Gradient Descent(26229): loss=0.028703966135086644\n",
      "Stochastic Gradient Descent(26230): loss=0.030865625690618768\n",
      "Stochastic Gradient Descent(26231): loss=10.737473645870104\n",
      "Stochastic Gradient Descent(26232): loss=15.888700184878617\n",
      "Stochastic Gradient Descent(26233): loss=5.766259659016802\n",
      "Stochastic Gradient Descent(26234): loss=29.343499301429645\n",
      "Stochastic Gradient Descent(26235): loss=11.880577446940078\n",
      "Stochastic Gradient Descent(26236): loss=3.984162572962828\n",
      "Stochastic Gradient Descent(26237): loss=0.0003060739057708909\n",
      "Stochastic Gradient Descent(26238): loss=0.16028728854104865\n",
      "Stochastic Gradient Descent(26239): loss=5.832270998931146\n",
      "Stochastic Gradient Descent(26240): loss=5.552289380708661\n",
      "Stochastic Gradient Descent(26241): loss=3.534660870770659\n",
      "Stochastic Gradient Descent(26242): loss=0.011632524738897441\n",
      "Stochastic Gradient Descent(26243): loss=0.04477084649875612\n",
      "Stochastic Gradient Descent(26244): loss=0.002038480853324111\n",
      "Stochastic Gradient Descent(26245): loss=9.040694318405528\n",
      "Stochastic Gradient Descent(26246): loss=5.938850273348568\n",
      "Stochastic Gradient Descent(26247): loss=0.31145409009447766\n",
      "Stochastic Gradient Descent(26248): loss=1.243276217520404\n",
      "Stochastic Gradient Descent(26249): loss=0.5244683516346875\n",
      "Stochastic Gradient Descent(26250): loss=4.253274914402036\n",
      "Stochastic Gradient Descent(26251): loss=0.7023787662397384\n",
      "Stochastic Gradient Descent(26252): loss=1.6692297322651475\n",
      "Stochastic Gradient Descent(26253): loss=0.12121586429275408\n",
      "Stochastic Gradient Descent(26254): loss=0.10912063204411539\n",
      "Stochastic Gradient Descent(26255): loss=5.183811099327061\n",
      "Stochastic Gradient Descent(26256): loss=6.124612770736832\n",
      "Stochastic Gradient Descent(26257): loss=1.7991994777998945\n",
      "Stochastic Gradient Descent(26258): loss=0.5474231468288342\n",
      "Stochastic Gradient Descent(26259): loss=0.5744786528747102\n",
      "Stochastic Gradient Descent(26260): loss=3.183578180525557\n",
      "Stochastic Gradient Descent(26261): loss=0.320407665292151\n",
      "Stochastic Gradient Descent(26262): loss=75.64675966698266\n",
      "Stochastic Gradient Descent(26263): loss=10.2221993667274\n",
      "Stochastic Gradient Descent(26264): loss=1.0560039238492713\n",
      "Stochastic Gradient Descent(26265): loss=4.838164404888332\n",
      "Stochastic Gradient Descent(26266): loss=17.293465487844497\n",
      "Stochastic Gradient Descent(26267): loss=0.3439219775864236\n",
      "Stochastic Gradient Descent(26268): loss=1.491572594722648\n",
      "Stochastic Gradient Descent(26269): loss=6.211098526676496\n",
      "Stochastic Gradient Descent(26270): loss=6.077425070503052\n",
      "Stochastic Gradient Descent(26271): loss=15.792113770306925\n",
      "Stochastic Gradient Descent(26272): loss=5.494889436766596\n",
      "Stochastic Gradient Descent(26273): loss=2.8327281016233936\n",
      "Stochastic Gradient Descent(26274): loss=8.153672736989813\n",
      "Stochastic Gradient Descent(26275): loss=2.4593984526365182\n",
      "Stochastic Gradient Descent(26276): loss=2.7136375605612826\n",
      "Stochastic Gradient Descent(26277): loss=0.0032593234805749073\n",
      "Stochastic Gradient Descent(26278): loss=26.403810060523945\n",
      "Stochastic Gradient Descent(26279): loss=3.0937051281729433\n",
      "Stochastic Gradient Descent(26280): loss=1.2330797659805033\n",
      "Stochastic Gradient Descent(26281): loss=1.6940728701465884\n",
      "Stochastic Gradient Descent(26282): loss=0.0779300384692876\n",
      "Stochastic Gradient Descent(26283): loss=0.29141301779066586\n",
      "Stochastic Gradient Descent(26284): loss=2.01727377015865\n",
      "Stochastic Gradient Descent(26285): loss=4.395689074093701\n",
      "Stochastic Gradient Descent(26286): loss=1.9760529169050256\n",
      "Stochastic Gradient Descent(26287): loss=0.5834180150165112\n",
      "Stochastic Gradient Descent(26288): loss=0.48156378152445023\n",
      "Stochastic Gradient Descent(26289): loss=9.253626112614617\n",
      "Stochastic Gradient Descent(26290): loss=9.29698962324554\n",
      "Stochastic Gradient Descent(26291): loss=0.06127695702857341\n",
      "Stochastic Gradient Descent(26292): loss=0.9787180955682172\n",
      "Stochastic Gradient Descent(26293): loss=2.326825368296766\n",
      "Stochastic Gradient Descent(26294): loss=23.94991525558682\n",
      "Stochastic Gradient Descent(26295): loss=6.988246280271593\n",
      "Stochastic Gradient Descent(26296): loss=2.8993021315553418\n",
      "Stochastic Gradient Descent(26297): loss=0.5126159421232955\n",
      "Stochastic Gradient Descent(26298): loss=2.7415684581285067\n",
      "Stochastic Gradient Descent(26299): loss=0.17301432440890918\n",
      "Stochastic Gradient Descent(26300): loss=0.6009672474461133\n",
      "Stochastic Gradient Descent(26301): loss=1.0604487808505125\n",
      "Stochastic Gradient Descent(26302): loss=5.155274074848964\n",
      "Stochastic Gradient Descent(26303): loss=22.178574578140474\n",
      "Stochastic Gradient Descent(26304): loss=0.6290797190375625\n",
      "Stochastic Gradient Descent(26305): loss=0.03629486660506252\n",
      "Stochastic Gradient Descent(26306): loss=2.2911039329921317\n",
      "Stochastic Gradient Descent(26307): loss=0.3354268276907751\n",
      "Stochastic Gradient Descent(26308): loss=34.42793528043619\n",
      "Stochastic Gradient Descent(26309): loss=5.849058042434794e-05\n",
      "Stochastic Gradient Descent(26310): loss=3.3385178775744184\n",
      "Stochastic Gradient Descent(26311): loss=0.2954569929929129\n",
      "Stochastic Gradient Descent(26312): loss=0.7657258378307336\n",
      "Stochastic Gradient Descent(26313): loss=0.3837606100799767\n",
      "Stochastic Gradient Descent(26314): loss=24.48377394742643\n",
      "Stochastic Gradient Descent(26315): loss=0.26766393076496603\n",
      "Stochastic Gradient Descent(26316): loss=0.27984622790559094\n",
      "Stochastic Gradient Descent(26317): loss=1.2214045357298156\n",
      "Stochastic Gradient Descent(26318): loss=1.243228501136071\n",
      "Stochastic Gradient Descent(26319): loss=8.218340584302359\n",
      "Stochastic Gradient Descent(26320): loss=0.3558723749465693\n",
      "Stochastic Gradient Descent(26321): loss=0.00011769940575456658\n",
      "Stochastic Gradient Descent(26322): loss=1.805136910955116\n",
      "Stochastic Gradient Descent(26323): loss=18.925328983105576\n",
      "Stochastic Gradient Descent(26324): loss=6.48525402681901\n",
      "Stochastic Gradient Descent(26325): loss=8.488517608428971\n",
      "Stochastic Gradient Descent(26326): loss=1.2618264902860812\n",
      "Stochastic Gradient Descent(26327): loss=0.04631599099329332\n",
      "Stochastic Gradient Descent(26328): loss=0.4281384486431294\n",
      "Stochastic Gradient Descent(26329): loss=2.8880620391382448\n",
      "Stochastic Gradient Descent(26330): loss=15.213110122146086\n",
      "Stochastic Gradient Descent(26331): loss=4.772322264371987\n",
      "Stochastic Gradient Descent(26332): loss=6.167103712399758\n",
      "Stochastic Gradient Descent(26333): loss=1.7161309313559956\n",
      "Stochastic Gradient Descent(26334): loss=4.679475032174661\n",
      "Stochastic Gradient Descent(26335): loss=10.679314748696893\n",
      "Stochastic Gradient Descent(26336): loss=0.010033985938461672\n",
      "Stochastic Gradient Descent(26337): loss=0.24305419220570063\n",
      "Stochastic Gradient Descent(26338): loss=11.161714469718948\n",
      "Stochastic Gradient Descent(26339): loss=0.2621066960192085\n",
      "Stochastic Gradient Descent(26340): loss=5.338369224382465\n",
      "Stochastic Gradient Descent(26341): loss=4.060429463339072\n",
      "Stochastic Gradient Descent(26342): loss=1.1505407186100192\n",
      "Stochastic Gradient Descent(26343): loss=2.8835237869517716\n",
      "Stochastic Gradient Descent(26344): loss=1.268113136968079\n",
      "Stochastic Gradient Descent(26345): loss=0.09334430700236075\n",
      "Stochastic Gradient Descent(26346): loss=2.0106360881184444\n",
      "Stochastic Gradient Descent(26347): loss=0.8531696569873447\n",
      "Stochastic Gradient Descent(26348): loss=5.082649011085052\n",
      "Stochastic Gradient Descent(26349): loss=0.18355724480475763\n",
      "Stochastic Gradient Descent(26350): loss=29.64739678141069\n",
      "Stochastic Gradient Descent(26351): loss=8.003637973973833\n",
      "Stochastic Gradient Descent(26352): loss=50.35314558166233\n",
      "Stochastic Gradient Descent(26353): loss=1.3533035651810403\n",
      "Stochastic Gradient Descent(26354): loss=0.07307179992205724\n",
      "Stochastic Gradient Descent(26355): loss=0.002172827895595722\n",
      "Stochastic Gradient Descent(26356): loss=5.59964748325529\n",
      "Stochastic Gradient Descent(26357): loss=30.656835044267815\n",
      "Stochastic Gradient Descent(26358): loss=0.26362010555272547\n",
      "Stochastic Gradient Descent(26359): loss=6.206018068913583\n",
      "Stochastic Gradient Descent(26360): loss=1.5027472182962676\n",
      "Stochastic Gradient Descent(26361): loss=11.158694885710833\n",
      "Stochastic Gradient Descent(26362): loss=0.5859798525794447\n",
      "Stochastic Gradient Descent(26363): loss=10.097113909501463\n",
      "Stochastic Gradient Descent(26364): loss=0.6664646934325101\n",
      "Stochastic Gradient Descent(26365): loss=0.1178951013515208\n",
      "Stochastic Gradient Descent(26366): loss=0.19274404771084527\n",
      "Stochastic Gradient Descent(26367): loss=1.189423167705553\n",
      "Stochastic Gradient Descent(26368): loss=1.8898361008451114\n",
      "Stochastic Gradient Descent(26369): loss=1.2904847988610395\n",
      "Stochastic Gradient Descent(26370): loss=2.5527467864238\n",
      "Stochastic Gradient Descent(26371): loss=3.3226749234551796\n",
      "Stochastic Gradient Descent(26372): loss=0.26799342178534274\n",
      "Stochastic Gradient Descent(26373): loss=1.1087010757524152\n",
      "Stochastic Gradient Descent(26374): loss=0.01636225064609466\n",
      "Stochastic Gradient Descent(26375): loss=1.5508828612584\n",
      "Stochastic Gradient Descent(26376): loss=2.422845744943028\n",
      "Stochastic Gradient Descent(26377): loss=0.6401657853935281\n",
      "Stochastic Gradient Descent(26378): loss=0.12095159111598253\n",
      "Stochastic Gradient Descent(26379): loss=0.5884797963739108\n",
      "Stochastic Gradient Descent(26380): loss=1.8521630325814298\n",
      "Stochastic Gradient Descent(26381): loss=1.4069813577766321\n",
      "Stochastic Gradient Descent(26382): loss=0.2195177826475439\n",
      "Stochastic Gradient Descent(26383): loss=0.10395093762821869\n",
      "Stochastic Gradient Descent(26384): loss=4.3362501389562995\n",
      "Stochastic Gradient Descent(26385): loss=0.023984160213688953\n",
      "Stochastic Gradient Descent(26386): loss=0.05155756940668326\n",
      "Stochastic Gradient Descent(26387): loss=0.5508598819792565\n",
      "Stochastic Gradient Descent(26388): loss=0.004473361027344841\n",
      "Stochastic Gradient Descent(26389): loss=5.092848356716112\n",
      "Stochastic Gradient Descent(26390): loss=3.879325085807055\n",
      "Stochastic Gradient Descent(26391): loss=0.8628238599302419\n",
      "Stochastic Gradient Descent(26392): loss=0.5177586369638121\n",
      "Stochastic Gradient Descent(26393): loss=7.924958851696898\n",
      "Stochastic Gradient Descent(26394): loss=0.42382990555742717\n",
      "Stochastic Gradient Descent(26395): loss=7.299291059148793\n",
      "Stochastic Gradient Descent(26396): loss=8.715273022141988\n",
      "Stochastic Gradient Descent(26397): loss=0.007564989558281422\n",
      "Stochastic Gradient Descent(26398): loss=9.0464897578871\n",
      "Stochastic Gradient Descent(26399): loss=0.8947455471782163\n",
      "Stochastic Gradient Descent(26400): loss=0.7072446195539726\n",
      "Stochastic Gradient Descent(26401): loss=13.21229079075996\n",
      "Stochastic Gradient Descent(26402): loss=0.00037649734779739234\n",
      "Stochastic Gradient Descent(26403): loss=0.22593982495428627\n",
      "Stochastic Gradient Descent(26404): loss=7.682158038965548\n",
      "Stochastic Gradient Descent(26405): loss=35.85586303081674\n",
      "Stochastic Gradient Descent(26406): loss=11.441435143172\n",
      "Stochastic Gradient Descent(26407): loss=1.6991556407553237\n",
      "Stochastic Gradient Descent(26408): loss=2.7836514036887725\n",
      "Stochastic Gradient Descent(26409): loss=0.8365097333497042\n",
      "Stochastic Gradient Descent(26410): loss=0.0009186255068505424\n",
      "Stochastic Gradient Descent(26411): loss=0.23146652650766578\n",
      "Stochastic Gradient Descent(26412): loss=24.28032674012903\n",
      "Stochastic Gradient Descent(26413): loss=0.6222296344171848\n",
      "Stochastic Gradient Descent(26414): loss=17.12841610649312\n",
      "Stochastic Gradient Descent(26415): loss=2.3449408716572786\n",
      "Stochastic Gradient Descent(26416): loss=0.9094092584598067\n",
      "Stochastic Gradient Descent(26417): loss=0.41886550993551674\n",
      "Stochastic Gradient Descent(26418): loss=0.00889296346020001\n",
      "Stochastic Gradient Descent(26419): loss=9.0875975214427\n",
      "Stochastic Gradient Descent(26420): loss=6.196487324464009\n",
      "Stochastic Gradient Descent(26421): loss=5.182468240773907\n",
      "Stochastic Gradient Descent(26422): loss=6.6658007291487085\n",
      "Stochastic Gradient Descent(26423): loss=11.402851986460197\n",
      "Stochastic Gradient Descent(26424): loss=0.0075086550047267385\n",
      "Stochastic Gradient Descent(26425): loss=1.0154178254301531\n",
      "Stochastic Gradient Descent(26426): loss=1.8153775565061612\n",
      "Stochastic Gradient Descent(26427): loss=13.342628110097857\n",
      "Stochastic Gradient Descent(26428): loss=5.529550183218884\n",
      "Stochastic Gradient Descent(26429): loss=0.32490580962132626\n",
      "Stochastic Gradient Descent(26430): loss=0.0001743863719117404\n",
      "Stochastic Gradient Descent(26431): loss=0.011077924529207053\n",
      "Stochastic Gradient Descent(26432): loss=75.45577875245353\n",
      "Stochastic Gradient Descent(26433): loss=0.8646278632168971\n",
      "Stochastic Gradient Descent(26434): loss=1.1506598191055195\n",
      "Stochastic Gradient Descent(26435): loss=2.2897460007101014\n",
      "Stochastic Gradient Descent(26436): loss=0.17814356676973447\n",
      "Stochastic Gradient Descent(26437): loss=10.684967905186106\n",
      "Stochastic Gradient Descent(26438): loss=10.53656725582788\n",
      "Stochastic Gradient Descent(26439): loss=0.072313053964903\n",
      "Stochastic Gradient Descent(26440): loss=0.148774094689093\n",
      "Stochastic Gradient Descent(26441): loss=1.9890483560729169\n",
      "Stochastic Gradient Descent(26442): loss=0.6739078636046073\n",
      "Stochastic Gradient Descent(26443): loss=0.8233801640273194\n",
      "Stochastic Gradient Descent(26444): loss=2.339546681514801\n",
      "Stochastic Gradient Descent(26445): loss=1.6177434639095634\n",
      "Stochastic Gradient Descent(26446): loss=3.195404997595702\n",
      "Stochastic Gradient Descent(26447): loss=3.3356259474230954\n",
      "Stochastic Gradient Descent(26448): loss=0.005061835862079737\n",
      "Stochastic Gradient Descent(26449): loss=2.3977015208254753\n",
      "Stochastic Gradient Descent(26450): loss=2.093714001190977\n",
      "Stochastic Gradient Descent(26451): loss=0.05497869521950272\n",
      "Stochastic Gradient Descent(26452): loss=1.7101866560490155\n",
      "Stochastic Gradient Descent(26453): loss=0.7294266785145087\n",
      "Stochastic Gradient Descent(26454): loss=0.31322406503935324\n",
      "Stochastic Gradient Descent(26455): loss=0.014017493663556542\n",
      "Stochastic Gradient Descent(26456): loss=1.392690988024296\n",
      "Stochastic Gradient Descent(26457): loss=6.005357483998984\n",
      "Stochastic Gradient Descent(26458): loss=1.57528986734285\n",
      "Stochastic Gradient Descent(26459): loss=9.455864982449732\n",
      "Stochastic Gradient Descent(26460): loss=6.098941145673858\n",
      "Stochastic Gradient Descent(26461): loss=3.8359671082007294\n",
      "Stochastic Gradient Descent(26462): loss=17.304886860481083\n",
      "Stochastic Gradient Descent(26463): loss=0.03232348444558182\n",
      "Stochastic Gradient Descent(26464): loss=0.15185503001527445\n",
      "Stochastic Gradient Descent(26465): loss=2.804579854655767\n",
      "Stochastic Gradient Descent(26466): loss=0.0773563297528301\n",
      "Stochastic Gradient Descent(26467): loss=1.8394742767715093\n",
      "Stochastic Gradient Descent(26468): loss=14.98223918614888\n",
      "Stochastic Gradient Descent(26469): loss=11.570243048842162\n",
      "Stochastic Gradient Descent(26470): loss=3.7238275826760145\n",
      "Stochastic Gradient Descent(26471): loss=9.128558116816608\n",
      "Stochastic Gradient Descent(26472): loss=3.335896369847601\n",
      "Stochastic Gradient Descent(26473): loss=11.602468327290284\n",
      "Stochastic Gradient Descent(26474): loss=5.589212816345775\n",
      "Stochastic Gradient Descent(26475): loss=6.897038360834983\n",
      "Stochastic Gradient Descent(26476): loss=0.8819565629006646\n",
      "Stochastic Gradient Descent(26477): loss=0.13329799973731501\n",
      "Stochastic Gradient Descent(26478): loss=0.5868026941332454\n",
      "Stochastic Gradient Descent(26479): loss=5.047662255241408\n",
      "Stochastic Gradient Descent(26480): loss=36.92099379695155\n",
      "Stochastic Gradient Descent(26481): loss=6.29173273535313\n",
      "Stochastic Gradient Descent(26482): loss=1.3421515936959068\n",
      "Stochastic Gradient Descent(26483): loss=0.04255329156452384\n",
      "Stochastic Gradient Descent(26484): loss=17.591701751045804\n",
      "Stochastic Gradient Descent(26485): loss=18.869910768465843\n",
      "Stochastic Gradient Descent(26486): loss=0.4549801790697994\n",
      "Stochastic Gradient Descent(26487): loss=6.532845876044703\n",
      "Stochastic Gradient Descent(26488): loss=12.100765160269018\n",
      "Stochastic Gradient Descent(26489): loss=5.3451240909492705\n",
      "Stochastic Gradient Descent(26490): loss=1.2983489508638673\n",
      "Stochastic Gradient Descent(26491): loss=3.705230802296579\n",
      "Stochastic Gradient Descent(26492): loss=0.06000083995484955\n",
      "Stochastic Gradient Descent(26493): loss=17.72425790647006\n",
      "Stochastic Gradient Descent(26494): loss=0.08535943015857123\n",
      "Stochastic Gradient Descent(26495): loss=4.856314011638315\n",
      "Stochastic Gradient Descent(26496): loss=0.13297013068494434\n",
      "Stochastic Gradient Descent(26497): loss=2.5107499416078323\n",
      "Stochastic Gradient Descent(26498): loss=3.479756601826521\n",
      "Stochastic Gradient Descent(26499): loss=1.430512684100592\n",
      "Stochastic Gradient Descent(26500): loss=12.68641267693973\n",
      "Stochastic Gradient Descent(26501): loss=1.6982482830116385\n",
      "Stochastic Gradient Descent(26502): loss=0.6270743559383735\n",
      "Stochastic Gradient Descent(26503): loss=24.60900793354081\n",
      "Stochastic Gradient Descent(26504): loss=1.7444961442647287\n",
      "Stochastic Gradient Descent(26505): loss=4.671526552597861\n",
      "Stochastic Gradient Descent(26506): loss=17.809947563318488\n",
      "Stochastic Gradient Descent(26507): loss=3.2971554096738527\n",
      "Stochastic Gradient Descent(26508): loss=0.8759917217251367\n",
      "Stochastic Gradient Descent(26509): loss=2.232341850251187\n",
      "Stochastic Gradient Descent(26510): loss=0.0009034215263813385\n",
      "Stochastic Gradient Descent(26511): loss=0.024262901238461677\n",
      "Stochastic Gradient Descent(26512): loss=0.16932972921514822\n",
      "Stochastic Gradient Descent(26513): loss=0.9080165267492017\n",
      "Stochastic Gradient Descent(26514): loss=1.677682658577347\n",
      "Stochastic Gradient Descent(26515): loss=1.4403297701418591\n",
      "Stochastic Gradient Descent(26516): loss=3.109473587055959\n",
      "Stochastic Gradient Descent(26517): loss=12.890019582431425\n",
      "Stochastic Gradient Descent(26518): loss=0.012590206667935115\n",
      "Stochastic Gradient Descent(26519): loss=7.617877148215178\n",
      "Stochastic Gradient Descent(26520): loss=1.6359222486324079\n",
      "Stochastic Gradient Descent(26521): loss=0.25486502830878616\n",
      "Stochastic Gradient Descent(26522): loss=0.6192283514476111\n",
      "Stochastic Gradient Descent(26523): loss=0.4008724275714994\n",
      "Stochastic Gradient Descent(26524): loss=0.04919060789008691\n",
      "Stochastic Gradient Descent(26525): loss=0.8314616303050241\n",
      "Stochastic Gradient Descent(26526): loss=0.008580519974150986\n",
      "Stochastic Gradient Descent(26527): loss=1.6220740318922664\n",
      "Stochastic Gradient Descent(26528): loss=2.3254101630037995\n",
      "Stochastic Gradient Descent(26529): loss=0.20927312783584057\n",
      "Stochastic Gradient Descent(26530): loss=2.1897319791542085\n",
      "Stochastic Gradient Descent(26531): loss=2.9894080182267158\n",
      "Stochastic Gradient Descent(26532): loss=23.669037831675542\n",
      "Stochastic Gradient Descent(26533): loss=0.412959059288543\n",
      "Stochastic Gradient Descent(26534): loss=0.0891081417048781\n",
      "Stochastic Gradient Descent(26535): loss=0.36366574922895\n",
      "Stochastic Gradient Descent(26536): loss=2.7909828302607718\n",
      "Stochastic Gradient Descent(26537): loss=24.54404991199341\n",
      "Stochastic Gradient Descent(26538): loss=9.528084657218947\n",
      "Stochastic Gradient Descent(26539): loss=0.02597991326150173\n",
      "Stochastic Gradient Descent(26540): loss=4.2291750779722435\n",
      "Stochastic Gradient Descent(26541): loss=24.751322172930337\n",
      "Stochastic Gradient Descent(26542): loss=2.721531389448062\n",
      "Stochastic Gradient Descent(26543): loss=13.344052269877562\n",
      "Stochastic Gradient Descent(26544): loss=4.5567258035041345\n",
      "Stochastic Gradient Descent(26545): loss=13.544294301641722\n",
      "Stochastic Gradient Descent(26546): loss=2.0880527254303156\n",
      "Stochastic Gradient Descent(26547): loss=1.02051525496373\n",
      "Stochastic Gradient Descent(26548): loss=0.5607715008543052\n",
      "Stochastic Gradient Descent(26549): loss=11.527060127179851\n",
      "Stochastic Gradient Descent(26550): loss=8.58617108087566\n",
      "Stochastic Gradient Descent(26551): loss=6.239870354785067\n",
      "Stochastic Gradient Descent(26552): loss=2.773055888024692\n",
      "Stochastic Gradient Descent(26553): loss=2.4124397539330245\n",
      "Stochastic Gradient Descent(26554): loss=1.1170952772978275\n",
      "Stochastic Gradient Descent(26555): loss=12.361157566290872\n",
      "Stochastic Gradient Descent(26556): loss=12.10066214227011\n",
      "Stochastic Gradient Descent(26557): loss=0.24081518427573947\n",
      "Stochastic Gradient Descent(26558): loss=0.15123729240406283\n",
      "Stochastic Gradient Descent(26559): loss=1.7101848434978664\n",
      "Stochastic Gradient Descent(26560): loss=0.019276825784892433\n",
      "Stochastic Gradient Descent(26561): loss=4.231100677894807\n",
      "Stochastic Gradient Descent(26562): loss=1.5653559883433674\n",
      "Stochastic Gradient Descent(26563): loss=3.428072195123256\n",
      "Stochastic Gradient Descent(26564): loss=4.7694828945237635\n",
      "Stochastic Gradient Descent(26565): loss=1.196474240799183\n",
      "Stochastic Gradient Descent(26566): loss=0.002300569418493694\n",
      "Stochastic Gradient Descent(26567): loss=1.769662285743802\n",
      "Stochastic Gradient Descent(26568): loss=5.494101430445124\n",
      "Stochastic Gradient Descent(26569): loss=0.18694448545691011\n",
      "Stochastic Gradient Descent(26570): loss=2.521953140165686\n",
      "Stochastic Gradient Descent(26571): loss=2.323271820805703\n",
      "Stochastic Gradient Descent(26572): loss=2.7968687042177076\n",
      "Stochastic Gradient Descent(26573): loss=0.2837919754940277\n",
      "Stochastic Gradient Descent(26574): loss=10.735958653599912\n",
      "Stochastic Gradient Descent(26575): loss=1.3039676003800484\n",
      "Stochastic Gradient Descent(26576): loss=2.540504533975955\n",
      "Stochastic Gradient Descent(26577): loss=19.772845655180955\n",
      "Stochastic Gradient Descent(26578): loss=0.020456938506841573\n",
      "Stochastic Gradient Descent(26579): loss=1.7102803007172405\n",
      "Stochastic Gradient Descent(26580): loss=9.054852377279856\n",
      "Stochastic Gradient Descent(26581): loss=0.1908579050328656\n",
      "Stochastic Gradient Descent(26582): loss=74.33885096192749\n",
      "Stochastic Gradient Descent(26583): loss=0.00035843649765050133\n",
      "Stochastic Gradient Descent(26584): loss=0.0014730403464615844\n",
      "Stochastic Gradient Descent(26585): loss=10.327906205466144\n",
      "Stochastic Gradient Descent(26586): loss=3.7018192507022527\n",
      "Stochastic Gradient Descent(26587): loss=0.3365503859239648\n",
      "Stochastic Gradient Descent(26588): loss=0.542903039480816\n",
      "Stochastic Gradient Descent(26589): loss=0.023278962811911025\n",
      "Stochastic Gradient Descent(26590): loss=19.15221172543158\n",
      "Stochastic Gradient Descent(26591): loss=0.8153810941114983\n",
      "Stochastic Gradient Descent(26592): loss=1.8893899992534235\n",
      "Stochastic Gradient Descent(26593): loss=1.2843378454059768\n",
      "Stochastic Gradient Descent(26594): loss=10.488246866022518\n",
      "Stochastic Gradient Descent(26595): loss=34.37769001805387\n",
      "Stochastic Gradient Descent(26596): loss=10.973351209461336\n",
      "Stochastic Gradient Descent(26597): loss=20.08589156994869\n",
      "Stochastic Gradient Descent(26598): loss=2.5228231136956003\n",
      "Stochastic Gradient Descent(26599): loss=21.714328157925024\n",
      "Stochastic Gradient Descent(26600): loss=3.8435267682837178\n",
      "Stochastic Gradient Descent(26601): loss=7.174324316979754\n",
      "Stochastic Gradient Descent(26602): loss=1.29000616591535\n",
      "Stochastic Gradient Descent(26603): loss=2.877208348093942\n",
      "Stochastic Gradient Descent(26604): loss=5.342537541423498\n",
      "Stochastic Gradient Descent(26605): loss=2.71134714397326\n",
      "Stochastic Gradient Descent(26606): loss=4.231741057609803\n",
      "Stochastic Gradient Descent(26607): loss=0.05841629447539609\n",
      "Stochastic Gradient Descent(26608): loss=0.06147210961472757\n",
      "Stochastic Gradient Descent(26609): loss=0.09276009867389949\n",
      "Stochastic Gradient Descent(26610): loss=0.09833817898731306\n",
      "Stochastic Gradient Descent(26611): loss=5.417757441509041\n",
      "Stochastic Gradient Descent(26612): loss=0.0002089442986973243\n",
      "Stochastic Gradient Descent(26613): loss=10.75152888761925\n",
      "Stochastic Gradient Descent(26614): loss=0.18044646391794747\n",
      "Stochastic Gradient Descent(26615): loss=0.800819964125999\n",
      "Stochastic Gradient Descent(26616): loss=7.157811848660787\n",
      "Stochastic Gradient Descent(26617): loss=4.5049316052338115\n",
      "Stochastic Gradient Descent(26618): loss=1.6235889390946276\n",
      "Stochastic Gradient Descent(26619): loss=8.74739918356862e-06\n",
      "Stochastic Gradient Descent(26620): loss=0.32253385848086263\n",
      "Stochastic Gradient Descent(26621): loss=1.1613291347509571\n",
      "Stochastic Gradient Descent(26622): loss=3.0200149484570855\n",
      "Stochastic Gradient Descent(26623): loss=0.22234268218366784\n",
      "Stochastic Gradient Descent(26624): loss=8.817058698313797\n",
      "Stochastic Gradient Descent(26625): loss=0.058115122816275504\n",
      "Stochastic Gradient Descent(26626): loss=2.2834720103530963\n",
      "Stochastic Gradient Descent(26627): loss=2.3708160878490276\n",
      "Stochastic Gradient Descent(26628): loss=1.1684326123510835\n",
      "Stochastic Gradient Descent(26629): loss=1.2468763228753033\n",
      "Stochastic Gradient Descent(26630): loss=0.02493865351031175\n",
      "Stochastic Gradient Descent(26631): loss=1.8061651203323223\n",
      "Stochastic Gradient Descent(26632): loss=0.7071188686975064\n",
      "Stochastic Gradient Descent(26633): loss=7.710853020455846\n",
      "Stochastic Gradient Descent(26634): loss=18.930977267623042\n",
      "Stochastic Gradient Descent(26635): loss=0.48903784356534813\n",
      "Stochastic Gradient Descent(26636): loss=4.118533429649114\n",
      "Stochastic Gradient Descent(26637): loss=0.48462020395881306\n",
      "Stochastic Gradient Descent(26638): loss=1.3306192447122\n",
      "Stochastic Gradient Descent(26639): loss=9.180909519422759\n",
      "Stochastic Gradient Descent(26640): loss=0.5094709739801305\n",
      "Stochastic Gradient Descent(26641): loss=4.921610130143944\n",
      "Stochastic Gradient Descent(26642): loss=5.772196178900214\n",
      "Stochastic Gradient Descent(26643): loss=0.20468591602784122\n",
      "Stochastic Gradient Descent(26644): loss=1.2259613105884217\n",
      "Stochastic Gradient Descent(26645): loss=0.20036754407602048\n",
      "Stochastic Gradient Descent(26646): loss=3.008672227146076\n",
      "Stochastic Gradient Descent(26647): loss=0.1574177416898027\n",
      "Stochastic Gradient Descent(26648): loss=14.938518610643715\n",
      "Stochastic Gradient Descent(26649): loss=2.691762693270397\n",
      "Stochastic Gradient Descent(26650): loss=8.236998616397575\n",
      "Stochastic Gradient Descent(26651): loss=10.093726873701753\n",
      "Stochastic Gradient Descent(26652): loss=6.582310834027487\n",
      "Stochastic Gradient Descent(26653): loss=1.311551251963493\n",
      "Stochastic Gradient Descent(26654): loss=0.03423645388131808\n",
      "Stochastic Gradient Descent(26655): loss=9.90120735071317\n",
      "Stochastic Gradient Descent(26656): loss=2.250785634866307\n",
      "Stochastic Gradient Descent(26657): loss=0.7171825856592474\n",
      "Stochastic Gradient Descent(26658): loss=2.731257009127116\n",
      "Stochastic Gradient Descent(26659): loss=21.81869092192082\n",
      "Stochastic Gradient Descent(26660): loss=0.8353555391763265\n",
      "Stochastic Gradient Descent(26661): loss=1.258877524106753\n",
      "Stochastic Gradient Descent(26662): loss=11.784351755461753\n",
      "Stochastic Gradient Descent(26663): loss=18.25769083567427\n",
      "Stochastic Gradient Descent(26664): loss=0.924543056831066\n",
      "Stochastic Gradient Descent(26665): loss=7.886904514308085\n",
      "Stochastic Gradient Descent(26666): loss=0.002616524758776405\n",
      "Stochastic Gradient Descent(26667): loss=0.001639569238057702\n",
      "Stochastic Gradient Descent(26668): loss=15.862199770903832\n",
      "Stochastic Gradient Descent(26669): loss=2.0650929533692195\n",
      "Stochastic Gradient Descent(26670): loss=9.11022610207186\n",
      "Stochastic Gradient Descent(26671): loss=11.607586238703943\n",
      "Stochastic Gradient Descent(26672): loss=2.4321996610213055\n",
      "Stochastic Gradient Descent(26673): loss=35.822628150169194\n",
      "Stochastic Gradient Descent(26674): loss=4.100639414267384\n",
      "Stochastic Gradient Descent(26675): loss=3.392602006459302\n",
      "Stochastic Gradient Descent(26676): loss=8.892853195575904\n",
      "Stochastic Gradient Descent(26677): loss=3.7219815548010677\n",
      "Stochastic Gradient Descent(26678): loss=0.3567692076895654\n",
      "Stochastic Gradient Descent(26679): loss=1.2753604363515145\n",
      "Stochastic Gradient Descent(26680): loss=1.8823401591366697\n",
      "Stochastic Gradient Descent(26681): loss=2.586187185246068\n",
      "Stochastic Gradient Descent(26682): loss=0.25602155556086337\n",
      "Stochastic Gradient Descent(26683): loss=0.3769873444652565\n",
      "Stochastic Gradient Descent(26684): loss=1.0723301716784384\n",
      "Stochastic Gradient Descent(26685): loss=14.635605464174555\n",
      "Stochastic Gradient Descent(26686): loss=0.02926579720976687\n",
      "Stochastic Gradient Descent(26687): loss=1.3952799348788168e-05\n",
      "Stochastic Gradient Descent(26688): loss=24.65357547409136\n",
      "Stochastic Gradient Descent(26689): loss=12.096840202011249\n",
      "Stochastic Gradient Descent(26690): loss=0.4898253317518618\n",
      "Stochastic Gradient Descent(26691): loss=9.891661506725526\n",
      "Stochastic Gradient Descent(26692): loss=0.10217164328887614\n",
      "Stochastic Gradient Descent(26693): loss=2.7034600882175868\n",
      "Stochastic Gradient Descent(26694): loss=5.794441903785436\n",
      "Stochastic Gradient Descent(26695): loss=9.002506758247588\n",
      "Stochastic Gradient Descent(26696): loss=12.079663943643926\n",
      "Stochastic Gradient Descent(26697): loss=5.480659456536592\n",
      "Stochastic Gradient Descent(26698): loss=0.6324192836703675\n",
      "Stochastic Gradient Descent(26699): loss=2.359085048656824\n",
      "Stochastic Gradient Descent(26700): loss=14.501298760374175\n",
      "Stochastic Gradient Descent(26701): loss=0.030480174510188793\n",
      "Stochastic Gradient Descent(26702): loss=0.013662268199151235\n",
      "Stochastic Gradient Descent(26703): loss=0.5354473736035996\n",
      "Stochastic Gradient Descent(26704): loss=2.7762283235503213\n",
      "Stochastic Gradient Descent(26705): loss=7.114181403256546\n",
      "Stochastic Gradient Descent(26706): loss=0.6919095356842047\n",
      "Stochastic Gradient Descent(26707): loss=1.2843821480274848\n",
      "Stochastic Gradient Descent(26708): loss=0.7152079878183961\n",
      "Stochastic Gradient Descent(26709): loss=0.0002349452873943093\n",
      "Stochastic Gradient Descent(26710): loss=0.08800411216256099\n",
      "Stochastic Gradient Descent(26711): loss=0.5946238699182536\n",
      "Stochastic Gradient Descent(26712): loss=0.31558581651981776\n",
      "Stochastic Gradient Descent(26713): loss=0.17907966321938357\n",
      "Stochastic Gradient Descent(26714): loss=4.293473271379307\n",
      "Stochastic Gradient Descent(26715): loss=5.607963128139159\n",
      "Stochastic Gradient Descent(26716): loss=2.579083939382054\n",
      "Stochastic Gradient Descent(26717): loss=0.30998162997016093\n",
      "Stochastic Gradient Descent(26718): loss=2.3000564521500344\n",
      "Stochastic Gradient Descent(26719): loss=0.02757852778597572\n",
      "Stochastic Gradient Descent(26720): loss=41.10216017008912\n",
      "Stochastic Gradient Descent(26721): loss=0.03337114412036051\n",
      "Stochastic Gradient Descent(26722): loss=5.63077788881524\n",
      "Stochastic Gradient Descent(26723): loss=0.34918503282270347\n",
      "Stochastic Gradient Descent(26724): loss=0.8926590230346998\n",
      "Stochastic Gradient Descent(26725): loss=2.480739849928895\n",
      "Stochastic Gradient Descent(26726): loss=10.108043693644689\n",
      "Stochastic Gradient Descent(26727): loss=2.448502162979258\n",
      "Stochastic Gradient Descent(26728): loss=0.9045312267821864\n",
      "Stochastic Gradient Descent(26729): loss=14.246805340363817\n",
      "Stochastic Gradient Descent(26730): loss=0.0224990959077476\n",
      "Stochastic Gradient Descent(26731): loss=4.268045612838604\n",
      "Stochastic Gradient Descent(26732): loss=13.998069194584536\n",
      "Stochastic Gradient Descent(26733): loss=1.098247258953127\n",
      "Stochastic Gradient Descent(26734): loss=1.458552501859183\n",
      "Stochastic Gradient Descent(26735): loss=3.719175734699896\n",
      "Stochastic Gradient Descent(26736): loss=8.171984790074489e-05\n",
      "Stochastic Gradient Descent(26737): loss=16.4378512063451\n",
      "Stochastic Gradient Descent(26738): loss=4.637010391656453\n",
      "Stochastic Gradient Descent(26739): loss=3.6987353268014123\n",
      "Stochastic Gradient Descent(26740): loss=0.03199027046470598\n",
      "Stochastic Gradient Descent(26741): loss=12.917799897699044\n",
      "Stochastic Gradient Descent(26742): loss=6.084746126537221\n",
      "Stochastic Gradient Descent(26743): loss=4.995421825511201\n",
      "Stochastic Gradient Descent(26744): loss=0.23921760716435012\n",
      "Stochastic Gradient Descent(26745): loss=0.1964425138802202\n",
      "Stochastic Gradient Descent(26746): loss=0.09408592033589101\n",
      "Stochastic Gradient Descent(26747): loss=0.025309041826920176\n",
      "Stochastic Gradient Descent(26748): loss=5.496696791539874\n",
      "Stochastic Gradient Descent(26749): loss=0.230754555722688\n",
      "Stochastic Gradient Descent(26750): loss=5.202626118809608\n",
      "Stochastic Gradient Descent(26751): loss=0.00836607172972391\n",
      "Stochastic Gradient Descent(26752): loss=1.0155122402954733\n",
      "Stochastic Gradient Descent(26753): loss=7.637806209700265\n",
      "Stochastic Gradient Descent(26754): loss=7.056631723984546\n",
      "Stochastic Gradient Descent(26755): loss=0.21838336634384226\n",
      "Stochastic Gradient Descent(26756): loss=2.749301499279601\n",
      "Stochastic Gradient Descent(26757): loss=5.747231712397977\n",
      "Stochastic Gradient Descent(26758): loss=5.249467275145865\n",
      "Stochastic Gradient Descent(26759): loss=0.7269572702741656\n",
      "Stochastic Gradient Descent(26760): loss=0.5052480562527935\n",
      "Stochastic Gradient Descent(26761): loss=3.175067067856563\n",
      "Stochastic Gradient Descent(26762): loss=6.805108544542344\n",
      "Stochastic Gradient Descent(26763): loss=0.35664811979408456\n",
      "Stochastic Gradient Descent(26764): loss=0.9743934784357677\n",
      "Stochastic Gradient Descent(26765): loss=0.34086527151037027\n",
      "Stochastic Gradient Descent(26766): loss=4.985369144034445\n",
      "Stochastic Gradient Descent(26767): loss=0.010959347708919905\n",
      "Stochastic Gradient Descent(26768): loss=3.530150259829616\n",
      "Stochastic Gradient Descent(26769): loss=2.3693131760261643\n",
      "Stochastic Gradient Descent(26770): loss=1.4206137348849366\n",
      "Stochastic Gradient Descent(26771): loss=0.6974054669330252\n",
      "Stochastic Gradient Descent(26772): loss=0.5048520600054619\n",
      "Stochastic Gradient Descent(26773): loss=0.28328677475019626\n",
      "Stochastic Gradient Descent(26774): loss=1.2635546244096298\n",
      "Stochastic Gradient Descent(26775): loss=0.0449569394715242\n",
      "Stochastic Gradient Descent(26776): loss=8.633681886520797\n",
      "Stochastic Gradient Descent(26777): loss=0.8063388638516565\n",
      "Stochastic Gradient Descent(26778): loss=0.7674499299710419\n",
      "Stochastic Gradient Descent(26779): loss=39.507307627152436\n",
      "Stochastic Gradient Descent(26780): loss=3.186821710853118\n",
      "Stochastic Gradient Descent(26781): loss=4.9260900628090845\n",
      "Stochastic Gradient Descent(26782): loss=2.8842706332677155\n",
      "Stochastic Gradient Descent(26783): loss=6.469192605656689\n",
      "Stochastic Gradient Descent(26784): loss=2.6875566779348716\n",
      "Stochastic Gradient Descent(26785): loss=0.6444636967986045\n",
      "Stochastic Gradient Descent(26786): loss=0.2795560744690819\n",
      "Stochastic Gradient Descent(26787): loss=12.087531099468237\n",
      "Stochastic Gradient Descent(26788): loss=5.180824329293132\n",
      "Stochastic Gradient Descent(26789): loss=3.325090423585724\n",
      "Stochastic Gradient Descent(26790): loss=0.0009663132647458473\n",
      "Stochastic Gradient Descent(26791): loss=7.981700849670204\n",
      "Stochastic Gradient Descent(26792): loss=0.0002696571697704971\n",
      "Stochastic Gradient Descent(26793): loss=0.0005702311443058854\n",
      "Stochastic Gradient Descent(26794): loss=0.5919520545896324\n",
      "Stochastic Gradient Descent(26795): loss=4.886543655578838\n",
      "Stochastic Gradient Descent(26796): loss=0.011813277143268578\n",
      "Stochastic Gradient Descent(26797): loss=0.006716939677394772\n",
      "Stochastic Gradient Descent(26798): loss=5.562082245626925\n",
      "Stochastic Gradient Descent(26799): loss=5.479970479035426\n",
      "Stochastic Gradient Descent(26800): loss=1.401477977319935\n",
      "Stochastic Gradient Descent(26801): loss=4.4874753552695905\n",
      "Stochastic Gradient Descent(26802): loss=0.6350746700861892\n",
      "Stochastic Gradient Descent(26803): loss=1.5040342294661497\n",
      "Stochastic Gradient Descent(26804): loss=2.1664512380356635\n",
      "Stochastic Gradient Descent(26805): loss=0.07089491075958994\n",
      "Stochastic Gradient Descent(26806): loss=0.21711279638657402\n",
      "Stochastic Gradient Descent(26807): loss=0.0208548132151272\n",
      "Stochastic Gradient Descent(26808): loss=12.843049097627636\n",
      "Stochastic Gradient Descent(26809): loss=9.848994554718727\n",
      "Stochastic Gradient Descent(26810): loss=0.9878694444231123\n",
      "Stochastic Gradient Descent(26811): loss=0.010213354783527298\n",
      "Stochastic Gradient Descent(26812): loss=7.349934128086274\n",
      "Stochastic Gradient Descent(26813): loss=0.2070580566871166\n",
      "Stochastic Gradient Descent(26814): loss=0.008176222459409858\n",
      "Stochastic Gradient Descent(26815): loss=12.549084777240319\n",
      "Stochastic Gradient Descent(26816): loss=0.26302421248759994\n",
      "Stochastic Gradient Descent(26817): loss=0.7104704060079046\n",
      "Stochastic Gradient Descent(26818): loss=1.2852470934330085\n",
      "Stochastic Gradient Descent(26819): loss=5.568945107228053\n",
      "Stochastic Gradient Descent(26820): loss=2.636137876727215\n",
      "Stochastic Gradient Descent(26821): loss=0.045122809452369156\n",
      "Stochastic Gradient Descent(26822): loss=0.2504282424967174\n",
      "Stochastic Gradient Descent(26823): loss=0.012842864648787113\n",
      "Stochastic Gradient Descent(26824): loss=0.225162557549635\n",
      "Stochastic Gradient Descent(26825): loss=0.00586479297048364\n",
      "Stochastic Gradient Descent(26826): loss=0.9063894303876503\n",
      "Stochastic Gradient Descent(26827): loss=4.433297057366516\n",
      "Stochastic Gradient Descent(26828): loss=2.4361311149989215\n",
      "Stochastic Gradient Descent(26829): loss=3.5990909930986925\n",
      "Stochastic Gradient Descent(26830): loss=0.0001749161339387196\n",
      "Stochastic Gradient Descent(26831): loss=5.329874544971941\n",
      "Stochastic Gradient Descent(26832): loss=3.4710087414962625\n",
      "Stochastic Gradient Descent(26833): loss=5.63020441823691\n",
      "Stochastic Gradient Descent(26834): loss=2.026502045578982\n",
      "Stochastic Gradient Descent(26835): loss=0.3295758528556009\n",
      "Stochastic Gradient Descent(26836): loss=4.4064383157605445\n",
      "Stochastic Gradient Descent(26837): loss=26.70397250958648\n",
      "Stochastic Gradient Descent(26838): loss=2.6573155390147964\n",
      "Stochastic Gradient Descent(26839): loss=11.490015614733265\n",
      "Stochastic Gradient Descent(26840): loss=0.0824218728049466\n",
      "Stochastic Gradient Descent(26841): loss=4.633250488827373\n",
      "Stochastic Gradient Descent(26842): loss=1.7142199852326814\n",
      "Stochastic Gradient Descent(26843): loss=0.479072880352547\n",
      "Stochastic Gradient Descent(26844): loss=0.3395715729406781\n",
      "Stochastic Gradient Descent(26845): loss=3.351924174856688\n",
      "Stochastic Gradient Descent(26846): loss=3.7535600504441335\n",
      "Stochastic Gradient Descent(26847): loss=1.3576275738345736\n",
      "Stochastic Gradient Descent(26848): loss=1.6120456665349951\n",
      "Stochastic Gradient Descent(26849): loss=0.017060746691401105\n",
      "Stochastic Gradient Descent(26850): loss=4.711154169038705\n",
      "Stochastic Gradient Descent(26851): loss=11.183314264655468\n",
      "Stochastic Gradient Descent(26852): loss=9.295348018273986\n",
      "Stochastic Gradient Descent(26853): loss=2.8055534521739247\n",
      "Stochastic Gradient Descent(26854): loss=0.02529794467377285\n",
      "Stochastic Gradient Descent(26855): loss=0.8431462099239372\n",
      "Stochastic Gradient Descent(26856): loss=21.577153522801115\n",
      "Stochastic Gradient Descent(26857): loss=4.918710276092753\n",
      "Stochastic Gradient Descent(26858): loss=7.118585796330373\n",
      "Stochastic Gradient Descent(26859): loss=2.157855961481928\n",
      "Stochastic Gradient Descent(26860): loss=0.8283064113342812\n",
      "Stochastic Gradient Descent(26861): loss=3.144080293918268\n",
      "Stochastic Gradient Descent(26862): loss=3.2671668911979688\n",
      "Stochastic Gradient Descent(26863): loss=0.36202403774381453\n",
      "Stochastic Gradient Descent(26864): loss=1.1389375716105066\n",
      "Stochastic Gradient Descent(26865): loss=1.822994773391989\n",
      "Stochastic Gradient Descent(26866): loss=0.15394541434747713\n",
      "Stochastic Gradient Descent(26867): loss=0.2979182227567312\n",
      "Stochastic Gradient Descent(26868): loss=3.1288929592691526\n",
      "Stochastic Gradient Descent(26869): loss=19.938063992085418\n",
      "Stochastic Gradient Descent(26870): loss=7.008066958751897\n",
      "Stochastic Gradient Descent(26871): loss=7.5819889689918\n",
      "Stochastic Gradient Descent(26872): loss=0.6854621265564401\n",
      "Stochastic Gradient Descent(26873): loss=0.08362062050574627\n",
      "Stochastic Gradient Descent(26874): loss=3.2004393474868174\n",
      "Stochastic Gradient Descent(26875): loss=21.07725071838868\n",
      "Stochastic Gradient Descent(26876): loss=3.064689905839571\n",
      "Stochastic Gradient Descent(26877): loss=1.9418202858836244\n",
      "Stochastic Gradient Descent(26878): loss=1.261356877823729\n",
      "Stochastic Gradient Descent(26879): loss=1.940111506670594\n",
      "Stochastic Gradient Descent(26880): loss=5.143433854307828\n",
      "Stochastic Gradient Descent(26881): loss=0.028128396626023094\n",
      "Stochastic Gradient Descent(26882): loss=0.028976628330819737\n",
      "Stochastic Gradient Descent(26883): loss=1.2316081226035624\n",
      "Stochastic Gradient Descent(26884): loss=0.9712349724605953\n",
      "Stochastic Gradient Descent(26885): loss=0.06688314662957538\n",
      "Stochastic Gradient Descent(26886): loss=0.38835844693940036\n",
      "Stochastic Gradient Descent(26887): loss=0.35753396641768914\n",
      "Stochastic Gradient Descent(26888): loss=0.10415319020416246\n",
      "Stochastic Gradient Descent(26889): loss=0.5134823640440082\n",
      "Stochastic Gradient Descent(26890): loss=6.180514064809608\n",
      "Stochastic Gradient Descent(26891): loss=3.9258903355789476\n",
      "Stochastic Gradient Descent(26892): loss=2.7364364564418997\n",
      "Stochastic Gradient Descent(26893): loss=0.15653448264255374\n",
      "Stochastic Gradient Descent(26894): loss=1.2785654924372094\n",
      "Stochastic Gradient Descent(26895): loss=1.7502739629159794\n",
      "Stochastic Gradient Descent(26896): loss=1.6203482135298124\n",
      "Stochastic Gradient Descent(26897): loss=10.417592726592167\n",
      "Stochastic Gradient Descent(26898): loss=0.17858833858562329\n",
      "Stochastic Gradient Descent(26899): loss=9.420613031813128\n",
      "Stochastic Gradient Descent(26900): loss=0.11091807944491269\n",
      "Stochastic Gradient Descent(26901): loss=2.2719682832197323\n",
      "Stochastic Gradient Descent(26902): loss=8.79736979115215\n",
      "Stochastic Gradient Descent(26903): loss=3.1451997095563344\n",
      "Stochastic Gradient Descent(26904): loss=0.4162880854958005\n",
      "Stochastic Gradient Descent(26905): loss=0.35275894252359896\n",
      "Stochastic Gradient Descent(26906): loss=0.004652517885728688\n",
      "Stochastic Gradient Descent(26907): loss=5.8117745383025\n",
      "Stochastic Gradient Descent(26908): loss=0.4642705744970495\n",
      "Stochastic Gradient Descent(26909): loss=1.1844996709835982\n",
      "Stochastic Gradient Descent(26910): loss=4.432246102252727\n",
      "Stochastic Gradient Descent(26911): loss=3.615070950938328\n",
      "Stochastic Gradient Descent(26912): loss=0.06643064647332969\n",
      "Stochastic Gradient Descent(26913): loss=10.309011239917869\n",
      "Stochastic Gradient Descent(26914): loss=0.10825615979519318\n",
      "Stochastic Gradient Descent(26915): loss=26.02274902041961\n",
      "Stochastic Gradient Descent(26916): loss=0.3930930357321338\n",
      "Stochastic Gradient Descent(26917): loss=12.869765593898316\n",
      "Stochastic Gradient Descent(26918): loss=5.496915008285628\n",
      "Stochastic Gradient Descent(26919): loss=6.152253353031146\n",
      "Stochastic Gradient Descent(26920): loss=0.7219309292371978\n",
      "Stochastic Gradient Descent(26921): loss=2.046352242225813\n",
      "Stochastic Gradient Descent(26922): loss=0.013907417888999732\n",
      "Stochastic Gradient Descent(26923): loss=13.54939275553151\n",
      "Stochastic Gradient Descent(26924): loss=2.3217346309212323\n",
      "Stochastic Gradient Descent(26925): loss=0.37408427614010653\n",
      "Stochastic Gradient Descent(26926): loss=10.488523672663101\n",
      "Stochastic Gradient Descent(26927): loss=18.56280741007558\n",
      "Stochastic Gradient Descent(26928): loss=5.200991947007413\n",
      "Stochastic Gradient Descent(26929): loss=1.5238551274646783\n",
      "Stochastic Gradient Descent(26930): loss=4.812845626982342\n",
      "Stochastic Gradient Descent(26931): loss=24.90294190962741\n",
      "Stochastic Gradient Descent(26932): loss=2.1197209110971382\n",
      "Stochastic Gradient Descent(26933): loss=0.7302533147369437\n",
      "Stochastic Gradient Descent(26934): loss=0.40136545585161076\n",
      "Stochastic Gradient Descent(26935): loss=2.1860362151725714\n",
      "Stochastic Gradient Descent(26936): loss=6.883929478090193\n",
      "Stochastic Gradient Descent(26937): loss=7.906515588536723\n",
      "Stochastic Gradient Descent(26938): loss=1.0198374314703624\n",
      "Stochastic Gradient Descent(26939): loss=7.0807746340774935\n",
      "Stochastic Gradient Descent(26940): loss=3.20568379305114\n",
      "Stochastic Gradient Descent(26941): loss=3.4648872177168792\n",
      "Stochastic Gradient Descent(26942): loss=2.0920646635507754\n",
      "Stochastic Gradient Descent(26943): loss=16.93869857342945\n",
      "Stochastic Gradient Descent(26944): loss=1.2290497756447014\n",
      "Stochastic Gradient Descent(26945): loss=1.6298235646842718\n",
      "Stochastic Gradient Descent(26946): loss=26.018369478138286\n",
      "Stochastic Gradient Descent(26947): loss=0.5511460202897837\n",
      "Stochastic Gradient Descent(26948): loss=2.6029837258034423\n",
      "Stochastic Gradient Descent(26949): loss=9.691822388333984\n",
      "Stochastic Gradient Descent(26950): loss=26.162154933949225\n",
      "Stochastic Gradient Descent(26951): loss=5.57747919487678\n",
      "Stochastic Gradient Descent(26952): loss=3.627795881087983\n",
      "Stochastic Gradient Descent(26953): loss=16.650032350638185\n",
      "Stochastic Gradient Descent(26954): loss=12.626431655029785\n",
      "Stochastic Gradient Descent(26955): loss=3.490247901717135\n",
      "Stochastic Gradient Descent(26956): loss=16.99518614605094\n",
      "Stochastic Gradient Descent(26957): loss=18.250441832317176\n",
      "Stochastic Gradient Descent(26958): loss=0.4503906204130609\n",
      "Stochastic Gradient Descent(26959): loss=3.3200788998181956\n",
      "Stochastic Gradient Descent(26960): loss=0.21249520233020727\n",
      "Stochastic Gradient Descent(26961): loss=0.48587280245693554\n",
      "Stochastic Gradient Descent(26962): loss=3.647126740151571\n",
      "Stochastic Gradient Descent(26963): loss=11.753801447444229\n",
      "Stochastic Gradient Descent(26964): loss=0.6229739066793314\n",
      "Stochastic Gradient Descent(26965): loss=8.065914919637416\n",
      "Stochastic Gradient Descent(26966): loss=3.4876980480464823\n",
      "Stochastic Gradient Descent(26967): loss=0.3617364296680613\n",
      "Stochastic Gradient Descent(26968): loss=42.109065461358\n",
      "Stochastic Gradient Descent(26969): loss=2.3074091061438717\n",
      "Stochastic Gradient Descent(26970): loss=3.168306212545002\n",
      "Stochastic Gradient Descent(26971): loss=1.7346103624018128\n",
      "Stochastic Gradient Descent(26972): loss=7.022250087422301\n",
      "Stochastic Gradient Descent(26973): loss=20.839532289387094\n",
      "Stochastic Gradient Descent(26974): loss=0.0018409777404936093\n",
      "Stochastic Gradient Descent(26975): loss=0.14059224564481396\n",
      "Stochastic Gradient Descent(26976): loss=0.12774938706559505\n",
      "Stochastic Gradient Descent(26977): loss=26.18859352322493\n",
      "Stochastic Gradient Descent(26978): loss=0.30813215409262334\n",
      "Stochastic Gradient Descent(26979): loss=2.868350361067422\n",
      "Stochastic Gradient Descent(26980): loss=17.747721003702168\n",
      "Stochastic Gradient Descent(26981): loss=12.609823312747114\n",
      "Stochastic Gradient Descent(26982): loss=14.926918129612764\n",
      "Stochastic Gradient Descent(26983): loss=11.404486189122148\n",
      "Stochastic Gradient Descent(26984): loss=3.630837597986444\n",
      "Stochastic Gradient Descent(26985): loss=67.6665700367216\n",
      "Stochastic Gradient Descent(26986): loss=11.955291933657916\n",
      "Stochastic Gradient Descent(26987): loss=43.52707316068577\n",
      "Stochastic Gradient Descent(26988): loss=0.020106471537382587\n",
      "Stochastic Gradient Descent(26989): loss=0.0815724248331277\n",
      "Stochastic Gradient Descent(26990): loss=0.0025538756952952304\n",
      "Stochastic Gradient Descent(26991): loss=1.7961096443019842\n",
      "Stochastic Gradient Descent(26992): loss=7.950747345171106\n",
      "Stochastic Gradient Descent(26993): loss=9.199950067722947\n",
      "Stochastic Gradient Descent(26994): loss=5.416967369383458\n",
      "Stochastic Gradient Descent(26995): loss=2.2690812633697663\n",
      "Stochastic Gradient Descent(26996): loss=7.679310205299528\n",
      "Stochastic Gradient Descent(26997): loss=3.051281018364841\n",
      "Stochastic Gradient Descent(26998): loss=19.604976186986228\n",
      "Stochastic Gradient Descent(26999): loss=0.032413889756385025\n",
      "Stochastic Gradient Descent(27000): loss=8.323891255254752\n",
      "Stochastic Gradient Descent(27001): loss=0.0019462935669206484\n",
      "Stochastic Gradient Descent(27002): loss=0.06574296400834609\n",
      "Stochastic Gradient Descent(27003): loss=2.629683013421278\n",
      "Stochastic Gradient Descent(27004): loss=7.2281292913182495\n",
      "Stochastic Gradient Descent(27005): loss=18.36428028569172\n",
      "Stochastic Gradient Descent(27006): loss=6.519276678428453\n",
      "Stochastic Gradient Descent(27007): loss=6.134364702230799\n",
      "Stochastic Gradient Descent(27008): loss=12.1102838768093\n",
      "Stochastic Gradient Descent(27009): loss=0.0011451809597500943\n",
      "Stochastic Gradient Descent(27010): loss=0.14945121743709397\n",
      "Stochastic Gradient Descent(27011): loss=10.58848188945525\n",
      "Stochastic Gradient Descent(27012): loss=0.3009858739937877\n",
      "Stochastic Gradient Descent(27013): loss=0.16868732661830338\n",
      "Stochastic Gradient Descent(27014): loss=2.186068562372996\n",
      "Stochastic Gradient Descent(27015): loss=3.3616986276878182\n",
      "Stochastic Gradient Descent(27016): loss=0.13324688499734122\n",
      "Stochastic Gradient Descent(27017): loss=4.322006108590139\n",
      "Stochastic Gradient Descent(27018): loss=2.802464359034722\n",
      "Stochastic Gradient Descent(27019): loss=3.4379275551468376\n",
      "Stochastic Gradient Descent(27020): loss=2.917239806350772\n",
      "Stochastic Gradient Descent(27021): loss=9.599849505093848\n",
      "Stochastic Gradient Descent(27022): loss=1.6257622007362356\n",
      "Stochastic Gradient Descent(27023): loss=0.48813012774734466\n",
      "Stochastic Gradient Descent(27024): loss=9.855923380719371\n",
      "Stochastic Gradient Descent(27025): loss=2.076032030252332\n",
      "Stochastic Gradient Descent(27026): loss=1.9178435674541086\n",
      "Stochastic Gradient Descent(27027): loss=0.19599288491392702\n",
      "Stochastic Gradient Descent(27028): loss=0.97469911566877\n",
      "Stochastic Gradient Descent(27029): loss=5.807416050013074\n",
      "Stochastic Gradient Descent(27030): loss=4.035246841360327\n",
      "Stochastic Gradient Descent(27031): loss=0.8521500669455476\n",
      "Stochastic Gradient Descent(27032): loss=1.5260664928763936\n",
      "Stochastic Gradient Descent(27033): loss=0.001214874920127389\n",
      "Stochastic Gradient Descent(27034): loss=0.021777490229063595\n",
      "Stochastic Gradient Descent(27035): loss=4.026865467078969\n",
      "Stochastic Gradient Descent(27036): loss=1.1692068820197214\n",
      "Stochastic Gradient Descent(27037): loss=11.916575671969767\n",
      "Stochastic Gradient Descent(27038): loss=0.12039927564214914\n",
      "Stochastic Gradient Descent(27039): loss=2.324310111139056\n",
      "Stochastic Gradient Descent(27040): loss=0.9364450859503167\n",
      "Stochastic Gradient Descent(27041): loss=11.467152501513903\n",
      "Stochastic Gradient Descent(27042): loss=2.6311035643738614\n",
      "Stochastic Gradient Descent(27043): loss=0.31950671325368085\n",
      "Stochastic Gradient Descent(27044): loss=16.426894348613775\n",
      "Stochastic Gradient Descent(27045): loss=0.23060960278606896\n",
      "Stochastic Gradient Descent(27046): loss=0.7449715952839971\n",
      "Stochastic Gradient Descent(27047): loss=4.194478234111833\n",
      "Stochastic Gradient Descent(27048): loss=1.40242268756455\n",
      "Stochastic Gradient Descent(27049): loss=0.555165789716678\n",
      "Stochastic Gradient Descent(27050): loss=11.206423611495905\n",
      "Stochastic Gradient Descent(27051): loss=0.3806325604747241\n",
      "Stochastic Gradient Descent(27052): loss=7.055901004548943\n",
      "Stochastic Gradient Descent(27053): loss=0.04983086942312913\n",
      "Stochastic Gradient Descent(27054): loss=15.443812148022182\n",
      "Stochastic Gradient Descent(27055): loss=7.353299817191534\n",
      "Stochastic Gradient Descent(27056): loss=5.721214621596401\n",
      "Stochastic Gradient Descent(27057): loss=1.2931833527084773\n",
      "Stochastic Gradient Descent(27058): loss=0.30225605894023005\n",
      "Stochastic Gradient Descent(27059): loss=6.741246256697532\n",
      "Stochastic Gradient Descent(27060): loss=0.013027406406447017\n",
      "Stochastic Gradient Descent(27061): loss=2.170384665031598\n",
      "Stochastic Gradient Descent(27062): loss=19.335240335716524\n",
      "Stochastic Gradient Descent(27063): loss=0.3892035382865583\n",
      "Stochastic Gradient Descent(27064): loss=0.16440065629087489\n",
      "Stochastic Gradient Descent(27065): loss=4.817946420602191\n",
      "Stochastic Gradient Descent(27066): loss=0.19845498416013468\n",
      "Stochastic Gradient Descent(27067): loss=0.006000334983654529\n",
      "Stochastic Gradient Descent(27068): loss=1.907086973074425\n",
      "Stochastic Gradient Descent(27069): loss=0.6518548943981458\n",
      "Stochastic Gradient Descent(27070): loss=0.11285477595290942\n",
      "Stochastic Gradient Descent(27071): loss=15.434656700645597\n",
      "Stochastic Gradient Descent(27072): loss=11.309597987909232\n",
      "Stochastic Gradient Descent(27073): loss=2.768363745422204\n",
      "Stochastic Gradient Descent(27074): loss=6.0798277227643664\n",
      "Stochastic Gradient Descent(27075): loss=10.80765109138385\n",
      "Stochastic Gradient Descent(27076): loss=12.58336498524824\n",
      "Stochastic Gradient Descent(27077): loss=10.828193175525318\n",
      "Stochastic Gradient Descent(27078): loss=10.924309072019708\n",
      "Stochastic Gradient Descent(27079): loss=12.724075870526704\n",
      "Stochastic Gradient Descent(27080): loss=7.212374133880419\n",
      "Stochastic Gradient Descent(27081): loss=20.30422043434147\n",
      "Stochastic Gradient Descent(27082): loss=10.919537883695671\n",
      "Stochastic Gradient Descent(27083): loss=1.2398291018108099\n",
      "Stochastic Gradient Descent(27084): loss=19.353336561080265\n",
      "Stochastic Gradient Descent(27085): loss=6.816965630570906\n",
      "Stochastic Gradient Descent(27086): loss=0.5388264918009291\n",
      "Stochastic Gradient Descent(27087): loss=0.09101980346414935\n",
      "Stochastic Gradient Descent(27088): loss=0.6824427001003207\n",
      "Stochastic Gradient Descent(27089): loss=0.0006442269874732863\n",
      "Stochastic Gradient Descent(27090): loss=0.44377137939842126\n",
      "Stochastic Gradient Descent(27091): loss=11.795933200598702\n",
      "Stochastic Gradient Descent(27092): loss=1.9511562469680306\n",
      "Stochastic Gradient Descent(27093): loss=6.683639670999484\n",
      "Stochastic Gradient Descent(27094): loss=0.1303065503956313\n",
      "Stochastic Gradient Descent(27095): loss=7.05031441949029\n",
      "Stochastic Gradient Descent(27096): loss=4.741808239558552\n",
      "Stochastic Gradient Descent(27097): loss=4.6218880461887295\n",
      "Stochastic Gradient Descent(27098): loss=1.6410260521398647\n",
      "Stochastic Gradient Descent(27099): loss=0.010018289499194054\n",
      "Stochastic Gradient Descent(27100): loss=4.537591666264049\n",
      "Stochastic Gradient Descent(27101): loss=4.816168609271477\n",
      "Stochastic Gradient Descent(27102): loss=9.322252683081809\n",
      "Stochastic Gradient Descent(27103): loss=0.001066470352064919\n",
      "Stochastic Gradient Descent(27104): loss=1.7684920458990545\n",
      "Stochastic Gradient Descent(27105): loss=21.531254763944965\n",
      "Stochastic Gradient Descent(27106): loss=7.139275782537448\n",
      "Stochastic Gradient Descent(27107): loss=0.5983557669575134\n",
      "Stochastic Gradient Descent(27108): loss=0.09666878884870442\n",
      "Stochastic Gradient Descent(27109): loss=0.28495562788953516\n",
      "Stochastic Gradient Descent(27110): loss=3.9842692079429165\n",
      "Stochastic Gradient Descent(27111): loss=2.291265664045792\n",
      "Stochastic Gradient Descent(27112): loss=2.753148754506744\n",
      "Stochastic Gradient Descent(27113): loss=0.22609367642303232\n",
      "Stochastic Gradient Descent(27114): loss=0.00542133247360983\n",
      "Stochastic Gradient Descent(27115): loss=25.196633564768575\n",
      "Stochastic Gradient Descent(27116): loss=1.7105696730008844\n",
      "Stochastic Gradient Descent(27117): loss=0.04290055862829526\n",
      "Stochastic Gradient Descent(27118): loss=1.6691809370201987\n",
      "Stochastic Gradient Descent(27119): loss=0.46912845363113254\n",
      "Stochastic Gradient Descent(27120): loss=5.071813814630672\n",
      "Stochastic Gradient Descent(27121): loss=1.475267901246468\n",
      "Stochastic Gradient Descent(27122): loss=55.016771531976396\n",
      "Stochastic Gradient Descent(27123): loss=21.053283728359116\n",
      "Stochastic Gradient Descent(27124): loss=19.564295269567477\n",
      "Stochastic Gradient Descent(27125): loss=7.789883371761427\n",
      "Stochastic Gradient Descent(27126): loss=0.5391496145131045\n",
      "Stochastic Gradient Descent(27127): loss=0.5369314880477534\n",
      "Stochastic Gradient Descent(27128): loss=23.1290833282361\n",
      "Stochastic Gradient Descent(27129): loss=0.540684964888539\n",
      "Stochastic Gradient Descent(27130): loss=3.4157768013923966\n",
      "Stochastic Gradient Descent(27131): loss=7.005474531939628\n",
      "Stochastic Gradient Descent(27132): loss=3.7462850476727128\n",
      "Stochastic Gradient Descent(27133): loss=3.771903717483049\n",
      "Stochastic Gradient Descent(27134): loss=0.31161301448432877\n",
      "Stochastic Gradient Descent(27135): loss=0.6426438539189865\n",
      "Stochastic Gradient Descent(27136): loss=0.06393616310170405\n",
      "Stochastic Gradient Descent(27137): loss=0.013052339582307032\n",
      "Stochastic Gradient Descent(27138): loss=2.2886394497163307\n",
      "Stochastic Gradient Descent(27139): loss=0.7050904196182273\n",
      "Stochastic Gradient Descent(27140): loss=0.8742391112785045\n",
      "Stochastic Gradient Descent(27141): loss=0.32301092374808593\n",
      "Stochastic Gradient Descent(27142): loss=1.492038008562711\n",
      "Stochastic Gradient Descent(27143): loss=7.250548383139744\n",
      "Stochastic Gradient Descent(27144): loss=0.5175693377085043\n",
      "Stochastic Gradient Descent(27145): loss=0.936579598833418\n",
      "Stochastic Gradient Descent(27146): loss=7.749530892635519\n",
      "Stochastic Gradient Descent(27147): loss=1.8786579067518736\n",
      "Stochastic Gradient Descent(27148): loss=5.21314687610112\n",
      "Stochastic Gradient Descent(27149): loss=1.7322499856408788\n",
      "Stochastic Gradient Descent(27150): loss=0.039896970224913696\n",
      "Stochastic Gradient Descent(27151): loss=6.568753180488302\n",
      "Stochastic Gradient Descent(27152): loss=22.686447684100592\n",
      "Stochastic Gradient Descent(27153): loss=0.13305089667198222\n",
      "Stochastic Gradient Descent(27154): loss=1.9109938422025188\n",
      "Stochastic Gradient Descent(27155): loss=0.7496617386188968\n",
      "Stochastic Gradient Descent(27156): loss=0.9743596704283619\n",
      "Stochastic Gradient Descent(27157): loss=0.8809570463082588\n",
      "Stochastic Gradient Descent(27158): loss=0.037640959520762365\n",
      "Stochastic Gradient Descent(27159): loss=0.40728832756092836\n",
      "Stochastic Gradient Descent(27160): loss=0.6345390218052932\n",
      "Stochastic Gradient Descent(27161): loss=11.15144869223293\n",
      "Stochastic Gradient Descent(27162): loss=0.043245346692266\n",
      "Stochastic Gradient Descent(27163): loss=3.7240368004766036\n",
      "Stochastic Gradient Descent(27164): loss=9.307562067897544\n",
      "Stochastic Gradient Descent(27165): loss=1.9119554737609106\n",
      "Stochastic Gradient Descent(27166): loss=0.960354519321473\n",
      "Stochastic Gradient Descent(27167): loss=7.661386127673478\n",
      "Stochastic Gradient Descent(27168): loss=0.06265627902628258\n",
      "Stochastic Gradient Descent(27169): loss=1.6173364643024832\n",
      "Stochastic Gradient Descent(27170): loss=31.217070128155264\n",
      "Stochastic Gradient Descent(27171): loss=12.651221025206908\n",
      "Stochastic Gradient Descent(27172): loss=7.055316770824072\n",
      "Stochastic Gradient Descent(27173): loss=0.045185727436579476\n",
      "Stochastic Gradient Descent(27174): loss=0.0033001296367187144\n",
      "Stochastic Gradient Descent(27175): loss=0.000433252014933137\n",
      "Stochastic Gradient Descent(27176): loss=1.0129687243519272\n",
      "Stochastic Gradient Descent(27177): loss=2.637026628055818\n",
      "Stochastic Gradient Descent(27178): loss=14.537421213201066\n",
      "Stochastic Gradient Descent(27179): loss=0.027125867075991173\n",
      "Stochastic Gradient Descent(27180): loss=2.0696747230130628\n",
      "Stochastic Gradient Descent(27181): loss=6.202907945920121\n",
      "Stochastic Gradient Descent(27182): loss=19.631460720969983\n",
      "Stochastic Gradient Descent(27183): loss=0.2222052739885237\n",
      "Stochastic Gradient Descent(27184): loss=7.607607979250923\n",
      "Stochastic Gradient Descent(27185): loss=9.850628496708945\n",
      "Stochastic Gradient Descent(27186): loss=1.2455582950577415\n",
      "Stochastic Gradient Descent(27187): loss=9.041140104071383\n",
      "Stochastic Gradient Descent(27188): loss=0.2850916561791127\n",
      "Stochastic Gradient Descent(27189): loss=0.0025345734525081087\n",
      "Stochastic Gradient Descent(27190): loss=0.009163222136039104\n",
      "Stochastic Gradient Descent(27191): loss=2.5073847621529923\n",
      "Stochastic Gradient Descent(27192): loss=0.20943707184718083\n",
      "Stochastic Gradient Descent(27193): loss=11.055020748081681\n",
      "Stochastic Gradient Descent(27194): loss=0.9771947934647869\n",
      "Stochastic Gradient Descent(27195): loss=1.5102676218790267\n",
      "Stochastic Gradient Descent(27196): loss=3.338209426649309\n",
      "Stochastic Gradient Descent(27197): loss=3.126839996689099\n",
      "Stochastic Gradient Descent(27198): loss=4.505446453397536\n",
      "Stochastic Gradient Descent(27199): loss=3.1323592761336285\n",
      "Stochastic Gradient Descent(27200): loss=1.3028215073355232\n",
      "Stochastic Gradient Descent(27201): loss=7.818152348114374\n",
      "Stochastic Gradient Descent(27202): loss=0.7262479108380976\n",
      "Stochastic Gradient Descent(27203): loss=1.8055275371422836\n",
      "Stochastic Gradient Descent(27204): loss=1.2383109968925776\n",
      "Stochastic Gradient Descent(27205): loss=1.7334273406322538\n",
      "Stochastic Gradient Descent(27206): loss=0.8665680948392654\n",
      "Stochastic Gradient Descent(27207): loss=6.251411428937633\n",
      "Stochastic Gradient Descent(27208): loss=0.07218912557729922\n",
      "Stochastic Gradient Descent(27209): loss=0.003732629796235642\n",
      "Stochastic Gradient Descent(27210): loss=0.8478892526964018\n",
      "Stochastic Gradient Descent(27211): loss=0.16427560261139973\n",
      "Stochastic Gradient Descent(27212): loss=10.469108210964837\n",
      "Stochastic Gradient Descent(27213): loss=2.797413154833779\n",
      "Stochastic Gradient Descent(27214): loss=1.3196648157533573\n",
      "Stochastic Gradient Descent(27215): loss=3.036259426206539\n",
      "Stochastic Gradient Descent(27216): loss=0.1357535351113278\n",
      "Stochastic Gradient Descent(27217): loss=1.417449080130704\n",
      "Stochastic Gradient Descent(27218): loss=2.8368565353250474\n",
      "Stochastic Gradient Descent(27219): loss=2.0418213584659233\n",
      "Stochastic Gradient Descent(27220): loss=3.967721593206491\n",
      "Stochastic Gradient Descent(27221): loss=0.961367880916672\n",
      "Stochastic Gradient Descent(27222): loss=0.4612150288465974\n",
      "Stochastic Gradient Descent(27223): loss=2.6650755220654885\n",
      "Stochastic Gradient Descent(27224): loss=3.884545409788111\n",
      "Stochastic Gradient Descent(27225): loss=3.640255764014116\n",
      "Stochastic Gradient Descent(27226): loss=4.2909075937334995\n",
      "Stochastic Gradient Descent(27227): loss=0.0006295414045522398\n",
      "Stochastic Gradient Descent(27228): loss=0.02820679572044499\n",
      "Stochastic Gradient Descent(27229): loss=3.4565019198764864\n",
      "Stochastic Gradient Descent(27230): loss=11.520912910741467\n",
      "Stochastic Gradient Descent(27231): loss=4.459624178475422\n",
      "Stochastic Gradient Descent(27232): loss=3.381681177491357\n",
      "Stochastic Gradient Descent(27233): loss=0.37284594102977703\n",
      "Stochastic Gradient Descent(27234): loss=0.21637541969516164\n",
      "Stochastic Gradient Descent(27235): loss=0.9518928670818705\n",
      "Stochastic Gradient Descent(27236): loss=1.0363906948142172\n",
      "Stochastic Gradient Descent(27237): loss=0.30108671277094873\n",
      "Stochastic Gradient Descent(27238): loss=2.2055947898639725\n",
      "Stochastic Gradient Descent(27239): loss=2.178832376199328\n",
      "Stochastic Gradient Descent(27240): loss=2.5008754562048234\n",
      "Stochastic Gradient Descent(27241): loss=0.9656814850525308\n",
      "Stochastic Gradient Descent(27242): loss=3.7718180537060557\n",
      "Stochastic Gradient Descent(27243): loss=0.42067411581948727\n",
      "Stochastic Gradient Descent(27244): loss=0.853072906275113\n",
      "Stochastic Gradient Descent(27245): loss=0.7157830608394066\n",
      "Stochastic Gradient Descent(27246): loss=8.808235460744458\n",
      "Stochastic Gradient Descent(27247): loss=0.8391769648420246\n",
      "Stochastic Gradient Descent(27248): loss=0.6210099874084132\n",
      "Stochastic Gradient Descent(27249): loss=0.9929288501725222\n",
      "Stochastic Gradient Descent(27250): loss=0.11371817978486679\n",
      "Stochastic Gradient Descent(27251): loss=4.91615719806419\n",
      "Stochastic Gradient Descent(27252): loss=3.6499126349379254\n",
      "Stochastic Gradient Descent(27253): loss=13.556585328178226\n",
      "Stochastic Gradient Descent(27254): loss=7.654606690782308\n",
      "Stochastic Gradient Descent(27255): loss=0.02003336596959947\n",
      "Stochastic Gradient Descent(27256): loss=7.949802538172531\n",
      "Stochastic Gradient Descent(27257): loss=52.107538344996996\n",
      "Stochastic Gradient Descent(27258): loss=5.986058290931809\n",
      "Stochastic Gradient Descent(27259): loss=2.6997404574138386\n",
      "Stochastic Gradient Descent(27260): loss=0.27527291563018574\n",
      "Stochastic Gradient Descent(27261): loss=3.182306707017547\n",
      "Stochastic Gradient Descent(27262): loss=0.15656470115620652\n",
      "Stochastic Gradient Descent(27263): loss=0.07771240706012086\n",
      "Stochastic Gradient Descent(27264): loss=1.2459032568280557\n",
      "Stochastic Gradient Descent(27265): loss=1.7203704311126884\n",
      "Stochastic Gradient Descent(27266): loss=4.807462119235754\n",
      "Stochastic Gradient Descent(27267): loss=0.36586452625381055\n",
      "Stochastic Gradient Descent(27268): loss=11.310598870530743\n",
      "Stochastic Gradient Descent(27269): loss=4.570335930278504\n",
      "Stochastic Gradient Descent(27270): loss=10.131053973527791\n",
      "Stochastic Gradient Descent(27271): loss=0.49722721034858336\n",
      "Stochastic Gradient Descent(27272): loss=5.411272929199584\n",
      "Stochastic Gradient Descent(27273): loss=21.394228111449657\n",
      "Stochastic Gradient Descent(27274): loss=1.6669643756871078\n",
      "Stochastic Gradient Descent(27275): loss=1.0122085009639794\n",
      "Stochastic Gradient Descent(27276): loss=0.6999143361851976\n",
      "Stochastic Gradient Descent(27277): loss=0.9450250523125274\n",
      "Stochastic Gradient Descent(27278): loss=1.31087445344592\n",
      "Stochastic Gradient Descent(27279): loss=0.6393955693800927\n",
      "Stochastic Gradient Descent(27280): loss=0.10343963038944382\n",
      "Stochastic Gradient Descent(27281): loss=0.3547889353774232\n",
      "Stochastic Gradient Descent(27282): loss=0.8169501088380073\n",
      "Stochastic Gradient Descent(27283): loss=4.969648250509909\n",
      "Stochastic Gradient Descent(27284): loss=21.381087092175992\n",
      "Stochastic Gradient Descent(27285): loss=0.5975592160870395\n",
      "Stochastic Gradient Descent(27286): loss=0.027404459876916313\n",
      "Stochastic Gradient Descent(27287): loss=3.0321583137722468\n",
      "Stochastic Gradient Descent(27288): loss=3.450703398888494\n",
      "Stochastic Gradient Descent(27289): loss=1.8479452554448033\n",
      "Stochastic Gradient Descent(27290): loss=3.8570471181693264\n",
      "Stochastic Gradient Descent(27291): loss=23.912196777866903\n",
      "Stochastic Gradient Descent(27292): loss=1.2581133682148469\n",
      "Stochastic Gradient Descent(27293): loss=8.456531750931752\n",
      "Stochastic Gradient Descent(27294): loss=2.764203862399794\n",
      "Stochastic Gradient Descent(27295): loss=0.04076755879712064\n",
      "Stochastic Gradient Descent(27296): loss=1.5009288489370418\n",
      "Stochastic Gradient Descent(27297): loss=1.9424637480738114\n",
      "Stochastic Gradient Descent(27298): loss=0.004800409788500322\n",
      "Stochastic Gradient Descent(27299): loss=3.8791516485579853\n",
      "Stochastic Gradient Descent(27300): loss=0.635827622815681\n",
      "Stochastic Gradient Descent(27301): loss=0.29732235730210016\n",
      "Stochastic Gradient Descent(27302): loss=8.308364496201351\n",
      "Stochastic Gradient Descent(27303): loss=2.21937091138839\n",
      "Stochastic Gradient Descent(27304): loss=0.4400061305413527\n",
      "Stochastic Gradient Descent(27305): loss=1.2884840041816288\n",
      "Stochastic Gradient Descent(27306): loss=3.083753580603358\n",
      "Stochastic Gradient Descent(27307): loss=2.7092988048247757\n",
      "Stochastic Gradient Descent(27308): loss=1.396375789418831\n",
      "Stochastic Gradient Descent(27309): loss=5.354829853893266\n",
      "Stochastic Gradient Descent(27310): loss=2.4466880755573785\n",
      "Stochastic Gradient Descent(27311): loss=2.545861126226683\n",
      "Stochastic Gradient Descent(27312): loss=2.3494708103153705\n",
      "Stochastic Gradient Descent(27313): loss=3.09282282815517\n",
      "Stochastic Gradient Descent(27314): loss=1.3730389054299914\n",
      "Stochastic Gradient Descent(27315): loss=7.821258103554566\n",
      "Stochastic Gradient Descent(27316): loss=4.744296527042857\n",
      "Stochastic Gradient Descent(27317): loss=22.531945996397194\n",
      "Stochastic Gradient Descent(27318): loss=3.0268539734864026\n",
      "Stochastic Gradient Descent(27319): loss=4.304657411071795\n",
      "Stochastic Gradient Descent(27320): loss=3.3517165475023645\n",
      "Stochastic Gradient Descent(27321): loss=0.1437422249755978\n",
      "Stochastic Gradient Descent(27322): loss=9.060123936582215\n",
      "Stochastic Gradient Descent(27323): loss=7.561411062386755\n",
      "Stochastic Gradient Descent(27324): loss=0.11424960987916546\n",
      "Stochastic Gradient Descent(27325): loss=0.04152313400001353\n",
      "Stochastic Gradient Descent(27326): loss=2.88636041260557\n",
      "Stochastic Gradient Descent(27327): loss=1.2379270280680286\n",
      "Stochastic Gradient Descent(27328): loss=6.461299674945213\n",
      "Stochastic Gradient Descent(27329): loss=2.110249362668467\n",
      "Stochastic Gradient Descent(27330): loss=6.205483943972401\n",
      "Stochastic Gradient Descent(27331): loss=2.3844761169436968\n",
      "Stochastic Gradient Descent(27332): loss=2.6596311846782372\n",
      "Stochastic Gradient Descent(27333): loss=0.2954267526754376\n",
      "Stochastic Gradient Descent(27334): loss=0.18245297791426177\n",
      "Stochastic Gradient Descent(27335): loss=7.381596769883819\n",
      "Stochastic Gradient Descent(27336): loss=1.5969355282810118\n",
      "Stochastic Gradient Descent(27337): loss=0.36445198154363606\n",
      "Stochastic Gradient Descent(27338): loss=1.6572574404001106\n",
      "Stochastic Gradient Descent(27339): loss=0.49305156337350536\n",
      "Stochastic Gradient Descent(27340): loss=3.4514199269066803\n",
      "Stochastic Gradient Descent(27341): loss=0.11308010920019085\n",
      "Stochastic Gradient Descent(27342): loss=0.13058305723962882\n",
      "Stochastic Gradient Descent(27343): loss=0.9671545665782038\n",
      "Stochastic Gradient Descent(27344): loss=0.8207610916784428\n",
      "Stochastic Gradient Descent(27345): loss=2.1682278002713358\n",
      "Stochastic Gradient Descent(27346): loss=9.974450629323096\n",
      "Stochastic Gradient Descent(27347): loss=1.6321999521771866\n",
      "Stochastic Gradient Descent(27348): loss=8.688173373964297\n",
      "Stochastic Gradient Descent(27349): loss=1.7529191561770638\n",
      "Stochastic Gradient Descent(27350): loss=0.0002332998184201893\n",
      "Stochastic Gradient Descent(27351): loss=0.5837892023786178\n",
      "Stochastic Gradient Descent(27352): loss=3.895188034026219\n",
      "Stochastic Gradient Descent(27353): loss=1.0688180287633355\n",
      "Stochastic Gradient Descent(27354): loss=0.38936625180516227\n",
      "Stochastic Gradient Descent(27355): loss=3.3635981774693655\n",
      "Stochastic Gradient Descent(27356): loss=1.1503356473516013\n",
      "Stochastic Gradient Descent(27357): loss=22.136742422062714\n",
      "Stochastic Gradient Descent(27358): loss=9.367461663658151\n",
      "Stochastic Gradient Descent(27359): loss=3.2725285974593965\n",
      "Stochastic Gradient Descent(27360): loss=5.381997056941929\n",
      "Stochastic Gradient Descent(27361): loss=0.0846407512698491\n",
      "Stochastic Gradient Descent(27362): loss=8.637730264199353\n",
      "Stochastic Gradient Descent(27363): loss=0.395192695472309\n",
      "Stochastic Gradient Descent(27364): loss=0.9623245666584058\n",
      "Stochastic Gradient Descent(27365): loss=4.062259578028767\n",
      "Stochastic Gradient Descent(27366): loss=1.0184127970376367\n",
      "Stochastic Gradient Descent(27367): loss=7.068677247412173\n",
      "Stochastic Gradient Descent(27368): loss=11.682398602917294\n",
      "Stochastic Gradient Descent(27369): loss=1.1796960801885166\n",
      "Stochastic Gradient Descent(27370): loss=5.135115335354665\n",
      "Stochastic Gradient Descent(27371): loss=3.1709400150477762\n",
      "Stochastic Gradient Descent(27372): loss=23.19110180488053\n",
      "Stochastic Gradient Descent(27373): loss=0.9058242969254245\n",
      "Stochastic Gradient Descent(27374): loss=11.861702703943154\n",
      "Stochastic Gradient Descent(27375): loss=5.654497284777129\n",
      "Stochastic Gradient Descent(27376): loss=2.0390815028114955\n",
      "Stochastic Gradient Descent(27377): loss=2.4446400165654927\n",
      "Stochastic Gradient Descent(27378): loss=0.03447638755126222\n",
      "Stochastic Gradient Descent(27379): loss=0.13561213557100663\n",
      "Stochastic Gradient Descent(27380): loss=3.750870275933691\n",
      "Stochastic Gradient Descent(27381): loss=0.22826341878556192\n",
      "Stochastic Gradient Descent(27382): loss=13.29467578217643\n",
      "Stochastic Gradient Descent(27383): loss=2.8986239605153883\n",
      "Stochastic Gradient Descent(27384): loss=0.3215912018328625\n",
      "Stochastic Gradient Descent(27385): loss=0.21790197168526082\n",
      "Stochastic Gradient Descent(27386): loss=0.07817432511907385\n",
      "Stochastic Gradient Descent(27387): loss=5.049404030024265\n",
      "Stochastic Gradient Descent(27388): loss=1.557371041934037\n",
      "Stochastic Gradient Descent(27389): loss=2.427354379512182\n",
      "Stochastic Gradient Descent(27390): loss=1.0915979615076843\n",
      "Stochastic Gradient Descent(27391): loss=1.790743352215731\n",
      "Stochastic Gradient Descent(27392): loss=11.474732911251142\n",
      "Stochastic Gradient Descent(27393): loss=0.018321051512903974\n",
      "Stochastic Gradient Descent(27394): loss=0.6828610993992805\n",
      "Stochastic Gradient Descent(27395): loss=4.58737827520659\n",
      "Stochastic Gradient Descent(27396): loss=0.035247324012023275\n",
      "Stochastic Gradient Descent(27397): loss=3.3228808842591477\n",
      "Stochastic Gradient Descent(27398): loss=8.777424682843703\n",
      "Stochastic Gradient Descent(27399): loss=0.023378073916598827\n",
      "Stochastic Gradient Descent(27400): loss=1.1123056415188455\n",
      "Stochastic Gradient Descent(27401): loss=0.8091387416623207\n",
      "Stochastic Gradient Descent(27402): loss=5.6749163386600765\n",
      "Stochastic Gradient Descent(27403): loss=3.5603148655980084\n",
      "Stochastic Gradient Descent(27404): loss=6.223229620749573\n",
      "Stochastic Gradient Descent(27405): loss=0.051782942007305266\n",
      "Stochastic Gradient Descent(27406): loss=0.030156801912912434\n",
      "Stochastic Gradient Descent(27407): loss=11.063289121284166\n",
      "Stochastic Gradient Descent(27408): loss=15.479371055438813\n",
      "Stochastic Gradient Descent(27409): loss=0.0059719379760967495\n",
      "Stochastic Gradient Descent(27410): loss=1.212368389471163\n",
      "Stochastic Gradient Descent(27411): loss=14.406515840435496\n",
      "Stochastic Gradient Descent(27412): loss=0.20706291090527804\n",
      "Stochastic Gradient Descent(27413): loss=1.0373082815750272\n",
      "Stochastic Gradient Descent(27414): loss=0.13569895690997208\n",
      "Stochastic Gradient Descent(27415): loss=0.05930811553495051\n",
      "Stochastic Gradient Descent(27416): loss=5.209715631116793\n",
      "Stochastic Gradient Descent(27417): loss=7.16362841946502\n",
      "Stochastic Gradient Descent(27418): loss=5.611299207494898\n",
      "Stochastic Gradient Descent(27419): loss=0.24584674512387408\n",
      "Stochastic Gradient Descent(27420): loss=0.9023954440522426\n",
      "Stochastic Gradient Descent(27421): loss=0.20176329129853005\n",
      "Stochastic Gradient Descent(27422): loss=0.30358353795776105\n",
      "Stochastic Gradient Descent(27423): loss=14.302651183131823\n",
      "Stochastic Gradient Descent(27424): loss=3.764861327363298\n",
      "Stochastic Gradient Descent(27425): loss=0.11070740540343399\n",
      "Stochastic Gradient Descent(27426): loss=6.918598788522198\n",
      "Stochastic Gradient Descent(27427): loss=2.0947419292833747\n",
      "Stochastic Gradient Descent(27428): loss=2.7678377459991905\n",
      "Stochastic Gradient Descent(27429): loss=0.020450926723594387\n",
      "Stochastic Gradient Descent(27430): loss=10.345767960545862\n",
      "Stochastic Gradient Descent(27431): loss=1.806468622986851\n",
      "Stochastic Gradient Descent(27432): loss=2.204264313650052\n",
      "Stochastic Gradient Descent(27433): loss=6.5721345800480595\n",
      "Stochastic Gradient Descent(27434): loss=1.1860650960179389\n",
      "Stochastic Gradient Descent(27435): loss=1.2075784001695051\n",
      "Stochastic Gradient Descent(27436): loss=0.5110362766869925\n",
      "Stochastic Gradient Descent(27437): loss=0.1750007535302126\n",
      "Stochastic Gradient Descent(27438): loss=6.978328456271801\n",
      "Stochastic Gradient Descent(27439): loss=15.979064941262191\n",
      "Stochastic Gradient Descent(27440): loss=2.0490840762269125\n",
      "Stochastic Gradient Descent(27441): loss=11.71364710903435\n",
      "Stochastic Gradient Descent(27442): loss=0.6765763633936341\n",
      "Stochastic Gradient Descent(27443): loss=0.014266481177241637\n",
      "Stochastic Gradient Descent(27444): loss=2.4082065077266943\n",
      "Stochastic Gradient Descent(27445): loss=0.7039274843744456\n",
      "Stochastic Gradient Descent(27446): loss=4.90010572470046\n",
      "Stochastic Gradient Descent(27447): loss=3.0795365010402533\n",
      "Stochastic Gradient Descent(27448): loss=2.851715770342777\n",
      "Stochastic Gradient Descent(27449): loss=3.163558828037597\n",
      "Stochastic Gradient Descent(27450): loss=0.19738176854238648\n",
      "Stochastic Gradient Descent(27451): loss=0.004034938246178338\n",
      "Stochastic Gradient Descent(27452): loss=7.548801627742545\n",
      "Stochastic Gradient Descent(27453): loss=0.2915448188736585\n",
      "Stochastic Gradient Descent(27454): loss=2.0380329282548595\n",
      "Stochastic Gradient Descent(27455): loss=8.849330844924975\n",
      "Stochastic Gradient Descent(27456): loss=2.540214629526288\n",
      "Stochastic Gradient Descent(27457): loss=1.1878165587596272\n",
      "Stochastic Gradient Descent(27458): loss=1.2240482002340654\n",
      "Stochastic Gradient Descent(27459): loss=0.30438806986698225\n",
      "Stochastic Gradient Descent(27460): loss=0.4326750396542383\n",
      "Stochastic Gradient Descent(27461): loss=4.419888973342464\n",
      "Stochastic Gradient Descent(27462): loss=0.5283395444924418\n",
      "Stochastic Gradient Descent(27463): loss=4.54313095221902\n",
      "Stochastic Gradient Descent(27464): loss=4.855146574731511\n",
      "Stochastic Gradient Descent(27465): loss=0.02754355980249611\n",
      "Stochastic Gradient Descent(27466): loss=0.21797103377043403\n",
      "Stochastic Gradient Descent(27467): loss=3.5070416156944266\n",
      "Stochastic Gradient Descent(27468): loss=8.182380010868663\n",
      "Stochastic Gradient Descent(27469): loss=1.972408694781677\n",
      "Stochastic Gradient Descent(27470): loss=0.08960797801204677\n",
      "Stochastic Gradient Descent(27471): loss=2.9125824644980396\n",
      "Stochastic Gradient Descent(27472): loss=0.6742239883609069\n",
      "Stochastic Gradient Descent(27473): loss=1.2918823402769921\n",
      "Stochastic Gradient Descent(27474): loss=1.1837428968961297\n",
      "Stochastic Gradient Descent(27475): loss=2.244449038868022\n",
      "Stochastic Gradient Descent(27476): loss=0.23022991658201683\n",
      "Stochastic Gradient Descent(27477): loss=3.993831949977448\n",
      "Stochastic Gradient Descent(27478): loss=3.42284138311415\n",
      "Stochastic Gradient Descent(27479): loss=4.997804092598208\n",
      "Stochastic Gradient Descent(27480): loss=0.5909681366840795\n",
      "Stochastic Gradient Descent(27481): loss=1.4856674793370344\n",
      "Stochastic Gradient Descent(27482): loss=6.744459914061728\n",
      "Stochastic Gradient Descent(27483): loss=2.892116351637495\n",
      "Stochastic Gradient Descent(27484): loss=0.021958806674936073\n",
      "Stochastic Gradient Descent(27485): loss=4.415124690561429\n",
      "Stochastic Gradient Descent(27486): loss=0.1507882799688694\n",
      "Stochastic Gradient Descent(27487): loss=0.008240858243234936\n",
      "Stochastic Gradient Descent(27488): loss=1.9186340919628857\n",
      "Stochastic Gradient Descent(27489): loss=1.193083547523206\n",
      "Stochastic Gradient Descent(27490): loss=0.2569226956549216\n",
      "Stochastic Gradient Descent(27491): loss=1.0728568832048457\n",
      "Stochastic Gradient Descent(27492): loss=0.16739542935836907\n",
      "Stochastic Gradient Descent(27493): loss=0.2735681705587253\n",
      "Stochastic Gradient Descent(27494): loss=0.4732617257101094\n",
      "Stochastic Gradient Descent(27495): loss=8.995678765054965\n",
      "Stochastic Gradient Descent(27496): loss=0.2849764943082773\n",
      "Stochastic Gradient Descent(27497): loss=2.9953665729334347\n",
      "Stochastic Gradient Descent(27498): loss=9.081594816008844\n",
      "Stochastic Gradient Descent(27499): loss=0.04641092841672225\n",
      "Stochastic Gradient Descent(27500): loss=0.39441996777224714\n",
      "Stochastic Gradient Descent(27501): loss=0.06793611418200829\n",
      "Stochastic Gradient Descent(27502): loss=0.14211411988035857\n",
      "Stochastic Gradient Descent(27503): loss=14.025583939210343\n",
      "Stochastic Gradient Descent(27504): loss=5.572756467349968\n",
      "Stochastic Gradient Descent(27505): loss=0.5494243153188937\n",
      "Stochastic Gradient Descent(27506): loss=18.419996497227988\n",
      "Stochastic Gradient Descent(27507): loss=0.5531358669666377\n",
      "Stochastic Gradient Descent(27508): loss=0.3768140950675227\n",
      "Stochastic Gradient Descent(27509): loss=11.80621270667592\n",
      "Stochastic Gradient Descent(27510): loss=0.24790862963410984\n",
      "Stochastic Gradient Descent(27511): loss=1.8133223975109825\n",
      "Stochastic Gradient Descent(27512): loss=0.9189624010640371\n",
      "Stochastic Gradient Descent(27513): loss=3.6557389668764046\n",
      "Stochastic Gradient Descent(27514): loss=3.1992202887487946\n",
      "Stochastic Gradient Descent(27515): loss=0.5662645462584119\n",
      "Stochastic Gradient Descent(27516): loss=1.877344877221259\n",
      "Stochastic Gradient Descent(27517): loss=0.34428205474173745\n",
      "Stochastic Gradient Descent(27518): loss=4.2933061245457855\n",
      "Stochastic Gradient Descent(27519): loss=2.9623024992985725\n",
      "Stochastic Gradient Descent(27520): loss=16.98231325048101\n",
      "Stochastic Gradient Descent(27521): loss=0.0001102187463621586\n",
      "Stochastic Gradient Descent(27522): loss=0.41189493203567745\n",
      "Stochastic Gradient Descent(27523): loss=12.929829633443584\n",
      "Stochastic Gradient Descent(27524): loss=3.8475296865363524\n",
      "Stochastic Gradient Descent(27525): loss=0.004371765871792124\n",
      "Stochastic Gradient Descent(27526): loss=25.722669942116273\n",
      "Stochastic Gradient Descent(27527): loss=1.2712150370588624\n",
      "Stochastic Gradient Descent(27528): loss=7.519557167630004\n",
      "Stochastic Gradient Descent(27529): loss=0.04751464983703555\n",
      "Stochastic Gradient Descent(27530): loss=6.955096566264034\n",
      "Stochastic Gradient Descent(27531): loss=1.1830836461604648\n",
      "Stochastic Gradient Descent(27532): loss=4.3603136250773336\n",
      "Stochastic Gradient Descent(27533): loss=0.06283655278331042\n",
      "Stochastic Gradient Descent(27534): loss=0.19283296458929663\n",
      "Stochastic Gradient Descent(27535): loss=3.8748446081619194\n",
      "Stochastic Gradient Descent(27536): loss=0.036392243759809\n",
      "Stochastic Gradient Descent(27537): loss=0.17092326348364334\n",
      "Stochastic Gradient Descent(27538): loss=0.9524622164958473\n",
      "Stochastic Gradient Descent(27539): loss=2.37548514895473\n",
      "Stochastic Gradient Descent(27540): loss=0.4486066438667662\n",
      "Stochastic Gradient Descent(27541): loss=5.034648982803639\n",
      "Stochastic Gradient Descent(27542): loss=0.3679695794509861\n",
      "Stochastic Gradient Descent(27543): loss=2.5154020304416496\n",
      "Stochastic Gradient Descent(27544): loss=1.3143035946928567\n",
      "Stochastic Gradient Descent(27545): loss=0.27364550486776584\n",
      "Stochastic Gradient Descent(27546): loss=4.978588388431134\n",
      "Stochastic Gradient Descent(27547): loss=6.6924754724338875\n",
      "Stochastic Gradient Descent(27548): loss=5.511832843864115\n",
      "Stochastic Gradient Descent(27549): loss=1.4564754678384837\n",
      "Stochastic Gradient Descent(27550): loss=2.371393139723796\n",
      "Stochastic Gradient Descent(27551): loss=0.0001728957590796351\n",
      "Stochastic Gradient Descent(27552): loss=5.579235644186821\n",
      "Stochastic Gradient Descent(27553): loss=0.0964033402953032\n",
      "Stochastic Gradient Descent(27554): loss=21.240406942887695\n",
      "Stochastic Gradient Descent(27555): loss=2.27590676476985\n",
      "Stochastic Gradient Descent(27556): loss=0.020896403772638107\n",
      "Stochastic Gradient Descent(27557): loss=10.290763513124872\n",
      "Stochastic Gradient Descent(27558): loss=2.971062935210464\n",
      "Stochastic Gradient Descent(27559): loss=0.07891224380899829\n",
      "Stochastic Gradient Descent(27560): loss=25.207329622748784\n",
      "Stochastic Gradient Descent(27561): loss=3.2012803728373704\n",
      "Stochastic Gradient Descent(27562): loss=1.657437859971972\n",
      "Stochastic Gradient Descent(27563): loss=0.645415190679082\n",
      "Stochastic Gradient Descent(27564): loss=0.1856803478285295\n",
      "Stochastic Gradient Descent(27565): loss=0.2996944908030926\n",
      "Stochastic Gradient Descent(27566): loss=0.15594514224814418\n",
      "Stochastic Gradient Descent(27567): loss=2.8398396832875274\n",
      "Stochastic Gradient Descent(27568): loss=9.386498329996341\n",
      "Stochastic Gradient Descent(27569): loss=0.17487235992821387\n",
      "Stochastic Gradient Descent(27570): loss=42.00610347133087\n",
      "Stochastic Gradient Descent(27571): loss=0.0030847106626298386\n",
      "Stochastic Gradient Descent(27572): loss=3.7199080806873517\n",
      "Stochastic Gradient Descent(27573): loss=4.481358689571444\n",
      "Stochastic Gradient Descent(27574): loss=0.4803660334307175\n",
      "Stochastic Gradient Descent(27575): loss=3.04446437957302\n",
      "Stochastic Gradient Descent(27576): loss=2.376346803443949\n",
      "Stochastic Gradient Descent(27577): loss=14.915069210642047\n",
      "Stochastic Gradient Descent(27578): loss=2.501656257734546\n",
      "Stochastic Gradient Descent(27579): loss=19.464251859005884\n",
      "Stochastic Gradient Descent(27580): loss=7.34270097011563\n",
      "Stochastic Gradient Descent(27581): loss=3.2092854138003797\n",
      "Stochastic Gradient Descent(27582): loss=0.405854923244894\n",
      "Stochastic Gradient Descent(27583): loss=0.13971274698161643\n",
      "Stochastic Gradient Descent(27584): loss=2.241043821026747\n",
      "Stochastic Gradient Descent(27585): loss=4.3734703310391545\n",
      "Stochastic Gradient Descent(27586): loss=2.118147955944546\n",
      "Stochastic Gradient Descent(27587): loss=1.5319523714243717\n",
      "Stochastic Gradient Descent(27588): loss=26.20558109698022\n",
      "Stochastic Gradient Descent(27589): loss=0.4047159892577276\n",
      "Stochastic Gradient Descent(27590): loss=0.17703560700863852\n",
      "Stochastic Gradient Descent(27591): loss=3.089782987806568\n",
      "Stochastic Gradient Descent(27592): loss=0.7247163883718188\n",
      "Stochastic Gradient Descent(27593): loss=3.404535932872092\n",
      "Stochastic Gradient Descent(27594): loss=0.7332625858706516\n",
      "Stochastic Gradient Descent(27595): loss=4.336322234455746\n",
      "Stochastic Gradient Descent(27596): loss=0.26247197417672785\n",
      "Stochastic Gradient Descent(27597): loss=21.820546088251742\n",
      "Stochastic Gradient Descent(27598): loss=4.1504917030542705\n",
      "Stochastic Gradient Descent(27599): loss=3.5070154775087947\n",
      "Stochastic Gradient Descent(27600): loss=0.04081981493458611\n",
      "Stochastic Gradient Descent(27601): loss=0.03847305606848729\n",
      "Stochastic Gradient Descent(27602): loss=0.35142861454043056\n",
      "Stochastic Gradient Descent(27603): loss=0.08989621026072027\n",
      "Stochastic Gradient Descent(27604): loss=2.6611265843044145\n",
      "Stochastic Gradient Descent(27605): loss=2.00489860773669\n",
      "Stochastic Gradient Descent(27606): loss=0.9943511887758048\n",
      "Stochastic Gradient Descent(27607): loss=0.8070981448672119\n",
      "Stochastic Gradient Descent(27608): loss=1.7635059524889372\n",
      "Stochastic Gradient Descent(27609): loss=0.2052439221479981\n",
      "Stochastic Gradient Descent(27610): loss=4.590741017155936\n",
      "Stochastic Gradient Descent(27611): loss=19.254448257302037\n",
      "Stochastic Gradient Descent(27612): loss=0.45882690049671665\n",
      "Stochastic Gradient Descent(27613): loss=3.6712420813808957\n",
      "Stochastic Gradient Descent(27614): loss=0.32847564239880706\n",
      "Stochastic Gradient Descent(27615): loss=1.0767901757269929\n",
      "Stochastic Gradient Descent(27616): loss=1.025467859726187\n",
      "Stochastic Gradient Descent(27617): loss=1.4345631468761073\n",
      "Stochastic Gradient Descent(27618): loss=5.563019281583168\n",
      "Stochastic Gradient Descent(27619): loss=11.309808087791716\n",
      "Stochastic Gradient Descent(27620): loss=0.4653530887241854\n",
      "Stochastic Gradient Descent(27621): loss=8.047698011156475\n",
      "Stochastic Gradient Descent(27622): loss=5.927042286213605\n",
      "Stochastic Gradient Descent(27623): loss=43.069557189991144\n",
      "Stochastic Gradient Descent(27624): loss=0.2178207372004442\n",
      "Stochastic Gradient Descent(27625): loss=7.484061539425249\n",
      "Stochastic Gradient Descent(27626): loss=4.927453083153997\n",
      "Stochastic Gradient Descent(27627): loss=0.010080999215036315\n",
      "Stochastic Gradient Descent(27628): loss=11.667221093227116\n",
      "Stochastic Gradient Descent(27629): loss=0.9098260948588436\n",
      "Stochastic Gradient Descent(27630): loss=8.07767045743981\n",
      "Stochastic Gradient Descent(27631): loss=4.759227525992017\n",
      "Stochastic Gradient Descent(27632): loss=7.340032442969259\n",
      "Stochastic Gradient Descent(27633): loss=8.642609706175312\n",
      "Stochastic Gradient Descent(27634): loss=0.039288295190331265\n",
      "Stochastic Gradient Descent(27635): loss=1.2118249269765453\n",
      "Stochastic Gradient Descent(27636): loss=1.9899221908415914\n",
      "Stochastic Gradient Descent(27637): loss=1.1029201012809013\n",
      "Stochastic Gradient Descent(27638): loss=14.94114664802585\n",
      "Stochastic Gradient Descent(27639): loss=1.000748437440371\n",
      "Stochastic Gradient Descent(27640): loss=1.4467790899245998\n",
      "Stochastic Gradient Descent(27641): loss=0.2498382823303483\n",
      "Stochastic Gradient Descent(27642): loss=0.28132797602118503\n",
      "Stochastic Gradient Descent(27643): loss=0.4421974425871773\n",
      "Stochastic Gradient Descent(27644): loss=6.27924468424571\n",
      "Stochastic Gradient Descent(27645): loss=0.7611330868055884\n",
      "Stochastic Gradient Descent(27646): loss=2.069871084495479\n",
      "Stochastic Gradient Descent(27647): loss=9.183995606244821\n",
      "Stochastic Gradient Descent(27648): loss=1.7925195485712477\n",
      "Stochastic Gradient Descent(27649): loss=2.5240019468577803\n",
      "Stochastic Gradient Descent(27650): loss=14.943594122034591\n",
      "Stochastic Gradient Descent(27651): loss=2.9620208500035816\n",
      "Stochastic Gradient Descent(27652): loss=5.333888408621575\n",
      "Stochastic Gradient Descent(27653): loss=0.016731435562369803\n",
      "Stochastic Gradient Descent(27654): loss=3.954745161560067\n",
      "Stochastic Gradient Descent(27655): loss=13.140454944496994\n",
      "Stochastic Gradient Descent(27656): loss=2.7697316862446764\n",
      "Stochastic Gradient Descent(27657): loss=1.2202810458257498\n",
      "Stochastic Gradient Descent(27658): loss=3.0252165909289723\n",
      "Stochastic Gradient Descent(27659): loss=4.331407800116098\n",
      "Stochastic Gradient Descent(27660): loss=7.840473843926844\n",
      "Stochastic Gradient Descent(27661): loss=0.5334304978986036\n",
      "Stochastic Gradient Descent(27662): loss=29.504638731882814\n",
      "Stochastic Gradient Descent(27663): loss=2.822540460838137\n",
      "Stochastic Gradient Descent(27664): loss=5.892692309306472\n",
      "Stochastic Gradient Descent(27665): loss=3.47108224389586\n",
      "Stochastic Gradient Descent(27666): loss=0.1506242889893367\n",
      "Stochastic Gradient Descent(27667): loss=0.07642292952428881\n",
      "Stochastic Gradient Descent(27668): loss=2.449164601810358\n",
      "Stochastic Gradient Descent(27669): loss=0.12626722499316712\n",
      "Stochastic Gradient Descent(27670): loss=5.150963019786546\n",
      "Stochastic Gradient Descent(27671): loss=11.384128562781157\n",
      "Stochastic Gradient Descent(27672): loss=7.002310045415087\n",
      "Stochastic Gradient Descent(27673): loss=23.939972603007607\n",
      "Stochastic Gradient Descent(27674): loss=3.3516019354060864\n",
      "Stochastic Gradient Descent(27675): loss=0.0005068658435760226\n",
      "Stochastic Gradient Descent(27676): loss=1.6460864832390585\n",
      "Stochastic Gradient Descent(27677): loss=5.872088869631387\n",
      "Stochastic Gradient Descent(27678): loss=0.02154462790883311\n",
      "Stochastic Gradient Descent(27679): loss=5.45381870888503\n",
      "Stochastic Gradient Descent(27680): loss=4.8130819708344115\n",
      "Stochastic Gradient Descent(27681): loss=3.167376880634808\n",
      "Stochastic Gradient Descent(27682): loss=6.470518560512253\n",
      "Stochastic Gradient Descent(27683): loss=12.757376531404317\n",
      "Stochastic Gradient Descent(27684): loss=1.2233210167748412\n",
      "Stochastic Gradient Descent(27685): loss=21.389489378950802\n",
      "Stochastic Gradient Descent(27686): loss=5.444114395821273\n",
      "Stochastic Gradient Descent(27687): loss=5.632525325371982\n",
      "Stochastic Gradient Descent(27688): loss=0.03989063026089266\n",
      "Stochastic Gradient Descent(27689): loss=1.906491559479266\n",
      "Stochastic Gradient Descent(27690): loss=3.8788135009638878\n",
      "Stochastic Gradient Descent(27691): loss=0.6191565265132285\n",
      "Stochastic Gradient Descent(27692): loss=0.7393986413099343\n",
      "Stochastic Gradient Descent(27693): loss=3.6836074438288797\n",
      "Stochastic Gradient Descent(27694): loss=0.0281002844951174\n",
      "Stochastic Gradient Descent(27695): loss=0.708510613801703\n",
      "Stochastic Gradient Descent(27696): loss=1.9229290008665354\n",
      "Stochastic Gradient Descent(27697): loss=1.0003121059861295\n",
      "Stochastic Gradient Descent(27698): loss=4.172630615338193\n",
      "Stochastic Gradient Descent(27699): loss=1.1116309568244245\n",
      "Stochastic Gradient Descent(27700): loss=3.7102450270742695\n",
      "Stochastic Gradient Descent(27701): loss=4.896489697677878\n",
      "Stochastic Gradient Descent(27702): loss=0.9616611138435641\n",
      "Stochastic Gradient Descent(27703): loss=12.995386592573944\n",
      "Stochastic Gradient Descent(27704): loss=10.131228225361395\n",
      "Stochastic Gradient Descent(27705): loss=2.4563737539095434\n",
      "Stochastic Gradient Descent(27706): loss=0.6415415046055463\n",
      "Stochastic Gradient Descent(27707): loss=1.4803578600311706\n",
      "Stochastic Gradient Descent(27708): loss=3.541355169334017\n",
      "Stochastic Gradient Descent(27709): loss=0.5008874978622379\n",
      "Stochastic Gradient Descent(27710): loss=0.5126126186640446\n",
      "Stochastic Gradient Descent(27711): loss=1.129558483680742\n",
      "Stochastic Gradient Descent(27712): loss=1.8669463553261005\n",
      "Stochastic Gradient Descent(27713): loss=0.7919712339047593\n",
      "Stochastic Gradient Descent(27714): loss=7.264366646921797\n",
      "Stochastic Gradient Descent(27715): loss=2.304855336219879\n",
      "Stochastic Gradient Descent(27716): loss=0.5422497825935856\n",
      "Stochastic Gradient Descent(27717): loss=0.9528369765921197\n",
      "Stochastic Gradient Descent(27718): loss=4.150139860468135\n",
      "Stochastic Gradient Descent(27719): loss=0.2920827027198843\n",
      "Stochastic Gradient Descent(27720): loss=4.121213597709498\n",
      "Stochastic Gradient Descent(27721): loss=0.4646627431527166\n",
      "Stochastic Gradient Descent(27722): loss=3.4584563679071505\n",
      "Stochastic Gradient Descent(27723): loss=13.790414860668973\n",
      "Stochastic Gradient Descent(27724): loss=12.761629424741754\n",
      "Stochastic Gradient Descent(27725): loss=1.1410858516939324\n",
      "Stochastic Gradient Descent(27726): loss=0.7875526755903898\n",
      "Stochastic Gradient Descent(27727): loss=1.0026700108241338\n",
      "Stochastic Gradient Descent(27728): loss=6.53995139000745\n",
      "Stochastic Gradient Descent(27729): loss=1.439536988197411\n",
      "Stochastic Gradient Descent(27730): loss=0.008213192730000662\n",
      "Stochastic Gradient Descent(27731): loss=7.781215278754352\n",
      "Stochastic Gradient Descent(27732): loss=51.47889062667002\n",
      "Stochastic Gradient Descent(27733): loss=11.944172224173844\n",
      "Stochastic Gradient Descent(27734): loss=2.2327712346622044\n",
      "Stochastic Gradient Descent(27735): loss=0.02636319541607302\n",
      "Stochastic Gradient Descent(27736): loss=2.5562300431441383\n",
      "Stochastic Gradient Descent(27737): loss=3.2338795937511073\n",
      "Stochastic Gradient Descent(27738): loss=4.844924716826849\n",
      "Stochastic Gradient Descent(27739): loss=1.0509571167851477\n",
      "Stochastic Gradient Descent(27740): loss=0.3273183615846311\n",
      "Stochastic Gradient Descent(27741): loss=3.402116225821792\n",
      "Stochastic Gradient Descent(27742): loss=1.7120802745261567\n",
      "Stochastic Gradient Descent(27743): loss=1.42119941926617\n",
      "Stochastic Gradient Descent(27744): loss=4.524996581582756\n",
      "Stochastic Gradient Descent(27745): loss=1.2158296537415156\n",
      "Stochastic Gradient Descent(27746): loss=0.2970997108410524\n",
      "Stochastic Gradient Descent(27747): loss=2.366632677124771\n",
      "Stochastic Gradient Descent(27748): loss=3.0523223940975126\n",
      "Stochastic Gradient Descent(27749): loss=0.12295960430512437\n",
      "Stochastic Gradient Descent(27750): loss=1.7948404248126772\n",
      "Stochastic Gradient Descent(27751): loss=9.661916451273195\n",
      "Stochastic Gradient Descent(27752): loss=0.0012967738232035155\n",
      "Stochastic Gradient Descent(27753): loss=7.252643793100582\n",
      "Stochastic Gradient Descent(27754): loss=1.170766904658695\n",
      "Stochastic Gradient Descent(27755): loss=5.608149305062252\n",
      "Stochastic Gradient Descent(27756): loss=2.831235828391963\n",
      "Stochastic Gradient Descent(27757): loss=4.331817574626292\n",
      "Stochastic Gradient Descent(27758): loss=0.004096093144898469\n",
      "Stochastic Gradient Descent(27759): loss=2.9200748974508914\n",
      "Stochastic Gradient Descent(27760): loss=7.456710821040659\n",
      "Stochastic Gradient Descent(27761): loss=10.040290757308226\n",
      "Stochastic Gradient Descent(27762): loss=1.6562113336473745\n",
      "Stochastic Gradient Descent(27763): loss=1.6007787733541636\n",
      "Stochastic Gradient Descent(27764): loss=0.08098399779229676\n",
      "Stochastic Gradient Descent(27765): loss=3.380514386923624\n",
      "Stochastic Gradient Descent(27766): loss=12.109194537188003\n",
      "Stochastic Gradient Descent(27767): loss=1.8687794465522614\n",
      "Stochastic Gradient Descent(27768): loss=9.733270790729568\n",
      "Stochastic Gradient Descent(27769): loss=6.1777354335241865\n",
      "Stochastic Gradient Descent(27770): loss=0.012426293258904755\n",
      "Stochastic Gradient Descent(27771): loss=11.678329582882112\n",
      "Stochastic Gradient Descent(27772): loss=1.6539319553837295\n",
      "Stochastic Gradient Descent(27773): loss=2.13188779597124\n",
      "Stochastic Gradient Descent(27774): loss=7.526435973299396\n",
      "Stochastic Gradient Descent(27775): loss=0.00105620505604044\n",
      "Stochastic Gradient Descent(27776): loss=1.3334714452432397\n",
      "Stochastic Gradient Descent(27777): loss=0.8568265498436987\n",
      "Stochastic Gradient Descent(27778): loss=0.003372064866336011\n",
      "Stochastic Gradient Descent(27779): loss=0.8916316045595382\n",
      "Stochastic Gradient Descent(27780): loss=0.05290687397747297\n",
      "Stochastic Gradient Descent(27781): loss=0.016551606581855765\n",
      "Stochastic Gradient Descent(27782): loss=0.006149672890388852\n",
      "Stochastic Gradient Descent(27783): loss=0.46526897436123055\n",
      "Stochastic Gradient Descent(27784): loss=0.009813237513405121\n",
      "Stochastic Gradient Descent(27785): loss=15.337029227212755\n",
      "Stochastic Gradient Descent(27786): loss=0.1823631314043397\n",
      "Stochastic Gradient Descent(27787): loss=1.48616025471601\n",
      "Stochastic Gradient Descent(27788): loss=0.001972194903225435\n",
      "Stochastic Gradient Descent(27789): loss=3.0787911666044097\n",
      "Stochastic Gradient Descent(27790): loss=3.8931426987479503\n",
      "Stochastic Gradient Descent(27791): loss=8.249070755211482\n",
      "Stochastic Gradient Descent(27792): loss=3.5190580648788488\n",
      "Stochastic Gradient Descent(27793): loss=14.534182897692453\n",
      "Stochastic Gradient Descent(27794): loss=0.9062084726466614\n",
      "Stochastic Gradient Descent(27795): loss=0.013361742245145363\n",
      "Stochastic Gradient Descent(27796): loss=0.09944038815822329\n",
      "Stochastic Gradient Descent(27797): loss=23.616286823732374\n",
      "Stochastic Gradient Descent(27798): loss=34.879075812147306\n",
      "Stochastic Gradient Descent(27799): loss=0.26972383885621093\n",
      "Stochastic Gradient Descent(27800): loss=55.86976209934904\n",
      "Stochastic Gradient Descent(27801): loss=0.36804301718004084\n",
      "Stochastic Gradient Descent(27802): loss=5.496669786931104\n",
      "Stochastic Gradient Descent(27803): loss=0.0944773030466034\n",
      "Stochastic Gradient Descent(27804): loss=4.665775874813468\n",
      "Stochastic Gradient Descent(27805): loss=13.539834866851914\n",
      "Stochastic Gradient Descent(27806): loss=8.024957891305439\n",
      "Stochastic Gradient Descent(27807): loss=0.08440372485127721\n",
      "Stochastic Gradient Descent(27808): loss=11.738205102442064\n",
      "Stochastic Gradient Descent(27809): loss=0.44346110274595696\n",
      "Stochastic Gradient Descent(27810): loss=2.893475358992592\n",
      "Stochastic Gradient Descent(27811): loss=4.6262401100402855\n",
      "Stochastic Gradient Descent(27812): loss=1.058421489340911\n",
      "Stochastic Gradient Descent(27813): loss=7.823852839763133\n",
      "Stochastic Gradient Descent(27814): loss=2.6761424598624384\n",
      "Stochastic Gradient Descent(27815): loss=6.798793388053629\n",
      "Stochastic Gradient Descent(27816): loss=0.6261973055517077\n",
      "Stochastic Gradient Descent(27817): loss=0.13075123112503287\n",
      "Stochastic Gradient Descent(27818): loss=2.031718630470215\n",
      "Stochastic Gradient Descent(27819): loss=2.8765241298657545\n",
      "Stochastic Gradient Descent(27820): loss=1.3905830252600475\n",
      "Stochastic Gradient Descent(27821): loss=0.60648549039788\n",
      "Stochastic Gradient Descent(27822): loss=2.2270687230620956\n",
      "Stochastic Gradient Descent(27823): loss=9.881157044450742\n",
      "Stochastic Gradient Descent(27824): loss=6.84313840834305\n",
      "Stochastic Gradient Descent(27825): loss=5.991650234363021\n",
      "Stochastic Gradient Descent(27826): loss=0.0013311062588213425\n",
      "Stochastic Gradient Descent(27827): loss=6.108432948341287\n",
      "Stochastic Gradient Descent(27828): loss=12.062379999608181\n",
      "Stochastic Gradient Descent(27829): loss=13.656396016034993\n",
      "Stochastic Gradient Descent(27830): loss=0.38745361706799647\n",
      "Stochastic Gradient Descent(27831): loss=2.1965654235997083\n",
      "Stochastic Gradient Descent(27832): loss=1.9897351305142172\n",
      "Stochastic Gradient Descent(27833): loss=0.6399413376124252\n",
      "Stochastic Gradient Descent(27834): loss=9.68789134580723\n",
      "Stochastic Gradient Descent(27835): loss=9.439049949252574\n",
      "Stochastic Gradient Descent(27836): loss=2.453826734647753\n",
      "Stochastic Gradient Descent(27837): loss=1.359485068864515\n",
      "Stochastic Gradient Descent(27838): loss=3.8989143598042095\n",
      "Stochastic Gradient Descent(27839): loss=0.00017495076949861681\n",
      "Stochastic Gradient Descent(27840): loss=10.20248231824918\n",
      "Stochastic Gradient Descent(27841): loss=6.6210049524297645\n",
      "Stochastic Gradient Descent(27842): loss=2.205267295681608\n",
      "Stochastic Gradient Descent(27843): loss=10.135852464030304\n",
      "Stochastic Gradient Descent(27844): loss=1.626520543444309\n",
      "Stochastic Gradient Descent(27845): loss=0.4073824400140143\n",
      "Stochastic Gradient Descent(27846): loss=0.07476499430640084\n",
      "Stochastic Gradient Descent(27847): loss=2.46031541016946\n",
      "Stochastic Gradient Descent(27848): loss=0.14396438993324928\n",
      "Stochastic Gradient Descent(27849): loss=2.6751479637662525\n",
      "Stochastic Gradient Descent(27850): loss=11.359022523380428\n",
      "Stochastic Gradient Descent(27851): loss=12.519513664602297\n",
      "Stochastic Gradient Descent(27852): loss=0.37051123174788114\n",
      "Stochastic Gradient Descent(27853): loss=0.3669568014504807\n",
      "Stochastic Gradient Descent(27854): loss=6.274972198693128\n",
      "Stochastic Gradient Descent(27855): loss=0.3286241270057125\n",
      "Stochastic Gradient Descent(27856): loss=11.732760757553638\n",
      "Stochastic Gradient Descent(27857): loss=6.5933640771711985\n",
      "Stochastic Gradient Descent(27858): loss=1.4007836768615096\n",
      "Stochastic Gradient Descent(27859): loss=2.544314052286636\n",
      "Stochastic Gradient Descent(27860): loss=0.10905893065653245\n",
      "Stochastic Gradient Descent(27861): loss=1.1886657337241546\n",
      "Stochastic Gradient Descent(27862): loss=0.021305146231117466\n",
      "Stochastic Gradient Descent(27863): loss=0.019569168751928698\n",
      "Stochastic Gradient Descent(27864): loss=0.8675320482695714\n",
      "Stochastic Gradient Descent(27865): loss=0.45942589201742906\n",
      "Stochastic Gradient Descent(27866): loss=0.0124486513443161\n",
      "Stochastic Gradient Descent(27867): loss=1.2007660994633065\n",
      "Stochastic Gradient Descent(27868): loss=0.15507629690812488\n",
      "Stochastic Gradient Descent(27869): loss=7.757363902428838\n",
      "Stochastic Gradient Descent(27870): loss=0.6538727331213572\n",
      "Stochastic Gradient Descent(27871): loss=2.3933072454990225\n",
      "Stochastic Gradient Descent(27872): loss=2.6423090675051326\n",
      "Stochastic Gradient Descent(27873): loss=4.034292471498712\n",
      "Stochastic Gradient Descent(27874): loss=0.10509592744571965\n",
      "Stochastic Gradient Descent(27875): loss=1.0065461621285015\n",
      "Stochastic Gradient Descent(27876): loss=0.0835855088761987\n",
      "Stochastic Gradient Descent(27877): loss=24.963475726260324\n",
      "Stochastic Gradient Descent(27878): loss=0.053448966358265154\n",
      "Stochastic Gradient Descent(27879): loss=7.826654655851968\n",
      "Stochastic Gradient Descent(27880): loss=0.5616972297575671\n",
      "Stochastic Gradient Descent(27881): loss=12.217616500934914\n",
      "Stochastic Gradient Descent(27882): loss=0.3878064731445649\n",
      "Stochastic Gradient Descent(27883): loss=2.0782451368311103\n",
      "Stochastic Gradient Descent(27884): loss=0.07838698523533585\n",
      "Stochastic Gradient Descent(27885): loss=6.222399816085814\n",
      "Stochastic Gradient Descent(27886): loss=3.3062360417780985\n",
      "Stochastic Gradient Descent(27887): loss=6.993151834016285\n",
      "Stochastic Gradient Descent(27888): loss=21.967223048387694\n",
      "Stochastic Gradient Descent(27889): loss=2.385472927283702\n",
      "Stochastic Gradient Descent(27890): loss=0.24101572066231736\n",
      "Stochastic Gradient Descent(27891): loss=16.37308639347679\n",
      "Stochastic Gradient Descent(27892): loss=0.2750094784322447\n",
      "Stochastic Gradient Descent(27893): loss=0.11986816850517468\n",
      "Stochastic Gradient Descent(27894): loss=9.50519118883705\n",
      "Stochastic Gradient Descent(27895): loss=1.1383823011745962\n",
      "Stochastic Gradient Descent(27896): loss=4.241895231098994\n",
      "Stochastic Gradient Descent(27897): loss=1.0030243460392758\n",
      "Stochastic Gradient Descent(27898): loss=7.365828863767592\n",
      "Stochastic Gradient Descent(27899): loss=0.05056352785174908\n",
      "Stochastic Gradient Descent(27900): loss=6.3912642402079785\n",
      "Stochastic Gradient Descent(27901): loss=0.1137139569382264\n",
      "Stochastic Gradient Descent(27902): loss=3.423984144656451\n",
      "Stochastic Gradient Descent(27903): loss=0.39132163198834924\n",
      "Stochastic Gradient Descent(27904): loss=10.868548272561403\n",
      "Stochastic Gradient Descent(27905): loss=0.36902153977133445\n",
      "Stochastic Gradient Descent(27906): loss=0.30163966073100895\n",
      "Stochastic Gradient Descent(27907): loss=2.3022585578682397\n",
      "Stochastic Gradient Descent(27908): loss=5.996672243531047\n",
      "Stochastic Gradient Descent(27909): loss=1.41808215524112\n",
      "Stochastic Gradient Descent(27910): loss=3.7247358609098367\n",
      "Stochastic Gradient Descent(27911): loss=0.16376827798075771\n",
      "Stochastic Gradient Descent(27912): loss=0.32604117902236224\n",
      "Stochastic Gradient Descent(27913): loss=4.092402669428995\n",
      "Stochastic Gradient Descent(27914): loss=5.219444370126579\n",
      "Stochastic Gradient Descent(27915): loss=2.822719466571989\n",
      "Stochastic Gradient Descent(27916): loss=0.5525015950304206\n",
      "Stochastic Gradient Descent(27917): loss=0.60933285268964\n",
      "Stochastic Gradient Descent(27918): loss=2.7870838102845967\n",
      "Stochastic Gradient Descent(27919): loss=4.6910552229114195\n",
      "Stochastic Gradient Descent(27920): loss=8.635808741678572\n",
      "Stochastic Gradient Descent(27921): loss=0.1652048053543817\n",
      "Stochastic Gradient Descent(27922): loss=3.9645931499134375\n",
      "Stochastic Gradient Descent(27923): loss=2.9081078064031427\n",
      "Stochastic Gradient Descent(27924): loss=8.844644847462295\n",
      "Stochastic Gradient Descent(27925): loss=0.03540768254810245\n",
      "Stochastic Gradient Descent(27926): loss=6.947064194253024\n",
      "Stochastic Gradient Descent(27927): loss=5.930547514838762\n",
      "Stochastic Gradient Descent(27928): loss=0.4905737849478775\n",
      "Stochastic Gradient Descent(27929): loss=2.1622738466498137\n",
      "Stochastic Gradient Descent(27930): loss=0.7427774068808444\n",
      "Stochastic Gradient Descent(27931): loss=7.400184372587174\n",
      "Stochastic Gradient Descent(27932): loss=0.9472864594598487\n",
      "Stochastic Gradient Descent(27933): loss=1.407701378868495\n",
      "Stochastic Gradient Descent(27934): loss=0.02106734718000305\n",
      "Stochastic Gradient Descent(27935): loss=8.544030139957345\n",
      "Stochastic Gradient Descent(27936): loss=13.063192659889276\n",
      "Stochastic Gradient Descent(27937): loss=2.7625103593625497\n",
      "Stochastic Gradient Descent(27938): loss=0.31213074620831727\n",
      "Stochastic Gradient Descent(27939): loss=11.232355514189516\n",
      "Stochastic Gradient Descent(27940): loss=15.186714957789766\n",
      "Stochastic Gradient Descent(27941): loss=5.388661587135376\n",
      "Stochastic Gradient Descent(27942): loss=11.633249076216215\n",
      "Stochastic Gradient Descent(27943): loss=0.5080411702369316\n",
      "Stochastic Gradient Descent(27944): loss=141.84740627737816\n",
      "Stochastic Gradient Descent(27945): loss=178.84462842819147\n",
      "Stochastic Gradient Descent(27946): loss=8.65632052852935\n",
      "Stochastic Gradient Descent(27947): loss=2.2044698082618646\n",
      "Stochastic Gradient Descent(27948): loss=3.374426681475212\n",
      "Stochastic Gradient Descent(27949): loss=0.04250448132676959\n",
      "Stochastic Gradient Descent(27950): loss=0.10993844078428663\n",
      "Stochastic Gradient Descent(27951): loss=23.907857045030646\n",
      "Stochastic Gradient Descent(27952): loss=8.512168565590601\n",
      "Stochastic Gradient Descent(27953): loss=8.486407615755542\n",
      "Stochastic Gradient Descent(27954): loss=3.568176653902041\n",
      "Stochastic Gradient Descent(27955): loss=4.627816202324886\n",
      "Stochastic Gradient Descent(27956): loss=10.688089443103381\n",
      "Stochastic Gradient Descent(27957): loss=5.162324054494183\n",
      "Stochastic Gradient Descent(27958): loss=1.3792771847208052\n",
      "Stochastic Gradient Descent(27959): loss=36.72315160455486\n",
      "Stochastic Gradient Descent(27960): loss=1.8708806909192657\n",
      "Stochastic Gradient Descent(27961): loss=5.170871432447012\n",
      "Stochastic Gradient Descent(27962): loss=5.720323157246037\n",
      "Stochastic Gradient Descent(27963): loss=3.4152386071572316\n",
      "Stochastic Gradient Descent(27964): loss=12.275781863873345\n",
      "Stochastic Gradient Descent(27965): loss=0.0821293807529988\n",
      "Stochastic Gradient Descent(27966): loss=13.612717161361182\n",
      "Stochastic Gradient Descent(27967): loss=0.006857436801461679\n",
      "Stochastic Gradient Descent(27968): loss=0.9813989517639706\n",
      "Stochastic Gradient Descent(27969): loss=0.18118931176752862\n",
      "Stochastic Gradient Descent(27970): loss=4.845041674680868\n",
      "Stochastic Gradient Descent(27971): loss=0.4799495923022984\n",
      "Stochastic Gradient Descent(27972): loss=0.006332662083075028\n",
      "Stochastic Gradient Descent(27973): loss=8.85684172422536\n",
      "Stochastic Gradient Descent(27974): loss=0.5759977619792246\n",
      "Stochastic Gradient Descent(27975): loss=2.20097709855156\n",
      "Stochastic Gradient Descent(27976): loss=2.3484823549381515\n",
      "Stochastic Gradient Descent(27977): loss=2.5743244189347645\n",
      "Stochastic Gradient Descent(27978): loss=20.41968184078147\n",
      "Stochastic Gradient Descent(27979): loss=1.1974136710851744\n",
      "Stochastic Gradient Descent(27980): loss=11.755779769143228\n",
      "Stochastic Gradient Descent(27981): loss=7.022730675069364\n",
      "Stochastic Gradient Descent(27982): loss=1.168309238608968\n",
      "Stochastic Gradient Descent(27983): loss=2.2862230780713624\n",
      "Stochastic Gradient Descent(27984): loss=3.641520108395737\n",
      "Stochastic Gradient Descent(27985): loss=3.887796663697602\n",
      "Stochastic Gradient Descent(27986): loss=0.8711741177941161\n",
      "Stochastic Gradient Descent(27987): loss=0.8514802896357083\n",
      "Stochastic Gradient Descent(27988): loss=15.222184142224453\n",
      "Stochastic Gradient Descent(27989): loss=0.2908771979861703\n",
      "Stochastic Gradient Descent(27990): loss=21.205298136385014\n",
      "Stochastic Gradient Descent(27991): loss=12.466923621090373\n",
      "Stochastic Gradient Descent(27992): loss=4.244956707523008\n",
      "Stochastic Gradient Descent(27993): loss=0.04077826129717161\n",
      "Stochastic Gradient Descent(27994): loss=9.927866641223916\n",
      "Stochastic Gradient Descent(27995): loss=0.16931220097647354\n",
      "Stochastic Gradient Descent(27996): loss=4.2397782961411705\n",
      "Stochastic Gradient Descent(27997): loss=1.5258400350054426\n",
      "Stochastic Gradient Descent(27998): loss=8.387589491413298\n",
      "Stochastic Gradient Descent(27999): loss=6.720263946218571\n",
      "Stochastic Gradient Descent(28000): loss=5.647650704422492\n",
      "Stochastic Gradient Descent(28001): loss=0.3067999837814712\n",
      "Stochastic Gradient Descent(28002): loss=5.770230124408951\n",
      "Stochastic Gradient Descent(28003): loss=2.123215993514736\n",
      "Stochastic Gradient Descent(28004): loss=2.235721912771638\n",
      "Stochastic Gradient Descent(28005): loss=2.082886576157712\n",
      "Stochastic Gradient Descent(28006): loss=0.0011892721230562003\n",
      "Stochastic Gradient Descent(28007): loss=0.004818125901997238\n",
      "Stochastic Gradient Descent(28008): loss=5.583791644394082\n",
      "Stochastic Gradient Descent(28009): loss=0.0075777249356537586\n",
      "Stochastic Gradient Descent(28010): loss=0.7788885510854757\n",
      "Stochastic Gradient Descent(28011): loss=18.594295503088144\n",
      "Stochastic Gradient Descent(28012): loss=2.5677902025384656\n",
      "Stochastic Gradient Descent(28013): loss=11.54158649862661\n",
      "Stochastic Gradient Descent(28014): loss=2.500909435559592\n",
      "Stochastic Gradient Descent(28015): loss=4.316162494024911\n",
      "Stochastic Gradient Descent(28016): loss=0.15099915983091666\n",
      "Stochastic Gradient Descent(28017): loss=2.4172170523980467\n",
      "Stochastic Gradient Descent(28018): loss=2.162855590878826\n",
      "Stochastic Gradient Descent(28019): loss=8.711039704557846\n",
      "Stochastic Gradient Descent(28020): loss=0.3486594325066548\n",
      "Stochastic Gradient Descent(28021): loss=0.48838419615430834\n",
      "Stochastic Gradient Descent(28022): loss=2.225470174298247\n",
      "Stochastic Gradient Descent(28023): loss=1.14373595276983\n",
      "Stochastic Gradient Descent(28024): loss=3.9023583060610254\n",
      "Stochastic Gradient Descent(28025): loss=4.582638080473484\n",
      "Stochastic Gradient Descent(28026): loss=0.2827873248321364\n",
      "Stochastic Gradient Descent(28027): loss=2.8324631562382154\n",
      "Stochastic Gradient Descent(28028): loss=9.675240034376124\n",
      "Stochastic Gradient Descent(28029): loss=0.2910077414015514\n",
      "Stochastic Gradient Descent(28030): loss=19.347650512851466\n",
      "Stochastic Gradient Descent(28031): loss=1.1155097868189467\n",
      "Stochastic Gradient Descent(28032): loss=1.3669881320824537\n",
      "Stochastic Gradient Descent(28033): loss=0.22091440375635726\n",
      "Stochastic Gradient Descent(28034): loss=7.517442261173851\n",
      "Stochastic Gradient Descent(28035): loss=8.538668470105563\n",
      "Stochastic Gradient Descent(28036): loss=31.25538606706997\n",
      "Stochastic Gradient Descent(28037): loss=2.1130154576961844\n",
      "Stochastic Gradient Descent(28038): loss=0.010353682792870928\n",
      "Stochastic Gradient Descent(28039): loss=21.443891348111183\n",
      "Stochastic Gradient Descent(28040): loss=1.0662495514173813\n",
      "Stochastic Gradient Descent(28041): loss=10.098788428341408\n",
      "Stochastic Gradient Descent(28042): loss=1.0900160181327834\n",
      "Stochastic Gradient Descent(28043): loss=0.4722332524086461\n",
      "Stochastic Gradient Descent(28044): loss=11.15009089116942\n",
      "Stochastic Gradient Descent(28045): loss=8.820933192564912\n",
      "Stochastic Gradient Descent(28046): loss=6.087918355244365\n",
      "Stochastic Gradient Descent(28047): loss=0.10402003170498023\n",
      "Stochastic Gradient Descent(28048): loss=0.33187550204986926\n",
      "Stochastic Gradient Descent(28049): loss=2.8989350521980195\n",
      "Stochastic Gradient Descent(28050): loss=0.18540257968709592\n",
      "Stochastic Gradient Descent(28051): loss=5.0513614688495805\n",
      "Stochastic Gradient Descent(28052): loss=7.77688472479912\n",
      "Stochastic Gradient Descent(28053): loss=1.9812483521700457\n",
      "Stochastic Gradient Descent(28054): loss=8.735205090221394\n",
      "Stochastic Gradient Descent(28055): loss=38.697279153890115\n",
      "Stochastic Gradient Descent(28056): loss=2.3560932300573194\n",
      "Stochastic Gradient Descent(28057): loss=3.6670826996959835\n",
      "Stochastic Gradient Descent(28058): loss=0.793979877118435\n",
      "Stochastic Gradient Descent(28059): loss=0.7127308089244043\n",
      "Stochastic Gradient Descent(28060): loss=27.289326914226688\n",
      "Stochastic Gradient Descent(28061): loss=14.787364395725884\n",
      "Stochastic Gradient Descent(28062): loss=3.1294651836622047\n",
      "Stochastic Gradient Descent(28063): loss=0.13206025678225963\n",
      "Stochastic Gradient Descent(28064): loss=2.3694937181700566\n",
      "Stochastic Gradient Descent(28065): loss=1.0754104341425232\n",
      "Stochastic Gradient Descent(28066): loss=1.7311398487490506\n",
      "Stochastic Gradient Descent(28067): loss=19.230161965727188\n",
      "Stochastic Gradient Descent(28068): loss=2.4732320839465154\n",
      "Stochastic Gradient Descent(28069): loss=1.1770778715996206\n",
      "Stochastic Gradient Descent(28070): loss=6.645734919170766\n",
      "Stochastic Gradient Descent(28071): loss=29.565225218412074\n",
      "Stochastic Gradient Descent(28072): loss=0.010918007403164584\n",
      "Stochastic Gradient Descent(28073): loss=1.3904962915799528\n",
      "Stochastic Gradient Descent(28074): loss=0.15492355311906658\n",
      "Stochastic Gradient Descent(28075): loss=0.5264988253734976\n",
      "Stochastic Gradient Descent(28076): loss=0.44664980802896964\n",
      "Stochastic Gradient Descent(28077): loss=2.3644946424784368\n",
      "Stochastic Gradient Descent(28078): loss=5.823597046441675\n",
      "Stochastic Gradient Descent(28079): loss=4.56634574390477\n",
      "Stochastic Gradient Descent(28080): loss=2.0666528123541044\n",
      "Stochastic Gradient Descent(28081): loss=0.06099818052949903\n",
      "Stochastic Gradient Descent(28082): loss=0.2721934197729504\n",
      "Stochastic Gradient Descent(28083): loss=7.592429778132379\n",
      "Stochastic Gradient Descent(28084): loss=1.8021740713980232\n",
      "Stochastic Gradient Descent(28085): loss=16.64253628709538\n",
      "Stochastic Gradient Descent(28086): loss=1.548441004162628\n",
      "Stochastic Gradient Descent(28087): loss=2.114054166159494\n",
      "Stochastic Gradient Descent(28088): loss=3.7282005460294716\n",
      "Stochastic Gradient Descent(28089): loss=3.379907520317876\n",
      "Stochastic Gradient Descent(28090): loss=3.692177422926164\n",
      "Stochastic Gradient Descent(28091): loss=0.4308041763876465\n",
      "Stochastic Gradient Descent(28092): loss=1.2312485546511442\n",
      "Stochastic Gradient Descent(28093): loss=3.9563187196261635\n",
      "Stochastic Gradient Descent(28094): loss=0.14804442249044686\n",
      "Stochastic Gradient Descent(28095): loss=0.021027896647799724\n",
      "Stochastic Gradient Descent(28096): loss=0.38792140001510483\n",
      "Stochastic Gradient Descent(28097): loss=0.3067474432630179\n",
      "Stochastic Gradient Descent(28098): loss=1.917158102097117\n",
      "Stochastic Gradient Descent(28099): loss=1.592435089327339\n",
      "Stochastic Gradient Descent(28100): loss=0.3631956654599779\n",
      "Stochastic Gradient Descent(28101): loss=22.729616417018423\n",
      "Stochastic Gradient Descent(28102): loss=6.231130846556279\n",
      "Stochastic Gradient Descent(28103): loss=9.706460030859118\n",
      "Stochastic Gradient Descent(28104): loss=11.302841232654522\n",
      "Stochastic Gradient Descent(28105): loss=1.379124562233719\n",
      "Stochastic Gradient Descent(28106): loss=0.057488680228044466\n",
      "Stochastic Gradient Descent(28107): loss=0.44700667244761205\n",
      "Stochastic Gradient Descent(28108): loss=0.055433184703979725\n",
      "Stochastic Gradient Descent(28109): loss=0.09094751959696927\n",
      "Stochastic Gradient Descent(28110): loss=1.0059557315159409\n",
      "Stochastic Gradient Descent(28111): loss=5.053194047215564\n",
      "Stochastic Gradient Descent(28112): loss=3.6295350808241245\n",
      "Stochastic Gradient Descent(28113): loss=2.9503178617733266\n",
      "Stochastic Gradient Descent(28114): loss=0.03229344964763812\n",
      "Stochastic Gradient Descent(28115): loss=4.1657084243109415\n",
      "Stochastic Gradient Descent(28116): loss=12.014238042146115\n",
      "Stochastic Gradient Descent(28117): loss=0.9701055691933784\n",
      "Stochastic Gradient Descent(28118): loss=4.263832970433825\n",
      "Stochastic Gradient Descent(28119): loss=24.15257814093374\n",
      "Stochastic Gradient Descent(28120): loss=9.916619286183833\n",
      "Stochastic Gradient Descent(28121): loss=0.01881016021861064\n",
      "Stochastic Gradient Descent(28122): loss=1.7388728941728315\n",
      "Stochastic Gradient Descent(28123): loss=0.4986458442501872\n",
      "Stochastic Gradient Descent(28124): loss=14.906656649704312\n",
      "Stochastic Gradient Descent(28125): loss=0.07709347806629836\n",
      "Stochastic Gradient Descent(28126): loss=2.50346058669131\n",
      "Stochastic Gradient Descent(28127): loss=2.863939013067119\n",
      "Stochastic Gradient Descent(28128): loss=0.3610476560487683\n",
      "Stochastic Gradient Descent(28129): loss=0.00784144036786303\n",
      "Stochastic Gradient Descent(28130): loss=4.506812037724827\n",
      "Stochastic Gradient Descent(28131): loss=6.049399439726922\n",
      "Stochastic Gradient Descent(28132): loss=0.05644284186236521\n",
      "Stochastic Gradient Descent(28133): loss=13.216572413791154\n",
      "Stochastic Gradient Descent(28134): loss=1.303506211658223\n",
      "Stochastic Gradient Descent(28135): loss=1.8059900560127342\n",
      "Stochastic Gradient Descent(28136): loss=0.2163461374115751\n",
      "Stochastic Gradient Descent(28137): loss=0.8451966041491258\n",
      "Stochastic Gradient Descent(28138): loss=3.089219471637605\n",
      "Stochastic Gradient Descent(28139): loss=0.06475388169035252\n",
      "Stochastic Gradient Descent(28140): loss=0.49984478155149464\n",
      "Stochastic Gradient Descent(28141): loss=1.496161688595943\n",
      "Stochastic Gradient Descent(28142): loss=0.0699160696763528\n",
      "Stochastic Gradient Descent(28143): loss=1.1767999379730454\n",
      "Stochastic Gradient Descent(28144): loss=4.0824894100080815\n",
      "Stochastic Gradient Descent(28145): loss=0.1604560714616739\n",
      "Stochastic Gradient Descent(28146): loss=3.196543023607784\n",
      "Stochastic Gradient Descent(28147): loss=7.606500926199113\n",
      "Stochastic Gradient Descent(28148): loss=0.0004690238182345881\n",
      "Stochastic Gradient Descent(28149): loss=14.150040829243927\n",
      "Stochastic Gradient Descent(28150): loss=3.7029814244162784\n",
      "Stochastic Gradient Descent(28151): loss=0.008416589415411012\n",
      "Stochastic Gradient Descent(28152): loss=16.286143862097827\n",
      "Stochastic Gradient Descent(28153): loss=14.370409052955184\n",
      "Stochastic Gradient Descent(28154): loss=0.12470313446174497\n",
      "Stochastic Gradient Descent(28155): loss=0.045509339544334934\n",
      "Stochastic Gradient Descent(28156): loss=0.5173514221633978\n",
      "Stochastic Gradient Descent(28157): loss=0.38844578234914784\n",
      "Stochastic Gradient Descent(28158): loss=5.1353655571795125\n",
      "Stochastic Gradient Descent(28159): loss=0.3233572511918029\n",
      "Stochastic Gradient Descent(28160): loss=7.767840690623995\n",
      "Stochastic Gradient Descent(28161): loss=2.3621541477514243\n",
      "Stochastic Gradient Descent(28162): loss=0.7916680570210463\n",
      "Stochastic Gradient Descent(28163): loss=13.328283813720383\n",
      "Stochastic Gradient Descent(28164): loss=1.210810943460681\n",
      "Stochastic Gradient Descent(28165): loss=3.479477083807293\n",
      "Stochastic Gradient Descent(28166): loss=5.152522496785114\n",
      "Stochastic Gradient Descent(28167): loss=18.16394885923814\n",
      "Stochastic Gradient Descent(28168): loss=13.525889391488189\n",
      "Stochastic Gradient Descent(28169): loss=0.7877465608756135\n",
      "Stochastic Gradient Descent(28170): loss=1.7661542422773433\n",
      "Stochastic Gradient Descent(28171): loss=0.624922219983809\n",
      "Stochastic Gradient Descent(28172): loss=0.8613054398673878\n",
      "Stochastic Gradient Descent(28173): loss=6.278078250009435\n",
      "Stochastic Gradient Descent(28174): loss=0.6314195967409204\n",
      "Stochastic Gradient Descent(28175): loss=9.537268802024803\n",
      "Stochastic Gradient Descent(28176): loss=2.5142614697268524\n",
      "Stochastic Gradient Descent(28177): loss=4.725131564600944\n",
      "Stochastic Gradient Descent(28178): loss=1.3340174823600657\n",
      "Stochastic Gradient Descent(28179): loss=0.012205924681941626\n",
      "Stochastic Gradient Descent(28180): loss=2.066637920602096\n",
      "Stochastic Gradient Descent(28181): loss=9.868535514292427\n",
      "Stochastic Gradient Descent(28182): loss=0.47301266679150983\n",
      "Stochastic Gradient Descent(28183): loss=1.6901855489019422\n",
      "Stochastic Gradient Descent(28184): loss=0.9024080624008011\n",
      "Stochastic Gradient Descent(28185): loss=12.23147776759115\n",
      "Stochastic Gradient Descent(28186): loss=24.340437685816763\n",
      "Stochastic Gradient Descent(28187): loss=10.34256999872109\n",
      "Stochastic Gradient Descent(28188): loss=1.9538679099920335\n",
      "Stochastic Gradient Descent(28189): loss=2.563419397783515\n",
      "Stochastic Gradient Descent(28190): loss=1.022582038937326\n",
      "Stochastic Gradient Descent(28191): loss=4.878320020209746\n",
      "Stochastic Gradient Descent(28192): loss=1.4630123003288624\n",
      "Stochastic Gradient Descent(28193): loss=0.1444346299504079\n",
      "Stochastic Gradient Descent(28194): loss=0.0240359134312971\n",
      "Stochastic Gradient Descent(28195): loss=1.7831788309387326\n",
      "Stochastic Gradient Descent(28196): loss=0.3931636143705312\n",
      "Stochastic Gradient Descent(28197): loss=13.650296856029643\n",
      "Stochastic Gradient Descent(28198): loss=13.693881663327737\n",
      "Stochastic Gradient Descent(28199): loss=0.31555377802945717\n",
      "Stochastic Gradient Descent(28200): loss=11.568062281521932\n",
      "Stochastic Gradient Descent(28201): loss=0.6330388623750676\n",
      "Stochastic Gradient Descent(28202): loss=2.019611000084355\n",
      "Stochastic Gradient Descent(28203): loss=3.7603627662360877\n",
      "Stochastic Gradient Descent(28204): loss=15.081756840711414\n",
      "Stochastic Gradient Descent(28205): loss=0.4930438723730718\n",
      "Stochastic Gradient Descent(28206): loss=0.0297377731189968\n",
      "Stochastic Gradient Descent(28207): loss=0.9532434893333089\n",
      "Stochastic Gradient Descent(28208): loss=2.728264255844183\n",
      "Stochastic Gradient Descent(28209): loss=2.8740345726967464\n",
      "Stochastic Gradient Descent(28210): loss=3.060366156045041\n",
      "Stochastic Gradient Descent(28211): loss=5.650711592305832\n",
      "Stochastic Gradient Descent(28212): loss=3.893663209708577\n",
      "Stochastic Gradient Descent(28213): loss=0.0027530392761771828\n",
      "Stochastic Gradient Descent(28214): loss=0.5402315327247177\n",
      "Stochastic Gradient Descent(28215): loss=0.22627430551958774\n",
      "Stochastic Gradient Descent(28216): loss=0.09063699591641321\n",
      "Stochastic Gradient Descent(28217): loss=1.569871784930015\n",
      "Stochastic Gradient Descent(28218): loss=5.705356398719434\n",
      "Stochastic Gradient Descent(28219): loss=6.225818195196688\n",
      "Stochastic Gradient Descent(28220): loss=0.5149227073316328\n",
      "Stochastic Gradient Descent(28221): loss=8.270418428452443\n",
      "Stochastic Gradient Descent(28222): loss=0.1481447569292961\n",
      "Stochastic Gradient Descent(28223): loss=4.04662709860395\n",
      "Stochastic Gradient Descent(28224): loss=9.881170312921448\n",
      "Stochastic Gradient Descent(28225): loss=0.28075150237920704\n",
      "Stochastic Gradient Descent(28226): loss=1.4252963316945682\n",
      "Stochastic Gradient Descent(28227): loss=5.992809967706597\n",
      "Stochastic Gradient Descent(28228): loss=3.014929465479739\n",
      "Stochastic Gradient Descent(28229): loss=9.155592041375478\n",
      "Stochastic Gradient Descent(28230): loss=0.4347765337398367\n",
      "Stochastic Gradient Descent(28231): loss=0.146104862157374\n",
      "Stochastic Gradient Descent(28232): loss=2.059515937993341\n",
      "Stochastic Gradient Descent(28233): loss=6.88987480285888\n",
      "Stochastic Gradient Descent(28234): loss=0.7342122733541977\n",
      "Stochastic Gradient Descent(28235): loss=2.495715505255599\n",
      "Stochastic Gradient Descent(28236): loss=0.10502925250965646\n",
      "Stochastic Gradient Descent(28237): loss=2.149823230028191\n",
      "Stochastic Gradient Descent(28238): loss=9.037579221279328\n",
      "Stochastic Gradient Descent(28239): loss=23.974506544417444\n",
      "Stochastic Gradient Descent(28240): loss=0.7763455705213843\n",
      "Stochastic Gradient Descent(28241): loss=0.6195842799618807\n",
      "Stochastic Gradient Descent(28242): loss=0.1800296088807104\n",
      "Stochastic Gradient Descent(28243): loss=1.7181860158150046\n",
      "Stochastic Gradient Descent(28244): loss=0.5814419564892882\n",
      "Stochastic Gradient Descent(28245): loss=1.3067427173603081\n",
      "Stochastic Gradient Descent(28246): loss=0.9429350311918232\n",
      "Stochastic Gradient Descent(28247): loss=2.321097839726327\n",
      "Stochastic Gradient Descent(28248): loss=4.215262291686559\n",
      "Stochastic Gradient Descent(28249): loss=0.1167011989289905\n",
      "Stochastic Gradient Descent(28250): loss=1.9876850311113148\n",
      "Stochastic Gradient Descent(28251): loss=0.037661589091745606\n",
      "Stochastic Gradient Descent(28252): loss=2.1458637858460383\n",
      "Stochastic Gradient Descent(28253): loss=27.077595740568\n",
      "Stochastic Gradient Descent(28254): loss=170.5303237557308\n",
      "Stochastic Gradient Descent(28255): loss=24.079561844758704\n",
      "Stochastic Gradient Descent(28256): loss=3.8077975169944165\n",
      "Stochastic Gradient Descent(28257): loss=0.11273584619031721\n",
      "Stochastic Gradient Descent(28258): loss=3.6013402552472686\n",
      "Stochastic Gradient Descent(28259): loss=4.024984561862416\n",
      "Stochastic Gradient Descent(28260): loss=3.3675486015025107\n",
      "Stochastic Gradient Descent(28261): loss=0.6332545373547113\n",
      "Stochastic Gradient Descent(28262): loss=4.399613666356863\n",
      "Stochastic Gradient Descent(28263): loss=4.5036156580109115\n",
      "Stochastic Gradient Descent(28264): loss=0.8362799051922952\n",
      "Stochastic Gradient Descent(28265): loss=0.35458264773729997\n",
      "Stochastic Gradient Descent(28266): loss=15.941918208072744\n",
      "Stochastic Gradient Descent(28267): loss=10.47583986669194\n",
      "Stochastic Gradient Descent(28268): loss=0.09405306418856986\n",
      "Stochastic Gradient Descent(28269): loss=0.36226070081569856\n",
      "Stochastic Gradient Descent(28270): loss=0.34260223061368356\n",
      "Stochastic Gradient Descent(28271): loss=1.7769082080239818\n",
      "Stochastic Gradient Descent(28272): loss=2.09486357894261\n",
      "Stochastic Gradient Descent(28273): loss=0.11050291935236152\n",
      "Stochastic Gradient Descent(28274): loss=12.338931828304489\n",
      "Stochastic Gradient Descent(28275): loss=0.18473329387936444\n",
      "Stochastic Gradient Descent(28276): loss=6.3166362162127925\n",
      "Stochastic Gradient Descent(28277): loss=0.2419058780012277\n",
      "Stochastic Gradient Descent(28278): loss=2.8214675619546865\n",
      "Stochastic Gradient Descent(28279): loss=9.029653700010947\n",
      "Stochastic Gradient Descent(28280): loss=0.036011854523086875\n",
      "Stochastic Gradient Descent(28281): loss=7.1068420736414035\n",
      "Stochastic Gradient Descent(28282): loss=11.467272111644816\n",
      "Stochastic Gradient Descent(28283): loss=0.04207020558356648\n",
      "Stochastic Gradient Descent(28284): loss=1.0193026295167054\n",
      "Stochastic Gradient Descent(28285): loss=4.760399696386918\n",
      "Stochastic Gradient Descent(28286): loss=1.9320634622467823\n",
      "Stochastic Gradient Descent(28287): loss=0.7502619024109518\n",
      "Stochastic Gradient Descent(28288): loss=11.070061930083934\n",
      "Stochastic Gradient Descent(28289): loss=0.942940769267974\n",
      "Stochastic Gradient Descent(28290): loss=5.928298633826462\n",
      "Stochastic Gradient Descent(28291): loss=9.337617374160514\n",
      "Stochastic Gradient Descent(28292): loss=0.17924519069734576\n",
      "Stochastic Gradient Descent(28293): loss=0.0573818473811516\n",
      "Stochastic Gradient Descent(28294): loss=2.7288235293611187\n",
      "Stochastic Gradient Descent(28295): loss=0.8436433528247711\n",
      "Stochastic Gradient Descent(28296): loss=8.464211476375633\n",
      "Stochastic Gradient Descent(28297): loss=12.867804953640004\n",
      "Stochastic Gradient Descent(28298): loss=1.5737270048817813\n",
      "Stochastic Gradient Descent(28299): loss=1.1429184321498216\n",
      "Stochastic Gradient Descent(28300): loss=1.4382471097372158\n",
      "Stochastic Gradient Descent(28301): loss=0.44601223994666395\n",
      "Stochastic Gradient Descent(28302): loss=3.5236897148407365\n",
      "Stochastic Gradient Descent(28303): loss=0.14661521637481567\n",
      "Stochastic Gradient Descent(28304): loss=7.417578738939585\n",
      "Stochastic Gradient Descent(28305): loss=0.18484735453282114\n",
      "Stochastic Gradient Descent(28306): loss=7.25721349001616\n",
      "Stochastic Gradient Descent(28307): loss=1.8432330633206948\n",
      "Stochastic Gradient Descent(28308): loss=2.1224645715526638\n",
      "Stochastic Gradient Descent(28309): loss=7.151259148746685\n",
      "Stochastic Gradient Descent(28310): loss=1.320421495684738\n",
      "Stochastic Gradient Descent(28311): loss=0.11248906701888847\n",
      "Stochastic Gradient Descent(28312): loss=3.8873285568589613\n",
      "Stochastic Gradient Descent(28313): loss=3.772257913399584\n",
      "Stochastic Gradient Descent(28314): loss=21.111691337179966\n",
      "Stochastic Gradient Descent(28315): loss=0.018496854304871847\n",
      "Stochastic Gradient Descent(28316): loss=0.27420465221610674\n",
      "Stochastic Gradient Descent(28317): loss=5.303687159531878\n",
      "Stochastic Gradient Descent(28318): loss=2.095917036075544\n",
      "Stochastic Gradient Descent(28319): loss=12.005160094099082\n",
      "Stochastic Gradient Descent(28320): loss=1.110918714627153\n",
      "Stochastic Gradient Descent(28321): loss=1.489985385937133\n",
      "Stochastic Gradient Descent(28322): loss=3.7169870812978907\n",
      "Stochastic Gradient Descent(28323): loss=1.7172446718862242\n",
      "Stochastic Gradient Descent(28324): loss=14.898301521764704\n",
      "Stochastic Gradient Descent(28325): loss=1.7621177425212393\n",
      "Stochastic Gradient Descent(28326): loss=5.566153501625962\n",
      "Stochastic Gradient Descent(28327): loss=0.02200022447762\n",
      "Stochastic Gradient Descent(28328): loss=1.3520884598666576\n",
      "Stochastic Gradient Descent(28329): loss=24.120749088282547\n",
      "Stochastic Gradient Descent(28330): loss=0.7481413356219472\n",
      "Stochastic Gradient Descent(28331): loss=1.2296668939360895\n",
      "Stochastic Gradient Descent(28332): loss=0.21820159447559007\n",
      "Stochastic Gradient Descent(28333): loss=10.724799279783257\n",
      "Stochastic Gradient Descent(28334): loss=1.6153174756380129\n",
      "Stochastic Gradient Descent(28335): loss=4.97200530849217\n",
      "Stochastic Gradient Descent(28336): loss=7.246257588957656\n",
      "Stochastic Gradient Descent(28337): loss=10.44823701713296\n",
      "Stochastic Gradient Descent(28338): loss=6.4922112843120585\n",
      "Stochastic Gradient Descent(28339): loss=2.373430385705616\n",
      "Stochastic Gradient Descent(28340): loss=1.1477706689089688\n",
      "Stochastic Gradient Descent(28341): loss=10.226010812010063\n",
      "Stochastic Gradient Descent(28342): loss=0.02434992720639536\n",
      "Stochastic Gradient Descent(28343): loss=0.15636339542057318\n",
      "Stochastic Gradient Descent(28344): loss=10.237450046947826\n",
      "Stochastic Gradient Descent(28345): loss=3.6977352918442934\n",
      "Stochastic Gradient Descent(28346): loss=1.8292945496735684\n",
      "Stochastic Gradient Descent(28347): loss=8.138484015196148\n",
      "Stochastic Gradient Descent(28348): loss=12.889491719179725\n",
      "Stochastic Gradient Descent(28349): loss=1.1279680933669909\n",
      "Stochastic Gradient Descent(28350): loss=0.5533684366429801\n",
      "Stochastic Gradient Descent(28351): loss=1.1128415816277872\n",
      "Stochastic Gradient Descent(28352): loss=1.68209705981944\n",
      "Stochastic Gradient Descent(28353): loss=13.622131734326908\n",
      "Stochastic Gradient Descent(28354): loss=2.500862691125224\n",
      "Stochastic Gradient Descent(28355): loss=0.27503598092765064\n",
      "Stochastic Gradient Descent(28356): loss=3.367908978577009\n",
      "Stochastic Gradient Descent(28357): loss=1.7872470275454275\n",
      "Stochastic Gradient Descent(28358): loss=0.022120310305292254\n",
      "Stochastic Gradient Descent(28359): loss=0.3651353028006549\n",
      "Stochastic Gradient Descent(28360): loss=4.671800211860503\n",
      "Stochastic Gradient Descent(28361): loss=1.3115681382999647\n",
      "Stochastic Gradient Descent(28362): loss=1.3562094447285327\n",
      "Stochastic Gradient Descent(28363): loss=18.5084396590749\n",
      "Stochastic Gradient Descent(28364): loss=0.9634350256475956\n",
      "Stochastic Gradient Descent(28365): loss=0.10561138255033249\n",
      "Stochastic Gradient Descent(28366): loss=0.534593753878314\n",
      "Stochastic Gradient Descent(28367): loss=4.68198738544099\n",
      "Stochastic Gradient Descent(28368): loss=15.111335914685599\n",
      "Stochastic Gradient Descent(28369): loss=0.046159870480065264\n",
      "Stochastic Gradient Descent(28370): loss=2.8742288353942738\n",
      "Stochastic Gradient Descent(28371): loss=5.23360091987347\n",
      "Stochastic Gradient Descent(28372): loss=15.420314757871418\n",
      "Stochastic Gradient Descent(28373): loss=0.32291265996341845\n",
      "Stochastic Gradient Descent(28374): loss=2.3728686916012807\n",
      "Stochastic Gradient Descent(28375): loss=1.2270377810136028\n",
      "Stochastic Gradient Descent(28376): loss=0.35134824444385604\n",
      "Stochastic Gradient Descent(28377): loss=0.000866531392358251\n",
      "Stochastic Gradient Descent(28378): loss=5.6148763410763545\n",
      "Stochastic Gradient Descent(28379): loss=1.3609941091725433\n",
      "Stochastic Gradient Descent(28380): loss=7.177352282670606\n",
      "Stochastic Gradient Descent(28381): loss=1.9609387078830374\n",
      "Stochastic Gradient Descent(28382): loss=9.732987729396358\n",
      "Stochastic Gradient Descent(28383): loss=4.215784533625495\n",
      "Stochastic Gradient Descent(28384): loss=2.941181084117896\n",
      "Stochastic Gradient Descent(28385): loss=12.762630405686506\n",
      "Stochastic Gradient Descent(28386): loss=3.6174088194543437\n",
      "Stochastic Gradient Descent(28387): loss=1.9311934119414784\n",
      "Stochastic Gradient Descent(28388): loss=0.0262831874418181\n",
      "Stochastic Gradient Descent(28389): loss=0.03822771029724323\n",
      "Stochastic Gradient Descent(28390): loss=0.5280187975988226\n",
      "Stochastic Gradient Descent(28391): loss=4.597344538352975\n",
      "Stochastic Gradient Descent(28392): loss=1.1814569366615904\n",
      "Stochastic Gradient Descent(28393): loss=2.2395046778568495\n",
      "Stochastic Gradient Descent(28394): loss=0.35756032706305213\n",
      "Stochastic Gradient Descent(28395): loss=7.272947330714951\n",
      "Stochastic Gradient Descent(28396): loss=0.3380000343374171\n",
      "Stochastic Gradient Descent(28397): loss=20.3472759585479\n",
      "Stochastic Gradient Descent(28398): loss=0.32763795828970943\n",
      "Stochastic Gradient Descent(28399): loss=0.09129806300767992\n",
      "Stochastic Gradient Descent(28400): loss=2.258666907821383\n",
      "Stochastic Gradient Descent(28401): loss=0.2767659504351578\n",
      "Stochastic Gradient Descent(28402): loss=15.589718095376668\n",
      "Stochastic Gradient Descent(28403): loss=33.322597119781676\n",
      "Stochastic Gradient Descent(28404): loss=2.22041024437387\n",
      "Stochastic Gradient Descent(28405): loss=0.03492442219675366\n",
      "Stochastic Gradient Descent(28406): loss=2.4996917773212717\n",
      "Stochastic Gradient Descent(28407): loss=7.5091759022473346\n",
      "Stochastic Gradient Descent(28408): loss=1.2168880291307098\n",
      "Stochastic Gradient Descent(28409): loss=0.017304325508422116\n",
      "Stochastic Gradient Descent(28410): loss=1.1011104609678226\n",
      "Stochastic Gradient Descent(28411): loss=0.5918279790394245\n",
      "Stochastic Gradient Descent(28412): loss=0.07889015114276995\n",
      "Stochastic Gradient Descent(28413): loss=3.312082322470188\n",
      "Stochastic Gradient Descent(28414): loss=0.9030842118962825\n",
      "Stochastic Gradient Descent(28415): loss=2.3615763069093654\n",
      "Stochastic Gradient Descent(28416): loss=7.3325996042825325\n",
      "Stochastic Gradient Descent(28417): loss=1.8939150362668524\n",
      "Stochastic Gradient Descent(28418): loss=0.22290503600309303\n",
      "Stochastic Gradient Descent(28419): loss=3.7125529362583447\n",
      "Stochastic Gradient Descent(28420): loss=0.03911846370477034\n",
      "Stochastic Gradient Descent(28421): loss=9.177025102867072\n",
      "Stochastic Gradient Descent(28422): loss=7.191774204029289\n",
      "Stochastic Gradient Descent(28423): loss=2.4090583567087607\n",
      "Stochastic Gradient Descent(28424): loss=3.0350532835301167\n",
      "Stochastic Gradient Descent(28425): loss=6.278587453108294\n",
      "Stochastic Gradient Descent(28426): loss=0.01305938792352424\n",
      "Stochastic Gradient Descent(28427): loss=1.4533254261682276\n",
      "Stochastic Gradient Descent(28428): loss=1.7501771237838324\n",
      "Stochastic Gradient Descent(28429): loss=5.419089895747439\n",
      "Stochastic Gradient Descent(28430): loss=9.99117367040208\n",
      "Stochastic Gradient Descent(28431): loss=0.3758984465988363\n",
      "Stochastic Gradient Descent(28432): loss=2.5326032274019985\n",
      "Stochastic Gradient Descent(28433): loss=7.779381189525376\n",
      "Stochastic Gradient Descent(28434): loss=0.004785402667797494\n",
      "Stochastic Gradient Descent(28435): loss=13.716577129562513\n",
      "Stochastic Gradient Descent(28436): loss=0.0028006225648419076\n",
      "Stochastic Gradient Descent(28437): loss=14.217623494263693\n",
      "Stochastic Gradient Descent(28438): loss=8.514283699875893\n",
      "Stochastic Gradient Descent(28439): loss=4.492675037481399\n",
      "Stochastic Gradient Descent(28440): loss=3.1544329423102537\n",
      "Stochastic Gradient Descent(28441): loss=0.007664177093344752\n",
      "Stochastic Gradient Descent(28442): loss=7.190457992285715e-05\n",
      "Stochastic Gradient Descent(28443): loss=1.7760164725755403\n",
      "Stochastic Gradient Descent(28444): loss=0.7560857934129286\n",
      "Stochastic Gradient Descent(28445): loss=0.234129342594329\n",
      "Stochastic Gradient Descent(28446): loss=0.4684947932999434\n",
      "Stochastic Gradient Descent(28447): loss=2.963742309497475\n",
      "Stochastic Gradient Descent(28448): loss=1.1102202121465747\n",
      "Stochastic Gradient Descent(28449): loss=2.0703964608886647\n",
      "Stochastic Gradient Descent(28450): loss=4.475280641450593\n",
      "Stochastic Gradient Descent(28451): loss=1.0900582932724208\n",
      "Stochastic Gradient Descent(28452): loss=5.052476032067811\n",
      "Stochastic Gradient Descent(28453): loss=5.50415840198199\n",
      "Stochastic Gradient Descent(28454): loss=1.563171950858239\n",
      "Stochastic Gradient Descent(28455): loss=0.01915825691672677\n",
      "Stochastic Gradient Descent(28456): loss=2.378630802344277\n",
      "Stochastic Gradient Descent(28457): loss=1.4062522042708159\n",
      "Stochastic Gradient Descent(28458): loss=1.189435049696009\n",
      "Stochastic Gradient Descent(28459): loss=4.048282888954096\n",
      "Stochastic Gradient Descent(28460): loss=0.3438479308866468\n",
      "Stochastic Gradient Descent(28461): loss=0.7910884861275438\n",
      "Stochastic Gradient Descent(28462): loss=0.7370153611046841\n",
      "Stochastic Gradient Descent(28463): loss=0.13311426543571866\n",
      "Stochastic Gradient Descent(28464): loss=0.48217618928446426\n",
      "Stochastic Gradient Descent(28465): loss=1.5453745146092779\n",
      "Stochastic Gradient Descent(28466): loss=0.027436323981291245\n",
      "Stochastic Gradient Descent(28467): loss=0.4618573307925037\n",
      "Stochastic Gradient Descent(28468): loss=4.10733454940724\n",
      "Stochastic Gradient Descent(28469): loss=5.142960877406419\n",
      "Stochastic Gradient Descent(28470): loss=2.4059486166051918\n",
      "Stochastic Gradient Descent(28471): loss=6.09376864665433\n",
      "Stochastic Gradient Descent(28472): loss=0.4880082462178617\n",
      "Stochastic Gradient Descent(28473): loss=0.07175527756550339\n",
      "Stochastic Gradient Descent(28474): loss=0.4376833911896288\n",
      "Stochastic Gradient Descent(28475): loss=2.2934393606005674\n",
      "Stochastic Gradient Descent(28476): loss=0.581724764408334\n",
      "Stochastic Gradient Descent(28477): loss=0.009081952178792579\n",
      "Stochastic Gradient Descent(28478): loss=0.22680393241211463\n",
      "Stochastic Gradient Descent(28479): loss=0.09936149695526782\n",
      "Stochastic Gradient Descent(28480): loss=1.9892577346467044\n",
      "Stochastic Gradient Descent(28481): loss=0.004505754895558308\n",
      "Stochastic Gradient Descent(28482): loss=5.263482329990953\n",
      "Stochastic Gradient Descent(28483): loss=39.932145697537294\n",
      "Stochastic Gradient Descent(28484): loss=1.3574244724823454\n",
      "Stochastic Gradient Descent(28485): loss=4.738817203335449\n",
      "Stochastic Gradient Descent(28486): loss=2.694497792115841\n",
      "Stochastic Gradient Descent(28487): loss=2.8692957901833775\n",
      "Stochastic Gradient Descent(28488): loss=14.613817641427605\n",
      "Stochastic Gradient Descent(28489): loss=0.39780660503989784\n",
      "Stochastic Gradient Descent(28490): loss=0.00032568766193726814\n",
      "Stochastic Gradient Descent(28491): loss=0.013535049873723315\n",
      "Stochastic Gradient Descent(28492): loss=1.5625930129927796\n",
      "Stochastic Gradient Descent(28493): loss=2.3972205443642007\n",
      "Stochastic Gradient Descent(28494): loss=46.32358255142778\n",
      "Stochastic Gradient Descent(28495): loss=2.9481437901686465\n",
      "Stochastic Gradient Descent(28496): loss=11.671600505708774\n",
      "Stochastic Gradient Descent(28497): loss=3.184235637169678\n",
      "Stochastic Gradient Descent(28498): loss=26.905646218738983\n",
      "Stochastic Gradient Descent(28499): loss=0.9967148213988166\n",
      "Stochastic Gradient Descent(28500): loss=0.5984350540748196\n",
      "Stochastic Gradient Descent(28501): loss=0.022375715192045963\n",
      "Stochastic Gradient Descent(28502): loss=0.7093928133850167\n",
      "Stochastic Gradient Descent(28503): loss=3.858425204313404\n",
      "Stochastic Gradient Descent(28504): loss=11.14271332385134\n",
      "Stochastic Gradient Descent(28505): loss=13.30482146368779\n",
      "Stochastic Gradient Descent(28506): loss=2.1097373584571386\n",
      "Stochastic Gradient Descent(28507): loss=7.802711454042411\n",
      "Stochastic Gradient Descent(28508): loss=16.79105582056292\n",
      "Stochastic Gradient Descent(28509): loss=1.2041546470669475\n",
      "Stochastic Gradient Descent(28510): loss=1.1410344219620547\n",
      "Stochastic Gradient Descent(28511): loss=6.429674132710577\n",
      "Stochastic Gradient Descent(28512): loss=3.5172325860928413\n",
      "Stochastic Gradient Descent(28513): loss=0.02813382623662136\n",
      "Stochastic Gradient Descent(28514): loss=8.311209317062083\n",
      "Stochastic Gradient Descent(28515): loss=0.007721909207956229\n",
      "Stochastic Gradient Descent(28516): loss=0.6386963797116001\n",
      "Stochastic Gradient Descent(28517): loss=2.3436737390329414\n",
      "Stochastic Gradient Descent(28518): loss=4.379118812087685\n",
      "Stochastic Gradient Descent(28519): loss=4.185072264303478\n",
      "Stochastic Gradient Descent(28520): loss=56.06229064618802\n",
      "Stochastic Gradient Descent(28521): loss=8.349437034731294e-05\n",
      "Stochastic Gradient Descent(28522): loss=14.684554193908529\n",
      "Stochastic Gradient Descent(28523): loss=18.74141344665903\n",
      "Stochastic Gradient Descent(28524): loss=0.15344566384060934\n",
      "Stochastic Gradient Descent(28525): loss=20.31056456724951\n",
      "Stochastic Gradient Descent(28526): loss=41.19903340064483\n",
      "Stochastic Gradient Descent(28527): loss=0.6856029163536453\n",
      "Stochastic Gradient Descent(28528): loss=12.690333880343603\n",
      "Stochastic Gradient Descent(28529): loss=1.0052893967913794\n",
      "Stochastic Gradient Descent(28530): loss=5.264343196794804\n",
      "Stochastic Gradient Descent(28531): loss=4.058467632401376\n",
      "Stochastic Gradient Descent(28532): loss=0.0070724512176097215\n",
      "Stochastic Gradient Descent(28533): loss=11.45910390426488\n",
      "Stochastic Gradient Descent(28534): loss=2.727741286806833\n",
      "Stochastic Gradient Descent(28535): loss=3.993992528801915\n",
      "Stochastic Gradient Descent(28536): loss=2.309679901346873\n",
      "Stochastic Gradient Descent(28537): loss=3.477190196812188\n",
      "Stochastic Gradient Descent(28538): loss=0.01052473290319484\n",
      "Stochastic Gradient Descent(28539): loss=4.587981471493654\n",
      "Stochastic Gradient Descent(28540): loss=0.9401942128981872\n",
      "Stochastic Gradient Descent(28541): loss=1.7837388254664548\n",
      "Stochastic Gradient Descent(28542): loss=0.7828501296172101\n",
      "Stochastic Gradient Descent(28543): loss=0.8592484862591989\n",
      "Stochastic Gradient Descent(28544): loss=10.986135065746044\n",
      "Stochastic Gradient Descent(28545): loss=1.5407705199330175\n",
      "Stochastic Gradient Descent(28546): loss=0.1562664858622756\n",
      "Stochastic Gradient Descent(28547): loss=0.77113592500295\n",
      "Stochastic Gradient Descent(28548): loss=11.116263855413372\n",
      "Stochastic Gradient Descent(28549): loss=0.32831780379310177\n",
      "Stochastic Gradient Descent(28550): loss=0.08107541229643787\n",
      "Stochastic Gradient Descent(28551): loss=0.35935532977450135\n",
      "Stochastic Gradient Descent(28552): loss=21.88709013929476\n",
      "Stochastic Gradient Descent(28553): loss=70.15399263216082\n",
      "Stochastic Gradient Descent(28554): loss=16.739716271612217\n",
      "Stochastic Gradient Descent(28555): loss=6.208128055961502\n",
      "Stochastic Gradient Descent(28556): loss=0.17908381766681705\n",
      "Stochastic Gradient Descent(28557): loss=4.160957393556242\n",
      "Stochastic Gradient Descent(28558): loss=0.16787297688678243\n",
      "Stochastic Gradient Descent(28559): loss=0.032475198662739596\n",
      "Stochastic Gradient Descent(28560): loss=4.788512693380172\n",
      "Stochastic Gradient Descent(28561): loss=25.808185885151723\n",
      "Stochastic Gradient Descent(28562): loss=0.5355611105263652\n",
      "Stochastic Gradient Descent(28563): loss=7.6448573427948165\n",
      "Stochastic Gradient Descent(28564): loss=0.49021021402382575\n",
      "Stochastic Gradient Descent(28565): loss=9.939121299658792\n",
      "Stochastic Gradient Descent(28566): loss=7.53267368327606\n",
      "Stochastic Gradient Descent(28567): loss=0.13091001268853006\n",
      "Stochastic Gradient Descent(28568): loss=4.007200138610378\n",
      "Stochastic Gradient Descent(28569): loss=2.689234029513765\n",
      "Stochastic Gradient Descent(28570): loss=5.459153842951068\n",
      "Stochastic Gradient Descent(28571): loss=0.004720454875528033\n",
      "Stochastic Gradient Descent(28572): loss=0.23008613361930907\n",
      "Stochastic Gradient Descent(28573): loss=3.8374087371589236\n",
      "Stochastic Gradient Descent(28574): loss=2.6805802658573232\n",
      "Stochastic Gradient Descent(28575): loss=13.731549292024221\n",
      "Stochastic Gradient Descent(28576): loss=2.629109979418979\n",
      "Stochastic Gradient Descent(28577): loss=1.7308090316590732\n",
      "Stochastic Gradient Descent(28578): loss=11.254716501218448\n",
      "Stochastic Gradient Descent(28579): loss=0.5964789372564548\n",
      "Stochastic Gradient Descent(28580): loss=0.002719602135757891\n",
      "Stochastic Gradient Descent(28581): loss=51.72687375364349\n",
      "Stochastic Gradient Descent(28582): loss=20.162296618018903\n",
      "Stochastic Gradient Descent(28583): loss=2.3080936209524245\n",
      "Stochastic Gradient Descent(28584): loss=17.69075789761367\n",
      "Stochastic Gradient Descent(28585): loss=0.28284255676267517\n",
      "Stochastic Gradient Descent(28586): loss=1.6034384157865573\n",
      "Stochastic Gradient Descent(28587): loss=26.861962158692293\n",
      "Stochastic Gradient Descent(28588): loss=0.004025518216661111\n",
      "Stochastic Gradient Descent(28589): loss=0.0218398384623346\n",
      "Stochastic Gradient Descent(28590): loss=0.09337475654513384\n",
      "Stochastic Gradient Descent(28591): loss=0.24183539388939315\n",
      "Stochastic Gradient Descent(28592): loss=9.414314834272203\n",
      "Stochastic Gradient Descent(28593): loss=0.1449301536747795\n",
      "Stochastic Gradient Descent(28594): loss=4.437573281947454\n",
      "Stochastic Gradient Descent(28595): loss=0.43941071286551475\n",
      "Stochastic Gradient Descent(28596): loss=1.0196022481808595\n",
      "Stochastic Gradient Descent(28597): loss=0.10016970382327886\n",
      "Stochastic Gradient Descent(28598): loss=0.9034680334587438\n",
      "Stochastic Gradient Descent(28599): loss=0.0014679067059547375\n",
      "Stochastic Gradient Descent(28600): loss=0.7159075207159558\n",
      "Stochastic Gradient Descent(28601): loss=3.926128578656327\n",
      "Stochastic Gradient Descent(28602): loss=1.590497118718099\n",
      "Stochastic Gradient Descent(28603): loss=0.2005447509111619\n",
      "Stochastic Gradient Descent(28604): loss=7.515032164684063\n",
      "Stochastic Gradient Descent(28605): loss=0.1582607832001432\n",
      "Stochastic Gradient Descent(28606): loss=0.8101822220819497\n",
      "Stochastic Gradient Descent(28607): loss=1.2563776121596002\n",
      "Stochastic Gradient Descent(28608): loss=1.0285623663714925\n",
      "Stochastic Gradient Descent(28609): loss=8.042948622996743\n",
      "Stochastic Gradient Descent(28610): loss=2.809950613933122\n",
      "Stochastic Gradient Descent(28611): loss=0.0006445958110140123\n",
      "Stochastic Gradient Descent(28612): loss=0.008222140769007395\n",
      "Stochastic Gradient Descent(28613): loss=2.655749272224059\n",
      "Stochastic Gradient Descent(28614): loss=18.766193308625244\n",
      "Stochastic Gradient Descent(28615): loss=0.0005380823044063053\n",
      "Stochastic Gradient Descent(28616): loss=4.163144749371093\n",
      "Stochastic Gradient Descent(28617): loss=3.7029412042308243\n",
      "Stochastic Gradient Descent(28618): loss=3.5320486239057995\n",
      "Stochastic Gradient Descent(28619): loss=3.2300466545345756\n",
      "Stochastic Gradient Descent(28620): loss=1.1820227921223565\n",
      "Stochastic Gradient Descent(28621): loss=3.594201351289107\n",
      "Stochastic Gradient Descent(28622): loss=1.7868276690780192\n",
      "Stochastic Gradient Descent(28623): loss=0.13909673527834934\n",
      "Stochastic Gradient Descent(28624): loss=4.4738923572645275\n",
      "Stochastic Gradient Descent(28625): loss=5.538046966242125\n",
      "Stochastic Gradient Descent(28626): loss=1.3552401500021665\n",
      "Stochastic Gradient Descent(28627): loss=2.1190798544796343\n",
      "Stochastic Gradient Descent(28628): loss=0.1557805351910214\n",
      "Stochastic Gradient Descent(28629): loss=5.0694546348877765\n",
      "Stochastic Gradient Descent(28630): loss=5.270824650491134\n",
      "Stochastic Gradient Descent(28631): loss=0.3769737265394445\n",
      "Stochastic Gradient Descent(28632): loss=5.849433733246512\n",
      "Stochastic Gradient Descent(28633): loss=5.719331265367395\n",
      "Stochastic Gradient Descent(28634): loss=0.38528485361615883\n",
      "Stochastic Gradient Descent(28635): loss=3.559885077814266\n",
      "Stochastic Gradient Descent(28636): loss=9.891960565381432\n",
      "Stochastic Gradient Descent(28637): loss=4.387323977150191\n",
      "Stochastic Gradient Descent(28638): loss=0.050213782556233176\n",
      "Stochastic Gradient Descent(28639): loss=11.88698805771766\n",
      "Stochastic Gradient Descent(28640): loss=4.840043774312334\n",
      "Stochastic Gradient Descent(28641): loss=0.905345560979124\n",
      "Stochastic Gradient Descent(28642): loss=6.052132244942349\n",
      "Stochastic Gradient Descent(28643): loss=39.50526269931807\n",
      "Stochastic Gradient Descent(28644): loss=2.1123363224414597\n",
      "Stochastic Gradient Descent(28645): loss=0.00564542915494419\n",
      "Stochastic Gradient Descent(28646): loss=0.5583341420310903\n",
      "Stochastic Gradient Descent(28647): loss=1.6122434916670265\n",
      "Stochastic Gradient Descent(28648): loss=1.3960660374243454\n",
      "Stochastic Gradient Descent(28649): loss=1.712971807381461\n",
      "Stochastic Gradient Descent(28650): loss=2.754993874081627\n",
      "Stochastic Gradient Descent(28651): loss=0.5902345910918372\n",
      "Stochastic Gradient Descent(28652): loss=13.733470997670542\n",
      "Stochastic Gradient Descent(28653): loss=1.1591528349369973\n",
      "Stochastic Gradient Descent(28654): loss=0.5331202236487493\n",
      "Stochastic Gradient Descent(28655): loss=2.3527402684394088\n",
      "Stochastic Gradient Descent(28656): loss=0.9041276104904605\n",
      "Stochastic Gradient Descent(28657): loss=4.235038248504241\n",
      "Stochastic Gradient Descent(28658): loss=6.436814238567715\n",
      "Stochastic Gradient Descent(28659): loss=1.3127151744067995\n",
      "Stochastic Gradient Descent(28660): loss=10.416951914093412\n",
      "Stochastic Gradient Descent(28661): loss=0.07485084669490073\n",
      "Stochastic Gradient Descent(28662): loss=2.2551427573363885\n",
      "Stochastic Gradient Descent(28663): loss=2.7978576490468616\n",
      "Stochastic Gradient Descent(28664): loss=11.280668720742373\n",
      "Stochastic Gradient Descent(28665): loss=0.8414332924381284\n",
      "Stochastic Gradient Descent(28666): loss=0.1309135544199173\n",
      "Stochastic Gradient Descent(28667): loss=8.48042126023121\n",
      "Stochastic Gradient Descent(28668): loss=0.04528447410354464\n",
      "Stochastic Gradient Descent(28669): loss=42.373338357208624\n",
      "Stochastic Gradient Descent(28670): loss=12.764585942893342\n",
      "Stochastic Gradient Descent(28671): loss=0.18480556086137934\n",
      "Stochastic Gradient Descent(28672): loss=4.56046962286591\n",
      "Stochastic Gradient Descent(28673): loss=4.737102590227397\n",
      "Stochastic Gradient Descent(28674): loss=0.938041071584431\n",
      "Stochastic Gradient Descent(28675): loss=0.1136382987512757\n",
      "Stochastic Gradient Descent(28676): loss=2.532757542859223\n",
      "Stochastic Gradient Descent(28677): loss=1.1869436439025793\n",
      "Stochastic Gradient Descent(28678): loss=0.09914361500738655\n",
      "Stochastic Gradient Descent(28679): loss=2.808966522229743\n",
      "Stochastic Gradient Descent(28680): loss=0.2633441082536361\n",
      "Stochastic Gradient Descent(28681): loss=8.410667564565195\n",
      "Stochastic Gradient Descent(28682): loss=0.5146159284005379\n",
      "Stochastic Gradient Descent(28683): loss=0.8502342033749126\n",
      "Stochastic Gradient Descent(28684): loss=9.338861019581646\n",
      "Stochastic Gradient Descent(28685): loss=0.06917091920483343\n",
      "Stochastic Gradient Descent(28686): loss=2.2699285713727413\n",
      "Stochastic Gradient Descent(28687): loss=0.004596971960943197\n",
      "Stochastic Gradient Descent(28688): loss=6.8920147588505305\n",
      "Stochastic Gradient Descent(28689): loss=0.043974149686641824\n",
      "Stochastic Gradient Descent(28690): loss=8.207873830147982\n",
      "Stochastic Gradient Descent(28691): loss=3.9293957425996897\n",
      "Stochastic Gradient Descent(28692): loss=29.164993600262154\n",
      "Stochastic Gradient Descent(28693): loss=5.451462799972422\n",
      "Stochastic Gradient Descent(28694): loss=3.96731641595321\n",
      "Stochastic Gradient Descent(28695): loss=11.699781724202602\n",
      "Stochastic Gradient Descent(28696): loss=0.9724967153721864\n",
      "Stochastic Gradient Descent(28697): loss=21.957441747395926\n",
      "Stochastic Gradient Descent(28698): loss=5.657075004876932\n",
      "Stochastic Gradient Descent(28699): loss=2.5043591696445033\n",
      "Stochastic Gradient Descent(28700): loss=27.46998839953511\n",
      "Stochastic Gradient Descent(28701): loss=0.7338534991031476\n",
      "Stochastic Gradient Descent(28702): loss=1.0728699887158562e-06\n",
      "Stochastic Gradient Descent(28703): loss=2.2834149384443068\n",
      "Stochastic Gradient Descent(28704): loss=1.63410506978389\n",
      "Stochastic Gradient Descent(28705): loss=0.052964927046290404\n",
      "Stochastic Gradient Descent(28706): loss=0.04626731742020387\n",
      "Stochastic Gradient Descent(28707): loss=16.39287982757847\n",
      "Stochastic Gradient Descent(28708): loss=1.5070920437544604\n",
      "Stochastic Gradient Descent(28709): loss=0.529769296736944\n",
      "Stochastic Gradient Descent(28710): loss=0.6940321721928701\n",
      "Stochastic Gradient Descent(28711): loss=0.08658209640347142\n",
      "Stochastic Gradient Descent(28712): loss=0.3427175137363329\n",
      "Stochastic Gradient Descent(28713): loss=11.045113849868864\n",
      "Stochastic Gradient Descent(28714): loss=5.653341253362915\n",
      "Stochastic Gradient Descent(28715): loss=0.03737823859080444\n",
      "Stochastic Gradient Descent(28716): loss=2.4713608928866355\n",
      "Stochastic Gradient Descent(28717): loss=0.06354780632479802\n",
      "Stochastic Gradient Descent(28718): loss=1.369289509642407\n",
      "Stochastic Gradient Descent(28719): loss=1.4978293995646212\n",
      "Stochastic Gradient Descent(28720): loss=3.9731016929262943\n",
      "Stochastic Gradient Descent(28721): loss=17.1515093570262\n",
      "Stochastic Gradient Descent(28722): loss=4.386923376722222\n",
      "Stochastic Gradient Descent(28723): loss=10.379758883215066\n",
      "Stochastic Gradient Descent(28724): loss=5.803482188807651\n",
      "Stochastic Gradient Descent(28725): loss=0.4993193106995798\n",
      "Stochastic Gradient Descent(28726): loss=14.987299183820493\n",
      "Stochastic Gradient Descent(28727): loss=0.06510027875683352\n",
      "Stochastic Gradient Descent(28728): loss=0.31346765425370626\n",
      "Stochastic Gradient Descent(28729): loss=8.415363813207106\n",
      "Stochastic Gradient Descent(28730): loss=1.4055764397399644\n",
      "Stochastic Gradient Descent(28731): loss=1.174291825550107\n",
      "Stochastic Gradient Descent(28732): loss=6.678584325778132\n",
      "Stochastic Gradient Descent(28733): loss=2.1681412603976433\n",
      "Stochastic Gradient Descent(28734): loss=0.5665408266548397\n",
      "Stochastic Gradient Descent(28735): loss=5.775377140069994\n",
      "Stochastic Gradient Descent(28736): loss=6.815493990322327\n",
      "Stochastic Gradient Descent(28737): loss=3.8326586997722765\n",
      "Stochastic Gradient Descent(28738): loss=0.4652885727642995\n",
      "Stochastic Gradient Descent(28739): loss=2.130202762134088\n",
      "Stochastic Gradient Descent(28740): loss=5.253199074378337\n",
      "Stochastic Gradient Descent(28741): loss=3.468324357100192\n",
      "Stochastic Gradient Descent(28742): loss=2.165138418261121\n",
      "Stochastic Gradient Descent(28743): loss=2.016598188282738\n",
      "Stochastic Gradient Descent(28744): loss=0.2964453658028241\n",
      "Stochastic Gradient Descent(28745): loss=1.2193585390936874\n",
      "Stochastic Gradient Descent(28746): loss=5.003372523052245\n",
      "Stochastic Gradient Descent(28747): loss=0.3578648913938502\n",
      "Stochastic Gradient Descent(28748): loss=18.951836662326826\n",
      "Stochastic Gradient Descent(28749): loss=4.297560526228731\n",
      "Stochastic Gradient Descent(28750): loss=0.9437334610282336\n",
      "Stochastic Gradient Descent(28751): loss=1.0247386544202608\n",
      "Stochastic Gradient Descent(28752): loss=2.929869791179097\n",
      "Stochastic Gradient Descent(28753): loss=1.129313959961081\n",
      "Stochastic Gradient Descent(28754): loss=4.221392553843197\n",
      "Stochastic Gradient Descent(28755): loss=12.236757550529429\n",
      "Stochastic Gradient Descent(28756): loss=0.06434406504035517\n",
      "Stochastic Gradient Descent(28757): loss=6.59701956813722\n",
      "Stochastic Gradient Descent(28758): loss=34.00710555611705\n",
      "Stochastic Gradient Descent(28759): loss=0.08825244154873466\n",
      "Stochastic Gradient Descent(28760): loss=2.0999236537801016\n",
      "Stochastic Gradient Descent(28761): loss=3.6736638789372935\n",
      "Stochastic Gradient Descent(28762): loss=20.567120865005386\n",
      "Stochastic Gradient Descent(28763): loss=0.00836862143685646\n",
      "Stochastic Gradient Descent(28764): loss=7.296334264720382\n",
      "Stochastic Gradient Descent(28765): loss=0.4084730113631259\n",
      "Stochastic Gradient Descent(28766): loss=0.6254308703848716\n",
      "Stochastic Gradient Descent(28767): loss=2.1163299520416556\n",
      "Stochastic Gradient Descent(28768): loss=0.04456566404251341\n",
      "Stochastic Gradient Descent(28769): loss=0.8116626937895607\n",
      "Stochastic Gradient Descent(28770): loss=6.767483639435968\n",
      "Stochastic Gradient Descent(28771): loss=5.33860948784491\n",
      "Stochastic Gradient Descent(28772): loss=0.25712042500107757\n",
      "Stochastic Gradient Descent(28773): loss=0.28648339113023125\n",
      "Stochastic Gradient Descent(28774): loss=9.909249158288567\n",
      "Stochastic Gradient Descent(28775): loss=4.472836042219142\n",
      "Stochastic Gradient Descent(28776): loss=7.731679476114164\n",
      "Stochastic Gradient Descent(28777): loss=11.22802794639922\n",
      "Stochastic Gradient Descent(28778): loss=0.14465003827543615\n",
      "Stochastic Gradient Descent(28779): loss=2.121262524347152\n",
      "Stochastic Gradient Descent(28780): loss=2.896388587750115\n",
      "Stochastic Gradient Descent(28781): loss=5.013297909786976\n",
      "Stochastic Gradient Descent(28782): loss=0.35008101702477884\n",
      "Stochastic Gradient Descent(28783): loss=0.8673175796764491\n",
      "Stochastic Gradient Descent(28784): loss=0.7026660819933335\n",
      "Stochastic Gradient Descent(28785): loss=5.591162649746494\n",
      "Stochastic Gradient Descent(28786): loss=0.04746110651368437\n",
      "Stochastic Gradient Descent(28787): loss=2.9897521943869725\n",
      "Stochastic Gradient Descent(28788): loss=5.092255839372285\n",
      "Stochastic Gradient Descent(28789): loss=0.07209499519159525\n",
      "Stochastic Gradient Descent(28790): loss=0.001940729509071717\n",
      "Stochastic Gradient Descent(28791): loss=6.496574227164303\n",
      "Stochastic Gradient Descent(28792): loss=0.7813023701973043\n",
      "Stochastic Gradient Descent(28793): loss=1.1760264997602712\n",
      "Stochastic Gradient Descent(28794): loss=1.9270323961092648\n",
      "Stochastic Gradient Descent(28795): loss=0.5545661384001384\n",
      "Stochastic Gradient Descent(28796): loss=6.190653525534781\n",
      "Stochastic Gradient Descent(28797): loss=14.473232136336797\n",
      "Stochastic Gradient Descent(28798): loss=3.0459063392049655\n",
      "Stochastic Gradient Descent(28799): loss=3.5828403456050215\n",
      "Stochastic Gradient Descent(28800): loss=9.016856176313341\n",
      "Stochastic Gradient Descent(28801): loss=6.228095104148131\n",
      "Stochastic Gradient Descent(28802): loss=0.03877744359679649\n",
      "Stochastic Gradient Descent(28803): loss=2.510651253523145\n",
      "Stochastic Gradient Descent(28804): loss=1.4732815185162993\n",
      "Stochastic Gradient Descent(28805): loss=8.294905068134094\n",
      "Stochastic Gradient Descent(28806): loss=0.1256248245534709\n",
      "Stochastic Gradient Descent(28807): loss=1.251192665513714\n",
      "Stochastic Gradient Descent(28808): loss=2.395481004565462\n",
      "Stochastic Gradient Descent(28809): loss=0.27823304353307116\n",
      "Stochastic Gradient Descent(28810): loss=0.022689507208668972\n",
      "Stochastic Gradient Descent(28811): loss=6.619882951043293\n",
      "Stochastic Gradient Descent(28812): loss=4.197097730467632\n",
      "Stochastic Gradient Descent(28813): loss=13.63719063208356\n",
      "Stochastic Gradient Descent(28814): loss=0.022565085221135164\n",
      "Stochastic Gradient Descent(28815): loss=0.045856946812243664\n",
      "Stochastic Gradient Descent(28816): loss=1.3601355557627652\n",
      "Stochastic Gradient Descent(28817): loss=0.441632071171175\n",
      "Stochastic Gradient Descent(28818): loss=0.29147330305818653\n",
      "Stochastic Gradient Descent(28819): loss=18.566530144385982\n",
      "Stochastic Gradient Descent(28820): loss=4.964143939712482\n",
      "Stochastic Gradient Descent(28821): loss=0.17360859238939014\n",
      "Stochastic Gradient Descent(28822): loss=14.576428382460787\n",
      "Stochastic Gradient Descent(28823): loss=31.812293300791126\n",
      "Stochastic Gradient Descent(28824): loss=3.362377950363742e-06\n",
      "Stochastic Gradient Descent(28825): loss=1.0727140730563964\n",
      "Stochastic Gradient Descent(28826): loss=0.007921533873340145\n",
      "Stochastic Gradient Descent(28827): loss=12.630655657483086\n",
      "Stochastic Gradient Descent(28828): loss=2.9447652596675518\n",
      "Stochastic Gradient Descent(28829): loss=2.980146401638483\n",
      "Stochastic Gradient Descent(28830): loss=5.506065756510738\n",
      "Stochastic Gradient Descent(28831): loss=10.900913137102748\n",
      "Stochastic Gradient Descent(28832): loss=31.30329787747045\n",
      "Stochastic Gradient Descent(28833): loss=8.59010391019037\n",
      "Stochastic Gradient Descent(28834): loss=0.2971838129026346\n",
      "Stochastic Gradient Descent(28835): loss=0.14263171334921898\n",
      "Stochastic Gradient Descent(28836): loss=0.020897778550538922\n",
      "Stochastic Gradient Descent(28837): loss=17.141642385122815\n",
      "Stochastic Gradient Descent(28838): loss=2.4175696420731128\n",
      "Stochastic Gradient Descent(28839): loss=0.8208561564602364\n",
      "Stochastic Gradient Descent(28840): loss=29.55947814768368\n",
      "Stochastic Gradient Descent(28841): loss=1.7279989712810329\n",
      "Stochastic Gradient Descent(28842): loss=3.586985588326321\n",
      "Stochastic Gradient Descent(28843): loss=0.08033732877468615\n",
      "Stochastic Gradient Descent(28844): loss=0.32105614871064014\n",
      "Stochastic Gradient Descent(28845): loss=4.920310354785321\n",
      "Stochastic Gradient Descent(28846): loss=0.12644221578158438\n",
      "Stochastic Gradient Descent(28847): loss=0.9410940955475421\n",
      "Stochastic Gradient Descent(28848): loss=1.665729082803874\n",
      "Stochastic Gradient Descent(28849): loss=8.22918029570198\n",
      "Stochastic Gradient Descent(28850): loss=2.2333668556359094\n",
      "Stochastic Gradient Descent(28851): loss=11.423606473732953\n",
      "Stochastic Gradient Descent(28852): loss=0.1578003301344951\n",
      "Stochastic Gradient Descent(28853): loss=5.790935468434864\n",
      "Stochastic Gradient Descent(28854): loss=4.241312432014502\n",
      "Stochastic Gradient Descent(28855): loss=8.530166186315455e-06\n",
      "Stochastic Gradient Descent(28856): loss=21.85292853467553\n",
      "Stochastic Gradient Descent(28857): loss=10.379453724988045\n",
      "Stochastic Gradient Descent(28858): loss=0.9092045341046571\n",
      "Stochastic Gradient Descent(28859): loss=0.032891517632327914\n",
      "Stochastic Gradient Descent(28860): loss=0.12442142234211019\n",
      "Stochastic Gradient Descent(28861): loss=0.6951901562997185\n",
      "Stochastic Gradient Descent(28862): loss=0.20196785037237375\n",
      "Stochastic Gradient Descent(28863): loss=4.4529203521571095\n",
      "Stochastic Gradient Descent(28864): loss=1.0157535266816122\n",
      "Stochastic Gradient Descent(28865): loss=0.12440056485135335\n",
      "Stochastic Gradient Descent(28866): loss=0.14362539194231852\n",
      "Stochastic Gradient Descent(28867): loss=0.40509101752398047\n",
      "Stochastic Gradient Descent(28868): loss=1.844842284848741\n",
      "Stochastic Gradient Descent(28869): loss=7.482209784399622\n",
      "Stochastic Gradient Descent(28870): loss=0.6382560466735634\n",
      "Stochastic Gradient Descent(28871): loss=1.7163331989488375\n",
      "Stochastic Gradient Descent(28872): loss=2.9322427355656466\n",
      "Stochastic Gradient Descent(28873): loss=1.1402529634764487\n",
      "Stochastic Gradient Descent(28874): loss=3.3445824325628055\n",
      "Stochastic Gradient Descent(28875): loss=6.893915812098987\n",
      "Stochastic Gradient Descent(28876): loss=8.390050984918277e-07\n",
      "Stochastic Gradient Descent(28877): loss=2.786058242036496\n",
      "Stochastic Gradient Descent(28878): loss=8.95542678823806\n",
      "Stochastic Gradient Descent(28879): loss=1.4609412780261428\n",
      "Stochastic Gradient Descent(28880): loss=4.304856600138266\n",
      "Stochastic Gradient Descent(28881): loss=3.807677945521932\n",
      "Stochastic Gradient Descent(28882): loss=10.408676997662551\n",
      "Stochastic Gradient Descent(28883): loss=0.16817679928550627\n",
      "Stochastic Gradient Descent(28884): loss=0.23327434784235146\n",
      "Stochastic Gradient Descent(28885): loss=6.335848612193327\n",
      "Stochastic Gradient Descent(28886): loss=4.595866607652091\n",
      "Stochastic Gradient Descent(28887): loss=0.05234492361819624\n",
      "Stochastic Gradient Descent(28888): loss=2.2417902569373784\n",
      "Stochastic Gradient Descent(28889): loss=12.7863012010102\n",
      "Stochastic Gradient Descent(28890): loss=0.7459080723137492\n",
      "Stochastic Gradient Descent(28891): loss=2.5161125643697693\n",
      "Stochastic Gradient Descent(28892): loss=4.220324231741841\n",
      "Stochastic Gradient Descent(28893): loss=7.72767936658193\n",
      "Stochastic Gradient Descent(28894): loss=0.2524318838365565\n",
      "Stochastic Gradient Descent(28895): loss=0.023629196943565647\n",
      "Stochastic Gradient Descent(28896): loss=6.104148840854038\n",
      "Stochastic Gradient Descent(28897): loss=0.36886554799263627\n",
      "Stochastic Gradient Descent(28898): loss=3.786656033128715\n",
      "Stochastic Gradient Descent(28899): loss=2.698106231238557\n",
      "Stochastic Gradient Descent(28900): loss=0.8049613021098964\n",
      "Stochastic Gradient Descent(28901): loss=5.624545016961047\n",
      "Stochastic Gradient Descent(28902): loss=1.3022428418911358\n",
      "Stochastic Gradient Descent(28903): loss=8.575683791622945\n",
      "Stochastic Gradient Descent(28904): loss=17.47202065774127\n",
      "Stochastic Gradient Descent(28905): loss=1.8013273857888255\n",
      "Stochastic Gradient Descent(28906): loss=17.502048399535635\n",
      "Stochastic Gradient Descent(28907): loss=0.06104509585617223\n",
      "Stochastic Gradient Descent(28908): loss=0.9097755471799065\n",
      "Stochastic Gradient Descent(28909): loss=9.87424230636041\n",
      "Stochastic Gradient Descent(28910): loss=5.345870251611127\n",
      "Stochastic Gradient Descent(28911): loss=17.90000039166219\n",
      "Stochastic Gradient Descent(28912): loss=10.821932167178009\n",
      "Stochastic Gradient Descent(28913): loss=0.5978001268599511\n",
      "Stochastic Gradient Descent(28914): loss=2.0039018690633372\n",
      "Stochastic Gradient Descent(28915): loss=0.0029140386772299838\n",
      "Stochastic Gradient Descent(28916): loss=5.669575851409856\n",
      "Stochastic Gradient Descent(28917): loss=0.4208033329899497\n",
      "Stochastic Gradient Descent(28918): loss=3.5881320825653034\n",
      "Stochastic Gradient Descent(28919): loss=2.431509492350849\n",
      "Stochastic Gradient Descent(28920): loss=2.453988078515984\n",
      "Stochastic Gradient Descent(28921): loss=2.9459793845755735\n",
      "Stochastic Gradient Descent(28922): loss=17.385896043119516\n",
      "Stochastic Gradient Descent(28923): loss=1.8873873291425265\n",
      "Stochastic Gradient Descent(28924): loss=0.9202213861127754\n",
      "Stochastic Gradient Descent(28925): loss=7.383800867419286\n",
      "Stochastic Gradient Descent(28926): loss=15.688352409677131\n",
      "Stochastic Gradient Descent(28927): loss=25.615397210066657\n",
      "Stochastic Gradient Descent(28928): loss=8.030864042982225\n",
      "Stochastic Gradient Descent(28929): loss=1.2554096398483952\n",
      "Stochastic Gradient Descent(28930): loss=1.1929278713882985\n",
      "Stochastic Gradient Descent(28931): loss=2.4202406989172416\n",
      "Stochastic Gradient Descent(28932): loss=1.5241409865863347\n",
      "Stochastic Gradient Descent(28933): loss=10.807898850408465\n",
      "Stochastic Gradient Descent(28934): loss=4.92053010253404\n",
      "Stochastic Gradient Descent(28935): loss=0.025675300203162853\n",
      "Stochastic Gradient Descent(28936): loss=0.8049614745748054\n",
      "Stochastic Gradient Descent(28937): loss=7.4282289676500906\n",
      "Stochastic Gradient Descent(28938): loss=0.005510051075043922\n",
      "Stochastic Gradient Descent(28939): loss=0.18355429412545507\n",
      "Stochastic Gradient Descent(28940): loss=0.12936891776270892\n",
      "Stochastic Gradient Descent(28941): loss=0.000180576219533365\n",
      "Stochastic Gradient Descent(28942): loss=16.16131436205491\n",
      "Stochastic Gradient Descent(28943): loss=10.868812451872735\n",
      "Stochastic Gradient Descent(28944): loss=2.9608561782940095\n",
      "Stochastic Gradient Descent(28945): loss=0.31136388783470403\n",
      "Stochastic Gradient Descent(28946): loss=14.115682014853153\n",
      "Stochastic Gradient Descent(28947): loss=0.4916362441024461\n",
      "Stochastic Gradient Descent(28948): loss=0.17489508075677423\n",
      "Stochastic Gradient Descent(28949): loss=4.103575455665635e-05\n",
      "Stochastic Gradient Descent(28950): loss=2.5731074646650343\n",
      "Stochastic Gradient Descent(28951): loss=0.24772117181017694\n",
      "Stochastic Gradient Descent(28952): loss=1.7447086783638075\n",
      "Stochastic Gradient Descent(28953): loss=2.524352862132199\n",
      "Stochastic Gradient Descent(28954): loss=0.005003895370597181\n",
      "Stochastic Gradient Descent(28955): loss=0.0030198760326775567\n",
      "Stochastic Gradient Descent(28956): loss=4.059563501568219\n",
      "Stochastic Gradient Descent(28957): loss=1.449995978060914\n",
      "Stochastic Gradient Descent(28958): loss=0.3086752354066237\n",
      "Stochastic Gradient Descent(28959): loss=0.05171504743624152\n",
      "Stochastic Gradient Descent(28960): loss=0.4333491480047933\n",
      "Stochastic Gradient Descent(28961): loss=13.712212619064722\n",
      "Stochastic Gradient Descent(28962): loss=0.6848868626903788\n",
      "Stochastic Gradient Descent(28963): loss=24.8913810986323\n",
      "Stochastic Gradient Descent(28964): loss=5.801385151778878\n",
      "Stochastic Gradient Descent(28965): loss=5.940374287701157\n",
      "Stochastic Gradient Descent(28966): loss=0.0019334292683942037\n",
      "Stochastic Gradient Descent(28967): loss=0.9679753762157947\n",
      "Stochastic Gradient Descent(28968): loss=6.524854941864181\n",
      "Stochastic Gradient Descent(28969): loss=107.26849897371778\n",
      "Stochastic Gradient Descent(28970): loss=0.6148313373750262\n",
      "Stochastic Gradient Descent(28971): loss=3.120445378696175\n",
      "Stochastic Gradient Descent(28972): loss=12.323703265131309\n",
      "Stochastic Gradient Descent(28973): loss=0.045914506794491564\n",
      "Stochastic Gradient Descent(28974): loss=0.5264698953125304\n",
      "Stochastic Gradient Descent(28975): loss=0.5125219711662905\n",
      "Stochastic Gradient Descent(28976): loss=1.3049158757417076\n",
      "Stochastic Gradient Descent(28977): loss=0.09846000471133372\n",
      "Stochastic Gradient Descent(28978): loss=0.24302783911749085\n",
      "Stochastic Gradient Descent(28979): loss=7.618394871598179\n",
      "Stochastic Gradient Descent(28980): loss=1.6312696825262023\n",
      "Stochastic Gradient Descent(28981): loss=0.39030172730160473\n",
      "Stochastic Gradient Descent(28982): loss=3.537189381482392\n",
      "Stochastic Gradient Descent(28983): loss=12.058270605574625\n",
      "Stochastic Gradient Descent(28984): loss=0.3125350229228568\n",
      "Stochastic Gradient Descent(28985): loss=0.1632356347752346\n",
      "Stochastic Gradient Descent(28986): loss=5.0360040286202326\n",
      "Stochastic Gradient Descent(28987): loss=1.4392740411088836\n",
      "Stochastic Gradient Descent(28988): loss=2.2800267300042196\n",
      "Stochastic Gradient Descent(28989): loss=1.227710620705362\n",
      "Stochastic Gradient Descent(28990): loss=2.9121067264397156\n",
      "Stochastic Gradient Descent(28991): loss=0.37222311648493395\n",
      "Stochastic Gradient Descent(28992): loss=10.42994150776149\n",
      "Stochastic Gradient Descent(28993): loss=0.7253659253401652\n",
      "Stochastic Gradient Descent(28994): loss=0.2494546681869384\n",
      "Stochastic Gradient Descent(28995): loss=11.211840935579149\n",
      "Stochastic Gradient Descent(28996): loss=0.496566111939674\n",
      "Stochastic Gradient Descent(28997): loss=2.7884536627900633\n",
      "Stochastic Gradient Descent(28998): loss=1.2154035971472135\n",
      "Stochastic Gradient Descent(28999): loss=1.1536061991443274\n",
      "Stochastic Gradient Descent(29000): loss=0.06587830694511351\n",
      "Stochastic Gradient Descent(29001): loss=0.025240360400565626\n",
      "Stochastic Gradient Descent(29002): loss=2.6789657924203922\n",
      "Stochastic Gradient Descent(29003): loss=0.1338558577033351\n",
      "Stochastic Gradient Descent(29004): loss=0.2651540450917079\n",
      "Stochastic Gradient Descent(29005): loss=2.0658544715013636\n",
      "Stochastic Gradient Descent(29006): loss=10.51112177111015\n",
      "Stochastic Gradient Descent(29007): loss=3.6973516756087954\n",
      "Stochastic Gradient Descent(29008): loss=6.489347120515801\n",
      "Stochastic Gradient Descent(29009): loss=0.10611708965826024\n",
      "Stochastic Gradient Descent(29010): loss=2.45292908527872\n",
      "Stochastic Gradient Descent(29011): loss=0.06802325675136651\n",
      "Stochastic Gradient Descent(29012): loss=3.6603163782628076\n",
      "Stochastic Gradient Descent(29013): loss=3.3859320972775127\n",
      "Stochastic Gradient Descent(29014): loss=0.08064665660333427\n",
      "Stochastic Gradient Descent(29015): loss=0.24395465105964179\n",
      "Stochastic Gradient Descent(29016): loss=2.292703716338228\n",
      "Stochastic Gradient Descent(29017): loss=4.316426796735608\n",
      "Stochastic Gradient Descent(29018): loss=7.414273915923414\n",
      "Stochastic Gradient Descent(29019): loss=1.6360272520496417\n",
      "Stochastic Gradient Descent(29020): loss=0.4609363168889515\n",
      "Stochastic Gradient Descent(29021): loss=3.552254965997247\n",
      "Stochastic Gradient Descent(29022): loss=2.710902360261216\n",
      "Stochastic Gradient Descent(29023): loss=3.859519148004496\n",
      "Stochastic Gradient Descent(29024): loss=4.450395701297842\n",
      "Stochastic Gradient Descent(29025): loss=2.3901284779955265\n",
      "Stochastic Gradient Descent(29026): loss=0.06306574390649447\n",
      "Stochastic Gradient Descent(29027): loss=3.1005876834662356\n",
      "Stochastic Gradient Descent(29028): loss=0.4534903675117286\n",
      "Stochastic Gradient Descent(29029): loss=0.14049987200580677\n",
      "Stochastic Gradient Descent(29030): loss=1.8187378217954833\n",
      "Stochastic Gradient Descent(29031): loss=51.47061258013506\n",
      "Stochastic Gradient Descent(29032): loss=6.445743889347599\n",
      "Stochastic Gradient Descent(29033): loss=0.9082693177517072\n",
      "Stochastic Gradient Descent(29034): loss=0.20883135034232217\n",
      "Stochastic Gradient Descent(29035): loss=4.461426550411744e-05\n",
      "Stochastic Gradient Descent(29036): loss=0.002891885864098464\n",
      "Stochastic Gradient Descent(29037): loss=13.466917010696685\n",
      "Stochastic Gradient Descent(29038): loss=2.543122660241991\n",
      "Stochastic Gradient Descent(29039): loss=6.640343863943606\n",
      "Stochastic Gradient Descent(29040): loss=4.691435449108276\n",
      "Stochastic Gradient Descent(29041): loss=21.57471436986834\n",
      "Stochastic Gradient Descent(29042): loss=5.486082242428625\n",
      "Stochastic Gradient Descent(29043): loss=2.1035308463540727\n",
      "Stochastic Gradient Descent(29044): loss=0.048971092986830965\n",
      "Stochastic Gradient Descent(29045): loss=2.0658426219790513\n",
      "Stochastic Gradient Descent(29046): loss=0.918675275229076\n",
      "Stochastic Gradient Descent(29047): loss=7.09434261344937\n",
      "Stochastic Gradient Descent(29048): loss=13.721341626379814\n",
      "Stochastic Gradient Descent(29049): loss=0.01037608018507465\n",
      "Stochastic Gradient Descent(29050): loss=5.912290143626225\n",
      "Stochastic Gradient Descent(29051): loss=0.13440261980451365\n",
      "Stochastic Gradient Descent(29052): loss=4.547699783210473\n",
      "Stochastic Gradient Descent(29053): loss=9.512850709030888\n",
      "Stochastic Gradient Descent(29054): loss=0.205270300882091\n",
      "Stochastic Gradient Descent(29055): loss=5.131782178153274\n",
      "Stochastic Gradient Descent(29056): loss=1.543134288878253\n",
      "Stochastic Gradient Descent(29057): loss=3.636198391019274\n",
      "Stochastic Gradient Descent(29058): loss=5.681926796273021\n",
      "Stochastic Gradient Descent(29059): loss=0.009600045675227044\n",
      "Stochastic Gradient Descent(29060): loss=5.929338699710295\n",
      "Stochastic Gradient Descent(29061): loss=8.055536333375821\n",
      "Stochastic Gradient Descent(29062): loss=48.969226053065945\n",
      "Stochastic Gradient Descent(29063): loss=1.389583392862017\n",
      "Stochastic Gradient Descent(29064): loss=11.632715578504396\n",
      "Stochastic Gradient Descent(29065): loss=2.6442080641235988\n",
      "Stochastic Gradient Descent(29066): loss=13.192296835628582\n",
      "Stochastic Gradient Descent(29067): loss=63.17259197801321\n",
      "Stochastic Gradient Descent(29068): loss=1.0721002855869721\n",
      "Stochastic Gradient Descent(29069): loss=5.456813445952022\n",
      "Stochastic Gradient Descent(29070): loss=4.835392012018309\n",
      "Stochastic Gradient Descent(29071): loss=1.5444292487677989\n",
      "Stochastic Gradient Descent(29072): loss=0.14700333507208865\n",
      "Stochastic Gradient Descent(29073): loss=1.4587615740513666\n",
      "Stochastic Gradient Descent(29074): loss=0.04874459322223398\n",
      "Stochastic Gradient Descent(29075): loss=0.009336957067687845\n",
      "Stochastic Gradient Descent(29076): loss=0.021724168831716816\n",
      "Stochastic Gradient Descent(29077): loss=3.065341903405278\n",
      "Stochastic Gradient Descent(29078): loss=3.783129705432017\n",
      "Stochastic Gradient Descent(29079): loss=10.47314442617655\n",
      "Stochastic Gradient Descent(29080): loss=1.4956151809725442\n",
      "Stochastic Gradient Descent(29081): loss=0.15999827542382214\n",
      "Stochastic Gradient Descent(29082): loss=11.932721245145615\n",
      "Stochastic Gradient Descent(29083): loss=0.13858939257314187\n",
      "Stochastic Gradient Descent(29084): loss=6.058122644170688\n",
      "Stochastic Gradient Descent(29085): loss=5.7657355340172645\n",
      "Stochastic Gradient Descent(29086): loss=4.227285898106335\n",
      "Stochastic Gradient Descent(29087): loss=11.476240555391021\n",
      "Stochastic Gradient Descent(29088): loss=0.24820116361114486\n",
      "Stochastic Gradient Descent(29089): loss=0.15145685152757\n",
      "Stochastic Gradient Descent(29090): loss=0.5124027829935051\n",
      "Stochastic Gradient Descent(29091): loss=5.905973541571732\n",
      "Stochastic Gradient Descent(29092): loss=0.440046062023975\n",
      "Stochastic Gradient Descent(29093): loss=9.188660862535471\n",
      "Stochastic Gradient Descent(29094): loss=0.9567184656015042\n",
      "Stochastic Gradient Descent(29095): loss=2.049639756756942\n",
      "Stochastic Gradient Descent(29096): loss=0.3131209693922818\n",
      "Stochastic Gradient Descent(29097): loss=34.60731310008033\n",
      "Stochastic Gradient Descent(29098): loss=21.027559092037187\n",
      "Stochastic Gradient Descent(29099): loss=4.546442678701443\n",
      "Stochastic Gradient Descent(29100): loss=1.8662812878512185\n",
      "Stochastic Gradient Descent(29101): loss=9.300237970263803\n",
      "Stochastic Gradient Descent(29102): loss=0.9109592523464702\n",
      "Stochastic Gradient Descent(29103): loss=0.0635739981069978\n",
      "Stochastic Gradient Descent(29104): loss=1.1453630636045355\n",
      "Stochastic Gradient Descent(29105): loss=2.2106393890150544\n",
      "Stochastic Gradient Descent(29106): loss=12.406531457786459\n",
      "Stochastic Gradient Descent(29107): loss=0.31605296971715313\n",
      "Stochastic Gradient Descent(29108): loss=6.458447967197509\n",
      "Stochastic Gradient Descent(29109): loss=0.1149162625001052\n",
      "Stochastic Gradient Descent(29110): loss=22.67460425464742\n",
      "Stochastic Gradient Descent(29111): loss=10.97934571929624\n",
      "Stochastic Gradient Descent(29112): loss=6.933010972495263\n",
      "Stochastic Gradient Descent(29113): loss=0.07707099024085767\n",
      "Stochastic Gradient Descent(29114): loss=0.29313704674790575\n",
      "Stochastic Gradient Descent(29115): loss=3.071782272320103\n",
      "Stochastic Gradient Descent(29116): loss=47.76303710202387\n",
      "Stochastic Gradient Descent(29117): loss=1.2358804643010122\n",
      "Stochastic Gradient Descent(29118): loss=9.644639859056923\n",
      "Stochastic Gradient Descent(29119): loss=0.0903632230081436\n",
      "Stochastic Gradient Descent(29120): loss=21.79909257711917\n",
      "Stochastic Gradient Descent(29121): loss=1.2819824706145448\n",
      "Stochastic Gradient Descent(29122): loss=5.148345282309696\n",
      "Stochastic Gradient Descent(29123): loss=1.4816536960352378\n",
      "Stochastic Gradient Descent(29124): loss=1.307083891477744\n",
      "Stochastic Gradient Descent(29125): loss=0.0013963673568981657\n",
      "Stochastic Gradient Descent(29126): loss=1.449983158535734\n",
      "Stochastic Gradient Descent(29127): loss=1.4273681504384905\n",
      "Stochastic Gradient Descent(29128): loss=7.665127867333462\n",
      "Stochastic Gradient Descent(29129): loss=0.770671788950858\n",
      "Stochastic Gradient Descent(29130): loss=0.27070716535826256\n",
      "Stochastic Gradient Descent(29131): loss=1.8079833084850143\n",
      "Stochastic Gradient Descent(29132): loss=3.7154435922066136\n",
      "Stochastic Gradient Descent(29133): loss=0.08791717936614012\n",
      "Stochastic Gradient Descent(29134): loss=2.0595689647827355\n",
      "Stochastic Gradient Descent(29135): loss=0.1569354464067382\n",
      "Stochastic Gradient Descent(29136): loss=3.9012553919064947\n",
      "Stochastic Gradient Descent(29137): loss=1.3473976420230418\n",
      "Stochastic Gradient Descent(29138): loss=12.178300062325299\n",
      "Stochastic Gradient Descent(29139): loss=3.359143867081032\n",
      "Stochastic Gradient Descent(29140): loss=0.6997216893031812\n",
      "Stochastic Gradient Descent(29141): loss=2.829169334700532\n",
      "Stochastic Gradient Descent(29142): loss=0.07154891205808572\n",
      "Stochastic Gradient Descent(29143): loss=6.8423018158714894\n",
      "Stochastic Gradient Descent(29144): loss=28.27109485694797\n",
      "Stochastic Gradient Descent(29145): loss=1.8793719795370558\n",
      "Stochastic Gradient Descent(29146): loss=1.5290725370807148\n",
      "Stochastic Gradient Descent(29147): loss=4.335047639075237\n",
      "Stochastic Gradient Descent(29148): loss=0.8038961190997278\n",
      "Stochastic Gradient Descent(29149): loss=1.8794331940221305\n",
      "Stochastic Gradient Descent(29150): loss=14.558461645667748\n",
      "Stochastic Gradient Descent(29151): loss=0.23711472539379314\n",
      "Stochastic Gradient Descent(29152): loss=3.720913670334706\n",
      "Stochastic Gradient Descent(29153): loss=2.1522229984828787\n",
      "Stochastic Gradient Descent(29154): loss=14.606158180449066\n",
      "Stochastic Gradient Descent(29155): loss=6.059992467424176\n",
      "Stochastic Gradient Descent(29156): loss=0.1602321402754524\n",
      "Stochastic Gradient Descent(29157): loss=2.528772024163129\n",
      "Stochastic Gradient Descent(29158): loss=4.167408700269513\n",
      "Stochastic Gradient Descent(29159): loss=0.0011063467928793555\n",
      "Stochastic Gradient Descent(29160): loss=0.4817286733193057\n",
      "Stochastic Gradient Descent(29161): loss=0.567978930410304\n",
      "Stochastic Gradient Descent(29162): loss=0.3883929571055323\n",
      "Stochastic Gradient Descent(29163): loss=0.11494759467214853\n",
      "Stochastic Gradient Descent(29164): loss=0.14988552573286926\n",
      "Stochastic Gradient Descent(29165): loss=6.304165242222348\n",
      "Stochastic Gradient Descent(29166): loss=0.0009299816930774056\n",
      "Stochastic Gradient Descent(29167): loss=0.10925447009033586\n",
      "Stochastic Gradient Descent(29168): loss=0.5754279496562466\n",
      "Stochastic Gradient Descent(29169): loss=6.829530996448582\n",
      "Stochastic Gradient Descent(29170): loss=21.373738253985074\n",
      "Stochastic Gradient Descent(29171): loss=2.2721289493729966\n",
      "Stochastic Gradient Descent(29172): loss=6.125144445157052\n",
      "Stochastic Gradient Descent(29173): loss=23.339726269521645\n",
      "Stochastic Gradient Descent(29174): loss=0.267859181354128\n",
      "Stochastic Gradient Descent(29175): loss=1.8254167624140758\n",
      "Stochastic Gradient Descent(29176): loss=0.5159025653589988\n",
      "Stochastic Gradient Descent(29177): loss=0.17946567554447318\n",
      "Stochastic Gradient Descent(29178): loss=3.88227401456869\n",
      "Stochastic Gradient Descent(29179): loss=0.6044658671958183\n",
      "Stochastic Gradient Descent(29180): loss=1.7533487909255268\n",
      "Stochastic Gradient Descent(29181): loss=6.984845908756766\n",
      "Stochastic Gradient Descent(29182): loss=0.035663328068661036\n",
      "Stochastic Gradient Descent(29183): loss=0.11061892083688181\n",
      "Stochastic Gradient Descent(29184): loss=6.2857171450916764\n",
      "Stochastic Gradient Descent(29185): loss=0.093695352642494\n",
      "Stochastic Gradient Descent(29186): loss=0.04302997668054213\n",
      "Stochastic Gradient Descent(29187): loss=0.36605775502722765\n",
      "Stochastic Gradient Descent(29188): loss=0.5095832431536558\n",
      "Stochastic Gradient Descent(29189): loss=0.19855591888264074\n",
      "Stochastic Gradient Descent(29190): loss=1.596653154799049\n",
      "Stochastic Gradient Descent(29191): loss=3.503007563352804\n",
      "Stochastic Gradient Descent(29192): loss=1.2241386892273012\n",
      "Stochastic Gradient Descent(29193): loss=0.2893340330571709\n",
      "Stochastic Gradient Descent(29194): loss=8.185126569033322\n",
      "Stochastic Gradient Descent(29195): loss=4.082051374978496\n",
      "Stochastic Gradient Descent(29196): loss=14.992887351851445\n",
      "Stochastic Gradient Descent(29197): loss=2.6827583985876293\n",
      "Stochastic Gradient Descent(29198): loss=0.2192917231812419\n",
      "Stochastic Gradient Descent(29199): loss=1.2352951490438395\n",
      "Stochastic Gradient Descent(29200): loss=1.4146070124763928\n",
      "Stochastic Gradient Descent(29201): loss=2.168938245112966\n",
      "Stochastic Gradient Descent(29202): loss=1.7074798665971977\n",
      "Stochastic Gradient Descent(29203): loss=0.13418645839240714\n",
      "Stochastic Gradient Descent(29204): loss=0.24383807060109108\n",
      "Stochastic Gradient Descent(29205): loss=27.963603688109295\n",
      "Stochastic Gradient Descent(29206): loss=0.0005314222504609801\n",
      "Stochastic Gradient Descent(29207): loss=2.975126815465476\n",
      "Stochastic Gradient Descent(29208): loss=0.6833949585812485\n",
      "Stochastic Gradient Descent(29209): loss=0.4895878479483493\n",
      "Stochastic Gradient Descent(29210): loss=30.35593365248334\n",
      "Stochastic Gradient Descent(29211): loss=5.844497941554283\n",
      "Stochastic Gradient Descent(29212): loss=7.201454091502291\n",
      "Stochastic Gradient Descent(29213): loss=3.8211679249422024\n",
      "Stochastic Gradient Descent(29214): loss=27.14368992303111\n",
      "Stochastic Gradient Descent(29215): loss=15.508970083514871\n",
      "Stochastic Gradient Descent(29216): loss=4.015934661727756\n",
      "Stochastic Gradient Descent(29217): loss=4.743164113594228\n",
      "Stochastic Gradient Descent(29218): loss=1.7694898479705108\n",
      "Stochastic Gradient Descent(29219): loss=12.649864533507065\n",
      "Stochastic Gradient Descent(29220): loss=3.9195671890450967\n",
      "Stochastic Gradient Descent(29221): loss=5.910830843212106\n",
      "Stochastic Gradient Descent(29222): loss=1.0116003978022372\n",
      "Stochastic Gradient Descent(29223): loss=2.061147971186886\n",
      "Stochastic Gradient Descent(29224): loss=0.6789129656456634\n",
      "Stochastic Gradient Descent(29225): loss=0.0031389727751768583\n",
      "Stochastic Gradient Descent(29226): loss=0.5559089721514981\n",
      "Stochastic Gradient Descent(29227): loss=6.1114427519984975\n",
      "Stochastic Gradient Descent(29228): loss=1.824585748417397\n",
      "Stochastic Gradient Descent(29229): loss=0.4489448809152512\n",
      "Stochastic Gradient Descent(29230): loss=0.015815834256189806\n",
      "Stochastic Gradient Descent(29231): loss=23.984693176616393\n",
      "Stochastic Gradient Descent(29232): loss=15.61504034271497\n",
      "Stochastic Gradient Descent(29233): loss=1.7953631711218323\n",
      "Stochastic Gradient Descent(29234): loss=0.00012260718298685768\n",
      "Stochastic Gradient Descent(29235): loss=17.52590658836753\n",
      "Stochastic Gradient Descent(29236): loss=2.86166041118173\n",
      "Stochastic Gradient Descent(29237): loss=3.5546801576279763\n",
      "Stochastic Gradient Descent(29238): loss=6.758148122424665\n",
      "Stochastic Gradient Descent(29239): loss=9.489160525473157\n",
      "Stochastic Gradient Descent(29240): loss=1.1716726534292536\n",
      "Stochastic Gradient Descent(29241): loss=3.70330758912255\n",
      "Stochastic Gradient Descent(29242): loss=9.212923074765948\n",
      "Stochastic Gradient Descent(29243): loss=11.713185958590078\n",
      "Stochastic Gradient Descent(29244): loss=0.00911777766581452\n",
      "Stochastic Gradient Descent(29245): loss=7.259967429527938\n",
      "Stochastic Gradient Descent(29246): loss=4.172076841932836\n",
      "Stochastic Gradient Descent(29247): loss=1.696122921804135\n",
      "Stochastic Gradient Descent(29248): loss=1.7865977430509947\n",
      "Stochastic Gradient Descent(29249): loss=5.7711766743890935\n",
      "Stochastic Gradient Descent(29250): loss=0.8901550287169506\n",
      "Stochastic Gradient Descent(29251): loss=10.42565330743591\n",
      "Stochastic Gradient Descent(29252): loss=6.088396979488404\n",
      "Stochastic Gradient Descent(29253): loss=7.8421503028226764\n",
      "Stochastic Gradient Descent(29254): loss=3.9234284862309665\n",
      "Stochastic Gradient Descent(29255): loss=0.5342518250296037\n",
      "Stochastic Gradient Descent(29256): loss=0.7629191144206253\n",
      "Stochastic Gradient Descent(29257): loss=0.9486103884466991\n",
      "Stochastic Gradient Descent(29258): loss=0.3406419110429354\n",
      "Stochastic Gradient Descent(29259): loss=6.890040141993168\n",
      "Stochastic Gradient Descent(29260): loss=24.04548662102803\n",
      "Stochastic Gradient Descent(29261): loss=1.454618232729827\n",
      "Stochastic Gradient Descent(29262): loss=1.0479265480859774\n",
      "Stochastic Gradient Descent(29263): loss=3.1687241140541293\n",
      "Stochastic Gradient Descent(29264): loss=26.469898198642635\n",
      "Stochastic Gradient Descent(29265): loss=42.78739936470323\n",
      "Stochastic Gradient Descent(29266): loss=5.829568256027337\n",
      "Stochastic Gradient Descent(29267): loss=8.298820167565067\n",
      "Stochastic Gradient Descent(29268): loss=0.12535912103304303\n",
      "Stochastic Gradient Descent(29269): loss=6.150074983627646\n",
      "Stochastic Gradient Descent(29270): loss=8.834445228197978\n",
      "Stochastic Gradient Descent(29271): loss=4.65729467518515\n",
      "Stochastic Gradient Descent(29272): loss=3.453644550741481\n",
      "Stochastic Gradient Descent(29273): loss=15.066750861542157\n",
      "Stochastic Gradient Descent(29274): loss=2.4421014329396757\n",
      "Stochastic Gradient Descent(29275): loss=16.631708073048717\n",
      "Stochastic Gradient Descent(29276): loss=3.6368158681732305\n",
      "Stochastic Gradient Descent(29277): loss=0.3240402693343942\n",
      "Stochastic Gradient Descent(29278): loss=0.003928556608056035\n",
      "Stochastic Gradient Descent(29279): loss=0.20397733795980322\n",
      "Stochastic Gradient Descent(29280): loss=0.8858008858395635\n",
      "Stochastic Gradient Descent(29281): loss=9.998763359740833\n",
      "Stochastic Gradient Descent(29282): loss=6.485773471824571\n",
      "Stochastic Gradient Descent(29283): loss=0.15832535337444686\n",
      "Stochastic Gradient Descent(29284): loss=0.29260491286841706\n",
      "Stochastic Gradient Descent(29285): loss=14.902275811522902\n",
      "Stochastic Gradient Descent(29286): loss=9.195241613143947\n",
      "Stochastic Gradient Descent(29287): loss=10.622223428756246\n",
      "Stochastic Gradient Descent(29288): loss=0.393261225371733\n",
      "Stochastic Gradient Descent(29289): loss=2.547442330951959\n",
      "Stochastic Gradient Descent(29290): loss=0.006670964718425642\n",
      "Stochastic Gradient Descent(29291): loss=6.894884409757428\n",
      "Stochastic Gradient Descent(29292): loss=20.24264561835736\n",
      "Stochastic Gradient Descent(29293): loss=10.680183623258298\n",
      "Stochastic Gradient Descent(29294): loss=7.05668933541438\n",
      "Stochastic Gradient Descent(29295): loss=0.33994230470447934\n",
      "Stochastic Gradient Descent(29296): loss=7.784752625037475\n",
      "Stochastic Gradient Descent(29297): loss=0.14062117723707895\n",
      "Stochastic Gradient Descent(29298): loss=0.2275315695108124\n",
      "Stochastic Gradient Descent(29299): loss=4.851432723010158\n",
      "Stochastic Gradient Descent(29300): loss=0.657354014618951\n",
      "Stochastic Gradient Descent(29301): loss=2.927068815948551\n",
      "Stochastic Gradient Descent(29302): loss=7.662113631294208\n",
      "Stochastic Gradient Descent(29303): loss=4.1065360982360986\n",
      "Stochastic Gradient Descent(29304): loss=44.92661391696138\n",
      "Stochastic Gradient Descent(29305): loss=14.76277458066614\n",
      "Stochastic Gradient Descent(29306): loss=0.013315648805077566\n",
      "Stochastic Gradient Descent(29307): loss=2.592323350690938\n",
      "Stochastic Gradient Descent(29308): loss=0.37136807511092784\n",
      "Stochastic Gradient Descent(29309): loss=4.856496586184522\n",
      "Stochastic Gradient Descent(29310): loss=3.7465869097836384\n",
      "Stochastic Gradient Descent(29311): loss=0.0031323150436404767\n",
      "Stochastic Gradient Descent(29312): loss=1.2481712752554528\n",
      "Stochastic Gradient Descent(29313): loss=0.6098765388047593\n",
      "Stochastic Gradient Descent(29314): loss=1.7092704147907334\n",
      "Stochastic Gradient Descent(29315): loss=7.273008831336094\n",
      "Stochastic Gradient Descent(29316): loss=3.8842624504508843\n",
      "Stochastic Gradient Descent(29317): loss=7.185991061727628\n",
      "Stochastic Gradient Descent(29318): loss=36.66073613138543\n",
      "Stochastic Gradient Descent(29319): loss=0.6867127439791743\n",
      "Stochastic Gradient Descent(29320): loss=2.4666896636040683\n",
      "Stochastic Gradient Descent(29321): loss=7.012837290535168\n",
      "Stochastic Gradient Descent(29322): loss=5.1286837377383\n",
      "Stochastic Gradient Descent(29323): loss=35.84974188763793\n",
      "Stochastic Gradient Descent(29324): loss=0.8554649731347606\n",
      "Stochastic Gradient Descent(29325): loss=2.8711791455745375\n",
      "Stochastic Gradient Descent(29326): loss=24.12522985870441\n",
      "Stochastic Gradient Descent(29327): loss=1.7762941970984942\n",
      "Stochastic Gradient Descent(29328): loss=25.378272075965192\n",
      "Stochastic Gradient Descent(29329): loss=2.146662405885488\n",
      "Stochastic Gradient Descent(29330): loss=21.0165882630705\n",
      "Stochastic Gradient Descent(29331): loss=2.5005883674471465\n",
      "Stochastic Gradient Descent(29332): loss=1.116220742510953\n",
      "Stochastic Gradient Descent(29333): loss=8.742619587735739\n",
      "Stochastic Gradient Descent(29334): loss=0.3636530689161425\n",
      "Stochastic Gradient Descent(29335): loss=2.950314226260905\n",
      "Stochastic Gradient Descent(29336): loss=4.138750567181483\n",
      "Stochastic Gradient Descent(29337): loss=0.628562008541251\n",
      "Stochastic Gradient Descent(29338): loss=0.14710529198606076\n",
      "Stochastic Gradient Descent(29339): loss=2.5532978171662952\n",
      "Stochastic Gradient Descent(29340): loss=0.0009039132781653026\n",
      "Stochastic Gradient Descent(29341): loss=0.5828477727164415\n",
      "Stochastic Gradient Descent(29342): loss=14.506540517754194\n",
      "Stochastic Gradient Descent(29343): loss=0.3629839255333966\n",
      "Stochastic Gradient Descent(29344): loss=1.18524619713671\n",
      "Stochastic Gradient Descent(29345): loss=5.9742254961820755\n",
      "Stochastic Gradient Descent(29346): loss=3.9934716926692064\n",
      "Stochastic Gradient Descent(29347): loss=0.03902225065173954\n",
      "Stochastic Gradient Descent(29348): loss=0.34638995520966415\n",
      "Stochastic Gradient Descent(29349): loss=10.775828612932925\n",
      "Stochastic Gradient Descent(29350): loss=2.0745361279780643\n",
      "Stochastic Gradient Descent(29351): loss=0.4558454714902914\n",
      "Stochastic Gradient Descent(29352): loss=1.44259284223643\n",
      "Stochastic Gradient Descent(29353): loss=0.3736446910661231\n",
      "Stochastic Gradient Descent(29354): loss=0.0030381702173060006\n",
      "Stochastic Gradient Descent(29355): loss=5.6510657339469255\n",
      "Stochastic Gradient Descent(29356): loss=0.40659159515961507\n",
      "Stochastic Gradient Descent(29357): loss=0.018443852839906846\n",
      "Stochastic Gradient Descent(29358): loss=2.232739465026457\n",
      "Stochastic Gradient Descent(29359): loss=3.5682964979480074\n",
      "Stochastic Gradient Descent(29360): loss=0.1020538344044867\n",
      "Stochastic Gradient Descent(29361): loss=3.7402776795823676\n",
      "Stochastic Gradient Descent(29362): loss=1.258634916082639\n",
      "Stochastic Gradient Descent(29363): loss=0.32491119030292004\n",
      "Stochastic Gradient Descent(29364): loss=5.116019829569499\n",
      "Stochastic Gradient Descent(29365): loss=4.226048831083687\n",
      "Stochastic Gradient Descent(29366): loss=0.13341279038815798\n",
      "Stochastic Gradient Descent(29367): loss=0.9012579784703759\n",
      "Stochastic Gradient Descent(29368): loss=0.38981634577746527\n",
      "Stochastic Gradient Descent(29369): loss=0.5251155000200487\n",
      "Stochastic Gradient Descent(29370): loss=0.5855714181148819\n",
      "Stochastic Gradient Descent(29371): loss=0.31465444732004144\n",
      "Stochastic Gradient Descent(29372): loss=2.7420542494721536\n",
      "Stochastic Gradient Descent(29373): loss=8.706361649160662\n",
      "Stochastic Gradient Descent(29374): loss=0.001844733737688947\n",
      "Stochastic Gradient Descent(29375): loss=6.338990267807602\n",
      "Stochastic Gradient Descent(29376): loss=0.00022239257127278157\n",
      "Stochastic Gradient Descent(29377): loss=0.3191986107013383\n",
      "Stochastic Gradient Descent(29378): loss=3.0760086273947977\n",
      "Stochastic Gradient Descent(29379): loss=0.4129696627428459\n",
      "Stochastic Gradient Descent(29380): loss=0.00838834665496837\n",
      "Stochastic Gradient Descent(29381): loss=0.47630273079959556\n",
      "Stochastic Gradient Descent(29382): loss=0.3206602285210602\n",
      "Stochastic Gradient Descent(29383): loss=3.4033476755265215\n",
      "Stochastic Gradient Descent(29384): loss=0.3208113887998105\n",
      "Stochastic Gradient Descent(29385): loss=5.523784538918801\n",
      "Stochastic Gradient Descent(29386): loss=1.6793762403987864\n",
      "Stochastic Gradient Descent(29387): loss=0.14559700338358053\n",
      "Stochastic Gradient Descent(29388): loss=0.029685700895651063\n",
      "Stochastic Gradient Descent(29389): loss=0.03823392629280271\n",
      "Stochastic Gradient Descent(29390): loss=5.0529085576907065\n",
      "Stochastic Gradient Descent(29391): loss=0.5773558347122649\n",
      "Stochastic Gradient Descent(29392): loss=3.4880786824871786\n",
      "Stochastic Gradient Descent(29393): loss=1.2164632112310005\n",
      "Stochastic Gradient Descent(29394): loss=1.3374884563952307\n",
      "Stochastic Gradient Descent(29395): loss=2.7995662304198263\n",
      "Stochastic Gradient Descent(29396): loss=2.942203705383111\n",
      "Stochastic Gradient Descent(29397): loss=15.312963702825234\n",
      "Stochastic Gradient Descent(29398): loss=5.762728569533547\n",
      "Stochastic Gradient Descent(29399): loss=0.0045578419388476255\n",
      "Stochastic Gradient Descent(29400): loss=15.611250036821714\n",
      "Stochastic Gradient Descent(29401): loss=4.152398004535429\n",
      "Stochastic Gradient Descent(29402): loss=0.01824496765837642\n",
      "Stochastic Gradient Descent(29403): loss=7.285628021758873\n",
      "Stochastic Gradient Descent(29404): loss=1.0023882419545682\n",
      "Stochastic Gradient Descent(29405): loss=0.6477873009465196\n",
      "Stochastic Gradient Descent(29406): loss=0.23330953710363797\n",
      "Stochastic Gradient Descent(29407): loss=10.532778543196343\n",
      "Stochastic Gradient Descent(29408): loss=0.20434242666194236\n",
      "Stochastic Gradient Descent(29409): loss=1.0969121766990078\n",
      "Stochastic Gradient Descent(29410): loss=7.042936112962506\n",
      "Stochastic Gradient Descent(29411): loss=0.24815928333950815\n",
      "Stochastic Gradient Descent(29412): loss=7.370160157206262\n",
      "Stochastic Gradient Descent(29413): loss=0.24783226312120976\n",
      "Stochastic Gradient Descent(29414): loss=0.017196304482050957\n",
      "Stochastic Gradient Descent(29415): loss=1.4904209690711638\n",
      "Stochastic Gradient Descent(29416): loss=0.23986173243387326\n",
      "Stochastic Gradient Descent(29417): loss=0.09676510444749967\n",
      "Stochastic Gradient Descent(29418): loss=2.4773778153461836\n",
      "Stochastic Gradient Descent(29419): loss=0.2532455153107729\n",
      "Stochastic Gradient Descent(29420): loss=1.3463064763778616\n",
      "Stochastic Gradient Descent(29421): loss=0.038871148991841804\n",
      "Stochastic Gradient Descent(29422): loss=2.1588520595126157\n",
      "Stochastic Gradient Descent(29423): loss=5.674850078272524\n",
      "Stochastic Gradient Descent(29424): loss=5.421905374064606\n",
      "Stochastic Gradient Descent(29425): loss=0.003764764987614171\n",
      "Stochastic Gradient Descent(29426): loss=1.6913996591791276\n",
      "Stochastic Gradient Descent(29427): loss=11.232582821875711\n",
      "Stochastic Gradient Descent(29428): loss=0.6770434501466517\n",
      "Stochastic Gradient Descent(29429): loss=1.1248195627204274\n",
      "Stochastic Gradient Descent(29430): loss=2.5459399079442337\n",
      "Stochastic Gradient Descent(29431): loss=5.742489151013179\n",
      "Stochastic Gradient Descent(29432): loss=2.402029666432896\n",
      "Stochastic Gradient Descent(29433): loss=2.820539710438464e-05\n",
      "Stochastic Gradient Descent(29434): loss=4.74332057283224\n",
      "Stochastic Gradient Descent(29435): loss=5.417767670031042\n",
      "Stochastic Gradient Descent(29436): loss=5.403072252041949\n",
      "Stochastic Gradient Descent(29437): loss=3.2559703149252646\n",
      "Stochastic Gradient Descent(29438): loss=11.340975273605236\n",
      "Stochastic Gradient Descent(29439): loss=1.021786650148258\n",
      "Stochastic Gradient Descent(29440): loss=0.09645590185316147\n",
      "Stochastic Gradient Descent(29441): loss=1.4008024080280301\n",
      "Stochastic Gradient Descent(29442): loss=6.992332697596159\n",
      "Stochastic Gradient Descent(29443): loss=0.921662214934056\n",
      "Stochastic Gradient Descent(29444): loss=2.7127687808327057\n",
      "Stochastic Gradient Descent(29445): loss=7.551544585741324\n",
      "Stochastic Gradient Descent(29446): loss=2.8267860996199325\n",
      "Stochastic Gradient Descent(29447): loss=1.0560086198732863\n",
      "Stochastic Gradient Descent(29448): loss=8.050810962874614\n",
      "Stochastic Gradient Descent(29449): loss=3.6318139066504824\n",
      "Stochastic Gradient Descent(29450): loss=1.7475985013210031\n",
      "Stochastic Gradient Descent(29451): loss=0.16146314359023994\n",
      "Stochastic Gradient Descent(29452): loss=5.602672303882689\n",
      "Stochastic Gradient Descent(29453): loss=5.170925114796729\n",
      "Stochastic Gradient Descent(29454): loss=2.7119028875090745\n",
      "Stochastic Gradient Descent(29455): loss=0.9616502923674722\n",
      "Stochastic Gradient Descent(29456): loss=0.3330005511351552\n",
      "Stochastic Gradient Descent(29457): loss=4.928041612794014\n",
      "Stochastic Gradient Descent(29458): loss=0.6148531702557039\n",
      "Stochastic Gradient Descent(29459): loss=13.586741762247259\n",
      "Stochastic Gradient Descent(29460): loss=7.204744679869517\n",
      "Stochastic Gradient Descent(29461): loss=1.2679337999257845\n",
      "Stochastic Gradient Descent(29462): loss=3.23201283042274\n",
      "Stochastic Gradient Descent(29463): loss=1.018923450258611\n",
      "Stochastic Gradient Descent(29464): loss=5.080111321395341\n",
      "Stochastic Gradient Descent(29465): loss=2.5082426977507137\n",
      "Stochastic Gradient Descent(29466): loss=0.020023334217902446\n",
      "Stochastic Gradient Descent(29467): loss=1.8897569082344587\n",
      "Stochastic Gradient Descent(29468): loss=10.912406402497632\n",
      "Stochastic Gradient Descent(29469): loss=61.65472150954312\n",
      "Stochastic Gradient Descent(29470): loss=20.60574616296014\n",
      "Stochastic Gradient Descent(29471): loss=110.33811276740506\n",
      "Stochastic Gradient Descent(29472): loss=3.0405748867734506\n",
      "Stochastic Gradient Descent(29473): loss=8.888788306439336\n",
      "Stochastic Gradient Descent(29474): loss=7.6159966715179\n",
      "Stochastic Gradient Descent(29475): loss=1.030226446643189\n",
      "Stochastic Gradient Descent(29476): loss=0.33578521020334856\n",
      "Stochastic Gradient Descent(29477): loss=0.0826706280339146\n",
      "Stochastic Gradient Descent(29478): loss=5.235721859950364\n",
      "Stochastic Gradient Descent(29479): loss=0.09359814543377919\n",
      "Stochastic Gradient Descent(29480): loss=6.560054022906493\n",
      "Stochastic Gradient Descent(29481): loss=1.8328105948947926\n",
      "Stochastic Gradient Descent(29482): loss=14.735964106088717\n",
      "Stochastic Gradient Descent(29483): loss=5.655346707285613\n",
      "Stochastic Gradient Descent(29484): loss=9.374759278078292\n",
      "Stochastic Gradient Descent(29485): loss=7.970637851576479\n",
      "Stochastic Gradient Descent(29486): loss=1.3935294539195229\n",
      "Stochastic Gradient Descent(29487): loss=1.5900654206118123\n",
      "Stochastic Gradient Descent(29488): loss=0.7383679645087527\n",
      "Stochastic Gradient Descent(29489): loss=8.481119875624676\n",
      "Stochastic Gradient Descent(29490): loss=0.356015452632456\n",
      "Stochastic Gradient Descent(29491): loss=0.8712352335966469\n",
      "Stochastic Gradient Descent(29492): loss=0.18178306338882388\n",
      "Stochastic Gradient Descent(29493): loss=3.9126062798467673\n",
      "Stochastic Gradient Descent(29494): loss=14.450960679028647\n",
      "Stochastic Gradient Descent(29495): loss=0.08138755317080766\n",
      "Stochastic Gradient Descent(29496): loss=1.8333108605761494\n",
      "Stochastic Gradient Descent(29497): loss=0.10103337366704804\n",
      "Stochastic Gradient Descent(29498): loss=3.2083621573372665\n",
      "Stochastic Gradient Descent(29499): loss=1.2044249855116964\n",
      "Stochastic Gradient Descent(29500): loss=0.004472099074663403\n",
      "Stochastic Gradient Descent(29501): loss=0.5876285712649535\n",
      "Stochastic Gradient Descent(29502): loss=2.539959534847725\n",
      "Stochastic Gradient Descent(29503): loss=1.6419167249902393\n",
      "Stochastic Gradient Descent(29504): loss=0.0884668525513485\n",
      "Stochastic Gradient Descent(29505): loss=45.07441269947486\n",
      "Stochastic Gradient Descent(29506): loss=4.96316435991962\n",
      "Stochastic Gradient Descent(29507): loss=5.796841320594496e-06\n",
      "Stochastic Gradient Descent(29508): loss=0.16804612058796503\n",
      "Stochastic Gradient Descent(29509): loss=0.3989612282652405\n",
      "Stochastic Gradient Descent(29510): loss=4.464906548242608\n",
      "Stochastic Gradient Descent(29511): loss=81.08968225137133\n",
      "Stochastic Gradient Descent(29512): loss=86.1225065872824\n",
      "Stochastic Gradient Descent(29513): loss=4.7312559005965715\n",
      "Stochastic Gradient Descent(29514): loss=1.436393172162778\n",
      "Stochastic Gradient Descent(29515): loss=10.302294211533678\n",
      "Stochastic Gradient Descent(29516): loss=4.117994915803784\n",
      "Stochastic Gradient Descent(29517): loss=5.276439692531763\n",
      "Stochastic Gradient Descent(29518): loss=0.014640470580772537\n",
      "Stochastic Gradient Descent(29519): loss=0.9467393673292851\n",
      "Stochastic Gradient Descent(29520): loss=22.92716744119226\n",
      "Stochastic Gradient Descent(29521): loss=1.0565288117425422\n",
      "Stochastic Gradient Descent(29522): loss=2.2384992032260955\n",
      "Stochastic Gradient Descent(29523): loss=0.10821518501446178\n",
      "Stochastic Gradient Descent(29524): loss=4.759679373348385\n",
      "Stochastic Gradient Descent(29525): loss=2.193596771095978\n",
      "Stochastic Gradient Descent(29526): loss=5.903752781324483\n",
      "Stochastic Gradient Descent(29527): loss=4.172860606592859\n",
      "Stochastic Gradient Descent(29528): loss=0.41140360570068285\n",
      "Stochastic Gradient Descent(29529): loss=12.87185653411337\n",
      "Stochastic Gradient Descent(29530): loss=17.380783219515333\n",
      "Stochastic Gradient Descent(29531): loss=0.38121935712317273\n",
      "Stochastic Gradient Descent(29532): loss=0.014485953013004213\n",
      "Stochastic Gradient Descent(29533): loss=0.23455005714603447\n",
      "Stochastic Gradient Descent(29534): loss=0.28153030681871405\n",
      "Stochastic Gradient Descent(29535): loss=0.6751042098265033\n",
      "Stochastic Gradient Descent(29536): loss=13.354811129035543\n",
      "Stochastic Gradient Descent(29537): loss=4.364821373698608\n",
      "Stochastic Gradient Descent(29538): loss=10.225663082376261\n",
      "Stochastic Gradient Descent(29539): loss=15.941611937129382\n",
      "Stochastic Gradient Descent(29540): loss=0.8601672434252948\n",
      "Stochastic Gradient Descent(29541): loss=1.4570894757040165\n",
      "Stochastic Gradient Descent(29542): loss=17.993501090923843\n",
      "Stochastic Gradient Descent(29543): loss=1.30251049345021\n",
      "Stochastic Gradient Descent(29544): loss=0.3264856980846931\n",
      "Stochastic Gradient Descent(29545): loss=0.1090886965296866\n",
      "Stochastic Gradient Descent(29546): loss=2.735224911600282\n",
      "Stochastic Gradient Descent(29547): loss=26.37821569557904\n",
      "Stochastic Gradient Descent(29548): loss=0.08493057477287001\n",
      "Stochastic Gradient Descent(29549): loss=0.35421776215336614\n",
      "Stochastic Gradient Descent(29550): loss=5.368555345799129\n",
      "Stochastic Gradient Descent(29551): loss=0.10149574818062851\n",
      "Stochastic Gradient Descent(29552): loss=10.180677148601125\n",
      "Stochastic Gradient Descent(29553): loss=1.2602105232249754\n",
      "Stochastic Gradient Descent(29554): loss=0.49270265759188686\n",
      "Stochastic Gradient Descent(29555): loss=1.7221629141296804\n",
      "Stochastic Gradient Descent(29556): loss=20.5789948817039\n",
      "Stochastic Gradient Descent(29557): loss=0.07370565416514185\n",
      "Stochastic Gradient Descent(29558): loss=14.943260516344036\n",
      "Stochastic Gradient Descent(29559): loss=7.436926555479989\n",
      "Stochastic Gradient Descent(29560): loss=9.978121533992974\n",
      "Stochastic Gradient Descent(29561): loss=8.35620403732528\n",
      "Stochastic Gradient Descent(29562): loss=18.330165183828978\n",
      "Stochastic Gradient Descent(29563): loss=0.1476302807218271\n",
      "Stochastic Gradient Descent(29564): loss=0.025799310461878534\n",
      "Stochastic Gradient Descent(29565): loss=7.367772752888079\n",
      "Stochastic Gradient Descent(29566): loss=3.4322523648775345\n",
      "Stochastic Gradient Descent(29567): loss=5.432891131449934\n",
      "Stochastic Gradient Descent(29568): loss=1.500954515623056\n",
      "Stochastic Gradient Descent(29569): loss=0.842000362928039\n",
      "Stochastic Gradient Descent(29570): loss=1.126995824501908\n",
      "Stochastic Gradient Descent(29571): loss=2.0199021673977464\n",
      "Stochastic Gradient Descent(29572): loss=0.40285842203362476\n",
      "Stochastic Gradient Descent(29573): loss=6.37064090502743\n",
      "Stochastic Gradient Descent(29574): loss=6.625955316804876\n",
      "Stochastic Gradient Descent(29575): loss=7.724614482834313\n",
      "Stochastic Gradient Descent(29576): loss=29.281268826395642\n",
      "Stochastic Gradient Descent(29577): loss=19.45027816686368\n",
      "Stochastic Gradient Descent(29578): loss=9.242375908595182\n",
      "Stochastic Gradient Descent(29579): loss=18.96386808625394\n",
      "Stochastic Gradient Descent(29580): loss=5.845507390941064\n",
      "Stochastic Gradient Descent(29581): loss=6.643578955550653\n",
      "Stochastic Gradient Descent(29582): loss=8.90761338276565\n",
      "Stochastic Gradient Descent(29583): loss=43.35602903554386\n",
      "Stochastic Gradient Descent(29584): loss=0.009908552212289229\n",
      "Stochastic Gradient Descent(29585): loss=38.85428311615808\n",
      "Stochastic Gradient Descent(29586): loss=1.2900798302073624\n",
      "Stochastic Gradient Descent(29587): loss=16.041327734271526\n",
      "Stochastic Gradient Descent(29588): loss=11.293941778865173\n",
      "Stochastic Gradient Descent(29589): loss=1.9602214133534406\n",
      "Stochastic Gradient Descent(29590): loss=1.2875654594599766\n",
      "Stochastic Gradient Descent(29591): loss=0.252667443282184\n",
      "Stochastic Gradient Descent(29592): loss=6.095274249663258\n",
      "Stochastic Gradient Descent(29593): loss=0.0007745583483404663\n",
      "Stochastic Gradient Descent(29594): loss=0.25883755574861933\n",
      "Stochastic Gradient Descent(29595): loss=0.02252930087512178\n",
      "Stochastic Gradient Descent(29596): loss=3.405172352378221\n",
      "Stochastic Gradient Descent(29597): loss=0.003831698210215693\n",
      "Stochastic Gradient Descent(29598): loss=0.02623300358203659\n",
      "Stochastic Gradient Descent(29599): loss=4.06681937367402\n",
      "Stochastic Gradient Descent(29600): loss=7.153224747791419\n",
      "Stochastic Gradient Descent(29601): loss=0.03421229368161154\n",
      "Stochastic Gradient Descent(29602): loss=0.0012958482077518871\n",
      "Stochastic Gradient Descent(29603): loss=0.2747880977274985\n",
      "Stochastic Gradient Descent(29604): loss=11.567753424113766\n",
      "Stochastic Gradient Descent(29605): loss=1.5541431144206344\n",
      "Stochastic Gradient Descent(29606): loss=0.17774121932836823\n",
      "Stochastic Gradient Descent(29607): loss=1.2606177335432232\n",
      "Stochastic Gradient Descent(29608): loss=4.194514078276677\n",
      "Stochastic Gradient Descent(29609): loss=14.642310780255183\n",
      "Stochastic Gradient Descent(29610): loss=7.213385451857878\n",
      "Stochastic Gradient Descent(29611): loss=3.700212831636765\n",
      "Stochastic Gradient Descent(29612): loss=3.2339114092634738\n",
      "Stochastic Gradient Descent(29613): loss=3.4884381278910492\n",
      "Stochastic Gradient Descent(29614): loss=0.44916302769710575\n",
      "Stochastic Gradient Descent(29615): loss=9.9255412707563\n",
      "Stochastic Gradient Descent(29616): loss=0.00620173098019515\n",
      "Stochastic Gradient Descent(29617): loss=1.5676535312170274\n",
      "Stochastic Gradient Descent(29618): loss=2.164888003918301\n",
      "Stochastic Gradient Descent(29619): loss=0.9053423316230481\n",
      "Stochastic Gradient Descent(29620): loss=3.5352799731195454\n",
      "Stochastic Gradient Descent(29621): loss=2.748554813542297\n",
      "Stochastic Gradient Descent(29622): loss=0.00017079508265246444\n",
      "Stochastic Gradient Descent(29623): loss=2.977278670847289\n",
      "Stochastic Gradient Descent(29624): loss=0.002024178698929065\n",
      "Stochastic Gradient Descent(29625): loss=13.147354157513337\n",
      "Stochastic Gradient Descent(29626): loss=8.769548047767465\n",
      "Stochastic Gradient Descent(29627): loss=7.574072632206902\n",
      "Stochastic Gradient Descent(29628): loss=6.231625519128509\n",
      "Stochastic Gradient Descent(29629): loss=9.825710562897784\n",
      "Stochastic Gradient Descent(29630): loss=25.511321698690143\n",
      "Stochastic Gradient Descent(29631): loss=5.276931423269501\n",
      "Stochastic Gradient Descent(29632): loss=6.165788122839646\n",
      "Stochastic Gradient Descent(29633): loss=2.3665063035228555\n",
      "Stochastic Gradient Descent(29634): loss=10.206803172689371\n",
      "Stochastic Gradient Descent(29635): loss=0.08591527373299238\n",
      "Stochastic Gradient Descent(29636): loss=2.1170572230283833\n",
      "Stochastic Gradient Descent(29637): loss=5.567087986712915\n",
      "Stochastic Gradient Descent(29638): loss=3.4736793502988013\n",
      "Stochastic Gradient Descent(29639): loss=1.0825205589940734\n",
      "Stochastic Gradient Descent(29640): loss=1.3915563761568024\n",
      "Stochastic Gradient Descent(29641): loss=9.96776198776753\n",
      "Stochastic Gradient Descent(29642): loss=5.073856555304513\n",
      "Stochastic Gradient Descent(29643): loss=0.05581429719191044\n",
      "Stochastic Gradient Descent(29644): loss=8.139584803414046\n",
      "Stochastic Gradient Descent(29645): loss=4.57981927635753\n",
      "Stochastic Gradient Descent(29646): loss=0.5824373038872543\n",
      "Stochastic Gradient Descent(29647): loss=1.9014903170977322\n",
      "Stochastic Gradient Descent(29648): loss=0.15710914641399706\n",
      "Stochastic Gradient Descent(29649): loss=1.6868888207784993\n",
      "Stochastic Gradient Descent(29650): loss=0.11106310122322731\n",
      "Stochastic Gradient Descent(29651): loss=25.36381444131159\n",
      "Stochastic Gradient Descent(29652): loss=0.6997863078016587\n",
      "Stochastic Gradient Descent(29653): loss=0.01672614424179755\n",
      "Stochastic Gradient Descent(29654): loss=1.6100292092873094\n",
      "Stochastic Gradient Descent(29655): loss=12.210041891509263\n",
      "Stochastic Gradient Descent(29656): loss=19.165253923690795\n",
      "Stochastic Gradient Descent(29657): loss=0.39189072522002344\n",
      "Stochastic Gradient Descent(29658): loss=10.328689162451766\n",
      "Stochastic Gradient Descent(29659): loss=2.6527055531230888\n",
      "Stochastic Gradient Descent(29660): loss=0.933765832549349\n",
      "Stochastic Gradient Descent(29661): loss=1.6103653458961216\n",
      "Stochastic Gradient Descent(29662): loss=0.3349889041633166\n",
      "Stochastic Gradient Descent(29663): loss=2.1123042475829146\n",
      "Stochastic Gradient Descent(29664): loss=1.1540256137899059\n",
      "Stochastic Gradient Descent(29665): loss=9.751692596917016\n",
      "Stochastic Gradient Descent(29666): loss=0.040396639830792745\n",
      "Stochastic Gradient Descent(29667): loss=0.18090217066470957\n",
      "Stochastic Gradient Descent(29668): loss=5.161834419715219\n",
      "Stochastic Gradient Descent(29669): loss=4.105643954872274\n",
      "Stochastic Gradient Descent(29670): loss=3.557684934326952\n",
      "Stochastic Gradient Descent(29671): loss=0.2566226420488341\n",
      "Stochastic Gradient Descent(29672): loss=0.015867725172522513\n",
      "Stochastic Gradient Descent(29673): loss=0.17865186714584913\n",
      "Stochastic Gradient Descent(29674): loss=2.93513152516067\n",
      "Stochastic Gradient Descent(29675): loss=5.219636356333038\n",
      "Stochastic Gradient Descent(29676): loss=9.144720218701757\n",
      "Stochastic Gradient Descent(29677): loss=26.362244040326708\n",
      "Stochastic Gradient Descent(29678): loss=0.04166450162706091\n",
      "Stochastic Gradient Descent(29679): loss=0.21634674911957055\n",
      "Stochastic Gradient Descent(29680): loss=3.3310165776186973\n",
      "Stochastic Gradient Descent(29681): loss=0.07249095740967708\n",
      "Stochastic Gradient Descent(29682): loss=5.2298960989075205\n",
      "Stochastic Gradient Descent(29683): loss=7.304359783538204\n",
      "Stochastic Gradient Descent(29684): loss=4.131839609443148\n",
      "Stochastic Gradient Descent(29685): loss=53.2501713806263\n",
      "Stochastic Gradient Descent(29686): loss=17.98832713752202\n",
      "Stochastic Gradient Descent(29687): loss=4.794598809892731\n",
      "Stochastic Gradient Descent(29688): loss=0.06519857885810655\n",
      "Stochastic Gradient Descent(29689): loss=6.713780416215292\n",
      "Stochastic Gradient Descent(29690): loss=24.30971324079257\n",
      "Stochastic Gradient Descent(29691): loss=10.633485132852211\n",
      "Stochastic Gradient Descent(29692): loss=0.29959501192550336\n",
      "Stochastic Gradient Descent(29693): loss=0.3018123150619216\n",
      "Stochastic Gradient Descent(29694): loss=0.1643340553877329\n",
      "Stochastic Gradient Descent(29695): loss=24.4249994431281\n",
      "Stochastic Gradient Descent(29696): loss=0.040192333566191694\n",
      "Stochastic Gradient Descent(29697): loss=0.014217250339902458\n",
      "Stochastic Gradient Descent(29698): loss=1.2090600052127733\n",
      "Stochastic Gradient Descent(29699): loss=1.9485791352578365\n",
      "Stochastic Gradient Descent(29700): loss=12.086915189383282\n",
      "Stochastic Gradient Descent(29701): loss=0.7678387384852416\n",
      "Stochastic Gradient Descent(29702): loss=2.0121718524424352\n",
      "Stochastic Gradient Descent(29703): loss=0.680973687335857\n",
      "Stochastic Gradient Descent(29704): loss=3.981988024647684\n",
      "Stochastic Gradient Descent(29705): loss=0.3275287718775155\n",
      "Stochastic Gradient Descent(29706): loss=53.68621662935165\n",
      "Stochastic Gradient Descent(29707): loss=0.38031953095143234\n",
      "Stochastic Gradient Descent(29708): loss=1.0599341817953776\n",
      "Stochastic Gradient Descent(29709): loss=24.28051536762041\n",
      "Stochastic Gradient Descent(29710): loss=0.9411942616441411\n",
      "Stochastic Gradient Descent(29711): loss=14.338059575683298\n",
      "Stochastic Gradient Descent(29712): loss=19.334803605017346\n",
      "Stochastic Gradient Descent(29713): loss=1.6933721945350408\n",
      "Stochastic Gradient Descent(29714): loss=2.9201309091108767\n",
      "Stochastic Gradient Descent(29715): loss=1.1847190051079475\n",
      "Stochastic Gradient Descent(29716): loss=1.0270761505958723\n",
      "Stochastic Gradient Descent(29717): loss=0.49535135661852225\n",
      "Stochastic Gradient Descent(29718): loss=3.7417614851427463\n",
      "Stochastic Gradient Descent(29719): loss=5.055161942690289\n",
      "Stochastic Gradient Descent(29720): loss=0.9325855321616651\n",
      "Stochastic Gradient Descent(29721): loss=25.007275878847647\n",
      "Stochastic Gradient Descent(29722): loss=0.8703648388416344\n",
      "Stochastic Gradient Descent(29723): loss=2.1474566265872395\n",
      "Stochastic Gradient Descent(29724): loss=0.011174032168889047\n",
      "Stochastic Gradient Descent(29725): loss=5.6195297411954845\n",
      "Stochastic Gradient Descent(29726): loss=14.485206487514798\n",
      "Stochastic Gradient Descent(29727): loss=0.2585748832778276\n",
      "Stochastic Gradient Descent(29728): loss=3.209367453306601\n",
      "Stochastic Gradient Descent(29729): loss=0.8886444387803143\n",
      "Stochastic Gradient Descent(29730): loss=16.140460065786183\n",
      "Stochastic Gradient Descent(29731): loss=0.0012631359155833497\n",
      "Stochastic Gradient Descent(29732): loss=14.481166910556812\n",
      "Stochastic Gradient Descent(29733): loss=0.41735094120796545\n",
      "Stochastic Gradient Descent(29734): loss=0.1882702492746485\n",
      "Stochastic Gradient Descent(29735): loss=2.976472871905118\n",
      "Stochastic Gradient Descent(29736): loss=11.198981381191773\n",
      "Stochastic Gradient Descent(29737): loss=45.36016547423346\n",
      "Stochastic Gradient Descent(29738): loss=5.110457682430597\n",
      "Stochastic Gradient Descent(29739): loss=2.226885254710494\n",
      "Stochastic Gradient Descent(29740): loss=11.33914859875946\n",
      "Stochastic Gradient Descent(29741): loss=0.2548722723056936\n",
      "Stochastic Gradient Descent(29742): loss=0.11627741464133302\n",
      "Stochastic Gradient Descent(29743): loss=2.7617577569288425\n",
      "Stochastic Gradient Descent(29744): loss=0.7007752189604832\n",
      "Stochastic Gradient Descent(29745): loss=1.4647729327093288\n",
      "Stochastic Gradient Descent(29746): loss=1.5044517095863843\n",
      "Stochastic Gradient Descent(29747): loss=2.3112808538432654\n",
      "Stochastic Gradient Descent(29748): loss=12.169308761609326\n",
      "Stochastic Gradient Descent(29749): loss=10.102987120300048\n",
      "Stochastic Gradient Descent(29750): loss=2.153878674994969\n",
      "Stochastic Gradient Descent(29751): loss=0.6924593511020457\n",
      "Stochastic Gradient Descent(29752): loss=1.5694379961055798\n",
      "Stochastic Gradient Descent(29753): loss=0.000853968346219387\n",
      "Stochastic Gradient Descent(29754): loss=12.464230040633772\n",
      "Stochastic Gradient Descent(29755): loss=0.78576724873835\n",
      "Stochastic Gradient Descent(29756): loss=2.945710208227173\n",
      "Stochastic Gradient Descent(29757): loss=0.04876147090655102\n",
      "Stochastic Gradient Descent(29758): loss=4.5342320638587505\n",
      "Stochastic Gradient Descent(29759): loss=3.8700657768788176\n",
      "Stochastic Gradient Descent(29760): loss=19.205842991468984\n",
      "Stochastic Gradient Descent(29761): loss=0.09283472189255902\n",
      "Stochastic Gradient Descent(29762): loss=0.00019755088193683178\n",
      "Stochastic Gradient Descent(29763): loss=3.2137236173800665\n",
      "Stochastic Gradient Descent(29764): loss=0.07246556178556862\n",
      "Stochastic Gradient Descent(29765): loss=16.074288350177582\n",
      "Stochastic Gradient Descent(29766): loss=2.162205726631599\n",
      "Stochastic Gradient Descent(29767): loss=5.487033762384778\n",
      "Stochastic Gradient Descent(29768): loss=1.0437837386580482\n",
      "Stochastic Gradient Descent(29769): loss=0.28945821850224934\n",
      "Stochastic Gradient Descent(29770): loss=0.02307536438694012\n",
      "Stochastic Gradient Descent(29771): loss=0.0846672152254099\n",
      "Stochastic Gradient Descent(29772): loss=4.147790977841444\n",
      "Stochastic Gradient Descent(29773): loss=0.17348195990606785\n",
      "Stochastic Gradient Descent(29774): loss=3.83872034579221\n",
      "Stochastic Gradient Descent(29775): loss=5.026264701782325\n",
      "Stochastic Gradient Descent(29776): loss=0.4781658236328798\n",
      "Stochastic Gradient Descent(29777): loss=2.7316904129901873\n",
      "Stochastic Gradient Descent(29778): loss=0.4596695759516777\n",
      "Stochastic Gradient Descent(29779): loss=6.154265543145752\n",
      "Stochastic Gradient Descent(29780): loss=1.39853777994556\n",
      "Stochastic Gradient Descent(29781): loss=1.5055987195751692\n",
      "Stochastic Gradient Descent(29782): loss=11.496442673551336\n",
      "Stochastic Gradient Descent(29783): loss=0.5993242173844947\n",
      "Stochastic Gradient Descent(29784): loss=4.882069224544757\n",
      "Stochastic Gradient Descent(29785): loss=0.0942486325558982\n",
      "Stochastic Gradient Descent(29786): loss=1.708954020235265\n",
      "Stochastic Gradient Descent(29787): loss=0.17328765250679481\n",
      "Stochastic Gradient Descent(29788): loss=1.4334780379672039\n",
      "Stochastic Gradient Descent(29789): loss=23.720699899399538\n",
      "Stochastic Gradient Descent(29790): loss=0.43823432510773147\n",
      "Stochastic Gradient Descent(29791): loss=5.543461764682892\n",
      "Stochastic Gradient Descent(29792): loss=6.3513985525316965\n",
      "Stochastic Gradient Descent(29793): loss=0.02639983674864355\n",
      "Stochastic Gradient Descent(29794): loss=4.395421297178921\n",
      "Stochastic Gradient Descent(29795): loss=10.996463736750801\n",
      "Stochastic Gradient Descent(29796): loss=11.93422045032709\n",
      "Stochastic Gradient Descent(29797): loss=7.296623921492412\n",
      "Stochastic Gradient Descent(29798): loss=3.9018813711167395\n",
      "Stochastic Gradient Descent(29799): loss=0.0002102007086715883\n",
      "Stochastic Gradient Descent(29800): loss=9.071839496429389\n",
      "Stochastic Gradient Descent(29801): loss=2.118205878977465\n",
      "Stochastic Gradient Descent(29802): loss=2.254558876278106\n",
      "Stochastic Gradient Descent(29803): loss=3.371990050681321\n",
      "Stochastic Gradient Descent(29804): loss=0.0010844728630335732\n",
      "Stochastic Gradient Descent(29805): loss=0.17907449849479368\n",
      "Stochastic Gradient Descent(29806): loss=0.37004000463602316\n",
      "Stochastic Gradient Descent(29807): loss=0.1361804093270819\n",
      "Stochastic Gradient Descent(29808): loss=0.33636442708320774\n",
      "Stochastic Gradient Descent(29809): loss=0.5654056201231309\n",
      "Stochastic Gradient Descent(29810): loss=22.29504499633755\n",
      "Stochastic Gradient Descent(29811): loss=0.02631591846057355\n",
      "Stochastic Gradient Descent(29812): loss=3.209148468688245\n",
      "Stochastic Gradient Descent(29813): loss=0.1527829001573461\n",
      "Stochastic Gradient Descent(29814): loss=0.3822538816832288\n",
      "Stochastic Gradient Descent(29815): loss=15.39719007507682\n",
      "Stochastic Gradient Descent(29816): loss=0.01057636444027555\n",
      "Stochastic Gradient Descent(29817): loss=5.6433841295529765\n",
      "Stochastic Gradient Descent(29818): loss=0.894556752117117\n",
      "Stochastic Gradient Descent(29819): loss=0.1800899867219068\n",
      "Stochastic Gradient Descent(29820): loss=0.012003574634494472\n",
      "Stochastic Gradient Descent(29821): loss=0.4372387386295395\n",
      "Stochastic Gradient Descent(29822): loss=3.316500534645012\n",
      "Stochastic Gradient Descent(29823): loss=0.18148957332391646\n",
      "Stochastic Gradient Descent(29824): loss=28.9145760277538\n",
      "Stochastic Gradient Descent(29825): loss=0.1666349670415531\n",
      "Stochastic Gradient Descent(29826): loss=1.458769077663715\n",
      "Stochastic Gradient Descent(29827): loss=2.2977050602703444\n",
      "Stochastic Gradient Descent(29828): loss=8.560834370942862\n",
      "Stochastic Gradient Descent(29829): loss=2.6454017539203742\n",
      "Stochastic Gradient Descent(29830): loss=0.6288761445111496\n",
      "Stochastic Gradient Descent(29831): loss=3.3350258613821677\n",
      "Stochastic Gradient Descent(29832): loss=8.184307953883271\n",
      "Stochastic Gradient Descent(29833): loss=1.712412747342474\n",
      "Stochastic Gradient Descent(29834): loss=3.2589452798715426\n",
      "Stochastic Gradient Descent(29835): loss=0.6322626241944951\n",
      "Stochastic Gradient Descent(29836): loss=0.25975561403058584\n",
      "Stochastic Gradient Descent(29837): loss=0.6688357201729163\n",
      "Stochastic Gradient Descent(29838): loss=5.940982319068741\n",
      "Stochastic Gradient Descent(29839): loss=0.6546074229247372\n",
      "Stochastic Gradient Descent(29840): loss=20.225701676724302\n",
      "Stochastic Gradient Descent(29841): loss=0.9822401190268432\n",
      "Stochastic Gradient Descent(29842): loss=11.023556113847174\n",
      "Stochastic Gradient Descent(29843): loss=2.094300619912399\n",
      "Stochastic Gradient Descent(29844): loss=0.029097810870148698\n",
      "Stochastic Gradient Descent(29845): loss=9.217077384535255\n",
      "Stochastic Gradient Descent(29846): loss=0.029889860728172887\n",
      "Stochastic Gradient Descent(29847): loss=3.212247991953523\n",
      "Stochastic Gradient Descent(29848): loss=26.380114978640773\n",
      "Stochastic Gradient Descent(29849): loss=9.087203456120212\n",
      "Stochastic Gradient Descent(29850): loss=8.319511010386886\n",
      "Stochastic Gradient Descent(29851): loss=1.1882279236765856\n",
      "Stochastic Gradient Descent(29852): loss=0.810215235496608\n",
      "Stochastic Gradient Descent(29853): loss=5.917517203766504\n",
      "Stochastic Gradient Descent(29854): loss=5.164030250453041\n",
      "Stochastic Gradient Descent(29855): loss=5.456297749493782\n",
      "Stochastic Gradient Descent(29856): loss=22.711026505136385\n",
      "Stochastic Gradient Descent(29857): loss=0.16268684657352633\n",
      "Stochastic Gradient Descent(29858): loss=2.725730786220399\n",
      "Stochastic Gradient Descent(29859): loss=3.955111006778398\n",
      "Stochastic Gradient Descent(29860): loss=5.8068997307196595\n",
      "Stochastic Gradient Descent(29861): loss=12.839071233775469\n",
      "Stochastic Gradient Descent(29862): loss=2.6960488019975526\n",
      "Stochastic Gradient Descent(29863): loss=4.806355752973437\n",
      "Stochastic Gradient Descent(29864): loss=2.222696217823547\n",
      "Stochastic Gradient Descent(29865): loss=25.477871911146693\n",
      "Stochastic Gradient Descent(29866): loss=3.514407677187722\n",
      "Stochastic Gradient Descent(29867): loss=4.7292034675137975\n",
      "Stochastic Gradient Descent(29868): loss=0.3085901847914485\n",
      "Stochastic Gradient Descent(29869): loss=1.2251945627746916\n",
      "Stochastic Gradient Descent(29870): loss=0.00743662643198262\n",
      "Stochastic Gradient Descent(29871): loss=2.2337792839668165\n",
      "Stochastic Gradient Descent(29872): loss=0.33722363657656845\n",
      "Stochastic Gradient Descent(29873): loss=5.069959728594184\n",
      "Stochastic Gradient Descent(29874): loss=0.001735309342407826\n",
      "Stochastic Gradient Descent(29875): loss=0.001760365226517871\n",
      "Stochastic Gradient Descent(29876): loss=4.443697854240483\n",
      "Stochastic Gradient Descent(29877): loss=0.8338985369762493\n",
      "Stochastic Gradient Descent(29878): loss=7.580847133346413\n",
      "Stochastic Gradient Descent(29879): loss=1.5387227810328021\n",
      "Stochastic Gradient Descent(29880): loss=0.0009492107525753007\n",
      "Stochastic Gradient Descent(29881): loss=1.7970875089509528\n",
      "Stochastic Gradient Descent(29882): loss=3.3836762655275368\n",
      "Stochastic Gradient Descent(29883): loss=7.620531252923242\n",
      "Stochastic Gradient Descent(29884): loss=0.014899628483151546\n",
      "Stochastic Gradient Descent(29885): loss=2.288320862807064\n",
      "Stochastic Gradient Descent(29886): loss=2.6683592009437724\n",
      "Stochastic Gradient Descent(29887): loss=1.4991285118907713\n",
      "Stochastic Gradient Descent(29888): loss=1.1846272543917753\n",
      "Stochastic Gradient Descent(29889): loss=0.419318256233382\n",
      "Stochastic Gradient Descent(29890): loss=0.9288400467765171\n",
      "Stochastic Gradient Descent(29891): loss=2.1545987008026284\n",
      "Stochastic Gradient Descent(29892): loss=9.59889726279904\n",
      "Stochastic Gradient Descent(29893): loss=6.138775757763286\n",
      "Stochastic Gradient Descent(29894): loss=0.8217890532752413\n",
      "Stochastic Gradient Descent(29895): loss=0.06979730403129698\n",
      "Stochastic Gradient Descent(29896): loss=7.124705372572699\n",
      "Stochastic Gradient Descent(29897): loss=0.3873157695435765\n",
      "Stochastic Gradient Descent(29898): loss=0.08523292283653483\n",
      "Stochastic Gradient Descent(29899): loss=5.352153515984175\n",
      "Stochastic Gradient Descent(29900): loss=0.7591203197192044\n",
      "Stochastic Gradient Descent(29901): loss=0.11205529662162982\n",
      "Stochastic Gradient Descent(29902): loss=1.2228888101789088\n",
      "Stochastic Gradient Descent(29903): loss=0.16588925202206223\n",
      "Stochastic Gradient Descent(29904): loss=4.421992582278649\n",
      "Stochastic Gradient Descent(29905): loss=1.644720615127467\n",
      "Stochastic Gradient Descent(29906): loss=2.94281130321025\n",
      "Stochastic Gradient Descent(29907): loss=10.171286463746542\n",
      "Stochastic Gradient Descent(29908): loss=4.815094401788804\n",
      "Stochastic Gradient Descent(29909): loss=3.0582530321312635\n",
      "Stochastic Gradient Descent(29910): loss=0.28491286934006727\n",
      "Stochastic Gradient Descent(29911): loss=3.069378160253651\n",
      "Stochastic Gradient Descent(29912): loss=1.0318468629150703\n",
      "Stochastic Gradient Descent(29913): loss=0.08393236568491028\n",
      "Stochastic Gradient Descent(29914): loss=0.19403300480499994\n",
      "Stochastic Gradient Descent(29915): loss=2.581166373714726\n",
      "Stochastic Gradient Descent(29916): loss=2.719020715795784e-05\n",
      "Stochastic Gradient Descent(29917): loss=1.1722327837869033\n",
      "Stochastic Gradient Descent(29918): loss=0.4283931293921806\n",
      "Stochastic Gradient Descent(29919): loss=3.3091428190567647\n",
      "Stochastic Gradient Descent(29920): loss=19.627702315437986\n",
      "Stochastic Gradient Descent(29921): loss=0.11305910437728107\n",
      "Stochastic Gradient Descent(29922): loss=0.5236591622981619\n",
      "Stochastic Gradient Descent(29923): loss=1.4822871276916665\n",
      "Stochastic Gradient Descent(29924): loss=24.68214595603564\n",
      "Stochastic Gradient Descent(29925): loss=8.470260828732588\n",
      "Stochastic Gradient Descent(29926): loss=0.9049148137591428\n",
      "Stochastic Gradient Descent(29927): loss=0.2392822837463051\n",
      "Stochastic Gradient Descent(29928): loss=1.0920125551275526\n",
      "Stochastic Gradient Descent(29929): loss=0.012129809243338223\n",
      "Stochastic Gradient Descent(29930): loss=1.3494248783133238\n",
      "Stochastic Gradient Descent(29931): loss=5.843841392231713\n",
      "Stochastic Gradient Descent(29932): loss=8.157289141175378\n",
      "Stochastic Gradient Descent(29933): loss=0.126560597157041\n",
      "Stochastic Gradient Descent(29934): loss=0.3517034465890283\n",
      "Stochastic Gradient Descent(29935): loss=12.277216478541558\n",
      "Stochastic Gradient Descent(29936): loss=4.204239994831716\n",
      "Stochastic Gradient Descent(29937): loss=1.7169074538523215\n",
      "Stochastic Gradient Descent(29938): loss=0.3144549403807173\n",
      "Stochastic Gradient Descent(29939): loss=0.010871305989629473\n",
      "Stochastic Gradient Descent(29940): loss=0.38778357645101164\n",
      "Stochastic Gradient Descent(29941): loss=11.171591096832767\n",
      "Stochastic Gradient Descent(29942): loss=1.143361249657052\n",
      "Stochastic Gradient Descent(29943): loss=1.0795637935055455\n",
      "Stochastic Gradient Descent(29944): loss=6.134995099903174\n",
      "Stochastic Gradient Descent(29945): loss=0.957571271847422\n",
      "Stochastic Gradient Descent(29946): loss=12.93416121504056\n",
      "Stochastic Gradient Descent(29947): loss=4.882478537168104\n",
      "Stochastic Gradient Descent(29948): loss=1.1052293148085766\n",
      "Stochastic Gradient Descent(29949): loss=9.875971542502066\n",
      "Stochastic Gradient Descent(29950): loss=18.452475953561134\n",
      "Stochastic Gradient Descent(29951): loss=0.0665778041977153\n",
      "Stochastic Gradient Descent(29952): loss=0.026547142377021903\n",
      "Stochastic Gradient Descent(29953): loss=6.457299252802451\n",
      "Stochastic Gradient Descent(29954): loss=16.662572448542686\n",
      "Stochastic Gradient Descent(29955): loss=4.096081068952134\n",
      "Stochastic Gradient Descent(29956): loss=7.145328585581612\n",
      "Stochastic Gradient Descent(29957): loss=1.6603870632260394\n",
      "Stochastic Gradient Descent(29958): loss=0.2688418183551849\n",
      "Stochastic Gradient Descent(29959): loss=1.3573982252470407\n",
      "Stochastic Gradient Descent(29960): loss=0.9564878496294913\n",
      "Stochastic Gradient Descent(29961): loss=0.0021232915446616017\n",
      "Stochastic Gradient Descent(29962): loss=0.13230697403843183\n",
      "Stochastic Gradient Descent(29963): loss=3.6469463177416053\n",
      "Stochastic Gradient Descent(29964): loss=4.172362297330657\n",
      "Stochastic Gradient Descent(29965): loss=0.7449745638884302\n",
      "Stochastic Gradient Descent(29966): loss=0.05577243259711372\n",
      "Stochastic Gradient Descent(29967): loss=0.6919135411514007\n",
      "Stochastic Gradient Descent(29968): loss=0.016726371313112318\n",
      "Stochastic Gradient Descent(29969): loss=4.308542669025186\n",
      "Stochastic Gradient Descent(29970): loss=14.197839640037296\n",
      "Stochastic Gradient Descent(29971): loss=7.8363468491072865\n",
      "Stochastic Gradient Descent(29972): loss=9.559047007262876\n",
      "Stochastic Gradient Descent(29973): loss=0.8698113383158341\n",
      "Stochastic Gradient Descent(29974): loss=0.03802660710926618\n",
      "Stochastic Gradient Descent(29975): loss=36.76814770729571\n",
      "Stochastic Gradient Descent(29976): loss=2.4675194308516164\n",
      "Stochastic Gradient Descent(29977): loss=16.37871228589422\n",
      "Stochastic Gradient Descent(29978): loss=0.00038996560702755765\n",
      "Stochastic Gradient Descent(29979): loss=3.422773383245688\n",
      "Stochastic Gradient Descent(29980): loss=0.7799156681490865\n",
      "Stochastic Gradient Descent(29981): loss=27.80706966028638\n",
      "Stochastic Gradient Descent(29982): loss=0.00011235071526628081\n",
      "Stochastic Gradient Descent(29983): loss=0.08393965804067693\n",
      "Stochastic Gradient Descent(29984): loss=0.9332411096935086\n",
      "Stochastic Gradient Descent(29985): loss=33.340710084281135\n",
      "Stochastic Gradient Descent(29986): loss=3.432562414374221\n",
      "Stochastic Gradient Descent(29987): loss=104.73910218501527\n",
      "Stochastic Gradient Descent(29988): loss=24.518266099441984\n",
      "Stochastic Gradient Descent(29989): loss=9.2140628948004\n",
      "Stochastic Gradient Descent(29990): loss=59.52962194282244\n",
      "Stochastic Gradient Descent(29991): loss=0.032308668784357175\n",
      "Stochastic Gradient Descent(29992): loss=0.010996673920457913\n",
      "Stochastic Gradient Descent(29993): loss=36.28919051270435\n",
      "Stochastic Gradient Descent(29994): loss=2.64863597404962\n",
      "Stochastic Gradient Descent(29995): loss=7.468031385536504\n",
      "Stochastic Gradient Descent(29996): loss=4.940340015915201\n",
      "Stochastic Gradient Descent(29997): loss=0.01683411280362147\n",
      "Stochastic Gradient Descent(29998): loss=3.066493822470415\n",
      "Stochastic Gradient Descent(29999): loss=11.057607145364463\n",
      "Stochastic Gradient Descent(30000): loss=5.781562080641928\n",
      "Stochastic Gradient Descent(30001): loss=0.31992067130387986\n",
      "Stochastic Gradient Descent(30002): loss=0.5488317185134572\n",
      "Stochastic Gradient Descent(30003): loss=3.7956116530082373\n",
      "Stochastic Gradient Descent(30004): loss=0.659536058880969\n",
      "Stochastic Gradient Descent(30005): loss=3.473042614870906\n",
      "Stochastic Gradient Descent(30006): loss=0.5894326471420663\n",
      "Stochastic Gradient Descent(30007): loss=0.7490283438243629\n",
      "Stochastic Gradient Descent(30008): loss=14.738785283621269\n",
      "Stochastic Gradient Descent(30009): loss=0.9336750619801457\n",
      "Stochastic Gradient Descent(30010): loss=2.993276112870621\n",
      "Stochastic Gradient Descent(30011): loss=8.53532656083345\n",
      "Stochastic Gradient Descent(30012): loss=0.3688190496092406\n",
      "Stochastic Gradient Descent(30013): loss=0.48299648603442186\n",
      "Stochastic Gradient Descent(30014): loss=0.48084382606710346\n",
      "Stochastic Gradient Descent(30015): loss=0.0019018867246080727\n",
      "Stochastic Gradient Descent(30016): loss=0.010608961835102628\n",
      "Stochastic Gradient Descent(30017): loss=0.5137296995711214\n",
      "Stochastic Gradient Descent(30018): loss=1.355706332416022e-05\n",
      "Stochastic Gradient Descent(30019): loss=1.1183347655541092\n",
      "Stochastic Gradient Descent(30020): loss=0.9677300850522574\n",
      "Stochastic Gradient Descent(30021): loss=0.46155218716762\n",
      "Stochastic Gradient Descent(30022): loss=1.367176729699659\n",
      "Stochastic Gradient Descent(30023): loss=10.80357202670691\n",
      "Stochastic Gradient Descent(30024): loss=0.32604130276242194\n",
      "Stochastic Gradient Descent(30025): loss=0.08378162159192867\n",
      "Stochastic Gradient Descent(30026): loss=3.5654942535479974\n",
      "Stochastic Gradient Descent(30027): loss=0.6419234357332104\n",
      "Stochastic Gradient Descent(30028): loss=0.1091005059586397\n",
      "Stochastic Gradient Descent(30029): loss=2.654703171207431\n",
      "Stochastic Gradient Descent(30030): loss=1.192085158609457\n",
      "Stochastic Gradient Descent(30031): loss=0.45185714965470697\n",
      "Stochastic Gradient Descent(30032): loss=0.1939251121476314\n",
      "Stochastic Gradient Descent(30033): loss=2.3057267290772288\n",
      "Stochastic Gradient Descent(30034): loss=1.0766678688662636\n",
      "Stochastic Gradient Descent(30035): loss=2.7526781061832355\n",
      "Stochastic Gradient Descent(30036): loss=0.038636772679197885\n",
      "Stochastic Gradient Descent(30037): loss=8.630521712880046\n",
      "Stochastic Gradient Descent(30038): loss=19.594607063568027\n",
      "Stochastic Gradient Descent(30039): loss=2.6842335541528866\n",
      "Stochastic Gradient Descent(30040): loss=19.435165043021904\n",
      "Stochastic Gradient Descent(30041): loss=6.0442368573125895\n",
      "Stochastic Gradient Descent(30042): loss=8.371869926869529\n",
      "Stochastic Gradient Descent(30043): loss=4.39374316898827\n",
      "Stochastic Gradient Descent(30044): loss=0.6113878318041587\n",
      "Stochastic Gradient Descent(30045): loss=9.572916572970316\n",
      "Stochastic Gradient Descent(30046): loss=2.322273988051439\n",
      "Stochastic Gradient Descent(30047): loss=15.893067953549272\n",
      "Stochastic Gradient Descent(30048): loss=1.6075046752503455\n",
      "Stochastic Gradient Descent(30049): loss=13.2542772113691\n",
      "Stochastic Gradient Descent(30050): loss=6.261555603222159\n",
      "Stochastic Gradient Descent(30051): loss=7.45253507390287\n",
      "Stochastic Gradient Descent(30052): loss=11.705821872980293\n",
      "Stochastic Gradient Descent(30053): loss=1.1781009375947125\n",
      "Stochastic Gradient Descent(30054): loss=6.382462142217267\n",
      "Stochastic Gradient Descent(30055): loss=0.9557937009840713\n",
      "Stochastic Gradient Descent(30056): loss=0.5929436799644211\n",
      "Stochastic Gradient Descent(30057): loss=0.46965018206361975\n",
      "Stochastic Gradient Descent(30058): loss=8.566056540731081\n",
      "Stochastic Gradient Descent(30059): loss=9.604921713891585\n",
      "Stochastic Gradient Descent(30060): loss=0.7460960937039929\n",
      "Stochastic Gradient Descent(30061): loss=0.1092022550539409\n",
      "Stochastic Gradient Descent(30062): loss=6.502296547893543\n",
      "Stochastic Gradient Descent(30063): loss=0.41552065959018614\n",
      "Stochastic Gradient Descent(30064): loss=0.06697818344646107\n",
      "Stochastic Gradient Descent(30065): loss=0.3510857326536916\n",
      "Stochastic Gradient Descent(30066): loss=2.47759750201308\n",
      "Stochastic Gradient Descent(30067): loss=5.362352363119383\n",
      "Stochastic Gradient Descent(30068): loss=0.062486489566379276\n",
      "Stochastic Gradient Descent(30069): loss=11.99030161816281\n",
      "Stochastic Gradient Descent(30070): loss=12.915142085023508\n",
      "Stochastic Gradient Descent(30071): loss=0.14883912423712592\n",
      "Stochastic Gradient Descent(30072): loss=0.0013869804406849155\n",
      "Stochastic Gradient Descent(30073): loss=4.55433179254202\n",
      "Stochastic Gradient Descent(30074): loss=10.964712892793985\n",
      "Stochastic Gradient Descent(30075): loss=0.3717873381429827\n",
      "Stochastic Gradient Descent(30076): loss=0.9984742958963483\n",
      "Stochastic Gradient Descent(30077): loss=0.0007593552824738492\n",
      "Stochastic Gradient Descent(30078): loss=0.5444663182705498\n",
      "Stochastic Gradient Descent(30079): loss=1.5937533868375189\n",
      "Stochastic Gradient Descent(30080): loss=6.482145871397625\n",
      "Stochastic Gradient Descent(30081): loss=0.1058563915396562\n",
      "Stochastic Gradient Descent(30082): loss=3.615354632704701\n",
      "Stochastic Gradient Descent(30083): loss=0.12364350229014077\n",
      "Stochastic Gradient Descent(30084): loss=138.47911586651298\n",
      "Stochastic Gradient Descent(30085): loss=60.08362652528997\n",
      "Stochastic Gradient Descent(30086): loss=0.4333592998090621\n",
      "Stochastic Gradient Descent(30087): loss=29.394876955665268\n",
      "Stochastic Gradient Descent(30088): loss=8.407355430450705\n",
      "Stochastic Gradient Descent(30089): loss=19.49254586182058\n",
      "Stochastic Gradient Descent(30090): loss=0.5474699836288517\n",
      "Stochastic Gradient Descent(30091): loss=0.2953356429411838\n",
      "Stochastic Gradient Descent(30092): loss=8.866571493934709\n",
      "Stochastic Gradient Descent(30093): loss=0.07123240551500926\n",
      "Stochastic Gradient Descent(30094): loss=7.903831464752208\n",
      "Stochastic Gradient Descent(30095): loss=1.2679672300182259\n",
      "Stochastic Gradient Descent(30096): loss=3.3709172641090266\n",
      "Stochastic Gradient Descent(30097): loss=8.921283584556305\n",
      "Stochastic Gradient Descent(30098): loss=2.3541130063324074\n",
      "Stochastic Gradient Descent(30099): loss=1.033404551386805\n",
      "Stochastic Gradient Descent(30100): loss=14.552953085722033\n",
      "Stochastic Gradient Descent(30101): loss=0.13140287861303748\n",
      "Stochastic Gradient Descent(30102): loss=0.46254656970385827\n",
      "Stochastic Gradient Descent(30103): loss=4.831825124591018\n",
      "Stochastic Gradient Descent(30104): loss=8.37500465346595\n",
      "Stochastic Gradient Descent(30105): loss=3.568298361687883\n",
      "Stochastic Gradient Descent(30106): loss=0.5294637001899186\n",
      "Stochastic Gradient Descent(30107): loss=22.62749992085534\n",
      "Stochastic Gradient Descent(30108): loss=0.5228190237187852\n",
      "Stochastic Gradient Descent(30109): loss=2.73454662525072\n",
      "Stochastic Gradient Descent(30110): loss=3.030916220741068\n",
      "Stochastic Gradient Descent(30111): loss=5.126371201692699\n",
      "Stochastic Gradient Descent(30112): loss=7.082762712031613\n",
      "Stochastic Gradient Descent(30113): loss=1.2047195826394956\n",
      "Stochastic Gradient Descent(30114): loss=8.848426804454114\n",
      "Stochastic Gradient Descent(30115): loss=19.672709147684436\n",
      "Stochastic Gradient Descent(30116): loss=15.702218565728838\n",
      "Stochastic Gradient Descent(30117): loss=1.431214771277106\n",
      "Stochastic Gradient Descent(30118): loss=15.203866675830723\n",
      "Stochastic Gradient Descent(30119): loss=13.905615718882434\n",
      "Stochastic Gradient Descent(30120): loss=3.029797108102996\n",
      "Stochastic Gradient Descent(30121): loss=0.07006497889090335\n",
      "Stochastic Gradient Descent(30122): loss=7.662269880637281\n",
      "Stochastic Gradient Descent(30123): loss=5.259582612328862\n",
      "Stochastic Gradient Descent(30124): loss=6.075168187476521\n",
      "Stochastic Gradient Descent(30125): loss=0.011768480893702159\n",
      "Stochastic Gradient Descent(30126): loss=7.729520409861486\n",
      "Stochastic Gradient Descent(30127): loss=3.7190136352354166\n",
      "Stochastic Gradient Descent(30128): loss=17.92501769246856\n",
      "Stochastic Gradient Descent(30129): loss=1.5285466609947005\n",
      "Stochastic Gradient Descent(30130): loss=0.9254481094336874\n",
      "Stochastic Gradient Descent(30131): loss=0.004097418614763649\n",
      "Stochastic Gradient Descent(30132): loss=3.4721418498347525\n",
      "Stochastic Gradient Descent(30133): loss=0.1644308556576113\n",
      "Stochastic Gradient Descent(30134): loss=33.79153333766746\n",
      "Stochastic Gradient Descent(30135): loss=4.585801466331233\n",
      "Stochastic Gradient Descent(30136): loss=0.010509890839449958\n",
      "Stochastic Gradient Descent(30137): loss=3.974723425690638\n",
      "Stochastic Gradient Descent(30138): loss=4.091071994237724\n",
      "Stochastic Gradient Descent(30139): loss=8.566619202062382\n",
      "Stochastic Gradient Descent(30140): loss=4.179482623140519\n",
      "Stochastic Gradient Descent(30141): loss=2.491002803828651\n",
      "Stochastic Gradient Descent(30142): loss=19.16331623551203\n",
      "Stochastic Gradient Descent(30143): loss=0.9125178182851442\n",
      "Stochastic Gradient Descent(30144): loss=2.668434515702028\n",
      "Stochastic Gradient Descent(30145): loss=0.8991694867298898\n",
      "Stochastic Gradient Descent(30146): loss=0.7341699602867753\n",
      "Stochastic Gradient Descent(30147): loss=6.695270490359248\n",
      "Stochastic Gradient Descent(30148): loss=3.8399925429405983\n",
      "Stochastic Gradient Descent(30149): loss=0.8255573281632889\n",
      "Stochastic Gradient Descent(30150): loss=0.009250318356213089\n",
      "Stochastic Gradient Descent(30151): loss=3.181746248896953\n",
      "Stochastic Gradient Descent(30152): loss=1.9934577630043777\n",
      "Stochastic Gradient Descent(30153): loss=0.9309778936330161\n",
      "Stochastic Gradient Descent(30154): loss=0.08652372100548551\n",
      "Stochastic Gradient Descent(30155): loss=1.234843647907116\n",
      "Stochastic Gradient Descent(30156): loss=1.5366900592261534\n",
      "Stochastic Gradient Descent(30157): loss=0.02438576736554179\n",
      "Stochastic Gradient Descent(30158): loss=0.20432718478394182\n",
      "Stochastic Gradient Descent(30159): loss=13.036502728940205\n",
      "Stochastic Gradient Descent(30160): loss=0.837868112514317\n",
      "Stochastic Gradient Descent(30161): loss=1.2261643662094754\n",
      "Stochastic Gradient Descent(30162): loss=0.9614403930985912\n",
      "Stochastic Gradient Descent(30163): loss=0.14860382021061683\n",
      "Stochastic Gradient Descent(30164): loss=7.865967644667031\n",
      "Stochastic Gradient Descent(30165): loss=0.011438767866190053\n",
      "Stochastic Gradient Descent(30166): loss=2.277314704609535\n",
      "Stochastic Gradient Descent(30167): loss=17.722317640875936\n",
      "Stochastic Gradient Descent(30168): loss=0.1586863743259983\n",
      "Stochastic Gradient Descent(30169): loss=5.615091824457918\n",
      "Stochastic Gradient Descent(30170): loss=5.397699083363611\n",
      "Stochastic Gradient Descent(30171): loss=0.0031204646008176416\n",
      "Stochastic Gradient Descent(30172): loss=2.7961256486005936\n",
      "Stochastic Gradient Descent(30173): loss=5.216888961109269\n",
      "Stochastic Gradient Descent(30174): loss=1.825250888159363\n",
      "Stochastic Gradient Descent(30175): loss=2.027366657843764\n",
      "Stochastic Gradient Descent(30176): loss=1.8577391925352038\n",
      "Stochastic Gradient Descent(30177): loss=9.871482982439\n",
      "Stochastic Gradient Descent(30178): loss=3.5010538117447574\n",
      "Stochastic Gradient Descent(30179): loss=0.010024333426641618\n",
      "Stochastic Gradient Descent(30180): loss=0.45888706004903146\n",
      "Stochastic Gradient Descent(30181): loss=3.126948832763641\n",
      "Stochastic Gradient Descent(30182): loss=1.3801369914573727\n",
      "Stochastic Gradient Descent(30183): loss=4.753832296950178\n",
      "Stochastic Gradient Descent(30184): loss=0.8179603723645776\n",
      "Stochastic Gradient Descent(30185): loss=9.23942600679999\n",
      "Stochastic Gradient Descent(30186): loss=6.568555195022912\n",
      "Stochastic Gradient Descent(30187): loss=0.8311857057703612\n",
      "Stochastic Gradient Descent(30188): loss=0.002375623450890669\n",
      "Stochastic Gradient Descent(30189): loss=6.89765228908676\n",
      "Stochastic Gradient Descent(30190): loss=3.6175279108478215\n",
      "Stochastic Gradient Descent(30191): loss=6.554143636887472\n",
      "Stochastic Gradient Descent(30192): loss=3.50087599491289\n",
      "Stochastic Gradient Descent(30193): loss=5.15553789786905\n",
      "Stochastic Gradient Descent(30194): loss=1.5403906268822614\n",
      "Stochastic Gradient Descent(30195): loss=12.357205195953865\n",
      "Stochastic Gradient Descent(30196): loss=0.9375343930602019\n",
      "Stochastic Gradient Descent(30197): loss=6.542280951608178\n",
      "Stochastic Gradient Descent(30198): loss=22.51975244588412\n",
      "Stochastic Gradient Descent(30199): loss=1.0642158237746315\n",
      "Stochastic Gradient Descent(30200): loss=0.3630623196493217\n",
      "Stochastic Gradient Descent(30201): loss=3.0781939633072324\n",
      "Stochastic Gradient Descent(30202): loss=1.8195923846502515\n",
      "Stochastic Gradient Descent(30203): loss=8.420449121025399\n",
      "Stochastic Gradient Descent(30204): loss=6.389099320407001\n",
      "Stochastic Gradient Descent(30205): loss=1.3158504522493657\n",
      "Stochastic Gradient Descent(30206): loss=0.010051024208288406\n",
      "Stochastic Gradient Descent(30207): loss=0.006495592478820548\n",
      "Stochastic Gradient Descent(30208): loss=3.1046343672820544\n",
      "Stochastic Gradient Descent(30209): loss=7.423328495344428\n",
      "Stochastic Gradient Descent(30210): loss=0.00032184487119438324\n",
      "Stochastic Gradient Descent(30211): loss=8.001336038767276\n",
      "Stochastic Gradient Descent(30212): loss=11.0472149341244\n",
      "Stochastic Gradient Descent(30213): loss=7.120143919103096\n",
      "Stochastic Gradient Descent(30214): loss=0.1570780517801248\n",
      "Stochastic Gradient Descent(30215): loss=24.71399109464965\n",
      "Stochastic Gradient Descent(30216): loss=0.7602010900130338\n",
      "Stochastic Gradient Descent(30217): loss=17.67752976121552\n",
      "Stochastic Gradient Descent(30218): loss=5.22661130543579\n",
      "Stochastic Gradient Descent(30219): loss=3.7755962769509614\n",
      "Stochastic Gradient Descent(30220): loss=0.025900298626565017\n",
      "Stochastic Gradient Descent(30221): loss=7.9818507506798815\n",
      "Stochastic Gradient Descent(30222): loss=11.756303748345399\n",
      "Stochastic Gradient Descent(30223): loss=0.008736865148218986\n",
      "Stochastic Gradient Descent(30224): loss=23.913281837436955\n",
      "Stochastic Gradient Descent(30225): loss=0.49620274544670334\n",
      "Stochastic Gradient Descent(30226): loss=7.356439037388371\n",
      "Stochastic Gradient Descent(30227): loss=0.03634551559590287\n",
      "Stochastic Gradient Descent(30228): loss=4.809698599370705\n",
      "Stochastic Gradient Descent(30229): loss=11.315842376974482\n",
      "Stochastic Gradient Descent(30230): loss=0.320044042823835\n",
      "Stochastic Gradient Descent(30231): loss=7.947608884156149\n",
      "Stochastic Gradient Descent(30232): loss=28.923083713901253\n",
      "Stochastic Gradient Descent(30233): loss=0.5934878321539799\n",
      "Stochastic Gradient Descent(30234): loss=1.0619507680443647\n",
      "Stochastic Gradient Descent(30235): loss=1.1254777106365206\n",
      "Stochastic Gradient Descent(30236): loss=8.550328984661865\n",
      "Stochastic Gradient Descent(30237): loss=0.8842340313339484\n",
      "Stochastic Gradient Descent(30238): loss=0.03133805129371543\n",
      "Stochastic Gradient Descent(30239): loss=10.448716986108133\n",
      "Stochastic Gradient Descent(30240): loss=0.697746959021489\n",
      "Stochastic Gradient Descent(30241): loss=9.023927430367793\n",
      "Stochastic Gradient Descent(30242): loss=3.352316656960046\n",
      "Stochastic Gradient Descent(30243): loss=0.04189722755554881\n",
      "Stochastic Gradient Descent(30244): loss=6.676356465775692\n",
      "Stochastic Gradient Descent(30245): loss=5.817326566231889\n",
      "Stochastic Gradient Descent(30246): loss=17.417575883259467\n",
      "Stochastic Gradient Descent(30247): loss=3.4120512863955654\n",
      "Stochastic Gradient Descent(30248): loss=0.05741511814679952\n",
      "Stochastic Gradient Descent(30249): loss=1.4644501001640033\n",
      "Stochastic Gradient Descent(30250): loss=0.2728334005900798\n",
      "Stochastic Gradient Descent(30251): loss=44.3805474224737\n",
      "Stochastic Gradient Descent(30252): loss=7.271502391914192\n",
      "Stochastic Gradient Descent(30253): loss=0.3911398142299348\n",
      "Stochastic Gradient Descent(30254): loss=12.505884521437258\n",
      "Stochastic Gradient Descent(30255): loss=0.8839932232556826\n",
      "Stochastic Gradient Descent(30256): loss=1.2784686398678173\n",
      "Stochastic Gradient Descent(30257): loss=7.027585115463477\n",
      "Stochastic Gradient Descent(30258): loss=0.9471433953361826\n",
      "Stochastic Gradient Descent(30259): loss=7.898659588716242\n",
      "Stochastic Gradient Descent(30260): loss=3.82710816544357\n",
      "Stochastic Gradient Descent(30261): loss=0.7307971865247604\n",
      "Stochastic Gradient Descent(30262): loss=0.1140846966929605\n",
      "Stochastic Gradient Descent(30263): loss=3.349475221781313\n",
      "Stochastic Gradient Descent(30264): loss=2.0439476585879786\n",
      "Stochastic Gradient Descent(30265): loss=11.145017316241654\n",
      "Stochastic Gradient Descent(30266): loss=0.8474779217466036\n",
      "Stochastic Gradient Descent(30267): loss=0.1699589717539399\n",
      "Stochastic Gradient Descent(30268): loss=0.09812239971056719\n",
      "Stochastic Gradient Descent(30269): loss=4.767652476360185\n",
      "Stochastic Gradient Descent(30270): loss=25.995106252459063\n",
      "Stochastic Gradient Descent(30271): loss=0.15283532425748067\n",
      "Stochastic Gradient Descent(30272): loss=0.7987513052305126\n",
      "Stochastic Gradient Descent(30273): loss=0.8831634242724486\n",
      "Stochastic Gradient Descent(30274): loss=0.22472392197655425\n",
      "Stochastic Gradient Descent(30275): loss=0.050477943479195035\n",
      "Stochastic Gradient Descent(30276): loss=1.0149864884056574\n",
      "Stochastic Gradient Descent(30277): loss=0.09090566090697275\n",
      "Stochastic Gradient Descent(30278): loss=1.0411460033643605\n",
      "Stochastic Gradient Descent(30279): loss=4.862785852092818\n",
      "Stochastic Gradient Descent(30280): loss=0.8164943718207123\n",
      "Stochastic Gradient Descent(30281): loss=5.53877717026632\n",
      "Stochastic Gradient Descent(30282): loss=1.667385899916678\n",
      "Stochastic Gradient Descent(30283): loss=65.4804218254643\n",
      "Stochastic Gradient Descent(30284): loss=1.2821464201478916\n",
      "Stochastic Gradient Descent(30285): loss=32.07873187948111\n",
      "Stochastic Gradient Descent(30286): loss=1.123631074682016\n",
      "Stochastic Gradient Descent(30287): loss=0.133381841654039\n",
      "Stochastic Gradient Descent(30288): loss=1.4897524410832785\n",
      "Stochastic Gradient Descent(30289): loss=0.025962331966726145\n",
      "Stochastic Gradient Descent(30290): loss=1.5041311124139443\n",
      "Stochastic Gradient Descent(30291): loss=4.668302443939872\n",
      "Stochastic Gradient Descent(30292): loss=4.57049998602702\n",
      "Stochastic Gradient Descent(30293): loss=12.740392257565405\n",
      "Stochastic Gradient Descent(30294): loss=0.25027272071277445\n",
      "Stochastic Gradient Descent(30295): loss=11.245901137674247\n",
      "Stochastic Gradient Descent(30296): loss=14.700271699671738\n",
      "Stochastic Gradient Descent(30297): loss=0.3730478277199568\n",
      "Stochastic Gradient Descent(30298): loss=3.8028000362147854\n",
      "Stochastic Gradient Descent(30299): loss=0.1452059494183152\n",
      "Stochastic Gradient Descent(30300): loss=4.476886320919622\n",
      "Stochastic Gradient Descent(30301): loss=0.055771555701747785\n",
      "Stochastic Gradient Descent(30302): loss=12.725629264497725\n",
      "Stochastic Gradient Descent(30303): loss=6.720354824287322\n",
      "Stochastic Gradient Descent(30304): loss=0.137199059894994\n",
      "Stochastic Gradient Descent(30305): loss=2.823742701069495\n",
      "Stochastic Gradient Descent(30306): loss=13.702029111892026\n",
      "Stochastic Gradient Descent(30307): loss=0.24269395432995483\n",
      "Stochastic Gradient Descent(30308): loss=6.384432645819962\n",
      "Stochastic Gradient Descent(30309): loss=5.889678122244731\n",
      "Stochastic Gradient Descent(30310): loss=9.391419988339377\n",
      "Stochastic Gradient Descent(30311): loss=3.1418591437308288\n",
      "Stochastic Gradient Descent(30312): loss=6.541552690123895\n",
      "Stochastic Gradient Descent(30313): loss=0.24436600142326487\n",
      "Stochastic Gradient Descent(30314): loss=0.02393052092666411\n",
      "Stochastic Gradient Descent(30315): loss=1.3688195863629056\n",
      "Stochastic Gradient Descent(30316): loss=4.705767207980942\n",
      "Stochastic Gradient Descent(30317): loss=1.5528790878062304\n",
      "Stochastic Gradient Descent(30318): loss=3.743646410293056\n",
      "Stochastic Gradient Descent(30319): loss=0.38468829343580957\n",
      "Stochastic Gradient Descent(30320): loss=8.020424875727983\n",
      "Stochastic Gradient Descent(30321): loss=0.6297618859775095\n",
      "Stochastic Gradient Descent(30322): loss=0.532987315862007\n",
      "Stochastic Gradient Descent(30323): loss=1.3252132521092372\n",
      "Stochastic Gradient Descent(30324): loss=10.469317230843549\n",
      "Stochastic Gradient Descent(30325): loss=13.54397669986716\n",
      "Stochastic Gradient Descent(30326): loss=1.2353521449404103\n",
      "Stochastic Gradient Descent(30327): loss=5.323138825324379\n",
      "Stochastic Gradient Descent(30328): loss=0.8109165754566957\n",
      "Stochastic Gradient Descent(30329): loss=1.1840331636486638\n",
      "Stochastic Gradient Descent(30330): loss=7.435492584710066\n",
      "Stochastic Gradient Descent(30331): loss=2.0042847148304426\n",
      "Stochastic Gradient Descent(30332): loss=12.100739206043455\n",
      "Stochastic Gradient Descent(30333): loss=10.33121458083387\n",
      "Stochastic Gradient Descent(30334): loss=1.099196758483205\n",
      "Stochastic Gradient Descent(30335): loss=11.479985858669385\n",
      "Stochastic Gradient Descent(30336): loss=6.3868691516659695\n",
      "Stochastic Gradient Descent(30337): loss=0.2679113714275303\n",
      "Stochastic Gradient Descent(30338): loss=1.9187504363257069\n",
      "Stochastic Gradient Descent(30339): loss=0.08003978378300242\n",
      "Stochastic Gradient Descent(30340): loss=44.28491978982445\n",
      "Stochastic Gradient Descent(30341): loss=6.776159118292861\n",
      "Stochastic Gradient Descent(30342): loss=0.026416499808773627\n",
      "Stochastic Gradient Descent(30343): loss=1.6215031623917993\n",
      "Stochastic Gradient Descent(30344): loss=0.3216496694310646\n",
      "Stochastic Gradient Descent(30345): loss=5.282951035364688\n",
      "Stochastic Gradient Descent(30346): loss=3.04688272600655\n",
      "Stochastic Gradient Descent(30347): loss=6.303720153533147\n",
      "Stochastic Gradient Descent(30348): loss=1.575927113711741\n",
      "Stochastic Gradient Descent(30349): loss=0.40688142861629895\n",
      "Stochastic Gradient Descent(30350): loss=3.9029767104532462\n",
      "Stochastic Gradient Descent(30351): loss=0.0772725321684232\n",
      "Stochastic Gradient Descent(30352): loss=0.15689684579806473\n",
      "Stochastic Gradient Descent(30353): loss=1.3287888622900057\n",
      "Stochastic Gradient Descent(30354): loss=0.023233648956827096\n",
      "Stochastic Gradient Descent(30355): loss=0.0033122711358593965\n",
      "Stochastic Gradient Descent(30356): loss=1.2726395585059922\n",
      "Stochastic Gradient Descent(30357): loss=13.197273185416625\n",
      "Stochastic Gradient Descent(30358): loss=2.815859573037324\n",
      "Stochastic Gradient Descent(30359): loss=0.8329628540289297\n",
      "Stochastic Gradient Descent(30360): loss=1.3975210358577335\n",
      "Stochastic Gradient Descent(30361): loss=6.18330800108463\n",
      "Stochastic Gradient Descent(30362): loss=31.08955458824567\n",
      "Stochastic Gradient Descent(30363): loss=2.0320172492300435\n",
      "Stochastic Gradient Descent(30364): loss=1.9602786124334586\n",
      "Stochastic Gradient Descent(30365): loss=0.0031041722472770383\n",
      "Stochastic Gradient Descent(30366): loss=1.7069807161271264\n",
      "Stochastic Gradient Descent(30367): loss=8.747989992705563\n",
      "Stochastic Gradient Descent(30368): loss=0.0038962718827402526\n",
      "Stochastic Gradient Descent(30369): loss=2.6579156663318626\n",
      "Stochastic Gradient Descent(30370): loss=0.5868865881728396\n",
      "Stochastic Gradient Descent(30371): loss=3.5651755337209448\n",
      "Stochastic Gradient Descent(30372): loss=2.962739249477914\n",
      "Stochastic Gradient Descent(30373): loss=0.003297800212541752\n",
      "Stochastic Gradient Descent(30374): loss=2.805435263983633\n",
      "Stochastic Gradient Descent(30375): loss=0.7962492976562359\n",
      "Stochastic Gradient Descent(30376): loss=7.567145684264021\n",
      "Stochastic Gradient Descent(30377): loss=0.08105978456449876\n",
      "Stochastic Gradient Descent(30378): loss=0.01233437029310665\n",
      "Stochastic Gradient Descent(30379): loss=0.06148477191469384\n",
      "Stochastic Gradient Descent(30380): loss=6.557128033342374\n",
      "Stochastic Gradient Descent(30381): loss=4.152625937945324\n",
      "Stochastic Gradient Descent(30382): loss=0.03175040726919469\n",
      "Stochastic Gradient Descent(30383): loss=7.815129790763085\n",
      "Stochastic Gradient Descent(30384): loss=4.938197956922427\n",
      "Stochastic Gradient Descent(30385): loss=0.3778665094046309\n",
      "Stochastic Gradient Descent(30386): loss=1.9845785752466074\n",
      "Stochastic Gradient Descent(30387): loss=10.641996801144815\n",
      "Stochastic Gradient Descent(30388): loss=1.4174034681770415\n",
      "Stochastic Gradient Descent(30389): loss=2.681070313083001\n",
      "Stochastic Gradient Descent(30390): loss=0.3014328555119411\n",
      "Stochastic Gradient Descent(30391): loss=3.493265096396057\n",
      "Stochastic Gradient Descent(30392): loss=0.20942485042724898\n",
      "Stochastic Gradient Descent(30393): loss=0.022473534276332054\n",
      "Stochastic Gradient Descent(30394): loss=10.30921894817165\n",
      "Stochastic Gradient Descent(30395): loss=0.8932099200839169\n",
      "Stochastic Gradient Descent(30396): loss=0.26445778135075637\n",
      "Stochastic Gradient Descent(30397): loss=74.15337962119652\n",
      "Stochastic Gradient Descent(30398): loss=7.147653691437145\n",
      "Stochastic Gradient Descent(30399): loss=42.65815349360495\n",
      "Stochastic Gradient Descent(30400): loss=5.637782458378069\n",
      "Stochastic Gradient Descent(30401): loss=5.080732949438237\n",
      "Stochastic Gradient Descent(30402): loss=4.729323545001365\n",
      "Stochastic Gradient Descent(30403): loss=30.886213845254748\n",
      "Stochastic Gradient Descent(30404): loss=7.301457168244462\n",
      "Stochastic Gradient Descent(30405): loss=0.5768346769182862\n",
      "Stochastic Gradient Descent(30406): loss=1.164262977462291\n",
      "Stochastic Gradient Descent(30407): loss=0.012217332415086884\n",
      "Stochastic Gradient Descent(30408): loss=0.8974415154178831\n",
      "Stochastic Gradient Descent(30409): loss=3.446817940858576\n",
      "Stochastic Gradient Descent(30410): loss=6.6977058721048595\n",
      "Stochastic Gradient Descent(30411): loss=4.009552617026699\n",
      "Stochastic Gradient Descent(30412): loss=0.1925391579486837\n",
      "Stochastic Gradient Descent(30413): loss=0.15652221253435727\n",
      "Stochastic Gradient Descent(30414): loss=1.24472357212671\n",
      "Stochastic Gradient Descent(30415): loss=29.998439402116357\n",
      "Stochastic Gradient Descent(30416): loss=19.889904219967313\n",
      "Stochastic Gradient Descent(30417): loss=24.369148866841684\n",
      "Stochastic Gradient Descent(30418): loss=12.138713424071124\n",
      "Stochastic Gradient Descent(30419): loss=1.5822938394837538\n",
      "Stochastic Gradient Descent(30420): loss=35.39371827540595\n",
      "Stochastic Gradient Descent(30421): loss=23.015074236284615\n",
      "Stochastic Gradient Descent(30422): loss=0.0706666817122832\n",
      "Stochastic Gradient Descent(30423): loss=30.61009514004304\n",
      "Stochastic Gradient Descent(30424): loss=4.016505797508949\n",
      "Stochastic Gradient Descent(30425): loss=0.08649875111903928\n",
      "Stochastic Gradient Descent(30426): loss=2.4002752372204803\n",
      "Stochastic Gradient Descent(30427): loss=1.7796900275218634\n",
      "Stochastic Gradient Descent(30428): loss=43.77506294255628\n",
      "Stochastic Gradient Descent(30429): loss=2.88707194490523\n",
      "Stochastic Gradient Descent(30430): loss=0.014580951809751736\n",
      "Stochastic Gradient Descent(30431): loss=10.70045921138137\n",
      "Stochastic Gradient Descent(30432): loss=0.014747522110523548\n",
      "Stochastic Gradient Descent(30433): loss=6.441437553129257\n",
      "Stochastic Gradient Descent(30434): loss=2.811707103910999\n",
      "Stochastic Gradient Descent(30435): loss=0.45316880777547736\n",
      "Stochastic Gradient Descent(30436): loss=0.45175445353711857\n",
      "Stochastic Gradient Descent(30437): loss=3.310027583351306\n",
      "Stochastic Gradient Descent(30438): loss=15.112960505909182\n",
      "Stochastic Gradient Descent(30439): loss=4.247111479256623\n",
      "Stochastic Gradient Descent(30440): loss=0.4454317828571283\n",
      "Stochastic Gradient Descent(30441): loss=0.0242312044198061\n",
      "Stochastic Gradient Descent(30442): loss=0.000186244489961001\n",
      "Stochastic Gradient Descent(30443): loss=0.010009205468605132\n",
      "Stochastic Gradient Descent(30444): loss=5.158543915121156\n",
      "Stochastic Gradient Descent(30445): loss=8.660988369104105\n",
      "Stochastic Gradient Descent(30446): loss=1.535659087703552\n",
      "Stochastic Gradient Descent(30447): loss=0.35877658719233635\n",
      "Stochastic Gradient Descent(30448): loss=11.544919586578825\n",
      "Stochastic Gradient Descent(30449): loss=0.010932839052063988\n",
      "Stochastic Gradient Descent(30450): loss=4.738004288874038\n",
      "Stochastic Gradient Descent(30451): loss=1.7454967272333843\n",
      "Stochastic Gradient Descent(30452): loss=0.017217848447348333\n",
      "Stochastic Gradient Descent(30453): loss=8.477420486754228\n",
      "Stochastic Gradient Descent(30454): loss=0.0038471128462223806\n",
      "Stochastic Gradient Descent(30455): loss=0.02881331347703281\n",
      "Stochastic Gradient Descent(30456): loss=1.7065908657448552\n",
      "Stochastic Gradient Descent(30457): loss=0.9789535119824067\n",
      "Stochastic Gradient Descent(30458): loss=0.8305777159761407\n",
      "Stochastic Gradient Descent(30459): loss=1.0244662173981218\n",
      "Stochastic Gradient Descent(30460): loss=1.1451881978058256\n",
      "Stochastic Gradient Descent(30461): loss=4.307107185533116\n",
      "Stochastic Gradient Descent(30462): loss=2.471766239709295\n",
      "Stochastic Gradient Descent(30463): loss=1.911354476748104\n",
      "Stochastic Gradient Descent(30464): loss=4.262061447299715\n",
      "Stochastic Gradient Descent(30465): loss=0.43144545235644527\n",
      "Stochastic Gradient Descent(30466): loss=0.7605436359080161\n",
      "Stochastic Gradient Descent(30467): loss=11.062532718505668\n",
      "Stochastic Gradient Descent(30468): loss=0.6926433076474325\n",
      "Stochastic Gradient Descent(30469): loss=0.005493815851886211\n",
      "Stochastic Gradient Descent(30470): loss=0.12512275761616537\n",
      "Stochastic Gradient Descent(30471): loss=3.3046948707061676\n",
      "Stochastic Gradient Descent(30472): loss=30.828399449753075\n",
      "Stochastic Gradient Descent(30473): loss=1.1933589473588877\n",
      "Stochastic Gradient Descent(30474): loss=1.8626502775828153\n",
      "Stochastic Gradient Descent(30475): loss=4.9548706597588446\n",
      "Stochastic Gradient Descent(30476): loss=3.8106943530156\n",
      "Stochastic Gradient Descent(30477): loss=1.3093135626753258\n",
      "Stochastic Gradient Descent(30478): loss=0.268763851124006\n",
      "Stochastic Gradient Descent(30479): loss=0.507939681042689\n",
      "Stochastic Gradient Descent(30480): loss=0.0018707671064616407\n",
      "Stochastic Gradient Descent(30481): loss=29.765040745997727\n",
      "Stochastic Gradient Descent(30482): loss=0.6687759594834543\n",
      "Stochastic Gradient Descent(30483): loss=0.47473933861818446\n",
      "Stochastic Gradient Descent(30484): loss=2.701409234405539\n",
      "Stochastic Gradient Descent(30485): loss=0.35624139216076\n",
      "Stochastic Gradient Descent(30486): loss=3.811599130400899\n",
      "Stochastic Gradient Descent(30487): loss=0.021895773637560654\n",
      "Stochastic Gradient Descent(30488): loss=7.652754861786312\n",
      "Stochastic Gradient Descent(30489): loss=3.0504154851724787\n",
      "Stochastic Gradient Descent(30490): loss=8.103397437556561\n",
      "Stochastic Gradient Descent(30491): loss=0.7309677804401912\n",
      "Stochastic Gradient Descent(30492): loss=0.10585016070061665\n",
      "Stochastic Gradient Descent(30493): loss=0.06477531972643188\n",
      "Stochastic Gradient Descent(30494): loss=0.015369247203067836\n",
      "Stochastic Gradient Descent(30495): loss=8.017738388674328\n",
      "Stochastic Gradient Descent(30496): loss=0.2472054217194575\n",
      "Stochastic Gradient Descent(30497): loss=0.005061722125051129\n",
      "Stochastic Gradient Descent(30498): loss=1.1800501176791494\n",
      "Stochastic Gradient Descent(30499): loss=4.457125582796134\n",
      "Stochastic Gradient Descent(30500): loss=3.4770777437377913\n",
      "Stochastic Gradient Descent(30501): loss=4.853800089610885\n",
      "Stochastic Gradient Descent(30502): loss=1.6351599946080355\n",
      "Stochastic Gradient Descent(30503): loss=5.131477045337047\n",
      "Stochastic Gradient Descent(30504): loss=4.891285420936152\n",
      "Stochastic Gradient Descent(30505): loss=0.7317611690722301\n",
      "Stochastic Gradient Descent(30506): loss=0.008542263924409526\n",
      "Stochastic Gradient Descent(30507): loss=2.7096903012431848\n",
      "Stochastic Gradient Descent(30508): loss=0.0465074601237211\n",
      "Stochastic Gradient Descent(30509): loss=0.9020110713083537\n",
      "Stochastic Gradient Descent(30510): loss=0.9065755200219927\n",
      "Stochastic Gradient Descent(30511): loss=5.753300964258716\n",
      "Stochastic Gradient Descent(30512): loss=4.1291202758722125\n",
      "Stochastic Gradient Descent(30513): loss=13.225395495035253\n",
      "Stochastic Gradient Descent(30514): loss=0.0005471668455167596\n",
      "Stochastic Gradient Descent(30515): loss=0.31378940922851406\n",
      "Stochastic Gradient Descent(30516): loss=0.10159633215307387\n",
      "Stochastic Gradient Descent(30517): loss=0.0006936849520985041\n",
      "Stochastic Gradient Descent(30518): loss=4.459254886433737\n",
      "Stochastic Gradient Descent(30519): loss=1.470648934083548\n",
      "Stochastic Gradient Descent(30520): loss=0.6822906897606368\n",
      "Stochastic Gradient Descent(30521): loss=4.010714693943169\n",
      "Stochastic Gradient Descent(30522): loss=0.37725247106090687\n",
      "Stochastic Gradient Descent(30523): loss=0.17221677222980283\n",
      "Stochastic Gradient Descent(30524): loss=9.019608271902538\n",
      "Stochastic Gradient Descent(30525): loss=2.1647191788238067\n",
      "Stochastic Gradient Descent(30526): loss=0.0010243438603000407\n",
      "Stochastic Gradient Descent(30527): loss=15.360570251538304\n",
      "Stochastic Gradient Descent(30528): loss=0.1600876811903009\n",
      "Stochastic Gradient Descent(30529): loss=3.8812596129386168\n",
      "Stochastic Gradient Descent(30530): loss=5.212375292162366\n",
      "Stochastic Gradient Descent(30531): loss=6.230127806246708\n",
      "Stochastic Gradient Descent(30532): loss=1.1449971107120462\n",
      "Stochastic Gradient Descent(30533): loss=0.08546466775232302\n",
      "Stochastic Gradient Descent(30534): loss=0.03750708880497789\n",
      "Stochastic Gradient Descent(30535): loss=2.820507540695538\n",
      "Stochastic Gradient Descent(30536): loss=5.613812517228823\n",
      "Stochastic Gradient Descent(30537): loss=0.11456067806885767\n",
      "Stochastic Gradient Descent(30538): loss=2.160487453249614\n",
      "Stochastic Gradient Descent(30539): loss=0.11042222568296992\n",
      "Stochastic Gradient Descent(30540): loss=3.866923745983349\n",
      "Stochastic Gradient Descent(30541): loss=3.327278275096964\n",
      "Stochastic Gradient Descent(30542): loss=43.005962660730596\n",
      "Stochastic Gradient Descent(30543): loss=47.37554181423665\n",
      "Stochastic Gradient Descent(30544): loss=2.9915306045869814\n",
      "Stochastic Gradient Descent(30545): loss=2.1626267430901187\n",
      "Stochastic Gradient Descent(30546): loss=3.3611520445203977\n",
      "Stochastic Gradient Descent(30547): loss=0.7155090399168628\n",
      "Stochastic Gradient Descent(30548): loss=2.4098159757782143\n",
      "Stochastic Gradient Descent(30549): loss=4.288243028149335\n",
      "Stochastic Gradient Descent(30550): loss=4.062657346506859\n",
      "Stochastic Gradient Descent(30551): loss=14.65820799651041\n",
      "Stochastic Gradient Descent(30552): loss=0.36553462681469384\n",
      "Stochastic Gradient Descent(30553): loss=4.963753754531201\n",
      "Stochastic Gradient Descent(30554): loss=0.3791724909830532\n",
      "Stochastic Gradient Descent(30555): loss=0.0856672116390374\n",
      "Stochastic Gradient Descent(30556): loss=0.7206806696096462\n",
      "Stochastic Gradient Descent(30557): loss=1.9382583617388895\n",
      "Stochastic Gradient Descent(30558): loss=1.396942524013751\n",
      "Stochastic Gradient Descent(30559): loss=6.464267472918207\n",
      "Stochastic Gradient Descent(30560): loss=17.68141738768352\n",
      "Stochastic Gradient Descent(30561): loss=10.462296609197946\n",
      "Stochastic Gradient Descent(30562): loss=6.642697348802553\n",
      "Stochastic Gradient Descent(30563): loss=0.06276550273777191\n",
      "Stochastic Gradient Descent(30564): loss=0.00878152659295001\n",
      "Stochastic Gradient Descent(30565): loss=0.9658927127138914\n",
      "Stochastic Gradient Descent(30566): loss=17.537203626235033\n",
      "Stochastic Gradient Descent(30567): loss=3.6807107629371894\n",
      "Stochastic Gradient Descent(30568): loss=1.1694179302593632\n",
      "Stochastic Gradient Descent(30569): loss=19.406422256555665\n",
      "Stochastic Gradient Descent(30570): loss=0.019237108661533945\n",
      "Stochastic Gradient Descent(30571): loss=6.14216585633224\n",
      "Stochastic Gradient Descent(30572): loss=16.245324354316686\n",
      "Stochastic Gradient Descent(30573): loss=2.707061189136293\n",
      "Stochastic Gradient Descent(30574): loss=0.638472679650821\n",
      "Stochastic Gradient Descent(30575): loss=1.9173814710617172\n",
      "Stochastic Gradient Descent(30576): loss=2.048989608027668\n",
      "Stochastic Gradient Descent(30577): loss=0.008148607935078846\n",
      "Stochastic Gradient Descent(30578): loss=2.1240832169601673\n",
      "Stochastic Gradient Descent(30579): loss=13.363062348820216\n",
      "Stochastic Gradient Descent(30580): loss=0.008022024298686379\n",
      "Stochastic Gradient Descent(30581): loss=3.6805246517267216\n",
      "Stochastic Gradient Descent(30582): loss=23.842552910805203\n",
      "Stochastic Gradient Descent(30583): loss=1.7153377248433113\n",
      "Stochastic Gradient Descent(30584): loss=0.09561797185558407\n",
      "Stochastic Gradient Descent(30585): loss=51.30928551428748\n",
      "Stochastic Gradient Descent(30586): loss=0.10134921503743895\n",
      "Stochastic Gradient Descent(30587): loss=2.819745276699117\n",
      "Stochastic Gradient Descent(30588): loss=0.9676122487259674\n",
      "Stochastic Gradient Descent(30589): loss=9.76037428400294\n",
      "Stochastic Gradient Descent(30590): loss=0.5482138592165053\n",
      "Stochastic Gradient Descent(30591): loss=0.8187599341580915\n",
      "Stochastic Gradient Descent(30592): loss=0.44519812827417116\n",
      "Stochastic Gradient Descent(30593): loss=0.025496513562853252\n",
      "Stochastic Gradient Descent(30594): loss=1.775444686666622\n",
      "Stochastic Gradient Descent(30595): loss=6.822281705057683\n",
      "Stochastic Gradient Descent(30596): loss=9.250904625312002\n",
      "Stochastic Gradient Descent(30597): loss=0.23377775690991753\n",
      "Stochastic Gradient Descent(30598): loss=47.07567377013684\n",
      "Stochastic Gradient Descent(30599): loss=1.9629379596840428\n",
      "Stochastic Gradient Descent(30600): loss=3.9828886872616085\n",
      "Stochastic Gradient Descent(30601): loss=2.5121127912274575\n",
      "Stochastic Gradient Descent(30602): loss=0.06371558344755957\n",
      "Stochastic Gradient Descent(30603): loss=6.211381563678727\n",
      "Stochastic Gradient Descent(30604): loss=0.23611398469921946\n",
      "Stochastic Gradient Descent(30605): loss=17.557876459047147\n",
      "Stochastic Gradient Descent(30606): loss=5.31958094530779\n",
      "Stochastic Gradient Descent(30607): loss=0.21433306847757472\n",
      "Stochastic Gradient Descent(30608): loss=4.087154603178078\n",
      "Stochastic Gradient Descent(30609): loss=1.889535344087638\n",
      "Stochastic Gradient Descent(30610): loss=0.7033681651563547\n",
      "Stochastic Gradient Descent(30611): loss=1.8900375749266924\n",
      "Stochastic Gradient Descent(30612): loss=0.00041313265206677154\n",
      "Stochastic Gradient Descent(30613): loss=3.1584035093456557\n",
      "Stochastic Gradient Descent(30614): loss=1.4841982676632253\n",
      "Stochastic Gradient Descent(30615): loss=3.2865693210394444\n",
      "Stochastic Gradient Descent(30616): loss=1.0012055190676075\n",
      "Stochastic Gradient Descent(30617): loss=0.07581734149701395\n",
      "Stochastic Gradient Descent(30618): loss=5.272739485409618\n",
      "Stochastic Gradient Descent(30619): loss=1.7698417434674176\n",
      "Stochastic Gradient Descent(30620): loss=0.3126256563540487\n",
      "Stochastic Gradient Descent(30621): loss=0.025060899250496167\n",
      "Stochastic Gradient Descent(30622): loss=17.734434576158158\n",
      "Stochastic Gradient Descent(30623): loss=0.07130856961251618\n",
      "Stochastic Gradient Descent(30624): loss=0.49550858385441177\n",
      "Stochastic Gradient Descent(30625): loss=8.001808502281813\n",
      "Stochastic Gradient Descent(30626): loss=1.8355024994212565\n",
      "Stochastic Gradient Descent(30627): loss=0.4280945612973211\n",
      "Stochastic Gradient Descent(30628): loss=3.2888544905985344\n",
      "Stochastic Gradient Descent(30629): loss=1.7508129387330011\n",
      "Stochastic Gradient Descent(30630): loss=0.8807061225161863\n",
      "Stochastic Gradient Descent(30631): loss=8.021866418907809\n",
      "Stochastic Gradient Descent(30632): loss=2.675932995679981\n",
      "Stochastic Gradient Descent(30633): loss=7.575252098918542\n",
      "Stochastic Gradient Descent(30634): loss=0.001163623341604106\n",
      "Stochastic Gradient Descent(30635): loss=0.20098894018001906\n",
      "Stochastic Gradient Descent(30636): loss=16.803181474298725\n",
      "Stochastic Gradient Descent(30637): loss=0.6526773973886786\n",
      "Stochastic Gradient Descent(30638): loss=0.2204291830399193\n",
      "Stochastic Gradient Descent(30639): loss=2.1968095154732796\n",
      "Stochastic Gradient Descent(30640): loss=3.3941618436082055\n",
      "Stochastic Gradient Descent(30641): loss=1.5294676865916765\n",
      "Stochastic Gradient Descent(30642): loss=0.7532531425511072\n",
      "Stochastic Gradient Descent(30643): loss=8.643580243235464\n",
      "Stochastic Gradient Descent(30644): loss=2.624316156014153\n",
      "Stochastic Gradient Descent(30645): loss=0.10062578722999914\n",
      "Stochastic Gradient Descent(30646): loss=1.113676650757077\n",
      "Stochastic Gradient Descent(30647): loss=5.52977953249638\n",
      "Stochastic Gradient Descent(30648): loss=0.4744103183290099\n",
      "Stochastic Gradient Descent(30649): loss=1.9795400028322232\n",
      "Stochastic Gradient Descent(30650): loss=0.19800087740462488\n",
      "Stochastic Gradient Descent(30651): loss=1.6289016128024745\n",
      "Stochastic Gradient Descent(30652): loss=11.74039370109486\n",
      "Stochastic Gradient Descent(30653): loss=4.9121455438066945\n",
      "Stochastic Gradient Descent(30654): loss=2.7530971315168165\n",
      "Stochastic Gradient Descent(30655): loss=17.478213168141266\n",
      "Stochastic Gradient Descent(30656): loss=0.07787018951970319\n",
      "Stochastic Gradient Descent(30657): loss=5.918670893951752\n",
      "Stochastic Gradient Descent(30658): loss=3.2652319096772406\n",
      "Stochastic Gradient Descent(30659): loss=25.57780568088255\n",
      "Stochastic Gradient Descent(30660): loss=0.7616764662652561\n",
      "Stochastic Gradient Descent(30661): loss=0.22097218468584834\n",
      "Stochastic Gradient Descent(30662): loss=0.02408052384665647\n",
      "Stochastic Gradient Descent(30663): loss=0.7129400081665913\n",
      "Stochastic Gradient Descent(30664): loss=0.09327915161569648\n",
      "Stochastic Gradient Descent(30665): loss=12.303411578176123\n",
      "Stochastic Gradient Descent(30666): loss=6.664071593769926\n",
      "Stochastic Gradient Descent(30667): loss=15.090667411503029\n",
      "Stochastic Gradient Descent(30668): loss=0.45430236852707634\n",
      "Stochastic Gradient Descent(30669): loss=5.611948496315857\n",
      "Stochastic Gradient Descent(30670): loss=0.5039465426516461\n",
      "Stochastic Gradient Descent(30671): loss=33.420851981852714\n",
      "Stochastic Gradient Descent(30672): loss=9.722763699614399\n",
      "Stochastic Gradient Descent(30673): loss=2.1949538386014265\n",
      "Stochastic Gradient Descent(30674): loss=0.3805756274247581\n",
      "Stochastic Gradient Descent(30675): loss=2.638936644559923\n",
      "Stochastic Gradient Descent(30676): loss=4.288551572621189\n",
      "Stochastic Gradient Descent(30677): loss=7.9982947307104935\n",
      "Stochastic Gradient Descent(30678): loss=0.2588590351653142\n",
      "Stochastic Gradient Descent(30679): loss=0.16857916873429857\n",
      "Stochastic Gradient Descent(30680): loss=1.088750097115929\n",
      "Stochastic Gradient Descent(30681): loss=0.8516121064511464\n",
      "Stochastic Gradient Descent(30682): loss=6.057621205638454\n",
      "Stochastic Gradient Descent(30683): loss=11.285365578792812\n",
      "Stochastic Gradient Descent(30684): loss=0.12391513322932782\n",
      "Stochastic Gradient Descent(30685): loss=0.1833783379743865\n",
      "Stochastic Gradient Descent(30686): loss=0.4884680727934081\n",
      "Stochastic Gradient Descent(30687): loss=3.222512772625425\n",
      "Stochastic Gradient Descent(30688): loss=23.36866259545187\n",
      "Stochastic Gradient Descent(30689): loss=9.977384561971203\n",
      "Stochastic Gradient Descent(30690): loss=3.272364746910049\n",
      "Stochastic Gradient Descent(30691): loss=0.6627519049150152\n",
      "Stochastic Gradient Descent(30692): loss=13.746592741153375\n",
      "Stochastic Gradient Descent(30693): loss=1.8433354144862009\n",
      "Stochastic Gradient Descent(30694): loss=4.946663347468345\n",
      "Stochastic Gradient Descent(30695): loss=1.2643813271554143\n",
      "Stochastic Gradient Descent(30696): loss=0.3489641885800526\n",
      "Stochastic Gradient Descent(30697): loss=0.005076686536951461\n",
      "Stochastic Gradient Descent(30698): loss=1.437003729510966\n",
      "Stochastic Gradient Descent(30699): loss=5.402486238503087\n",
      "Stochastic Gradient Descent(30700): loss=1.080921456804865\n",
      "Stochastic Gradient Descent(30701): loss=0.3297759283388228\n",
      "Stochastic Gradient Descent(30702): loss=0.7257337897510114\n",
      "Stochastic Gradient Descent(30703): loss=2.4350859762501424\n",
      "Stochastic Gradient Descent(30704): loss=2.1148445701607637\n",
      "Stochastic Gradient Descent(30705): loss=0.03243789566272626\n",
      "Stochastic Gradient Descent(30706): loss=2.9438367930888893\n",
      "Stochastic Gradient Descent(30707): loss=0.14954190676782123\n",
      "Stochastic Gradient Descent(30708): loss=24.684765955827373\n",
      "Stochastic Gradient Descent(30709): loss=0.3322456481654395\n",
      "Stochastic Gradient Descent(30710): loss=3.575081088796437\n",
      "Stochastic Gradient Descent(30711): loss=0.3424585607537986\n",
      "Stochastic Gradient Descent(30712): loss=0.2324128716812845\n",
      "Stochastic Gradient Descent(30713): loss=2.9800697896887858\n",
      "Stochastic Gradient Descent(30714): loss=0.1824204695922111\n",
      "Stochastic Gradient Descent(30715): loss=12.132236519492555\n",
      "Stochastic Gradient Descent(30716): loss=1.1951347748965782\n",
      "Stochastic Gradient Descent(30717): loss=0.06267083035874665\n",
      "Stochastic Gradient Descent(30718): loss=0.7722037664857343\n",
      "Stochastic Gradient Descent(30719): loss=2.9176179978985606\n",
      "Stochastic Gradient Descent(30720): loss=2.067337739089857\n",
      "Stochastic Gradient Descent(30721): loss=0.027764952181775923\n",
      "Stochastic Gradient Descent(30722): loss=11.442364074517354\n",
      "Stochastic Gradient Descent(30723): loss=0.7886214665402319\n",
      "Stochastic Gradient Descent(30724): loss=2.368376186891519\n",
      "Stochastic Gradient Descent(30725): loss=1.2663154417037732\n",
      "Stochastic Gradient Descent(30726): loss=2.0406604347760986\n",
      "Stochastic Gradient Descent(30727): loss=3.2262897200636953\n",
      "Stochastic Gradient Descent(30728): loss=8.820270602496317\n",
      "Stochastic Gradient Descent(30729): loss=3.8868205385477412\n",
      "Stochastic Gradient Descent(30730): loss=5.711267626892994\n",
      "Stochastic Gradient Descent(30731): loss=23.094793434470187\n",
      "Stochastic Gradient Descent(30732): loss=25.863339582572685\n",
      "Stochastic Gradient Descent(30733): loss=3.384563440725374\n",
      "Stochastic Gradient Descent(30734): loss=0.20526657590938846\n",
      "Stochastic Gradient Descent(30735): loss=27.297548597100988\n",
      "Stochastic Gradient Descent(30736): loss=5.702941299187163\n",
      "Stochastic Gradient Descent(30737): loss=6.973716636422297\n",
      "Stochastic Gradient Descent(30738): loss=5.019796684452786\n",
      "Stochastic Gradient Descent(30739): loss=7.0751964983654405\n",
      "Stochastic Gradient Descent(30740): loss=1.7098774708503397\n",
      "Stochastic Gradient Descent(30741): loss=0.21000244146107505\n",
      "Stochastic Gradient Descent(30742): loss=0.9944032101345235\n",
      "Stochastic Gradient Descent(30743): loss=2.064157064723052\n",
      "Stochastic Gradient Descent(30744): loss=8.580273925646976\n",
      "Stochastic Gradient Descent(30745): loss=6.888357891447387\n",
      "Stochastic Gradient Descent(30746): loss=0.10333389944847539\n",
      "Stochastic Gradient Descent(30747): loss=9.77568613732604\n",
      "Stochastic Gradient Descent(30748): loss=0.672921761474505\n",
      "Stochastic Gradient Descent(30749): loss=1.0125204364316482\n",
      "Stochastic Gradient Descent(30750): loss=1.299558794623367\n",
      "Stochastic Gradient Descent(30751): loss=0.7651751928938579\n",
      "Stochastic Gradient Descent(30752): loss=0.1115478419360472\n",
      "Stochastic Gradient Descent(30753): loss=1.3317992667025111\n",
      "Stochastic Gradient Descent(30754): loss=2.2962054912960155\n",
      "Stochastic Gradient Descent(30755): loss=1.0481917261984486\n",
      "Stochastic Gradient Descent(30756): loss=5.572501956476291\n",
      "Stochastic Gradient Descent(30757): loss=0.8552440113879411\n",
      "Stochastic Gradient Descent(30758): loss=0.024164904857021482\n",
      "Stochastic Gradient Descent(30759): loss=0.023993634041619496\n",
      "Stochastic Gradient Descent(30760): loss=0.7894277320504524\n",
      "Stochastic Gradient Descent(30761): loss=0.2532323028475397\n",
      "Stochastic Gradient Descent(30762): loss=3.2780157355843094\n",
      "Stochastic Gradient Descent(30763): loss=10.12375132399868\n",
      "Stochastic Gradient Descent(30764): loss=3.464152638036265\n",
      "Stochastic Gradient Descent(30765): loss=0.23053863092432203\n",
      "Stochastic Gradient Descent(30766): loss=0.33822093810643217\n",
      "Stochastic Gradient Descent(30767): loss=0.2924940696840867\n",
      "Stochastic Gradient Descent(30768): loss=0.33481032090406393\n",
      "Stochastic Gradient Descent(30769): loss=3.284857603031827\n",
      "Stochastic Gradient Descent(30770): loss=5.8025553177677525\n",
      "Stochastic Gradient Descent(30771): loss=0.13913226338080828\n",
      "Stochastic Gradient Descent(30772): loss=0.7655013869774967\n",
      "Stochastic Gradient Descent(30773): loss=1.2672393098009642\n",
      "Stochastic Gradient Descent(30774): loss=9.46481809973599\n",
      "Stochastic Gradient Descent(30775): loss=1.8222807634586458\n",
      "Stochastic Gradient Descent(30776): loss=5.701724844022926\n",
      "Stochastic Gradient Descent(30777): loss=0.566805995977313\n",
      "Stochastic Gradient Descent(30778): loss=1.3568811511421335\n",
      "Stochastic Gradient Descent(30779): loss=6.134059144469927\n",
      "Stochastic Gradient Descent(30780): loss=0.1957124079312856\n",
      "Stochastic Gradient Descent(30781): loss=1.0841890998725823\n",
      "Stochastic Gradient Descent(30782): loss=0.06727092817115185\n",
      "Stochastic Gradient Descent(30783): loss=1.75803169636924\n",
      "Stochastic Gradient Descent(30784): loss=2.0252072292221017\n",
      "Stochastic Gradient Descent(30785): loss=1.9846756436164215\n",
      "Stochastic Gradient Descent(30786): loss=0.09584172346063236\n",
      "Stochastic Gradient Descent(30787): loss=16.836094083717665\n",
      "Stochastic Gradient Descent(30788): loss=1.0292164775502533\n",
      "Stochastic Gradient Descent(30789): loss=2.165164546270218\n",
      "Stochastic Gradient Descent(30790): loss=0.3043685692419479\n",
      "Stochastic Gradient Descent(30791): loss=0.5690023668764634\n",
      "Stochastic Gradient Descent(30792): loss=1.5681295265812711\n",
      "Stochastic Gradient Descent(30793): loss=6.6443063923000905\n",
      "Stochastic Gradient Descent(30794): loss=0.1273940057475156\n",
      "Stochastic Gradient Descent(30795): loss=2.0011194660182445\n",
      "Stochastic Gradient Descent(30796): loss=7.352700703181051\n",
      "Stochastic Gradient Descent(30797): loss=0.24462599766747167\n",
      "Stochastic Gradient Descent(30798): loss=0.824065936960472\n",
      "Stochastic Gradient Descent(30799): loss=3.934644287646364\n",
      "Stochastic Gradient Descent(30800): loss=0.7003696569585796\n",
      "Stochastic Gradient Descent(30801): loss=7.120337204223741\n",
      "Stochastic Gradient Descent(30802): loss=4.756653200953236\n",
      "Stochastic Gradient Descent(30803): loss=0.03767878847232857\n",
      "Stochastic Gradient Descent(30804): loss=1.2645051633166686\n",
      "Stochastic Gradient Descent(30805): loss=0.013968598075903898\n",
      "Stochastic Gradient Descent(30806): loss=2.9177698606413265\n",
      "Stochastic Gradient Descent(30807): loss=2.5995624120102527\n",
      "Stochastic Gradient Descent(30808): loss=1.139525768163259\n",
      "Stochastic Gradient Descent(30809): loss=1.3467314037196332\n",
      "Stochastic Gradient Descent(30810): loss=0.10351080579912392\n",
      "Stochastic Gradient Descent(30811): loss=0.5668153432290491\n",
      "Stochastic Gradient Descent(30812): loss=1.2757174611180606\n",
      "Stochastic Gradient Descent(30813): loss=3.6551324394963522\n",
      "Stochastic Gradient Descent(30814): loss=2.7730871642390373\n",
      "Stochastic Gradient Descent(30815): loss=0.897625729940529\n",
      "Stochastic Gradient Descent(30816): loss=8.618289904017661\n",
      "Stochastic Gradient Descent(30817): loss=0.6053718071645063\n",
      "Stochastic Gradient Descent(30818): loss=1.3873798266429824\n",
      "Stochastic Gradient Descent(30819): loss=0.5938687944110905\n",
      "Stochastic Gradient Descent(30820): loss=1.9028944701829513\n",
      "Stochastic Gradient Descent(30821): loss=0.010605193297786529\n",
      "Stochastic Gradient Descent(30822): loss=0.06784987083181718\n",
      "Stochastic Gradient Descent(30823): loss=1.6948072496672053\n",
      "Stochastic Gradient Descent(30824): loss=1.022495310721659\n",
      "Stochastic Gradient Descent(30825): loss=0.6114203289511928\n",
      "Stochastic Gradient Descent(30826): loss=0.15948452522212742\n",
      "Stochastic Gradient Descent(30827): loss=0.12330467207609502\n",
      "Stochastic Gradient Descent(30828): loss=5.940267990669237\n",
      "Stochastic Gradient Descent(30829): loss=4.0968194783627965\n",
      "Stochastic Gradient Descent(30830): loss=4.824361895038473\n",
      "Stochastic Gradient Descent(30831): loss=1.0633468048866555\n",
      "Stochastic Gradient Descent(30832): loss=0.6744401685813948\n",
      "Stochastic Gradient Descent(30833): loss=0.5456957293436636\n",
      "Stochastic Gradient Descent(30834): loss=0.9157659660406187\n",
      "Stochastic Gradient Descent(30835): loss=13.41984548715172\n",
      "Stochastic Gradient Descent(30836): loss=7.850931131589473\n",
      "Stochastic Gradient Descent(30837): loss=3.706715054565233\n",
      "Stochastic Gradient Descent(30838): loss=5.3032379126043\n",
      "Stochastic Gradient Descent(30839): loss=0.001121899280664419\n",
      "Stochastic Gradient Descent(30840): loss=3.3626316694847613\n",
      "Stochastic Gradient Descent(30841): loss=1.3017731697043347\n",
      "Stochastic Gradient Descent(30842): loss=0.7572979475577214\n",
      "Stochastic Gradient Descent(30843): loss=3.5955442757630807\n",
      "Stochastic Gradient Descent(30844): loss=1.694902663158464\n",
      "Stochastic Gradient Descent(30845): loss=1.7510348242332523\n",
      "Stochastic Gradient Descent(30846): loss=1.1070336725393162\n",
      "Stochastic Gradient Descent(30847): loss=1.284567999377165\n",
      "Stochastic Gradient Descent(30848): loss=5.414724746088801\n",
      "Stochastic Gradient Descent(30849): loss=6.631668346153505\n",
      "Stochastic Gradient Descent(30850): loss=0.7450917440530145\n",
      "Stochastic Gradient Descent(30851): loss=0.20030890688237427\n",
      "Stochastic Gradient Descent(30852): loss=17.877639020247084\n",
      "Stochastic Gradient Descent(30853): loss=8.397925314796245\n",
      "Stochastic Gradient Descent(30854): loss=1.1781688854264298\n",
      "Stochastic Gradient Descent(30855): loss=2.408097355772506\n",
      "Stochastic Gradient Descent(30856): loss=2.9296649543489295\n",
      "Stochastic Gradient Descent(30857): loss=11.283078527265655\n",
      "Stochastic Gradient Descent(30858): loss=6.203407564075531\n",
      "Stochastic Gradient Descent(30859): loss=1.828111343385103\n",
      "Stochastic Gradient Descent(30860): loss=8.424689604899072\n",
      "Stochastic Gradient Descent(30861): loss=0.06268486118229799\n",
      "Stochastic Gradient Descent(30862): loss=6.382051914744155\n",
      "Stochastic Gradient Descent(30863): loss=2.021902756706939\n",
      "Stochastic Gradient Descent(30864): loss=0.20107949518640714\n",
      "Stochastic Gradient Descent(30865): loss=0.6515406354914146\n",
      "Stochastic Gradient Descent(30866): loss=0.3903623670834442\n",
      "Stochastic Gradient Descent(30867): loss=0.018478776273449973\n",
      "Stochastic Gradient Descent(30868): loss=3.807581736577858\n",
      "Stochastic Gradient Descent(30869): loss=6.638319413710571\n",
      "Stochastic Gradient Descent(30870): loss=0.044603577510317856\n",
      "Stochastic Gradient Descent(30871): loss=2.1235368056401756\n",
      "Stochastic Gradient Descent(30872): loss=13.62784171062666\n",
      "Stochastic Gradient Descent(30873): loss=8.4446185305148\n",
      "Stochastic Gradient Descent(30874): loss=3.304321716006438\n",
      "Stochastic Gradient Descent(30875): loss=3.7804589317536084\n",
      "Stochastic Gradient Descent(30876): loss=0.4611856096136518\n",
      "Stochastic Gradient Descent(30877): loss=0.14492910303063508\n",
      "Stochastic Gradient Descent(30878): loss=1.68294345415509\n",
      "Stochastic Gradient Descent(30879): loss=4.973279928669072\n",
      "Stochastic Gradient Descent(30880): loss=9.347406829161821\n",
      "Stochastic Gradient Descent(30881): loss=0.0043571598625401445\n",
      "Stochastic Gradient Descent(30882): loss=3.5332971775410846\n",
      "Stochastic Gradient Descent(30883): loss=4.212368173682459\n",
      "Stochastic Gradient Descent(30884): loss=0.8243046026664811\n",
      "Stochastic Gradient Descent(30885): loss=3.410670289831378\n",
      "Stochastic Gradient Descent(30886): loss=2.3391300232296293\n",
      "Stochastic Gradient Descent(30887): loss=0.002075166360008014\n",
      "Stochastic Gradient Descent(30888): loss=2.7259902822870643\n",
      "Stochastic Gradient Descent(30889): loss=0.9295725841566657\n",
      "Stochastic Gradient Descent(30890): loss=0.6720122258136051\n",
      "Stochastic Gradient Descent(30891): loss=6.319081419823859\n",
      "Stochastic Gradient Descent(30892): loss=0.19025038199594368\n",
      "Stochastic Gradient Descent(30893): loss=0.2512643037147925\n",
      "Stochastic Gradient Descent(30894): loss=1.6158129736370244\n",
      "Stochastic Gradient Descent(30895): loss=23.968273056920403\n",
      "Stochastic Gradient Descent(30896): loss=3.4995246526175685\n",
      "Stochastic Gradient Descent(30897): loss=0.16316895981143573\n",
      "Stochastic Gradient Descent(30898): loss=3.1571336797572753\n",
      "Stochastic Gradient Descent(30899): loss=2.6391521747048583\n",
      "Stochastic Gradient Descent(30900): loss=3.9532580751624136\n",
      "Stochastic Gradient Descent(30901): loss=4.006035740222177\n",
      "Stochastic Gradient Descent(30902): loss=19.16311603806431\n",
      "Stochastic Gradient Descent(30903): loss=4.082486320155146\n",
      "Stochastic Gradient Descent(30904): loss=0.784397488514884\n",
      "Stochastic Gradient Descent(30905): loss=0.05697225397612859\n",
      "Stochastic Gradient Descent(30906): loss=0.2541191146586669\n",
      "Stochastic Gradient Descent(30907): loss=13.483503905220571\n",
      "Stochastic Gradient Descent(30908): loss=1.483723141746032\n",
      "Stochastic Gradient Descent(30909): loss=0.00243366557322801\n",
      "Stochastic Gradient Descent(30910): loss=54.58668318591068\n",
      "Stochastic Gradient Descent(30911): loss=32.785696449349636\n",
      "Stochastic Gradient Descent(30912): loss=2.146479584801875\n",
      "Stochastic Gradient Descent(30913): loss=0.01142038274446202\n",
      "Stochastic Gradient Descent(30914): loss=1.827400602182438\n",
      "Stochastic Gradient Descent(30915): loss=14.6985822220927\n",
      "Stochastic Gradient Descent(30916): loss=0.9108826791157504\n",
      "Stochastic Gradient Descent(30917): loss=0.7695387415961753\n",
      "Stochastic Gradient Descent(30918): loss=7.681395580321121\n",
      "Stochastic Gradient Descent(30919): loss=12.260290122589767\n",
      "Stochastic Gradient Descent(30920): loss=3.1807559599792965\n",
      "Stochastic Gradient Descent(30921): loss=0.024304576712534397\n",
      "Stochastic Gradient Descent(30922): loss=1.9546157057949\n",
      "Stochastic Gradient Descent(30923): loss=0.2584783745497532\n",
      "Stochastic Gradient Descent(30924): loss=6.065732550994219\n",
      "Stochastic Gradient Descent(30925): loss=0.536383420866646\n",
      "Stochastic Gradient Descent(30926): loss=3.066457617984229\n",
      "Stochastic Gradient Descent(30927): loss=3.3810049721310245\n",
      "Stochastic Gradient Descent(30928): loss=4.735166871048511\n",
      "Stochastic Gradient Descent(30929): loss=0.09849805321515996\n",
      "Stochastic Gradient Descent(30930): loss=4.28031532165708\n",
      "Stochastic Gradient Descent(30931): loss=0.628469153286979\n",
      "Stochastic Gradient Descent(30932): loss=15.185511824776254\n",
      "Stochastic Gradient Descent(30933): loss=3.964902870710346\n",
      "Stochastic Gradient Descent(30934): loss=0.27504134600171276\n",
      "Stochastic Gradient Descent(30935): loss=1.2407224782283834\n",
      "Stochastic Gradient Descent(30936): loss=11.62308468234504\n",
      "Stochastic Gradient Descent(30937): loss=0.5326696003159318\n",
      "Stochastic Gradient Descent(30938): loss=0.838596577425356\n",
      "Stochastic Gradient Descent(30939): loss=3.7219886246920666\n",
      "Stochastic Gradient Descent(30940): loss=1.7088762377624809\n",
      "Stochastic Gradient Descent(30941): loss=0.00023836205442661314\n",
      "Stochastic Gradient Descent(30942): loss=1.5120602896144029\n",
      "Stochastic Gradient Descent(30943): loss=3.834108957795854\n",
      "Stochastic Gradient Descent(30944): loss=1.4483692107077784\n",
      "Stochastic Gradient Descent(30945): loss=13.201862234053996\n",
      "Stochastic Gradient Descent(30946): loss=3.010039355513987\n",
      "Stochastic Gradient Descent(30947): loss=0.26334075287147884\n",
      "Stochastic Gradient Descent(30948): loss=0.8242068326009855\n",
      "Stochastic Gradient Descent(30949): loss=19.191341975970605\n",
      "Stochastic Gradient Descent(30950): loss=0.9733959047148879\n",
      "Stochastic Gradient Descent(30951): loss=28.199900240588033\n",
      "Stochastic Gradient Descent(30952): loss=2.3974805476847263\n",
      "Stochastic Gradient Descent(30953): loss=15.14785112072897\n",
      "Stochastic Gradient Descent(30954): loss=0.028710777729436997\n",
      "Stochastic Gradient Descent(30955): loss=2.6913947733111923\n",
      "Stochastic Gradient Descent(30956): loss=5.622990800119874\n",
      "Stochastic Gradient Descent(30957): loss=27.059052454458346\n",
      "Stochastic Gradient Descent(30958): loss=3.3053851719216096\n",
      "Stochastic Gradient Descent(30959): loss=2.6075807260223662\n",
      "Stochastic Gradient Descent(30960): loss=0.03268005630887794\n",
      "Stochastic Gradient Descent(30961): loss=6.9206037546221495\n",
      "Stochastic Gradient Descent(30962): loss=0.004218453640836213\n",
      "Stochastic Gradient Descent(30963): loss=5.025126158240631\n",
      "Stochastic Gradient Descent(30964): loss=21.576790003629295\n",
      "Stochastic Gradient Descent(30965): loss=0.10130757089141394\n",
      "Stochastic Gradient Descent(30966): loss=1.7415898039736684\n",
      "Stochastic Gradient Descent(30967): loss=0.012430520262293876\n",
      "Stochastic Gradient Descent(30968): loss=2.6878127280569473\n",
      "Stochastic Gradient Descent(30969): loss=7.520217278925487\n",
      "Stochastic Gradient Descent(30970): loss=6.20045476538828\n",
      "Stochastic Gradient Descent(30971): loss=0.09251951877543724\n",
      "Stochastic Gradient Descent(30972): loss=0.46161747970379974\n",
      "Stochastic Gradient Descent(30973): loss=0.06098504557243368\n",
      "Stochastic Gradient Descent(30974): loss=13.115710490583059\n",
      "Stochastic Gradient Descent(30975): loss=13.089612882929073\n",
      "Stochastic Gradient Descent(30976): loss=0.00296775199304337\n",
      "Stochastic Gradient Descent(30977): loss=3.8482962523194053\n",
      "Stochastic Gradient Descent(30978): loss=4.203711357525116\n",
      "Stochastic Gradient Descent(30979): loss=13.52608471819596\n",
      "Stochastic Gradient Descent(30980): loss=4.522085303257763\n",
      "Stochastic Gradient Descent(30981): loss=17.202506124269796\n",
      "Stochastic Gradient Descent(30982): loss=13.076594633254672\n",
      "Stochastic Gradient Descent(30983): loss=2.9441589041441483\n",
      "Stochastic Gradient Descent(30984): loss=2.381800337120882\n",
      "Stochastic Gradient Descent(30985): loss=1.6285250576225414\n",
      "Stochastic Gradient Descent(30986): loss=0.17752535123613142\n",
      "Stochastic Gradient Descent(30987): loss=0.9711882186168607\n",
      "Stochastic Gradient Descent(30988): loss=2.560033703997669\n",
      "Stochastic Gradient Descent(30989): loss=18.472566368715487\n",
      "Stochastic Gradient Descent(30990): loss=3.6103917172224533\n",
      "Stochastic Gradient Descent(30991): loss=2.698642235691776\n",
      "Stochastic Gradient Descent(30992): loss=1.6964263047924366\n",
      "Stochastic Gradient Descent(30993): loss=16.745105822162216\n",
      "Stochastic Gradient Descent(30994): loss=14.623095310943901\n",
      "Stochastic Gradient Descent(30995): loss=0.7546360607924609\n",
      "Stochastic Gradient Descent(30996): loss=1.213048198756661\n",
      "Stochastic Gradient Descent(30997): loss=27.329296002967446\n",
      "Stochastic Gradient Descent(30998): loss=0.6617512277344565\n",
      "Stochastic Gradient Descent(30999): loss=4.592702866236384\n",
      "Stochastic Gradient Descent(31000): loss=5.970206660321119\n",
      "Stochastic Gradient Descent(31001): loss=0.3662870202801284\n",
      "Stochastic Gradient Descent(31002): loss=6.944785969347661\n",
      "Stochastic Gradient Descent(31003): loss=7.832370023256537\n",
      "Stochastic Gradient Descent(31004): loss=0.02601754833008734\n",
      "Stochastic Gradient Descent(31005): loss=0.36884833760805025\n",
      "Stochastic Gradient Descent(31006): loss=3.4915097693582995\n",
      "Stochastic Gradient Descent(31007): loss=2.3355544287858927\n",
      "Stochastic Gradient Descent(31008): loss=6.925181308211219\n",
      "Stochastic Gradient Descent(31009): loss=4.975940708259648\n",
      "Stochastic Gradient Descent(31010): loss=0.030243157302384204\n",
      "Stochastic Gradient Descent(31011): loss=1.1696918483387095\n",
      "Stochastic Gradient Descent(31012): loss=4.47072930746993\n",
      "Stochastic Gradient Descent(31013): loss=1.0252706750362794\n",
      "Stochastic Gradient Descent(31014): loss=2.3445545352356376\n",
      "Stochastic Gradient Descent(31015): loss=2.0902600568265055\n",
      "Stochastic Gradient Descent(31016): loss=0.023537075606272113\n",
      "Stochastic Gradient Descent(31017): loss=9.559359425699421\n",
      "Stochastic Gradient Descent(31018): loss=1.9888840162162265\n",
      "Stochastic Gradient Descent(31019): loss=3.5699261487303673\n",
      "Stochastic Gradient Descent(31020): loss=9.304806548864292\n",
      "Stochastic Gradient Descent(31021): loss=0.08514995148898023\n",
      "Stochastic Gradient Descent(31022): loss=2.527891215188762\n",
      "Stochastic Gradient Descent(31023): loss=1.6976170493092027\n",
      "Stochastic Gradient Descent(31024): loss=0.3041768694155978\n",
      "Stochastic Gradient Descent(31025): loss=1.2777455545876533\n",
      "Stochastic Gradient Descent(31026): loss=1.7918692952761048\n",
      "Stochastic Gradient Descent(31027): loss=4.295130595660474\n",
      "Stochastic Gradient Descent(31028): loss=8.997794322216858\n",
      "Stochastic Gradient Descent(31029): loss=21.364879907517103\n",
      "Stochastic Gradient Descent(31030): loss=5.030615997448703\n",
      "Stochastic Gradient Descent(31031): loss=11.054104748389497\n",
      "Stochastic Gradient Descent(31032): loss=6.3807370101519805\n",
      "Stochastic Gradient Descent(31033): loss=0.7896488417911469\n",
      "Stochastic Gradient Descent(31034): loss=0.6777961877622308\n",
      "Stochastic Gradient Descent(31035): loss=29.01038411172871\n",
      "Stochastic Gradient Descent(31036): loss=0.7102326498172107\n",
      "Stochastic Gradient Descent(31037): loss=5.31254614694537\n",
      "Stochastic Gradient Descent(31038): loss=5.289263603808161\n",
      "Stochastic Gradient Descent(31039): loss=3.1843969898726137\n",
      "Stochastic Gradient Descent(31040): loss=23.409438472676424\n",
      "Stochastic Gradient Descent(31041): loss=1.3630416067947664\n",
      "Stochastic Gradient Descent(31042): loss=15.364848946279665\n",
      "Stochastic Gradient Descent(31043): loss=4.837769921985273\n",
      "Stochastic Gradient Descent(31044): loss=4.224820685914493\n",
      "Stochastic Gradient Descent(31045): loss=5.638810011169942\n",
      "Stochastic Gradient Descent(31046): loss=0.5225581781276182\n",
      "Stochastic Gradient Descent(31047): loss=2.52873940563321\n",
      "Stochastic Gradient Descent(31048): loss=5.559577564337721\n",
      "Stochastic Gradient Descent(31049): loss=1.5830787045892867\n",
      "Stochastic Gradient Descent(31050): loss=1.4724251834859083\n",
      "Stochastic Gradient Descent(31051): loss=1.7726907814492405\n",
      "Stochastic Gradient Descent(31052): loss=3.794963635300014\n",
      "Stochastic Gradient Descent(31053): loss=2.0486008742327795\n",
      "Stochastic Gradient Descent(31054): loss=4.9866479020095635\n",
      "Stochastic Gradient Descent(31055): loss=5.455129158401741\n",
      "Stochastic Gradient Descent(31056): loss=1.0071173752772684\n",
      "Stochastic Gradient Descent(31057): loss=1.2030480042422182\n",
      "Stochastic Gradient Descent(31058): loss=3.7714135440891727\n",
      "Stochastic Gradient Descent(31059): loss=1.357736258891413\n",
      "Stochastic Gradient Descent(31060): loss=1.744329727714479\n",
      "Stochastic Gradient Descent(31061): loss=0.22259692203182235\n",
      "Stochastic Gradient Descent(31062): loss=1.696051806126709\n",
      "Stochastic Gradient Descent(31063): loss=0.029272933429644363\n",
      "Stochastic Gradient Descent(31064): loss=6.212733098303993\n",
      "Stochastic Gradient Descent(31065): loss=0.1737785227298577\n",
      "Stochastic Gradient Descent(31066): loss=5.282938059851957\n",
      "Stochastic Gradient Descent(31067): loss=59.51498863663944\n",
      "Stochastic Gradient Descent(31068): loss=7.034378095993909\n",
      "Stochastic Gradient Descent(31069): loss=0.04724759839494475\n",
      "Stochastic Gradient Descent(31070): loss=0.8523776015923918\n",
      "Stochastic Gradient Descent(31071): loss=0.10027602429354832\n",
      "Stochastic Gradient Descent(31072): loss=0.5844585357100716\n",
      "Stochastic Gradient Descent(31073): loss=0.25578567207333847\n",
      "Stochastic Gradient Descent(31074): loss=0.10533383431424503\n",
      "Stochastic Gradient Descent(31075): loss=5.9766486545639665\n",
      "Stochastic Gradient Descent(31076): loss=1.913521525149778\n",
      "Stochastic Gradient Descent(31077): loss=19.713654644491093\n",
      "Stochastic Gradient Descent(31078): loss=25.166625675647225\n",
      "Stochastic Gradient Descent(31079): loss=0.03314909253307575\n",
      "Stochastic Gradient Descent(31080): loss=4.843514009617296\n",
      "Stochastic Gradient Descent(31081): loss=0.1693308527038054\n",
      "Stochastic Gradient Descent(31082): loss=1.0481697630147206\n",
      "Stochastic Gradient Descent(31083): loss=9.64658560068544\n",
      "Stochastic Gradient Descent(31084): loss=4.545480192126942\n",
      "Stochastic Gradient Descent(31085): loss=4.671125337304452\n",
      "Stochastic Gradient Descent(31086): loss=4.93904651868783\n",
      "Stochastic Gradient Descent(31087): loss=2.899012310782097\n",
      "Stochastic Gradient Descent(31088): loss=0.5245625629884991\n",
      "Stochastic Gradient Descent(31089): loss=5.959127069481898\n",
      "Stochastic Gradient Descent(31090): loss=0.011159577497317568\n",
      "Stochastic Gradient Descent(31091): loss=5.2204552374047175\n",
      "Stochastic Gradient Descent(31092): loss=0.01189856319866317\n",
      "Stochastic Gradient Descent(31093): loss=6.7891225463732585\n",
      "Stochastic Gradient Descent(31094): loss=4.15229705704598\n",
      "Stochastic Gradient Descent(31095): loss=15.399346594496627\n",
      "Stochastic Gradient Descent(31096): loss=2.1470070676857906\n",
      "Stochastic Gradient Descent(31097): loss=0.4127915131479377\n",
      "Stochastic Gradient Descent(31098): loss=3.185959054724574\n",
      "Stochastic Gradient Descent(31099): loss=1.108281022063702\n",
      "Stochastic Gradient Descent(31100): loss=25.83083054988949\n",
      "Stochastic Gradient Descent(31101): loss=0.0603377581866708\n",
      "Stochastic Gradient Descent(31102): loss=4.756648710826347\n",
      "Stochastic Gradient Descent(31103): loss=3.6750322411791685\n",
      "Stochastic Gradient Descent(31104): loss=6.455186036065469\n",
      "Stochastic Gradient Descent(31105): loss=0.22729385232434335\n",
      "Stochastic Gradient Descent(31106): loss=0.7345577680165308\n",
      "Stochastic Gradient Descent(31107): loss=5.036746470190737\n",
      "Stochastic Gradient Descent(31108): loss=0.5717920670792969\n",
      "Stochastic Gradient Descent(31109): loss=0.7571460131163947\n",
      "Stochastic Gradient Descent(31110): loss=3.108340398854688\n",
      "Stochastic Gradient Descent(31111): loss=2.2416415771690965\n",
      "Stochastic Gradient Descent(31112): loss=4.008142218233865\n",
      "Stochastic Gradient Descent(31113): loss=2.2484276451236433\n",
      "Stochastic Gradient Descent(31114): loss=0.35157066864391784\n",
      "Stochastic Gradient Descent(31115): loss=1.3930766534082872\n",
      "Stochastic Gradient Descent(31116): loss=3.566929635014685\n",
      "Stochastic Gradient Descent(31117): loss=0.12472728407400185\n",
      "Stochastic Gradient Descent(31118): loss=7.130541878383997\n",
      "Stochastic Gradient Descent(31119): loss=3.4309312421130613\n",
      "Stochastic Gradient Descent(31120): loss=3.5844283127417\n",
      "Stochastic Gradient Descent(31121): loss=1.9929664830967238\n",
      "Stochastic Gradient Descent(31122): loss=0.33577098670575756\n",
      "Stochastic Gradient Descent(31123): loss=0.0563745408158063\n",
      "Stochastic Gradient Descent(31124): loss=0.010675195795309452\n",
      "Stochastic Gradient Descent(31125): loss=3.646993444284573\n",
      "Stochastic Gradient Descent(31126): loss=0.2425930394953295\n",
      "Stochastic Gradient Descent(31127): loss=0.7940805431209303\n",
      "Stochastic Gradient Descent(31128): loss=5.156605584913712\n",
      "Stochastic Gradient Descent(31129): loss=7.416235863688656\n",
      "Stochastic Gradient Descent(31130): loss=1.1071298309184479\n",
      "Stochastic Gradient Descent(31131): loss=8.39226724158267\n",
      "Stochastic Gradient Descent(31132): loss=6.326690194650326\n",
      "Stochastic Gradient Descent(31133): loss=10.51256892274963\n",
      "Stochastic Gradient Descent(31134): loss=16.535423044424974\n",
      "Stochastic Gradient Descent(31135): loss=0.2682367340140386\n",
      "Stochastic Gradient Descent(31136): loss=4.655880277554518\n",
      "Stochastic Gradient Descent(31137): loss=0.28269635565975165\n",
      "Stochastic Gradient Descent(31138): loss=6.377841499697311\n",
      "Stochastic Gradient Descent(31139): loss=0.1994879390426583\n",
      "Stochastic Gradient Descent(31140): loss=1.1772251064684196\n",
      "Stochastic Gradient Descent(31141): loss=0.05631554970254348\n",
      "Stochastic Gradient Descent(31142): loss=2.454896865852647\n",
      "Stochastic Gradient Descent(31143): loss=4.263304982820101\n",
      "Stochastic Gradient Descent(31144): loss=3.4529412509027515\n",
      "Stochastic Gradient Descent(31145): loss=3.14439948124055\n",
      "Stochastic Gradient Descent(31146): loss=9.01791437483956\n",
      "Stochastic Gradient Descent(31147): loss=3.229938812874064\n",
      "Stochastic Gradient Descent(31148): loss=0.005901431303314003\n",
      "Stochastic Gradient Descent(31149): loss=0.047893117120684205\n",
      "Stochastic Gradient Descent(31150): loss=1.2282136358431903\n",
      "Stochastic Gradient Descent(31151): loss=1.7568214498858035\n",
      "Stochastic Gradient Descent(31152): loss=0.0068655817497031355\n",
      "Stochastic Gradient Descent(31153): loss=0.37845761061669553\n",
      "Stochastic Gradient Descent(31154): loss=2.205438962991661\n",
      "Stochastic Gradient Descent(31155): loss=0.11144515111081943\n",
      "Stochastic Gradient Descent(31156): loss=1.1782499065052305\n",
      "Stochastic Gradient Descent(31157): loss=0.3653397175805005\n",
      "Stochastic Gradient Descent(31158): loss=7.0786903651391535\n",
      "Stochastic Gradient Descent(31159): loss=4.02209619138288\n",
      "Stochastic Gradient Descent(31160): loss=23.70443840504085\n",
      "Stochastic Gradient Descent(31161): loss=7.86166045355936\n",
      "Stochastic Gradient Descent(31162): loss=18.94227911088593\n",
      "Stochastic Gradient Descent(31163): loss=1.3146678354858015\n",
      "Stochastic Gradient Descent(31164): loss=5.201476932201949\n",
      "Stochastic Gradient Descent(31165): loss=15.728014792387237\n",
      "Stochastic Gradient Descent(31166): loss=29.261171114659735\n",
      "Stochastic Gradient Descent(31167): loss=0.9853003748743326\n",
      "Stochastic Gradient Descent(31168): loss=0.35693987122197074\n",
      "Stochastic Gradient Descent(31169): loss=5.511537895907468\n",
      "Stochastic Gradient Descent(31170): loss=6.290013710385753\n",
      "Stochastic Gradient Descent(31171): loss=4.744037495603178\n",
      "Stochastic Gradient Descent(31172): loss=6.883316934512935\n",
      "Stochastic Gradient Descent(31173): loss=2.7787939215085062\n",
      "Stochastic Gradient Descent(31174): loss=1.9164865417254902\n",
      "Stochastic Gradient Descent(31175): loss=0.0005637375185092026\n",
      "Stochastic Gradient Descent(31176): loss=19.232920300197833\n",
      "Stochastic Gradient Descent(31177): loss=0.5623392999464331\n",
      "Stochastic Gradient Descent(31178): loss=3.549790210099557\n",
      "Stochastic Gradient Descent(31179): loss=15.04391657879195\n",
      "Stochastic Gradient Descent(31180): loss=0.15316118584426486\n",
      "Stochastic Gradient Descent(31181): loss=11.162523810486967\n",
      "Stochastic Gradient Descent(31182): loss=0.34703266782863285\n",
      "Stochastic Gradient Descent(31183): loss=14.04397952420859\n",
      "Stochastic Gradient Descent(31184): loss=0.9420916462807502\n",
      "Stochastic Gradient Descent(31185): loss=0.48158779155538606\n",
      "Stochastic Gradient Descent(31186): loss=0.062166658181579966\n",
      "Stochastic Gradient Descent(31187): loss=0.9372341069183234\n",
      "Stochastic Gradient Descent(31188): loss=1.1220846940427727\n",
      "Stochastic Gradient Descent(31189): loss=0.21139066929133166\n",
      "Stochastic Gradient Descent(31190): loss=0.06613028797188554\n",
      "Stochastic Gradient Descent(31191): loss=0.07772167994809924\n",
      "Stochastic Gradient Descent(31192): loss=0.0010614540713145785\n",
      "Stochastic Gradient Descent(31193): loss=4.483583094640132\n",
      "Stochastic Gradient Descent(31194): loss=3.4765583105510545\n",
      "Stochastic Gradient Descent(31195): loss=8.08244993267574\n",
      "Stochastic Gradient Descent(31196): loss=0.7697931663955593\n",
      "Stochastic Gradient Descent(31197): loss=0.6156077075524227\n",
      "Stochastic Gradient Descent(31198): loss=3.323121242002642\n",
      "Stochastic Gradient Descent(31199): loss=0.01851399064227555\n",
      "Stochastic Gradient Descent(31200): loss=7.208821988751963\n",
      "Stochastic Gradient Descent(31201): loss=0.03583258284278256\n",
      "Stochastic Gradient Descent(31202): loss=3.4968290506503905\n",
      "Stochastic Gradient Descent(31203): loss=3.5148891372606466\n",
      "Stochastic Gradient Descent(31204): loss=0.5940028297003723\n",
      "Stochastic Gradient Descent(31205): loss=0.10983038789642649\n",
      "Stochastic Gradient Descent(31206): loss=0.4676171534390584\n",
      "Stochastic Gradient Descent(31207): loss=1.9203858017279831\n",
      "Stochastic Gradient Descent(31208): loss=0.11276476007918022\n",
      "Stochastic Gradient Descent(31209): loss=0.3173186478619744\n",
      "Stochastic Gradient Descent(31210): loss=0.46792137408935125\n",
      "Stochastic Gradient Descent(31211): loss=0.5369705685763688\n",
      "Stochastic Gradient Descent(31212): loss=10.001045136940567\n",
      "Stochastic Gradient Descent(31213): loss=0.11311937058518448\n",
      "Stochastic Gradient Descent(31214): loss=0.6591843204368012\n",
      "Stochastic Gradient Descent(31215): loss=11.298620013963752\n",
      "Stochastic Gradient Descent(31216): loss=1.2577360089253795\n",
      "Stochastic Gradient Descent(31217): loss=3.447274745038187\n",
      "Stochastic Gradient Descent(31218): loss=0.6544150881524656\n",
      "Stochastic Gradient Descent(31219): loss=2.574165962046567\n",
      "Stochastic Gradient Descent(31220): loss=3.6756800085960433\n",
      "Stochastic Gradient Descent(31221): loss=0.24165865273845302\n",
      "Stochastic Gradient Descent(31222): loss=0.5093028197748957\n",
      "Stochastic Gradient Descent(31223): loss=1.3122105447652008\n",
      "Stochastic Gradient Descent(31224): loss=5.625236334329458\n",
      "Stochastic Gradient Descent(31225): loss=1.9046365881901406\n",
      "Stochastic Gradient Descent(31226): loss=18.2162835196999\n",
      "Stochastic Gradient Descent(31227): loss=0.6804650845380416\n",
      "Stochastic Gradient Descent(31228): loss=2.0239392222571104\n",
      "Stochastic Gradient Descent(31229): loss=3.408586901813951\n",
      "Stochastic Gradient Descent(31230): loss=3.6465993206698113\n",
      "Stochastic Gradient Descent(31231): loss=0.8162662946184771\n",
      "Stochastic Gradient Descent(31232): loss=0.5770752714705837\n",
      "Stochastic Gradient Descent(31233): loss=3.9696472090536794\n",
      "Stochastic Gradient Descent(31234): loss=2.919132415551261\n",
      "Stochastic Gradient Descent(31235): loss=11.383972370715771\n",
      "Stochastic Gradient Descent(31236): loss=5.189591564824818\n",
      "Stochastic Gradient Descent(31237): loss=0.12105470411963712\n",
      "Stochastic Gradient Descent(31238): loss=0.6769746389804304\n",
      "Stochastic Gradient Descent(31239): loss=0.4007739063426818\n",
      "Stochastic Gradient Descent(31240): loss=23.96556225614949\n",
      "Stochastic Gradient Descent(31241): loss=7.793395153578496\n",
      "Stochastic Gradient Descent(31242): loss=5.403037410862411\n",
      "Stochastic Gradient Descent(31243): loss=0.10081360875339529\n",
      "Stochastic Gradient Descent(31244): loss=3.0107164029225477\n",
      "Stochastic Gradient Descent(31245): loss=6.4922908677001026\n",
      "Stochastic Gradient Descent(31246): loss=0.03120958113273856\n",
      "Stochastic Gradient Descent(31247): loss=0.14667654017350018\n",
      "Stochastic Gradient Descent(31248): loss=6.295950283451911\n",
      "Stochastic Gradient Descent(31249): loss=5.9364228817905875\n",
      "Stochastic Gradient Descent(31250): loss=28.096192260770213\n",
      "Stochastic Gradient Descent(31251): loss=26.042253176520884\n",
      "Stochastic Gradient Descent(31252): loss=2.083235852918753\n",
      "Stochastic Gradient Descent(31253): loss=14.2987655710977\n",
      "Stochastic Gradient Descent(31254): loss=3.1724423233723287\n",
      "Stochastic Gradient Descent(31255): loss=2.7544847965389\n",
      "Stochastic Gradient Descent(31256): loss=0.656892664831808\n",
      "Stochastic Gradient Descent(31257): loss=0.640444494185175\n",
      "Stochastic Gradient Descent(31258): loss=2.4396491590358305\n",
      "Stochastic Gradient Descent(31259): loss=8.015713787989297\n",
      "Stochastic Gradient Descent(31260): loss=2.6290548789293986\n",
      "Stochastic Gradient Descent(31261): loss=0.030214179143844774\n",
      "Stochastic Gradient Descent(31262): loss=0.5493521005797648\n",
      "Stochastic Gradient Descent(31263): loss=9.596298318648167\n",
      "Stochastic Gradient Descent(31264): loss=0.5728763591391234\n",
      "Stochastic Gradient Descent(31265): loss=1.2154092574185271\n",
      "Stochastic Gradient Descent(31266): loss=0.10700931739984688\n",
      "Stochastic Gradient Descent(31267): loss=0.020214470901779476\n",
      "Stochastic Gradient Descent(31268): loss=2.11921096806791\n",
      "Stochastic Gradient Descent(31269): loss=0.9190903267597514\n",
      "Stochastic Gradient Descent(31270): loss=1.5202477800908012\n",
      "Stochastic Gradient Descent(31271): loss=11.887944214139957\n",
      "Stochastic Gradient Descent(31272): loss=0.26362395513257036\n",
      "Stochastic Gradient Descent(31273): loss=0.025875064878478172\n",
      "Stochastic Gradient Descent(31274): loss=0.7412764367548265\n",
      "Stochastic Gradient Descent(31275): loss=0.04741444497350583\n",
      "Stochastic Gradient Descent(31276): loss=2.6989346508699756\n",
      "Stochastic Gradient Descent(31277): loss=10.803483276134829\n",
      "Stochastic Gradient Descent(31278): loss=1.3096210572658755\n",
      "Stochastic Gradient Descent(31279): loss=4.283057613537282\n",
      "Stochastic Gradient Descent(31280): loss=0.22276931105226008\n",
      "Stochastic Gradient Descent(31281): loss=0.043604653619710255\n",
      "Stochastic Gradient Descent(31282): loss=0.028653678632659274\n",
      "Stochastic Gradient Descent(31283): loss=1.194861088627709\n",
      "Stochastic Gradient Descent(31284): loss=0.12359901862738117\n",
      "Stochastic Gradient Descent(31285): loss=14.199963870846465\n",
      "Stochastic Gradient Descent(31286): loss=0.03517643847938063\n",
      "Stochastic Gradient Descent(31287): loss=0.19587409879501558\n",
      "Stochastic Gradient Descent(31288): loss=8.192622778903663\n",
      "Stochastic Gradient Descent(31289): loss=0.6784123390969664\n",
      "Stochastic Gradient Descent(31290): loss=1.7800938019744403\n",
      "Stochastic Gradient Descent(31291): loss=5.212179552834702\n",
      "Stochastic Gradient Descent(31292): loss=3.574753871535921\n",
      "Stochastic Gradient Descent(31293): loss=0.22830477119939013\n",
      "Stochastic Gradient Descent(31294): loss=0.633042536680153\n",
      "Stochastic Gradient Descent(31295): loss=6.504636199945679\n",
      "Stochastic Gradient Descent(31296): loss=0.8463415842496467\n",
      "Stochastic Gradient Descent(31297): loss=0.0002710127349317031\n",
      "Stochastic Gradient Descent(31298): loss=1.3729664620279518\n",
      "Stochastic Gradient Descent(31299): loss=0.6257010273848669\n",
      "Stochastic Gradient Descent(31300): loss=3.8275280288073046\n",
      "Stochastic Gradient Descent(31301): loss=3.5647031123070625\n",
      "Stochastic Gradient Descent(31302): loss=0.4720820049234815\n",
      "Stochastic Gradient Descent(31303): loss=2.0737417979741655\n",
      "Stochastic Gradient Descent(31304): loss=3.060239684214578\n",
      "Stochastic Gradient Descent(31305): loss=27.526492987268142\n",
      "Stochastic Gradient Descent(31306): loss=3.8711235933685044\n",
      "Stochastic Gradient Descent(31307): loss=1.987266452989807\n",
      "Stochastic Gradient Descent(31308): loss=7.356613840084026\n",
      "Stochastic Gradient Descent(31309): loss=3.8798739885028732\n",
      "Stochastic Gradient Descent(31310): loss=1.1214744595172392\n",
      "Stochastic Gradient Descent(31311): loss=1.0259861307664848\n",
      "Stochastic Gradient Descent(31312): loss=7.267007303432634\n",
      "Stochastic Gradient Descent(31313): loss=0.47397197739250885\n",
      "Stochastic Gradient Descent(31314): loss=4.533826082954759\n",
      "Stochastic Gradient Descent(31315): loss=3.745461980183229\n",
      "Stochastic Gradient Descent(31316): loss=0.0008313732355955789\n",
      "Stochastic Gradient Descent(31317): loss=5.793278859549013\n",
      "Stochastic Gradient Descent(31318): loss=0.9794690926183768\n",
      "Stochastic Gradient Descent(31319): loss=0.6487116123832448\n",
      "Stochastic Gradient Descent(31320): loss=5.569651054610226\n",
      "Stochastic Gradient Descent(31321): loss=4.306808578873141\n",
      "Stochastic Gradient Descent(31322): loss=5.292278513757263\n",
      "Stochastic Gradient Descent(31323): loss=5.874213323248704\n",
      "Stochastic Gradient Descent(31324): loss=3.0582983720067647\n",
      "Stochastic Gradient Descent(31325): loss=0.16934787344996471\n",
      "Stochastic Gradient Descent(31326): loss=0.462097239379585\n",
      "Stochastic Gradient Descent(31327): loss=0.07451535286547126\n",
      "Stochastic Gradient Descent(31328): loss=6.7948578006524505\n",
      "Stochastic Gradient Descent(31329): loss=0.058912963739777044\n",
      "Stochastic Gradient Descent(31330): loss=0.11126811584012627\n",
      "Stochastic Gradient Descent(31331): loss=4.861655118289442\n",
      "Stochastic Gradient Descent(31332): loss=15.58448964687888\n",
      "Stochastic Gradient Descent(31333): loss=0.3685155877442596\n",
      "Stochastic Gradient Descent(31334): loss=4.001846575037351\n",
      "Stochastic Gradient Descent(31335): loss=0.8215519882921929\n",
      "Stochastic Gradient Descent(31336): loss=10.516684427485773\n",
      "Stochastic Gradient Descent(31337): loss=0.02449417421066004\n",
      "Stochastic Gradient Descent(31338): loss=1.1196294212271443\n",
      "Stochastic Gradient Descent(31339): loss=3.418349636913286\n",
      "Stochastic Gradient Descent(31340): loss=2.919672320269489\n",
      "Stochastic Gradient Descent(31341): loss=0.00982661092059057\n",
      "Stochastic Gradient Descent(31342): loss=0.4255094936529927\n",
      "Stochastic Gradient Descent(31343): loss=0.16327451434187681\n",
      "Stochastic Gradient Descent(31344): loss=4.538696124624354\n",
      "Stochastic Gradient Descent(31345): loss=1.9876311364820074\n",
      "Stochastic Gradient Descent(31346): loss=0.011734714809735414\n",
      "Stochastic Gradient Descent(31347): loss=7.960215674187448\n",
      "Stochastic Gradient Descent(31348): loss=1.2491155904895321\n",
      "Stochastic Gradient Descent(31349): loss=1.4686937608075021\n",
      "Stochastic Gradient Descent(31350): loss=1.4139432680339454\n",
      "Stochastic Gradient Descent(31351): loss=2.591334912715983\n",
      "Stochastic Gradient Descent(31352): loss=2.3766085532388903\n",
      "Stochastic Gradient Descent(31353): loss=2.970517337747487\n",
      "Stochastic Gradient Descent(31354): loss=0.2901344004554057\n",
      "Stochastic Gradient Descent(31355): loss=0.27682985405829214\n",
      "Stochastic Gradient Descent(31356): loss=0.1992581775771377\n",
      "Stochastic Gradient Descent(31357): loss=1.462461627396365\n",
      "Stochastic Gradient Descent(31358): loss=4.975116677662184\n",
      "Stochastic Gradient Descent(31359): loss=0.8633120410300656\n",
      "Stochastic Gradient Descent(31360): loss=10.126241792458206\n",
      "Stochastic Gradient Descent(31361): loss=3.222223548462781\n",
      "Stochastic Gradient Descent(31362): loss=22.6175781780855\n",
      "Stochastic Gradient Descent(31363): loss=6.49952499921067\n",
      "Stochastic Gradient Descent(31364): loss=0.7037263024253136\n",
      "Stochastic Gradient Descent(31365): loss=2.194281267739411\n",
      "Stochastic Gradient Descent(31366): loss=19.797223724600936\n",
      "Stochastic Gradient Descent(31367): loss=3.4805342564383683\n",
      "Stochastic Gradient Descent(31368): loss=11.080631004754698\n",
      "Stochastic Gradient Descent(31369): loss=8.090560605660135\n",
      "Stochastic Gradient Descent(31370): loss=1.0354094229736117\n",
      "Stochastic Gradient Descent(31371): loss=0.04347375746114502\n",
      "Stochastic Gradient Descent(31372): loss=3.725382584519968\n",
      "Stochastic Gradient Descent(31373): loss=0.14893338627077973\n",
      "Stochastic Gradient Descent(31374): loss=34.31534207724192\n",
      "Stochastic Gradient Descent(31375): loss=0.0046650938905451085\n",
      "Stochastic Gradient Descent(31376): loss=5.831534063149333\n",
      "Stochastic Gradient Descent(31377): loss=0.5255599933964332\n",
      "Stochastic Gradient Descent(31378): loss=9.896539533948097\n",
      "Stochastic Gradient Descent(31379): loss=0.00026862753294551445\n",
      "Stochastic Gradient Descent(31380): loss=1.4023705655746643\n",
      "Stochastic Gradient Descent(31381): loss=0.13511225703196214\n",
      "Stochastic Gradient Descent(31382): loss=1.2808270521807226\n",
      "Stochastic Gradient Descent(31383): loss=0.13133541110037475\n",
      "Stochastic Gradient Descent(31384): loss=5.63885115859909\n",
      "Stochastic Gradient Descent(31385): loss=4.451462103072456\n",
      "Stochastic Gradient Descent(31386): loss=3.077685382676258\n",
      "Stochastic Gradient Descent(31387): loss=0.09291260045147\n",
      "Stochastic Gradient Descent(31388): loss=4.948451685158878\n",
      "Stochastic Gradient Descent(31389): loss=3.108027482974825\n",
      "Stochastic Gradient Descent(31390): loss=11.019282087850677\n",
      "Stochastic Gradient Descent(31391): loss=17.215428332503972\n",
      "Stochastic Gradient Descent(31392): loss=0.9280267443296206\n",
      "Stochastic Gradient Descent(31393): loss=0.8936129897665208\n",
      "Stochastic Gradient Descent(31394): loss=0.5320821487270823\n",
      "Stochastic Gradient Descent(31395): loss=4.815985496625008\n",
      "Stochastic Gradient Descent(31396): loss=10.756997184312421\n",
      "Stochastic Gradient Descent(31397): loss=0.6144251975206875\n",
      "Stochastic Gradient Descent(31398): loss=1.644367519146526\n",
      "Stochastic Gradient Descent(31399): loss=8.529233786839912\n",
      "Stochastic Gradient Descent(31400): loss=0.5122636057356811\n",
      "Stochastic Gradient Descent(31401): loss=0.09493664383651007\n",
      "Stochastic Gradient Descent(31402): loss=0.12682071402635428\n",
      "Stochastic Gradient Descent(31403): loss=0.1226839297061064\n",
      "Stochastic Gradient Descent(31404): loss=0.39614309414379717\n",
      "Stochastic Gradient Descent(31405): loss=2.169973882459685\n",
      "Stochastic Gradient Descent(31406): loss=21.17422065092062\n",
      "Stochastic Gradient Descent(31407): loss=0.3470998010503845\n",
      "Stochastic Gradient Descent(31408): loss=3.315175766916611\n",
      "Stochastic Gradient Descent(31409): loss=2.095704044852729\n",
      "Stochastic Gradient Descent(31410): loss=6.224910083302301\n",
      "Stochastic Gradient Descent(31411): loss=11.362567716967563\n",
      "Stochastic Gradient Descent(31412): loss=3.2759679784361695\n",
      "Stochastic Gradient Descent(31413): loss=0.4505758909065797\n",
      "Stochastic Gradient Descent(31414): loss=2.433692482255644\n",
      "Stochastic Gradient Descent(31415): loss=1.0423592472990997\n",
      "Stochastic Gradient Descent(31416): loss=7.240232764114639\n",
      "Stochastic Gradient Descent(31417): loss=4.902429959407448\n",
      "Stochastic Gradient Descent(31418): loss=7.097097474657223\n",
      "Stochastic Gradient Descent(31419): loss=1.7917174088177914\n",
      "Stochastic Gradient Descent(31420): loss=0.2933894034819404\n",
      "Stochastic Gradient Descent(31421): loss=3.0059061426963125\n",
      "Stochastic Gradient Descent(31422): loss=0.8714950364057021\n",
      "Stochastic Gradient Descent(31423): loss=0.0048633232685139525\n",
      "Stochastic Gradient Descent(31424): loss=3.387761887612391\n",
      "Stochastic Gradient Descent(31425): loss=0.14496179771351633\n",
      "Stochastic Gradient Descent(31426): loss=1.6800102350315045\n",
      "Stochastic Gradient Descent(31427): loss=9.315606296388221\n",
      "Stochastic Gradient Descent(31428): loss=10.336023115959998\n",
      "Stochastic Gradient Descent(31429): loss=0.07083502764645525\n",
      "Stochastic Gradient Descent(31430): loss=1.0483456714560604\n",
      "Stochastic Gradient Descent(31431): loss=7.423362811140666\n",
      "Stochastic Gradient Descent(31432): loss=4.331595765310098\n",
      "Stochastic Gradient Descent(31433): loss=2.310227614094528\n",
      "Stochastic Gradient Descent(31434): loss=5.406419024449486\n",
      "Stochastic Gradient Descent(31435): loss=1.974174428763162\n",
      "Stochastic Gradient Descent(31436): loss=0.7240318674861166\n",
      "Stochastic Gradient Descent(31437): loss=9.601217234022148\n",
      "Stochastic Gradient Descent(31438): loss=0.005181069288661426\n",
      "Stochastic Gradient Descent(31439): loss=2.6173453708417678\n",
      "Stochastic Gradient Descent(31440): loss=0.0013553897120087061\n",
      "Stochastic Gradient Descent(31441): loss=7.185090563327899\n",
      "Stochastic Gradient Descent(31442): loss=13.682659337128964\n",
      "Stochastic Gradient Descent(31443): loss=0.39867837222791946\n",
      "Stochastic Gradient Descent(31444): loss=0.003357685996890258\n",
      "Stochastic Gradient Descent(31445): loss=0.7474063211964267\n",
      "Stochastic Gradient Descent(31446): loss=2.0698765437135855\n",
      "Stochastic Gradient Descent(31447): loss=7.100697245331163\n",
      "Stochastic Gradient Descent(31448): loss=0.0198879060266198\n",
      "Stochastic Gradient Descent(31449): loss=2.486460891268949\n",
      "Stochastic Gradient Descent(31450): loss=6.903568401375546\n",
      "Stochastic Gradient Descent(31451): loss=4.422231048719321\n",
      "Stochastic Gradient Descent(31452): loss=1.6996187653923085\n",
      "Stochastic Gradient Descent(31453): loss=2.197631239851976\n",
      "Stochastic Gradient Descent(31454): loss=15.449272546172201\n",
      "Stochastic Gradient Descent(31455): loss=1.3926084877538585\n",
      "Stochastic Gradient Descent(31456): loss=1.8959236152565588\n",
      "Stochastic Gradient Descent(31457): loss=0.3348805635388344\n",
      "Stochastic Gradient Descent(31458): loss=5.268023204996715\n",
      "Stochastic Gradient Descent(31459): loss=0.23791328216353977\n",
      "Stochastic Gradient Descent(31460): loss=1.7094860464809603\n",
      "Stochastic Gradient Descent(31461): loss=1.4114186119195276\n",
      "Stochastic Gradient Descent(31462): loss=0.6177982722687004\n",
      "Stochastic Gradient Descent(31463): loss=10.89218252069145\n",
      "Stochastic Gradient Descent(31464): loss=7.8342093702745155\n",
      "Stochastic Gradient Descent(31465): loss=12.003480575238783\n",
      "Stochastic Gradient Descent(31466): loss=0.815968323967058\n",
      "Stochastic Gradient Descent(31467): loss=5.563701253741177\n",
      "Stochastic Gradient Descent(31468): loss=1.2773123403349478\n",
      "Stochastic Gradient Descent(31469): loss=0.4894599741733636\n",
      "Stochastic Gradient Descent(31470): loss=0.05070824751590952\n",
      "Stochastic Gradient Descent(31471): loss=2.8126452678341023\n",
      "Stochastic Gradient Descent(31472): loss=2.703459706379682\n",
      "Stochastic Gradient Descent(31473): loss=0.0004151933259477467\n",
      "Stochastic Gradient Descent(31474): loss=0.08710619517550962\n",
      "Stochastic Gradient Descent(31475): loss=1.6387679436870646\n",
      "Stochastic Gradient Descent(31476): loss=1.0149229852297241\n",
      "Stochastic Gradient Descent(31477): loss=0.9479115679277076\n",
      "Stochastic Gradient Descent(31478): loss=2.096509309984657\n",
      "Stochastic Gradient Descent(31479): loss=4.249818795123605\n",
      "Stochastic Gradient Descent(31480): loss=2.701990442358024\n",
      "Stochastic Gradient Descent(31481): loss=4.175787082946463\n",
      "Stochastic Gradient Descent(31482): loss=6.023808869648542\n",
      "Stochastic Gradient Descent(31483): loss=7.841701446212895\n",
      "Stochastic Gradient Descent(31484): loss=7.559108988916334\n",
      "Stochastic Gradient Descent(31485): loss=0.9484021967362964\n",
      "Stochastic Gradient Descent(31486): loss=0.42000654073378113\n",
      "Stochastic Gradient Descent(31487): loss=0.4153029648395737\n",
      "Stochastic Gradient Descent(31488): loss=4.812405207532928\n",
      "Stochastic Gradient Descent(31489): loss=8.934675570439722\n",
      "Stochastic Gradient Descent(31490): loss=1.107595001073514\n",
      "Stochastic Gradient Descent(31491): loss=3.534791795421142\n",
      "Stochastic Gradient Descent(31492): loss=0.3580740262192803\n",
      "Stochastic Gradient Descent(31493): loss=0.7083751145383315\n",
      "Stochastic Gradient Descent(31494): loss=8.970267128515976\n",
      "Stochastic Gradient Descent(31495): loss=2.5807774513203\n",
      "Stochastic Gradient Descent(31496): loss=1.064616819376416\n",
      "Stochastic Gradient Descent(31497): loss=3.3633233851682105\n",
      "Stochastic Gradient Descent(31498): loss=12.027548334940924\n",
      "Stochastic Gradient Descent(31499): loss=0.9276111005571889\n",
      "Stochastic Gradient Descent(31500): loss=0.0020535837069290065\n",
      "Stochastic Gradient Descent(31501): loss=1.3314575542544764\n",
      "Stochastic Gradient Descent(31502): loss=0.23682805702548398\n",
      "Stochastic Gradient Descent(31503): loss=0.00014794457554191437\n",
      "Stochastic Gradient Descent(31504): loss=3.9173536802452236\n",
      "Stochastic Gradient Descent(31505): loss=1.534227808222998\n",
      "Stochastic Gradient Descent(31506): loss=2.0899399660313014\n",
      "Stochastic Gradient Descent(31507): loss=0.3181826345725588\n",
      "Stochastic Gradient Descent(31508): loss=6.426752098264402\n",
      "Stochastic Gradient Descent(31509): loss=1.5246259168618912\n",
      "Stochastic Gradient Descent(31510): loss=0.005670436078028905\n",
      "Stochastic Gradient Descent(31511): loss=7.708302492341037\n",
      "Stochastic Gradient Descent(31512): loss=0.6243015329162194\n",
      "Stochastic Gradient Descent(31513): loss=1.4186705618636546\n",
      "Stochastic Gradient Descent(31514): loss=0.7047171204177465\n",
      "Stochastic Gradient Descent(31515): loss=3.5560308854968574\n",
      "Stochastic Gradient Descent(31516): loss=19.171670801724595\n",
      "Stochastic Gradient Descent(31517): loss=0.0499002091148032\n",
      "Stochastic Gradient Descent(31518): loss=3.980681650559305\n",
      "Stochastic Gradient Descent(31519): loss=7.924500886886149\n",
      "Stochastic Gradient Descent(31520): loss=5.787261408228443\n",
      "Stochastic Gradient Descent(31521): loss=2.0407077508104314\n",
      "Stochastic Gradient Descent(31522): loss=0.14879159499056863\n",
      "Stochastic Gradient Descent(31523): loss=1.1667227827383744\n",
      "Stochastic Gradient Descent(31524): loss=6.435824021323761\n",
      "Stochastic Gradient Descent(31525): loss=1.6511750804771015\n",
      "Stochastic Gradient Descent(31526): loss=10.475668482146371\n",
      "Stochastic Gradient Descent(31527): loss=4.557013235784162\n",
      "Stochastic Gradient Descent(31528): loss=0.01630416594102441\n",
      "Stochastic Gradient Descent(31529): loss=0.31884342902836044\n",
      "Stochastic Gradient Descent(31530): loss=34.21109377838007\n",
      "Stochastic Gradient Descent(31531): loss=0.6164626558300921\n",
      "Stochastic Gradient Descent(31532): loss=0.9443909127991849\n",
      "Stochastic Gradient Descent(31533): loss=31.02814371623712\n",
      "Stochastic Gradient Descent(31534): loss=2.191689349959451\n",
      "Stochastic Gradient Descent(31535): loss=5.597767885885648\n",
      "Stochastic Gradient Descent(31536): loss=0.5317635801801901\n",
      "Stochastic Gradient Descent(31537): loss=4.982529379874666\n",
      "Stochastic Gradient Descent(31538): loss=1.940714058041126\n",
      "Stochastic Gradient Descent(31539): loss=11.112937008375512\n",
      "Stochastic Gradient Descent(31540): loss=4.344510926299916\n",
      "Stochastic Gradient Descent(31541): loss=15.780883820244934\n",
      "Stochastic Gradient Descent(31542): loss=1.2980301043787035\n",
      "Stochastic Gradient Descent(31543): loss=4.334238007481153\n",
      "Stochastic Gradient Descent(31544): loss=1.988002317892711\n",
      "Stochastic Gradient Descent(31545): loss=0.2871263553884536\n",
      "Stochastic Gradient Descent(31546): loss=0.05118919894195533\n",
      "Stochastic Gradient Descent(31547): loss=1.269436824826017\n",
      "Stochastic Gradient Descent(31548): loss=0.018696940127779303\n",
      "Stochastic Gradient Descent(31549): loss=0.16064852247702038\n",
      "Stochastic Gradient Descent(31550): loss=0.061350251340797544\n",
      "Stochastic Gradient Descent(31551): loss=10.806278452025532\n",
      "Stochastic Gradient Descent(31552): loss=3.9871917681049465\n",
      "Stochastic Gradient Descent(31553): loss=1.008667684817588\n",
      "Stochastic Gradient Descent(31554): loss=8.14286502419353\n",
      "Stochastic Gradient Descent(31555): loss=0.015883490058415793\n",
      "Stochastic Gradient Descent(31556): loss=6.732071534862237\n",
      "Stochastic Gradient Descent(31557): loss=9.608699360515816\n",
      "Stochastic Gradient Descent(31558): loss=10.23650119843569\n",
      "Stochastic Gradient Descent(31559): loss=0.0008640003207443068\n",
      "Stochastic Gradient Descent(31560): loss=7.489225191316948\n",
      "Stochastic Gradient Descent(31561): loss=16.762257640573182\n",
      "Stochastic Gradient Descent(31562): loss=6.792135510696256\n",
      "Stochastic Gradient Descent(31563): loss=4.600109018492781\n",
      "Stochastic Gradient Descent(31564): loss=0.09699355916407117\n",
      "Stochastic Gradient Descent(31565): loss=12.62130062031408\n",
      "Stochastic Gradient Descent(31566): loss=0.12577183508526849\n",
      "Stochastic Gradient Descent(31567): loss=0.10386880317105657\n",
      "Stochastic Gradient Descent(31568): loss=0.012736997155819704\n",
      "Stochastic Gradient Descent(31569): loss=0.004421579757873699\n",
      "Stochastic Gradient Descent(31570): loss=19.85428634881966\n",
      "Stochastic Gradient Descent(31571): loss=13.406970453959891\n",
      "Stochastic Gradient Descent(31572): loss=1.1862092892978364\n",
      "Stochastic Gradient Descent(31573): loss=8.375644064083284\n",
      "Stochastic Gradient Descent(31574): loss=0.9415772763722547\n",
      "Stochastic Gradient Descent(31575): loss=2.8963430034782347e-06\n",
      "Stochastic Gradient Descent(31576): loss=0.3950114125528958\n",
      "Stochastic Gradient Descent(31577): loss=1.2956722688114293\n",
      "Stochastic Gradient Descent(31578): loss=18.830802136082124\n",
      "Stochastic Gradient Descent(31579): loss=1.3057928713651967\n",
      "Stochastic Gradient Descent(31580): loss=1.845386460060863\n",
      "Stochastic Gradient Descent(31581): loss=0.10147449063661534\n",
      "Stochastic Gradient Descent(31582): loss=0.18921235972536687\n",
      "Stochastic Gradient Descent(31583): loss=15.72316887926691\n",
      "Stochastic Gradient Descent(31584): loss=1.1786931472035744\n",
      "Stochastic Gradient Descent(31585): loss=1.483728908669945\n",
      "Stochastic Gradient Descent(31586): loss=12.945852004981745\n",
      "Stochastic Gradient Descent(31587): loss=9.07476747615496\n",
      "Stochastic Gradient Descent(31588): loss=1.4193496429676347\n",
      "Stochastic Gradient Descent(31589): loss=0.214624114315567\n",
      "Stochastic Gradient Descent(31590): loss=0.01811170791020762\n",
      "Stochastic Gradient Descent(31591): loss=0.2535982781046604\n",
      "Stochastic Gradient Descent(31592): loss=0.02863604280357796\n",
      "Stochastic Gradient Descent(31593): loss=2.358777190570144\n",
      "Stochastic Gradient Descent(31594): loss=1.2238520717523258\n",
      "Stochastic Gradient Descent(31595): loss=0.4598013227320019\n",
      "Stochastic Gradient Descent(31596): loss=4.980670599005969\n",
      "Stochastic Gradient Descent(31597): loss=32.25832562210917\n",
      "Stochastic Gradient Descent(31598): loss=4.377429930077518\n",
      "Stochastic Gradient Descent(31599): loss=3.6374382514972035\n",
      "Stochastic Gradient Descent(31600): loss=2.4555657037333005\n",
      "Stochastic Gradient Descent(31601): loss=0.16640631883812396\n",
      "Stochastic Gradient Descent(31602): loss=10.563235407981857\n",
      "Stochastic Gradient Descent(31603): loss=0.1693675314274261\n",
      "Stochastic Gradient Descent(31604): loss=5.175975567534724\n",
      "Stochastic Gradient Descent(31605): loss=0.201032551484476\n",
      "Stochastic Gradient Descent(31606): loss=1.2928576972241625\n",
      "Stochastic Gradient Descent(31607): loss=3.26208081504968\n",
      "Stochastic Gradient Descent(31608): loss=2.3089613287062067\n",
      "Stochastic Gradient Descent(31609): loss=2.3304330637832735\n",
      "Stochastic Gradient Descent(31610): loss=0.9693140063844347\n",
      "Stochastic Gradient Descent(31611): loss=0.19956092900333228\n",
      "Stochastic Gradient Descent(31612): loss=5.348884957430266\n",
      "Stochastic Gradient Descent(31613): loss=15.176450088597576\n",
      "Stochastic Gradient Descent(31614): loss=0.5922333130514741\n",
      "Stochastic Gradient Descent(31615): loss=10.421846147784498\n",
      "Stochastic Gradient Descent(31616): loss=1.0525140662624168\n",
      "Stochastic Gradient Descent(31617): loss=0.2454808060559141\n",
      "Stochastic Gradient Descent(31618): loss=8.777617260519344\n",
      "Stochastic Gradient Descent(31619): loss=1.8400493368425879\n",
      "Stochastic Gradient Descent(31620): loss=1.0665392456323655\n",
      "Stochastic Gradient Descent(31621): loss=5.06739656210038\n",
      "Stochastic Gradient Descent(31622): loss=0.46250359361310717\n",
      "Stochastic Gradient Descent(31623): loss=1.6068739581533258\n",
      "Stochastic Gradient Descent(31624): loss=31.378979183337908\n",
      "Stochastic Gradient Descent(31625): loss=0.00010669141419652502\n",
      "Stochastic Gradient Descent(31626): loss=9.0662596097948\n",
      "Stochastic Gradient Descent(31627): loss=3.149209155524029\n",
      "Stochastic Gradient Descent(31628): loss=0.2254098118013382\n",
      "Stochastic Gradient Descent(31629): loss=0.4339556916295056\n",
      "Stochastic Gradient Descent(31630): loss=0.812750697894169\n",
      "Stochastic Gradient Descent(31631): loss=32.70255206896309\n",
      "Stochastic Gradient Descent(31632): loss=0.6961350077100853\n",
      "Stochastic Gradient Descent(31633): loss=12.015741997812173\n",
      "Stochastic Gradient Descent(31634): loss=5.905450706902266\n",
      "Stochastic Gradient Descent(31635): loss=21.135635917703635\n",
      "Stochastic Gradient Descent(31636): loss=0.34868570236863405\n",
      "Stochastic Gradient Descent(31637): loss=1.5311846842726604\n",
      "Stochastic Gradient Descent(31638): loss=1.0876721604988646\n",
      "Stochastic Gradient Descent(31639): loss=0.49234121173357265\n",
      "Stochastic Gradient Descent(31640): loss=0.017135686867117295\n",
      "Stochastic Gradient Descent(31641): loss=1.6299198507569035\n",
      "Stochastic Gradient Descent(31642): loss=5.441184335577531\n",
      "Stochastic Gradient Descent(31643): loss=23.78533731714939\n",
      "Stochastic Gradient Descent(31644): loss=17.475584994555536\n",
      "Stochastic Gradient Descent(31645): loss=0.9697981006814964\n",
      "Stochastic Gradient Descent(31646): loss=6.2686787636534085\n",
      "Stochastic Gradient Descent(31647): loss=2.6815896433103994\n",
      "Stochastic Gradient Descent(31648): loss=0.05872754585433592\n",
      "Stochastic Gradient Descent(31649): loss=1.9325438305534302\n",
      "Stochastic Gradient Descent(31650): loss=0.5499598962444693\n",
      "Stochastic Gradient Descent(31651): loss=0.1389289983775425\n",
      "Stochastic Gradient Descent(31652): loss=8.84366806550085\n",
      "Stochastic Gradient Descent(31653): loss=0.10134616561577106\n",
      "Stochastic Gradient Descent(31654): loss=0.3150424678823618\n",
      "Stochastic Gradient Descent(31655): loss=32.947470429802735\n",
      "Stochastic Gradient Descent(31656): loss=0.07582224543465586\n",
      "Stochastic Gradient Descent(31657): loss=6.877502196465062\n",
      "Stochastic Gradient Descent(31658): loss=0.22483353417338545\n",
      "Stochastic Gradient Descent(31659): loss=0.8621912984605957\n",
      "Stochastic Gradient Descent(31660): loss=0.7302916172091614\n",
      "Stochastic Gradient Descent(31661): loss=3.7818416121270046\n",
      "Stochastic Gradient Descent(31662): loss=1.7396395113388399\n",
      "Stochastic Gradient Descent(31663): loss=1.2518590852750906\n",
      "Stochastic Gradient Descent(31664): loss=5.180927688178441\n",
      "Stochastic Gradient Descent(31665): loss=0.25129023495875136\n",
      "Stochastic Gradient Descent(31666): loss=0.004984266411814956\n",
      "Stochastic Gradient Descent(31667): loss=0.004428719407201673\n",
      "Stochastic Gradient Descent(31668): loss=1.6106271944542052\n",
      "Stochastic Gradient Descent(31669): loss=0.11017420907459252\n",
      "Stochastic Gradient Descent(31670): loss=0.21700339919151176\n",
      "Stochastic Gradient Descent(31671): loss=0.7145017076580996\n",
      "Stochastic Gradient Descent(31672): loss=0.1431679220515929\n",
      "Stochastic Gradient Descent(31673): loss=3.1868376039026507\n",
      "Stochastic Gradient Descent(31674): loss=0.6394504685609117\n",
      "Stochastic Gradient Descent(31675): loss=12.44901747029301\n",
      "Stochastic Gradient Descent(31676): loss=0.011654818004181982\n",
      "Stochastic Gradient Descent(31677): loss=11.055330430420575\n",
      "Stochastic Gradient Descent(31678): loss=3.8500855371098\n",
      "Stochastic Gradient Descent(31679): loss=0.07653601847161504\n",
      "Stochastic Gradient Descent(31680): loss=2.4911195070840395\n",
      "Stochastic Gradient Descent(31681): loss=1.606993767737522\n",
      "Stochastic Gradient Descent(31682): loss=3.265004819160142\n",
      "Stochastic Gradient Descent(31683): loss=5.649273438218139\n",
      "Stochastic Gradient Descent(31684): loss=0.34757877517461844\n",
      "Stochastic Gradient Descent(31685): loss=0.4493626036408913\n",
      "Stochastic Gradient Descent(31686): loss=2.6417570458709396\n",
      "Stochastic Gradient Descent(31687): loss=2.615466830909716\n",
      "Stochastic Gradient Descent(31688): loss=5.488471496420033\n",
      "Stochastic Gradient Descent(31689): loss=1.6747138970819129\n",
      "Stochastic Gradient Descent(31690): loss=10.769605897441116\n",
      "Stochastic Gradient Descent(31691): loss=5.068960865701777\n",
      "Stochastic Gradient Descent(31692): loss=0.08083485227360813\n",
      "Stochastic Gradient Descent(31693): loss=0.011388813151115488\n",
      "Stochastic Gradient Descent(31694): loss=0.1412179004990359\n",
      "Stochastic Gradient Descent(31695): loss=2.0321664641302783\n",
      "Stochastic Gradient Descent(31696): loss=0.7340418210050917\n",
      "Stochastic Gradient Descent(31697): loss=1.4696400006862866\n",
      "Stochastic Gradient Descent(31698): loss=6.611031046110002\n",
      "Stochastic Gradient Descent(31699): loss=2.760252444544891\n",
      "Stochastic Gradient Descent(31700): loss=0.4376673215166089\n",
      "Stochastic Gradient Descent(31701): loss=0.4978398320081331\n",
      "Stochastic Gradient Descent(31702): loss=0.1678709941514159\n",
      "Stochastic Gradient Descent(31703): loss=1.5575574160698566\n",
      "Stochastic Gradient Descent(31704): loss=4.532008459492156\n",
      "Stochastic Gradient Descent(31705): loss=1.8556907857116236\n",
      "Stochastic Gradient Descent(31706): loss=13.98527643488184\n",
      "Stochastic Gradient Descent(31707): loss=0.9028405881346961\n",
      "Stochastic Gradient Descent(31708): loss=2.607998980892298\n",
      "Stochastic Gradient Descent(31709): loss=1.2925553003713501\n",
      "Stochastic Gradient Descent(31710): loss=1.6561761091681808\n",
      "Stochastic Gradient Descent(31711): loss=0.32234715947161807\n",
      "Stochastic Gradient Descent(31712): loss=0.0978098958535329\n",
      "Stochastic Gradient Descent(31713): loss=2.0528156967718436\n",
      "Stochastic Gradient Descent(31714): loss=1.0807533186124276\n",
      "Stochastic Gradient Descent(31715): loss=13.624518504729915\n",
      "Stochastic Gradient Descent(31716): loss=0.006391234342683628\n",
      "Stochastic Gradient Descent(31717): loss=6.0033499914329544\n",
      "Stochastic Gradient Descent(31718): loss=0.005750752218271332\n",
      "Stochastic Gradient Descent(31719): loss=1.3197944925233753\n",
      "Stochastic Gradient Descent(31720): loss=0.4617044742203887\n",
      "Stochastic Gradient Descent(31721): loss=20.080423227920246\n",
      "Stochastic Gradient Descent(31722): loss=3.8866826644090993\n",
      "Stochastic Gradient Descent(31723): loss=25.26638108271104\n",
      "Stochastic Gradient Descent(31724): loss=0.18788266130817718\n",
      "Stochastic Gradient Descent(31725): loss=10.502452644371512\n",
      "Stochastic Gradient Descent(31726): loss=1.93383475324657\n",
      "Stochastic Gradient Descent(31727): loss=2.358735292429252\n",
      "Stochastic Gradient Descent(31728): loss=0.12199869440043819\n",
      "Stochastic Gradient Descent(31729): loss=17.10339042235572\n",
      "Stochastic Gradient Descent(31730): loss=12.041856756222774\n",
      "Stochastic Gradient Descent(31731): loss=0.12990815100726333\n",
      "Stochastic Gradient Descent(31732): loss=15.083347186287948\n",
      "Stochastic Gradient Descent(31733): loss=0.13126717392191647\n",
      "Stochastic Gradient Descent(31734): loss=1.2103904221810715\n",
      "Stochastic Gradient Descent(31735): loss=3.1898690181887104\n",
      "Stochastic Gradient Descent(31736): loss=0.2673002402224257\n",
      "Stochastic Gradient Descent(31737): loss=2.4403461026917266\n",
      "Stochastic Gradient Descent(31738): loss=12.484963628105064\n",
      "Stochastic Gradient Descent(31739): loss=0.10766381790800518\n",
      "Stochastic Gradient Descent(31740): loss=3.047448779347503\n",
      "Stochastic Gradient Descent(31741): loss=4.4232384974336245\n",
      "Stochastic Gradient Descent(31742): loss=0.9372072719457176\n",
      "Stochastic Gradient Descent(31743): loss=0.783552506043594\n",
      "Stochastic Gradient Descent(31744): loss=0.21893232082472672\n",
      "Stochastic Gradient Descent(31745): loss=0.04302195969275193\n",
      "Stochastic Gradient Descent(31746): loss=0.0029236808728305094\n",
      "Stochastic Gradient Descent(31747): loss=1.147176452984952\n",
      "Stochastic Gradient Descent(31748): loss=5.90005538815805\n",
      "Stochastic Gradient Descent(31749): loss=1.6126877301961942\n",
      "Stochastic Gradient Descent(31750): loss=0.8898579357284264\n",
      "Stochastic Gradient Descent(31751): loss=0.28018333996900285\n",
      "Stochastic Gradient Descent(31752): loss=5.207311665716866\n",
      "Stochastic Gradient Descent(31753): loss=1.9445067891898462\n",
      "Stochastic Gradient Descent(31754): loss=21.124641899059174\n",
      "Stochastic Gradient Descent(31755): loss=4.704865913918267\n",
      "Stochastic Gradient Descent(31756): loss=4.82183031798711\n",
      "Stochastic Gradient Descent(31757): loss=3.8900155675883203\n",
      "Stochastic Gradient Descent(31758): loss=3.7784094330148474\n",
      "Stochastic Gradient Descent(31759): loss=0.003690682791822974\n",
      "Stochastic Gradient Descent(31760): loss=2.788888612623014\n",
      "Stochastic Gradient Descent(31761): loss=1.4833894065293407\n",
      "Stochastic Gradient Descent(31762): loss=7.268125141147656\n",
      "Stochastic Gradient Descent(31763): loss=0.24586082383460495\n",
      "Stochastic Gradient Descent(31764): loss=0.09774213873918104\n",
      "Stochastic Gradient Descent(31765): loss=1.349602634651899\n",
      "Stochastic Gradient Descent(31766): loss=5.252371777494493\n",
      "Stochastic Gradient Descent(31767): loss=1.6235068754397664\n",
      "Stochastic Gradient Descent(31768): loss=52.69816453848569\n",
      "Stochastic Gradient Descent(31769): loss=2.729398621769853\n",
      "Stochastic Gradient Descent(31770): loss=7.2105196080914595\n",
      "Stochastic Gradient Descent(31771): loss=2.270589378035822\n",
      "Stochastic Gradient Descent(31772): loss=13.134959756119041\n",
      "Stochastic Gradient Descent(31773): loss=0.21297131626260477\n",
      "Stochastic Gradient Descent(31774): loss=11.924458090448464\n",
      "Stochastic Gradient Descent(31775): loss=12.568625068485622\n",
      "Stochastic Gradient Descent(31776): loss=2.296220302405136\n",
      "Stochastic Gradient Descent(31777): loss=6.706928707397513\n",
      "Stochastic Gradient Descent(31778): loss=0.14971600009164915\n",
      "Stochastic Gradient Descent(31779): loss=2.1172478009054934\n",
      "Stochastic Gradient Descent(31780): loss=0.33298819096959703\n",
      "Stochastic Gradient Descent(31781): loss=0.8339463692063181\n",
      "Stochastic Gradient Descent(31782): loss=0.1334988420979825\n",
      "Stochastic Gradient Descent(31783): loss=0.0021344968016202164\n",
      "Stochastic Gradient Descent(31784): loss=2.3667672545172915\n",
      "Stochastic Gradient Descent(31785): loss=2.006684382025439\n",
      "Stochastic Gradient Descent(31786): loss=0.8405171688867535\n",
      "Stochastic Gradient Descent(31787): loss=1.555167981636022\n",
      "Stochastic Gradient Descent(31788): loss=5.416124759960324\n",
      "Stochastic Gradient Descent(31789): loss=1.6420821217396593\n",
      "Stochastic Gradient Descent(31790): loss=0.2518693974247366\n",
      "Stochastic Gradient Descent(31791): loss=0.45499013716738185\n",
      "Stochastic Gradient Descent(31792): loss=23.391856898762228\n",
      "Stochastic Gradient Descent(31793): loss=26.765726560083127\n",
      "Stochastic Gradient Descent(31794): loss=7.461180653568372\n",
      "Stochastic Gradient Descent(31795): loss=15.88780768005413\n",
      "Stochastic Gradient Descent(31796): loss=6.028966817850308\n",
      "Stochastic Gradient Descent(31797): loss=31.478541700260624\n",
      "Stochastic Gradient Descent(31798): loss=3.4510896109348437\n",
      "Stochastic Gradient Descent(31799): loss=0.11438086739174244\n",
      "Stochastic Gradient Descent(31800): loss=13.759608983107025\n",
      "Stochastic Gradient Descent(31801): loss=1.5339572259575556\n",
      "Stochastic Gradient Descent(31802): loss=0.14296502648406859\n",
      "Stochastic Gradient Descent(31803): loss=2.696760623089997\n",
      "Stochastic Gradient Descent(31804): loss=1.3485192643627404\n",
      "Stochastic Gradient Descent(31805): loss=0.018436759530250212\n",
      "Stochastic Gradient Descent(31806): loss=3.663960267393414\n",
      "Stochastic Gradient Descent(31807): loss=0.25720595487306114\n",
      "Stochastic Gradient Descent(31808): loss=4.015634246620258\n",
      "Stochastic Gradient Descent(31809): loss=0.8463927426896946\n",
      "Stochastic Gradient Descent(31810): loss=0.7789472880947437\n",
      "Stochastic Gradient Descent(31811): loss=2.460863230980261\n",
      "Stochastic Gradient Descent(31812): loss=0.11167268532617146\n",
      "Stochastic Gradient Descent(31813): loss=8.497892875968919\n",
      "Stochastic Gradient Descent(31814): loss=10.393140034083276\n",
      "Stochastic Gradient Descent(31815): loss=1.1103747999704294\n",
      "Stochastic Gradient Descent(31816): loss=0.00986517170399844\n",
      "Stochastic Gradient Descent(31817): loss=4.432991457560446\n",
      "Stochastic Gradient Descent(31818): loss=0.8244256060513822\n",
      "Stochastic Gradient Descent(31819): loss=0.7418631334156942\n",
      "Stochastic Gradient Descent(31820): loss=1.7132515936030346\n",
      "Stochastic Gradient Descent(31821): loss=11.836010254115441\n",
      "Stochastic Gradient Descent(31822): loss=0.40557783900692207\n",
      "Stochastic Gradient Descent(31823): loss=0.17372090076373645\n",
      "Stochastic Gradient Descent(31824): loss=0.37114152471968376\n",
      "Stochastic Gradient Descent(31825): loss=2.0164979418948574\n",
      "Stochastic Gradient Descent(31826): loss=12.125794377379233\n",
      "Stochastic Gradient Descent(31827): loss=1.5321354449448545\n",
      "Stochastic Gradient Descent(31828): loss=0.45067615183218157\n",
      "Stochastic Gradient Descent(31829): loss=2.401073097923666\n",
      "Stochastic Gradient Descent(31830): loss=9.827839128356482\n",
      "Stochastic Gradient Descent(31831): loss=0.011702214651425406\n",
      "Stochastic Gradient Descent(31832): loss=2.3230979581436215\n",
      "Stochastic Gradient Descent(31833): loss=7.871179887103665\n",
      "Stochastic Gradient Descent(31834): loss=2.039860955576099\n",
      "Stochastic Gradient Descent(31835): loss=2.7092782075792514\n",
      "Stochastic Gradient Descent(31836): loss=3.019625981987855\n",
      "Stochastic Gradient Descent(31837): loss=0.061299636509134635\n",
      "Stochastic Gradient Descent(31838): loss=0.9295578926199441\n",
      "Stochastic Gradient Descent(31839): loss=0.10753318267978394\n",
      "Stochastic Gradient Descent(31840): loss=1.6682190594998754\n",
      "Stochastic Gradient Descent(31841): loss=3.321449887935641\n",
      "Stochastic Gradient Descent(31842): loss=0.6186851902103405\n",
      "Stochastic Gradient Descent(31843): loss=2.4645230942532015\n",
      "Stochastic Gradient Descent(31844): loss=18.576269203409428\n",
      "Stochastic Gradient Descent(31845): loss=2.3410453650786844\n",
      "Stochastic Gradient Descent(31846): loss=3.987539839049701\n",
      "Stochastic Gradient Descent(31847): loss=8.72640955273734\n",
      "Stochastic Gradient Descent(31848): loss=0.20962483918133906\n",
      "Stochastic Gradient Descent(31849): loss=0.6728674878737888\n",
      "Stochastic Gradient Descent(31850): loss=7.476111795896765\n",
      "Stochastic Gradient Descent(31851): loss=12.878987924181278\n",
      "Stochastic Gradient Descent(31852): loss=4.568259733628181\n",
      "Stochastic Gradient Descent(31853): loss=2.1333679501612868\n",
      "Stochastic Gradient Descent(31854): loss=0.54457921213115\n",
      "Stochastic Gradient Descent(31855): loss=0.009120642001014123\n",
      "Stochastic Gradient Descent(31856): loss=4.6202530765632775e-05\n",
      "Stochastic Gradient Descent(31857): loss=20.51078068740834\n",
      "Stochastic Gradient Descent(31858): loss=1.9520952450040987\n",
      "Stochastic Gradient Descent(31859): loss=1.138854381933061\n",
      "Stochastic Gradient Descent(31860): loss=6.405289337347854\n",
      "Stochastic Gradient Descent(31861): loss=1.77335012179155\n",
      "Stochastic Gradient Descent(31862): loss=0.09515553323708507\n",
      "Stochastic Gradient Descent(31863): loss=0.16955825655895973\n",
      "Stochastic Gradient Descent(31864): loss=0.25527214521582564\n",
      "Stochastic Gradient Descent(31865): loss=13.232467006490477\n",
      "Stochastic Gradient Descent(31866): loss=5.446401867350803\n",
      "Stochastic Gradient Descent(31867): loss=9.258304718493283\n",
      "Stochastic Gradient Descent(31868): loss=5.469762022787445\n",
      "Stochastic Gradient Descent(31869): loss=2.0187953548967146\n",
      "Stochastic Gradient Descent(31870): loss=3.0270318013032282\n",
      "Stochastic Gradient Descent(31871): loss=1.8913499367851792\n",
      "Stochastic Gradient Descent(31872): loss=1.9344831236914628\n",
      "Stochastic Gradient Descent(31873): loss=26.091934854644656\n",
      "Stochastic Gradient Descent(31874): loss=0.004973385013849361\n",
      "Stochastic Gradient Descent(31875): loss=0.07757799826676381\n",
      "Stochastic Gradient Descent(31876): loss=0.49255204007302605\n",
      "Stochastic Gradient Descent(31877): loss=0.5108249444068301\n",
      "Stochastic Gradient Descent(31878): loss=7.1687554589614075\n",
      "Stochastic Gradient Descent(31879): loss=0.0005294077643756747\n",
      "Stochastic Gradient Descent(31880): loss=1.7589873003326664\n",
      "Stochastic Gradient Descent(31881): loss=2.4803386649917782\n",
      "Stochastic Gradient Descent(31882): loss=0.7208464128684223\n",
      "Stochastic Gradient Descent(31883): loss=13.210298171221229\n",
      "Stochastic Gradient Descent(31884): loss=7.208235110844235\n",
      "Stochastic Gradient Descent(31885): loss=5.270828418883468\n",
      "Stochastic Gradient Descent(31886): loss=2.4491966700692362\n",
      "Stochastic Gradient Descent(31887): loss=0.24770109836172707\n",
      "Stochastic Gradient Descent(31888): loss=1.1768737264647875\n",
      "Stochastic Gradient Descent(31889): loss=1.4011233496883304\n",
      "Stochastic Gradient Descent(31890): loss=10.778181386214657\n",
      "Stochastic Gradient Descent(31891): loss=0.0026181062158341342\n",
      "Stochastic Gradient Descent(31892): loss=7.687821524491404\n",
      "Stochastic Gradient Descent(31893): loss=0.2531522497843974\n",
      "Stochastic Gradient Descent(31894): loss=1.2100330443423295\n",
      "Stochastic Gradient Descent(31895): loss=0.026299951425359357\n",
      "Stochastic Gradient Descent(31896): loss=1.725821569513364\n",
      "Stochastic Gradient Descent(31897): loss=5.104534868912075\n",
      "Stochastic Gradient Descent(31898): loss=0.3754524551741783\n",
      "Stochastic Gradient Descent(31899): loss=0.08795482694872846\n",
      "Stochastic Gradient Descent(31900): loss=0.21394147700558744\n",
      "Stochastic Gradient Descent(31901): loss=4.168455229505723\n",
      "Stochastic Gradient Descent(31902): loss=1.567164601663386\n",
      "Stochastic Gradient Descent(31903): loss=10.28901364563085\n",
      "Stochastic Gradient Descent(31904): loss=8.635780495855975\n",
      "Stochastic Gradient Descent(31905): loss=2.3419552173659186\n",
      "Stochastic Gradient Descent(31906): loss=19.493967877726455\n",
      "Stochastic Gradient Descent(31907): loss=0.35670192829973885\n",
      "Stochastic Gradient Descent(31908): loss=40.50389018937306\n",
      "Stochastic Gradient Descent(31909): loss=1.1763110563775236\n",
      "Stochastic Gradient Descent(31910): loss=0.400485723499242\n",
      "Stochastic Gradient Descent(31911): loss=2.759671913910718\n",
      "Stochastic Gradient Descent(31912): loss=1.149775843631328\n",
      "Stochastic Gradient Descent(31913): loss=4.0241690253535385\n",
      "Stochastic Gradient Descent(31914): loss=0.1564625643369322\n",
      "Stochastic Gradient Descent(31915): loss=3.215643478330815\n",
      "Stochastic Gradient Descent(31916): loss=3.2360092894748687\n",
      "Stochastic Gradient Descent(31917): loss=5.951170543097967\n",
      "Stochastic Gradient Descent(31918): loss=2.893275012250037\n",
      "Stochastic Gradient Descent(31919): loss=3.833485800889395\n",
      "Stochastic Gradient Descent(31920): loss=13.449203116372203\n",
      "Stochastic Gradient Descent(31921): loss=0.05754879742378446\n",
      "Stochastic Gradient Descent(31922): loss=1.2243187074057944\n",
      "Stochastic Gradient Descent(31923): loss=5.527343489547201\n",
      "Stochastic Gradient Descent(31924): loss=6.434396455459177\n",
      "Stochastic Gradient Descent(31925): loss=0.39728494442731754\n",
      "Stochastic Gradient Descent(31926): loss=3.5067299055609316\n",
      "Stochastic Gradient Descent(31927): loss=2.661131423089843\n",
      "Stochastic Gradient Descent(31928): loss=9.640775561320245\n",
      "Stochastic Gradient Descent(31929): loss=20.832589302157043\n",
      "Stochastic Gradient Descent(31930): loss=0.5087007242533345\n",
      "Stochastic Gradient Descent(31931): loss=0.4574670784124576\n",
      "Stochastic Gradient Descent(31932): loss=5.721954960755913\n",
      "Stochastic Gradient Descent(31933): loss=0.4727442516571403\n",
      "Stochastic Gradient Descent(31934): loss=0.9499240752904299\n",
      "Stochastic Gradient Descent(31935): loss=5.4312187177922615\n",
      "Stochastic Gradient Descent(31936): loss=1.1107398930081671\n",
      "Stochastic Gradient Descent(31937): loss=0.9795807555712049\n",
      "Stochastic Gradient Descent(31938): loss=0.14740972255133927\n",
      "Stochastic Gradient Descent(31939): loss=0.006362350140825281\n",
      "Stochastic Gradient Descent(31940): loss=2.682555761567324\n",
      "Stochastic Gradient Descent(31941): loss=5.114213523433257\n",
      "Stochastic Gradient Descent(31942): loss=1.9171659422079241\n",
      "Stochastic Gradient Descent(31943): loss=0.5220938161678057\n",
      "Stochastic Gradient Descent(31944): loss=1.4059475930443774\n",
      "Stochastic Gradient Descent(31945): loss=0.2765834150578877\n",
      "Stochastic Gradient Descent(31946): loss=7.750620250155763\n",
      "Stochastic Gradient Descent(31947): loss=0.13466740590950926\n",
      "Stochastic Gradient Descent(31948): loss=1.1798048041581681\n",
      "Stochastic Gradient Descent(31949): loss=0.1251519930073864\n",
      "Stochastic Gradient Descent(31950): loss=4.495324323252647\n",
      "Stochastic Gradient Descent(31951): loss=4.758262783130209\n",
      "Stochastic Gradient Descent(31952): loss=0.36197771014455704\n",
      "Stochastic Gradient Descent(31953): loss=0.040245821638818895\n",
      "Stochastic Gradient Descent(31954): loss=3.5678828436378915\n",
      "Stochastic Gradient Descent(31955): loss=2.4717286878957156\n",
      "Stochastic Gradient Descent(31956): loss=0.13903053487989017\n",
      "Stochastic Gradient Descent(31957): loss=0.030056815955672223\n",
      "Stochastic Gradient Descent(31958): loss=0.915441967254868\n",
      "Stochastic Gradient Descent(31959): loss=1.5697118475340928\n",
      "Stochastic Gradient Descent(31960): loss=3.815097653634634\n",
      "Stochastic Gradient Descent(31961): loss=0.15908937872301385\n",
      "Stochastic Gradient Descent(31962): loss=3.4380222002455834\n",
      "Stochastic Gradient Descent(31963): loss=1.5286859512843427\n",
      "Stochastic Gradient Descent(31964): loss=4.43477237708975\n",
      "Stochastic Gradient Descent(31965): loss=0.891499488905334\n",
      "Stochastic Gradient Descent(31966): loss=6.240462408421041\n",
      "Stochastic Gradient Descent(31967): loss=6.613010283860092\n",
      "Stochastic Gradient Descent(31968): loss=5.747331734419769\n",
      "Stochastic Gradient Descent(31969): loss=0.06874262467791602\n",
      "Stochastic Gradient Descent(31970): loss=2.181548426831324\n",
      "Stochastic Gradient Descent(31971): loss=0.07666891341078728\n",
      "Stochastic Gradient Descent(31972): loss=0.10150229224653974\n",
      "Stochastic Gradient Descent(31973): loss=2.2409988806209853\n",
      "Stochastic Gradient Descent(31974): loss=0.7753826923181834\n",
      "Stochastic Gradient Descent(31975): loss=0.00397480769538061\n",
      "Stochastic Gradient Descent(31976): loss=11.645704927344196\n",
      "Stochastic Gradient Descent(31977): loss=0.0024279234606149956\n",
      "Stochastic Gradient Descent(31978): loss=1.2503518304157697\n",
      "Stochastic Gradient Descent(31979): loss=0.22180085850527712\n",
      "Stochastic Gradient Descent(31980): loss=2.9273184062970747\n",
      "Stochastic Gradient Descent(31981): loss=9.136740454901071\n",
      "Stochastic Gradient Descent(31982): loss=14.248111937652439\n",
      "Stochastic Gradient Descent(31983): loss=0.024944562331952275\n",
      "Stochastic Gradient Descent(31984): loss=3.2742379509350266\n",
      "Stochastic Gradient Descent(31985): loss=0.493874141514542\n",
      "Stochastic Gradient Descent(31986): loss=2.8888082262562396\n",
      "Stochastic Gradient Descent(31987): loss=20.048317022657105\n",
      "Stochastic Gradient Descent(31988): loss=0.23711406329377988\n",
      "Stochastic Gradient Descent(31989): loss=6.689294628226481\n",
      "Stochastic Gradient Descent(31990): loss=12.446515010394082\n",
      "Stochastic Gradient Descent(31991): loss=2.682033296967289\n",
      "Stochastic Gradient Descent(31992): loss=5.548735982731335\n",
      "Stochastic Gradient Descent(31993): loss=13.46196410048323\n",
      "Stochastic Gradient Descent(31994): loss=0.1785025531694077\n",
      "Stochastic Gradient Descent(31995): loss=0.7977907791909531\n",
      "Stochastic Gradient Descent(31996): loss=13.632550903530708\n",
      "Stochastic Gradient Descent(31997): loss=5.6446996914126055\n",
      "Stochastic Gradient Descent(31998): loss=12.285530602278943\n",
      "Stochastic Gradient Descent(31999): loss=0.0981045858419104\n",
      "Stochastic Gradient Descent(32000): loss=0.4908200396196873\n",
      "Stochastic Gradient Descent(32001): loss=0.033456730981716004\n",
      "Stochastic Gradient Descent(32002): loss=0.3137975490818648\n",
      "Stochastic Gradient Descent(32003): loss=2.6345794808311855\n",
      "Stochastic Gradient Descent(32004): loss=8.93882987663802\n",
      "Stochastic Gradient Descent(32005): loss=0.1281239149450012\n",
      "Stochastic Gradient Descent(32006): loss=0.0009291351707061284\n",
      "Stochastic Gradient Descent(32007): loss=5.694095620149087e-07\n",
      "Stochastic Gradient Descent(32008): loss=7.254439849815538\n",
      "Stochastic Gradient Descent(32009): loss=4.343694366765757\n",
      "Stochastic Gradient Descent(32010): loss=12.42621993161291\n",
      "Stochastic Gradient Descent(32011): loss=2.4069917510182024\n",
      "Stochastic Gradient Descent(32012): loss=6.968846945928556\n",
      "Stochastic Gradient Descent(32013): loss=5.834298489036149\n",
      "Stochastic Gradient Descent(32014): loss=1.9137063775000098\n",
      "Stochastic Gradient Descent(32015): loss=0.24071384643057597\n",
      "Stochastic Gradient Descent(32016): loss=4.933792896196312\n",
      "Stochastic Gradient Descent(32017): loss=0.16048511120475237\n",
      "Stochastic Gradient Descent(32018): loss=2.633208623653219\n",
      "Stochastic Gradient Descent(32019): loss=1.7851542912390324\n",
      "Stochastic Gradient Descent(32020): loss=1.0774909917982436\n",
      "Stochastic Gradient Descent(32021): loss=0.05784153418816501\n",
      "Stochastic Gradient Descent(32022): loss=6.870456168201996\n",
      "Stochastic Gradient Descent(32023): loss=0.9384543794988923\n",
      "Stochastic Gradient Descent(32024): loss=0.24601652383215775\n",
      "Stochastic Gradient Descent(32025): loss=3.3562565777004534\n",
      "Stochastic Gradient Descent(32026): loss=1.671449401628768\n",
      "Stochastic Gradient Descent(32027): loss=2.505046634504205\n",
      "Stochastic Gradient Descent(32028): loss=0.26775904580800625\n",
      "Stochastic Gradient Descent(32029): loss=7.687140540322695\n",
      "Stochastic Gradient Descent(32030): loss=4.122398188745967\n",
      "Stochastic Gradient Descent(32031): loss=0.6197955734526933\n",
      "Stochastic Gradient Descent(32032): loss=4.603373393407748\n",
      "Stochastic Gradient Descent(32033): loss=0.16095847200737448\n",
      "Stochastic Gradient Descent(32034): loss=1.6387254628276688\n",
      "Stochastic Gradient Descent(32035): loss=0.7970778166148034\n",
      "Stochastic Gradient Descent(32036): loss=2.4061329328641126\n",
      "Stochastic Gradient Descent(32037): loss=1.9584101382716474\n",
      "Stochastic Gradient Descent(32038): loss=0.22199960960550003\n",
      "Stochastic Gradient Descent(32039): loss=3.1335326498070497\n",
      "Stochastic Gradient Descent(32040): loss=1.7349298772029438\n",
      "Stochastic Gradient Descent(32041): loss=2.0783422612720646\n",
      "Stochastic Gradient Descent(32042): loss=3.88901356500907\n",
      "Stochastic Gradient Descent(32043): loss=0.0136612936690624\n",
      "Stochastic Gradient Descent(32044): loss=0.13229196614597738\n",
      "Stochastic Gradient Descent(32045): loss=3.0636571778139743\n",
      "Stochastic Gradient Descent(32046): loss=0.01697399670673414\n",
      "Stochastic Gradient Descent(32047): loss=2.267136757263102\n",
      "Stochastic Gradient Descent(32048): loss=0.22976683575623544\n",
      "Stochastic Gradient Descent(32049): loss=0.1651721944127266\n",
      "Stochastic Gradient Descent(32050): loss=1.8717380243371013\n",
      "Stochastic Gradient Descent(32051): loss=0.002851439177402907\n",
      "Stochastic Gradient Descent(32052): loss=0.258579866670613\n",
      "Stochastic Gradient Descent(32053): loss=2.3071122958989774\n",
      "Stochastic Gradient Descent(32054): loss=0.06873256057550634\n",
      "Stochastic Gradient Descent(32055): loss=0.4726139893507843\n",
      "Stochastic Gradient Descent(32056): loss=0.17129947101885143\n",
      "Stochastic Gradient Descent(32057): loss=9.19338482300702\n",
      "Stochastic Gradient Descent(32058): loss=5.297556519462358\n",
      "Stochastic Gradient Descent(32059): loss=8.143866374662002\n",
      "Stochastic Gradient Descent(32060): loss=0.04473700117271698\n",
      "Stochastic Gradient Descent(32061): loss=7.769843468486099\n",
      "Stochastic Gradient Descent(32062): loss=0.047455566046285216\n",
      "Stochastic Gradient Descent(32063): loss=18.767567490342817\n",
      "Stochastic Gradient Descent(32064): loss=7.685437215171666\n",
      "Stochastic Gradient Descent(32065): loss=0.9161781143234308\n",
      "Stochastic Gradient Descent(32066): loss=8.771066334406683\n",
      "Stochastic Gradient Descent(32067): loss=1.2883071566871604\n",
      "Stochastic Gradient Descent(32068): loss=4.885146877354491\n",
      "Stochastic Gradient Descent(32069): loss=17.584220367026685\n",
      "Stochastic Gradient Descent(32070): loss=12.735677565271192\n",
      "Stochastic Gradient Descent(32071): loss=0.056186337478991426\n",
      "Stochastic Gradient Descent(32072): loss=2.394321193660968\n",
      "Stochastic Gradient Descent(32073): loss=0.002257393831712923\n",
      "Stochastic Gradient Descent(32074): loss=0.2324408983502308\n",
      "Stochastic Gradient Descent(32075): loss=3.6495701245829744\n",
      "Stochastic Gradient Descent(32076): loss=0.13390751956853195\n",
      "Stochastic Gradient Descent(32077): loss=3.586764291866855\n",
      "Stochastic Gradient Descent(32078): loss=6.457879682353928\n",
      "Stochastic Gradient Descent(32079): loss=1.6806092810531184\n",
      "Stochastic Gradient Descent(32080): loss=8.413647917054165\n",
      "Stochastic Gradient Descent(32081): loss=1.564200620166561\n",
      "Stochastic Gradient Descent(32082): loss=2.6419126819590333\n",
      "Stochastic Gradient Descent(32083): loss=0.13761294790238343\n",
      "Stochastic Gradient Descent(32084): loss=0.6762283847455837\n",
      "Stochastic Gradient Descent(32085): loss=4.309132317704812\n",
      "Stochastic Gradient Descent(32086): loss=6.37263294878869\n",
      "Stochastic Gradient Descent(32087): loss=0.6388115846624433\n",
      "Stochastic Gradient Descent(32088): loss=8.124932278446423\n",
      "Stochastic Gradient Descent(32089): loss=0.002517961247786682\n",
      "Stochastic Gradient Descent(32090): loss=20.90930723544493\n",
      "Stochastic Gradient Descent(32091): loss=3.29705655451689\n",
      "Stochastic Gradient Descent(32092): loss=0.6926294183786597\n",
      "Stochastic Gradient Descent(32093): loss=21.46150703920321\n",
      "Stochastic Gradient Descent(32094): loss=0.7695440469734028\n",
      "Stochastic Gradient Descent(32095): loss=4.723522908658943\n",
      "Stochastic Gradient Descent(32096): loss=11.429506062098383\n",
      "Stochastic Gradient Descent(32097): loss=5.421118918289086\n",
      "Stochastic Gradient Descent(32098): loss=9.05356471203104\n",
      "Stochastic Gradient Descent(32099): loss=0.918761785348282\n",
      "Stochastic Gradient Descent(32100): loss=0.04746904037666367\n",
      "Stochastic Gradient Descent(32101): loss=7.642126961687598\n",
      "Stochastic Gradient Descent(32102): loss=11.311557235318872\n",
      "Stochastic Gradient Descent(32103): loss=2.2248273465063253\n",
      "Stochastic Gradient Descent(32104): loss=0.666165495390758\n",
      "Stochastic Gradient Descent(32105): loss=21.7730353530554\n",
      "Stochastic Gradient Descent(32106): loss=2.76958967498987\n",
      "Stochastic Gradient Descent(32107): loss=0.1251630283340879\n",
      "Stochastic Gradient Descent(32108): loss=0.5224918045726437\n",
      "Stochastic Gradient Descent(32109): loss=11.149650068981924\n",
      "Stochastic Gradient Descent(32110): loss=8.534730563124723\n",
      "Stochastic Gradient Descent(32111): loss=21.68412163390383\n",
      "Stochastic Gradient Descent(32112): loss=3.0271765207267505\n",
      "Stochastic Gradient Descent(32113): loss=0.2605186792309811\n",
      "Stochastic Gradient Descent(32114): loss=0.06516365316750705\n",
      "Stochastic Gradient Descent(32115): loss=2.1299772423795673\n",
      "Stochastic Gradient Descent(32116): loss=4.657161354127939\n",
      "Stochastic Gradient Descent(32117): loss=1.3025189014642484\n",
      "Stochastic Gradient Descent(32118): loss=6.438453600264709\n",
      "Stochastic Gradient Descent(32119): loss=1.8158903982931462\n",
      "Stochastic Gradient Descent(32120): loss=15.08636827235436\n",
      "Stochastic Gradient Descent(32121): loss=0.0025640416011351074\n",
      "Stochastic Gradient Descent(32122): loss=2.828224013442746\n",
      "Stochastic Gradient Descent(32123): loss=3.511665132082028\n",
      "Stochastic Gradient Descent(32124): loss=0.80670740693085\n",
      "Stochastic Gradient Descent(32125): loss=0.20746785248926883\n",
      "Stochastic Gradient Descent(32126): loss=2.295411911575903\n",
      "Stochastic Gradient Descent(32127): loss=5.174479712204439\n",
      "Stochastic Gradient Descent(32128): loss=2.621240774189308\n",
      "Stochastic Gradient Descent(32129): loss=0.03407268543016999\n",
      "Stochastic Gradient Descent(32130): loss=3.799580561884371\n",
      "Stochastic Gradient Descent(32131): loss=1.7197067432649336\n",
      "Stochastic Gradient Descent(32132): loss=2.7592532274810417\n",
      "Stochastic Gradient Descent(32133): loss=7.3488468003581735\n",
      "Stochastic Gradient Descent(32134): loss=1.630283558673662\n",
      "Stochastic Gradient Descent(32135): loss=4.701978391180939\n",
      "Stochastic Gradient Descent(32136): loss=6.486174543245317\n",
      "Stochastic Gradient Descent(32137): loss=1.5502197312205153\n",
      "Stochastic Gradient Descent(32138): loss=5.321654227751478\n",
      "Stochastic Gradient Descent(32139): loss=0.003135651179312575\n",
      "Stochastic Gradient Descent(32140): loss=1.500925314287605\n",
      "Stochastic Gradient Descent(32141): loss=0.25380047895115415\n",
      "Stochastic Gradient Descent(32142): loss=0.9057692358115992\n",
      "Stochastic Gradient Descent(32143): loss=0.15179895413811698\n",
      "Stochastic Gradient Descent(32144): loss=0.3640724519724339\n",
      "Stochastic Gradient Descent(32145): loss=4.9540555691878545\n",
      "Stochastic Gradient Descent(32146): loss=1.3072403234869998\n",
      "Stochastic Gradient Descent(32147): loss=4.065935539349959\n",
      "Stochastic Gradient Descent(32148): loss=0.029849544780252004\n",
      "Stochastic Gradient Descent(32149): loss=6.305696287388765\n",
      "Stochastic Gradient Descent(32150): loss=1.337517718577622\n",
      "Stochastic Gradient Descent(32151): loss=6.117112773210431\n",
      "Stochastic Gradient Descent(32152): loss=0.34738963605931\n",
      "Stochastic Gradient Descent(32153): loss=1.06731478128955\n",
      "Stochastic Gradient Descent(32154): loss=1.3817059295346112\n",
      "Stochastic Gradient Descent(32155): loss=0.25236808873689065\n",
      "Stochastic Gradient Descent(32156): loss=14.597476643639448\n",
      "Stochastic Gradient Descent(32157): loss=8.271508590017342\n",
      "Stochastic Gradient Descent(32158): loss=10.519599957820269\n",
      "Stochastic Gradient Descent(32159): loss=3.4971361036206887\n",
      "Stochastic Gradient Descent(32160): loss=2.926295120205425\n",
      "Stochastic Gradient Descent(32161): loss=0.11131611769602925\n",
      "Stochastic Gradient Descent(32162): loss=14.397675288919883\n",
      "Stochastic Gradient Descent(32163): loss=1.790841982191384\n",
      "Stochastic Gradient Descent(32164): loss=2.932370121201916\n",
      "Stochastic Gradient Descent(32165): loss=0.7643035424778531\n",
      "Stochastic Gradient Descent(32166): loss=1.9290196766285697\n",
      "Stochastic Gradient Descent(32167): loss=4.6718022969742945\n",
      "Stochastic Gradient Descent(32168): loss=18.05036064695645\n",
      "Stochastic Gradient Descent(32169): loss=0.060142242717784115\n",
      "Stochastic Gradient Descent(32170): loss=0.17187508379239877\n",
      "Stochastic Gradient Descent(32171): loss=1.5011990117070824\n",
      "Stochastic Gradient Descent(32172): loss=12.133879197348998\n",
      "Stochastic Gradient Descent(32173): loss=0.2594973954851653\n",
      "Stochastic Gradient Descent(32174): loss=1.771945700224008\n",
      "Stochastic Gradient Descent(32175): loss=18.271070214222078\n",
      "Stochastic Gradient Descent(32176): loss=6.318156759427635\n",
      "Stochastic Gradient Descent(32177): loss=0.34858136829587516\n",
      "Stochastic Gradient Descent(32178): loss=5.326399843976691\n",
      "Stochastic Gradient Descent(32179): loss=1.1748797096094625\n",
      "Stochastic Gradient Descent(32180): loss=11.11716284228992\n",
      "Stochastic Gradient Descent(32181): loss=0.8407063671085088\n",
      "Stochastic Gradient Descent(32182): loss=2.664649495026922\n",
      "Stochastic Gradient Descent(32183): loss=3.1984230537909273\n",
      "Stochastic Gradient Descent(32184): loss=2.9101588317685834\n",
      "Stochastic Gradient Descent(32185): loss=0.6097040516755574\n",
      "Stochastic Gradient Descent(32186): loss=1.1791087670829916\n",
      "Stochastic Gradient Descent(32187): loss=7.630150437409193\n",
      "Stochastic Gradient Descent(32188): loss=1.0056095228197004\n",
      "Stochastic Gradient Descent(32189): loss=0.30565660413266965\n",
      "Stochastic Gradient Descent(32190): loss=8.676516679339791\n",
      "Stochastic Gradient Descent(32191): loss=0.13798686399096396\n",
      "Stochastic Gradient Descent(32192): loss=32.79708589568254\n",
      "Stochastic Gradient Descent(32193): loss=0.0002745112366426985\n",
      "Stochastic Gradient Descent(32194): loss=12.892668425901437\n",
      "Stochastic Gradient Descent(32195): loss=9.26132113165301\n",
      "Stochastic Gradient Descent(32196): loss=1.9964382379630294\n",
      "Stochastic Gradient Descent(32197): loss=11.117040295799562\n",
      "Stochastic Gradient Descent(32198): loss=17.412921509493387\n",
      "Stochastic Gradient Descent(32199): loss=0.23723716802508352\n",
      "Stochastic Gradient Descent(32200): loss=0.7858093277255205\n",
      "Stochastic Gradient Descent(32201): loss=2.4321903448817674\n",
      "Stochastic Gradient Descent(32202): loss=0.46064640044310445\n",
      "Stochastic Gradient Descent(32203): loss=2.48661489933663\n",
      "Stochastic Gradient Descent(32204): loss=0.0783994115627711\n",
      "Stochastic Gradient Descent(32205): loss=0.8004895958292626\n",
      "Stochastic Gradient Descent(32206): loss=1.229304252310452\n",
      "Stochastic Gradient Descent(32207): loss=0.6004635017443432\n",
      "Stochastic Gradient Descent(32208): loss=0.4045607564316525\n",
      "Stochastic Gradient Descent(32209): loss=3.9402282875333183\n",
      "Stochastic Gradient Descent(32210): loss=0.719595434068755\n",
      "Stochastic Gradient Descent(32211): loss=5.504896437042423\n",
      "Stochastic Gradient Descent(32212): loss=12.674073974175121\n",
      "Stochastic Gradient Descent(32213): loss=0.3776832298712667\n",
      "Stochastic Gradient Descent(32214): loss=0.7185409559964131\n",
      "Stochastic Gradient Descent(32215): loss=0.025415938294028426\n",
      "Stochastic Gradient Descent(32216): loss=2.964875319475375\n",
      "Stochastic Gradient Descent(32217): loss=0.9727544612034537\n",
      "Stochastic Gradient Descent(32218): loss=3.787429717384468\n",
      "Stochastic Gradient Descent(32219): loss=2.533809536109303\n",
      "Stochastic Gradient Descent(32220): loss=7.123414022809264\n",
      "Stochastic Gradient Descent(32221): loss=0.8846737506380956\n",
      "Stochastic Gradient Descent(32222): loss=0.6532813795678458\n",
      "Stochastic Gradient Descent(32223): loss=12.377099111213978\n",
      "Stochastic Gradient Descent(32224): loss=1.8755448502470922\n",
      "Stochastic Gradient Descent(32225): loss=3.7087314311893937\n",
      "Stochastic Gradient Descent(32226): loss=4.474436173915474\n",
      "Stochastic Gradient Descent(32227): loss=1.8420991510864868\n",
      "Stochastic Gradient Descent(32228): loss=0.5283397895879626\n",
      "Stochastic Gradient Descent(32229): loss=0.002817976786072129\n",
      "Stochastic Gradient Descent(32230): loss=3.1985170184502074\n",
      "Stochastic Gradient Descent(32231): loss=0.5226272026911694\n",
      "Stochastic Gradient Descent(32232): loss=2.2977023665293848\n",
      "Stochastic Gradient Descent(32233): loss=1.4295408581658533\n",
      "Stochastic Gradient Descent(32234): loss=3.5384766188574215\n",
      "Stochastic Gradient Descent(32235): loss=0.1701340476129465\n",
      "Stochastic Gradient Descent(32236): loss=0.5276416195594471\n",
      "Stochastic Gradient Descent(32237): loss=9.960396753773967\n",
      "Stochastic Gradient Descent(32238): loss=5.803370112164815\n",
      "Stochastic Gradient Descent(32239): loss=2.695750792274196\n",
      "Stochastic Gradient Descent(32240): loss=4.289571065033919\n",
      "Stochastic Gradient Descent(32241): loss=0.19718090614692274\n",
      "Stochastic Gradient Descent(32242): loss=0.5353937216914565\n",
      "Stochastic Gradient Descent(32243): loss=4.800189897071986\n",
      "Stochastic Gradient Descent(32244): loss=0.5074818685559136\n",
      "Stochastic Gradient Descent(32245): loss=2.5006918217598004\n",
      "Stochastic Gradient Descent(32246): loss=3.8537625188611218\n",
      "Stochastic Gradient Descent(32247): loss=1.2442066489065813\n",
      "Stochastic Gradient Descent(32248): loss=11.770621470580403\n",
      "Stochastic Gradient Descent(32249): loss=14.086671070161291\n",
      "Stochastic Gradient Descent(32250): loss=4.617829390223408\n",
      "Stochastic Gradient Descent(32251): loss=0.055778341936929744\n",
      "Stochastic Gradient Descent(32252): loss=1.168388231982747\n",
      "Stochastic Gradient Descent(32253): loss=0.5326560129678151\n",
      "Stochastic Gradient Descent(32254): loss=0.040347162034352256\n",
      "Stochastic Gradient Descent(32255): loss=0.026514924133677356\n",
      "Stochastic Gradient Descent(32256): loss=0.5063469758511396\n",
      "Stochastic Gradient Descent(32257): loss=1.042978592594451\n",
      "Stochastic Gradient Descent(32258): loss=0.06740080227078339\n",
      "Stochastic Gradient Descent(32259): loss=0.7236385967558091\n",
      "Stochastic Gradient Descent(32260): loss=2.1738223499853735\n",
      "Stochastic Gradient Descent(32261): loss=10.076226411211112\n",
      "Stochastic Gradient Descent(32262): loss=0.2639621978102657\n",
      "Stochastic Gradient Descent(32263): loss=2.2012490928291197\n",
      "Stochastic Gradient Descent(32264): loss=6.815007184665978\n",
      "Stochastic Gradient Descent(32265): loss=7.869787350663345\n",
      "Stochastic Gradient Descent(32266): loss=0.11520957787405442\n",
      "Stochastic Gradient Descent(32267): loss=2.8355681250174785\n",
      "Stochastic Gradient Descent(32268): loss=24.372737922727417\n",
      "Stochastic Gradient Descent(32269): loss=0.02442115750625841\n",
      "Stochastic Gradient Descent(32270): loss=10.42323237548343\n",
      "Stochastic Gradient Descent(32271): loss=7.19194029742078\n",
      "Stochastic Gradient Descent(32272): loss=0.8992332811742617\n",
      "Stochastic Gradient Descent(32273): loss=0.21329603593372112\n",
      "Stochastic Gradient Descent(32274): loss=3.0641689078691265\n",
      "Stochastic Gradient Descent(32275): loss=5.4905484531754\n",
      "Stochastic Gradient Descent(32276): loss=3.025236591694196\n",
      "Stochastic Gradient Descent(32277): loss=2.5747873017426275\n",
      "Stochastic Gradient Descent(32278): loss=0.9012202839487009\n",
      "Stochastic Gradient Descent(32279): loss=0.26753420142934586\n",
      "Stochastic Gradient Descent(32280): loss=2.294070558806142\n",
      "Stochastic Gradient Descent(32281): loss=6.1826684646682875\n",
      "Stochastic Gradient Descent(32282): loss=0.1776518969436054\n",
      "Stochastic Gradient Descent(32283): loss=0.2584600870544367\n",
      "Stochastic Gradient Descent(32284): loss=0.0020974657318107316\n",
      "Stochastic Gradient Descent(32285): loss=0.39495812729903584\n",
      "Stochastic Gradient Descent(32286): loss=9.803367168004753\n",
      "Stochastic Gradient Descent(32287): loss=0.26749878327025806\n",
      "Stochastic Gradient Descent(32288): loss=2.9973415548673326\n",
      "Stochastic Gradient Descent(32289): loss=2.608215240264452\n",
      "Stochastic Gradient Descent(32290): loss=3.2292130903779324\n",
      "Stochastic Gradient Descent(32291): loss=6.223174958704412\n",
      "Stochastic Gradient Descent(32292): loss=21.663486615662112\n",
      "Stochastic Gradient Descent(32293): loss=0.8494939381635472\n",
      "Stochastic Gradient Descent(32294): loss=5.0264918005390635\n",
      "Stochastic Gradient Descent(32295): loss=2.9061080749276993\n",
      "Stochastic Gradient Descent(32296): loss=8.205686805843198\n",
      "Stochastic Gradient Descent(32297): loss=5.833076416740554\n",
      "Stochastic Gradient Descent(32298): loss=2.791106146065364\n",
      "Stochastic Gradient Descent(32299): loss=2.3586782214281463\n",
      "Stochastic Gradient Descent(32300): loss=0.6653495243522094\n",
      "Stochastic Gradient Descent(32301): loss=6.056726076045339\n",
      "Stochastic Gradient Descent(32302): loss=1.7454514469399303\n",
      "Stochastic Gradient Descent(32303): loss=1.2766092489527125\n",
      "Stochastic Gradient Descent(32304): loss=0.34122815306945004\n",
      "Stochastic Gradient Descent(32305): loss=0.15730296778287284\n",
      "Stochastic Gradient Descent(32306): loss=4.702066913188687\n",
      "Stochastic Gradient Descent(32307): loss=3.7182974451165327\n",
      "Stochastic Gradient Descent(32308): loss=5.916928780478914\n",
      "Stochastic Gradient Descent(32309): loss=3.5677887810077245\n",
      "Stochastic Gradient Descent(32310): loss=6.88603319664432\n",
      "Stochastic Gradient Descent(32311): loss=0.009267109249435078\n",
      "Stochastic Gradient Descent(32312): loss=11.53483528482996\n",
      "Stochastic Gradient Descent(32313): loss=0.5239019088358584\n",
      "Stochastic Gradient Descent(32314): loss=5.98320199157078\n",
      "Stochastic Gradient Descent(32315): loss=7.332779785533724\n",
      "Stochastic Gradient Descent(32316): loss=10.962926162902486\n",
      "Stochastic Gradient Descent(32317): loss=0.8321042485750144\n",
      "Stochastic Gradient Descent(32318): loss=8.441169836146345\n",
      "Stochastic Gradient Descent(32319): loss=0.04350997788568909\n",
      "Stochastic Gradient Descent(32320): loss=0.4652406107525205\n",
      "Stochastic Gradient Descent(32321): loss=1.9292100852741296\n",
      "Stochastic Gradient Descent(32322): loss=15.25404484831363\n",
      "Stochastic Gradient Descent(32323): loss=25.019220930965478\n",
      "Stochastic Gradient Descent(32324): loss=0.28522993232224236\n",
      "Stochastic Gradient Descent(32325): loss=7.611490524098677\n",
      "Stochastic Gradient Descent(32326): loss=0.7103972920582533\n",
      "Stochastic Gradient Descent(32327): loss=0.07535091375343814\n",
      "Stochastic Gradient Descent(32328): loss=58.75895306739353\n",
      "Stochastic Gradient Descent(32329): loss=3.5643645852301833\n",
      "Stochastic Gradient Descent(32330): loss=0.00015563631275989238\n",
      "Stochastic Gradient Descent(32331): loss=0.3676887618063241\n",
      "Stochastic Gradient Descent(32332): loss=0.6555427842872845\n",
      "Stochastic Gradient Descent(32333): loss=0.9747346426699632\n",
      "Stochastic Gradient Descent(32334): loss=0.49418280030718753\n",
      "Stochastic Gradient Descent(32335): loss=1.3820330718155367\n",
      "Stochastic Gradient Descent(32336): loss=2.1282634821215383\n",
      "Stochastic Gradient Descent(32337): loss=2.9771048732581034\n",
      "Stochastic Gradient Descent(32338): loss=0.0066152020342175035\n",
      "Stochastic Gradient Descent(32339): loss=38.68798422516991\n",
      "Stochastic Gradient Descent(32340): loss=9.20596570107868\n",
      "Stochastic Gradient Descent(32341): loss=31.649145732261257\n",
      "Stochastic Gradient Descent(32342): loss=17.223128408671418\n",
      "Stochastic Gradient Descent(32343): loss=6.35792046165937\n",
      "Stochastic Gradient Descent(32344): loss=48.56716309601548\n",
      "Stochastic Gradient Descent(32345): loss=12.314799177546739\n",
      "Stochastic Gradient Descent(32346): loss=0.8877150031777936\n",
      "Stochastic Gradient Descent(32347): loss=1.2559565662494188\n",
      "Stochastic Gradient Descent(32348): loss=0.1341960936396598\n",
      "Stochastic Gradient Descent(32349): loss=0.018404355919621126\n",
      "Stochastic Gradient Descent(32350): loss=10.229374435403559\n",
      "Stochastic Gradient Descent(32351): loss=10.523539446105318\n",
      "Stochastic Gradient Descent(32352): loss=5.5445170178355205\n",
      "Stochastic Gradient Descent(32353): loss=0.10856998191722744\n",
      "Stochastic Gradient Descent(32354): loss=7.424013161791037\n",
      "Stochastic Gradient Descent(32355): loss=0.7199599287820162\n",
      "Stochastic Gradient Descent(32356): loss=0.4338410132771565\n",
      "Stochastic Gradient Descent(32357): loss=5.774462658991216\n",
      "Stochastic Gradient Descent(32358): loss=0.0024483662743721205\n",
      "Stochastic Gradient Descent(32359): loss=0.07827594686393886\n",
      "Stochastic Gradient Descent(32360): loss=2.091610414932265\n",
      "Stochastic Gradient Descent(32361): loss=5.893972678076009\n",
      "Stochastic Gradient Descent(32362): loss=1.1410740130026602\n",
      "Stochastic Gradient Descent(32363): loss=5.272599776016117\n",
      "Stochastic Gradient Descent(32364): loss=0.5302731833705915\n",
      "Stochastic Gradient Descent(32365): loss=0.059425019497411066\n",
      "Stochastic Gradient Descent(32366): loss=13.557259939228327\n",
      "Stochastic Gradient Descent(32367): loss=0.764451537301188\n",
      "Stochastic Gradient Descent(32368): loss=0.07815167161218652\n",
      "Stochastic Gradient Descent(32369): loss=2.151110245340549\n",
      "Stochastic Gradient Descent(32370): loss=2.7014465795141667\n",
      "Stochastic Gradient Descent(32371): loss=0.014621953934336096\n",
      "Stochastic Gradient Descent(32372): loss=0.9699932665651757\n",
      "Stochastic Gradient Descent(32373): loss=1.1559683329769668\n",
      "Stochastic Gradient Descent(32374): loss=1.087912249583222\n",
      "Stochastic Gradient Descent(32375): loss=0.6199659386742011\n",
      "Stochastic Gradient Descent(32376): loss=1.7733997654252935\n",
      "Stochastic Gradient Descent(32377): loss=13.165372364477998\n",
      "Stochastic Gradient Descent(32378): loss=0.2072146349558551\n",
      "Stochastic Gradient Descent(32379): loss=3.330114987193459\n",
      "Stochastic Gradient Descent(32380): loss=1.3048671121924722\n",
      "Stochastic Gradient Descent(32381): loss=1.4862920870769873\n",
      "Stochastic Gradient Descent(32382): loss=0.6387954814474119\n",
      "Stochastic Gradient Descent(32383): loss=1.6747884909843544\n",
      "Stochastic Gradient Descent(32384): loss=12.032625447662971\n",
      "Stochastic Gradient Descent(32385): loss=0.027917686928977885\n",
      "Stochastic Gradient Descent(32386): loss=0.07398767808312087\n",
      "Stochastic Gradient Descent(32387): loss=9.908660164128694\n",
      "Stochastic Gradient Descent(32388): loss=0.07895532889849342\n",
      "Stochastic Gradient Descent(32389): loss=55.01010867624023\n",
      "Stochastic Gradient Descent(32390): loss=3.404548707096877\n",
      "Stochastic Gradient Descent(32391): loss=4.044938897482548\n",
      "Stochastic Gradient Descent(32392): loss=21.71958091646562\n",
      "Stochastic Gradient Descent(32393): loss=12.431207054043735\n",
      "Stochastic Gradient Descent(32394): loss=0.05256207539668823\n",
      "Stochastic Gradient Descent(32395): loss=3.9985685358842034\n",
      "Stochastic Gradient Descent(32396): loss=0.3694649698374723\n",
      "Stochastic Gradient Descent(32397): loss=3.4506365360841915\n",
      "Stochastic Gradient Descent(32398): loss=5.460066021668733\n",
      "Stochastic Gradient Descent(32399): loss=1.4799279287836014\n",
      "Stochastic Gradient Descent(32400): loss=0.3437639328372129\n",
      "Stochastic Gradient Descent(32401): loss=0.041861691006814375\n",
      "Stochastic Gradient Descent(32402): loss=2.5079554577674648\n",
      "Stochastic Gradient Descent(32403): loss=0.3879304417134206\n",
      "Stochastic Gradient Descent(32404): loss=2.9792094275697107\n",
      "Stochastic Gradient Descent(32405): loss=11.977047103511271\n",
      "Stochastic Gradient Descent(32406): loss=0.14462970422404842\n",
      "Stochastic Gradient Descent(32407): loss=18.778183386510452\n",
      "Stochastic Gradient Descent(32408): loss=6.158938893594875\n",
      "Stochastic Gradient Descent(32409): loss=14.92479863039222\n",
      "Stochastic Gradient Descent(32410): loss=1.0473407940523962\n",
      "Stochastic Gradient Descent(32411): loss=9.07478513412477\n",
      "Stochastic Gradient Descent(32412): loss=19.44384216652839\n",
      "Stochastic Gradient Descent(32413): loss=1.0446059199200266\n",
      "Stochastic Gradient Descent(32414): loss=3.1036172494711725\n",
      "Stochastic Gradient Descent(32415): loss=9.837865831021887\n",
      "Stochastic Gradient Descent(32416): loss=2.3920128536495424\n",
      "Stochastic Gradient Descent(32417): loss=1.0551266508741861\n",
      "Stochastic Gradient Descent(32418): loss=0.024398439772945577\n",
      "Stochastic Gradient Descent(32419): loss=2.117713503605621\n",
      "Stochastic Gradient Descent(32420): loss=0.0034932684645299435\n",
      "Stochastic Gradient Descent(32421): loss=4.391256847088865\n",
      "Stochastic Gradient Descent(32422): loss=0.0004522980880249072\n",
      "Stochastic Gradient Descent(32423): loss=0.48851303352788567\n",
      "Stochastic Gradient Descent(32424): loss=0.26016185714558654\n",
      "Stochastic Gradient Descent(32425): loss=0.10384296069165537\n",
      "Stochastic Gradient Descent(32426): loss=13.092590499306066\n",
      "Stochastic Gradient Descent(32427): loss=2.3038161631955796\n",
      "Stochastic Gradient Descent(32428): loss=0.3912386663991118\n",
      "Stochastic Gradient Descent(32429): loss=2.229608300903594\n",
      "Stochastic Gradient Descent(32430): loss=1.1396575048328956\n",
      "Stochastic Gradient Descent(32431): loss=0.3456890586725486\n",
      "Stochastic Gradient Descent(32432): loss=0.14927160025733155\n",
      "Stochastic Gradient Descent(32433): loss=6.903039502259937\n",
      "Stochastic Gradient Descent(32434): loss=7.191548369989854\n",
      "Stochastic Gradient Descent(32435): loss=2.4261384148218355\n",
      "Stochastic Gradient Descent(32436): loss=2.2281653355732427\n",
      "Stochastic Gradient Descent(32437): loss=20.294108421719557\n",
      "Stochastic Gradient Descent(32438): loss=1.4033042085236271\n",
      "Stochastic Gradient Descent(32439): loss=0.09654283453604945\n",
      "Stochastic Gradient Descent(32440): loss=8.300561510864549\n",
      "Stochastic Gradient Descent(32441): loss=0.10323703720034212\n",
      "Stochastic Gradient Descent(32442): loss=2.2888611456115875\n",
      "Stochastic Gradient Descent(32443): loss=0.22991815653054828\n",
      "Stochastic Gradient Descent(32444): loss=0.7156381844931827\n",
      "Stochastic Gradient Descent(32445): loss=10.992913496340286\n",
      "Stochastic Gradient Descent(32446): loss=7.136226188192442\n",
      "Stochastic Gradient Descent(32447): loss=1.4737807959261915\n",
      "Stochastic Gradient Descent(32448): loss=3.872589896924726e-05\n",
      "Stochastic Gradient Descent(32449): loss=1.6511819895730107\n",
      "Stochastic Gradient Descent(32450): loss=1.2302427638840416\n",
      "Stochastic Gradient Descent(32451): loss=0.07849982033024876\n",
      "Stochastic Gradient Descent(32452): loss=0.8332723025925373\n",
      "Stochastic Gradient Descent(32453): loss=4.080246152108648\n",
      "Stochastic Gradient Descent(32454): loss=1.1481208182266276\n",
      "Stochastic Gradient Descent(32455): loss=0.008792188592240383\n",
      "Stochastic Gradient Descent(32456): loss=5.547652150179549\n",
      "Stochastic Gradient Descent(32457): loss=3.0264623524460252\n",
      "Stochastic Gradient Descent(32458): loss=0.0028202809822819616\n",
      "Stochastic Gradient Descent(32459): loss=0.8699160949616717\n",
      "Stochastic Gradient Descent(32460): loss=0.3352945857669327\n",
      "Stochastic Gradient Descent(32461): loss=0.007639995338328484\n",
      "Stochastic Gradient Descent(32462): loss=1.3233817336055596\n",
      "Stochastic Gradient Descent(32463): loss=18.077347318074434\n",
      "Stochastic Gradient Descent(32464): loss=0.18341786191143147\n",
      "Stochastic Gradient Descent(32465): loss=1.6548546020341635\n",
      "Stochastic Gradient Descent(32466): loss=1.123947457971442\n",
      "Stochastic Gradient Descent(32467): loss=0.7343681280632801\n",
      "Stochastic Gradient Descent(32468): loss=0.6471513148336031\n",
      "Stochastic Gradient Descent(32469): loss=4.13609601742886\n",
      "Stochastic Gradient Descent(32470): loss=0.6072688183786842\n",
      "Stochastic Gradient Descent(32471): loss=1.9234312113028667\n",
      "Stochastic Gradient Descent(32472): loss=4.77943181346361\n",
      "Stochastic Gradient Descent(32473): loss=1.467741291709223\n",
      "Stochastic Gradient Descent(32474): loss=27.742887454117273\n",
      "Stochastic Gradient Descent(32475): loss=36.63486821693855\n",
      "Stochastic Gradient Descent(32476): loss=16.439721680909162\n",
      "Stochastic Gradient Descent(32477): loss=0.0798788773990175\n",
      "Stochastic Gradient Descent(32478): loss=1.0961702994435334\n",
      "Stochastic Gradient Descent(32479): loss=3.4340682852732645\n",
      "Stochastic Gradient Descent(32480): loss=1.3825595405220852\n",
      "Stochastic Gradient Descent(32481): loss=1.2390418054707963\n",
      "Stochastic Gradient Descent(32482): loss=0.005114524518153117\n",
      "Stochastic Gradient Descent(32483): loss=0.8075687946375332\n",
      "Stochastic Gradient Descent(32484): loss=3.738329143824734\n",
      "Stochastic Gradient Descent(32485): loss=18.99393290612019\n",
      "Stochastic Gradient Descent(32486): loss=0.9895143219450147\n",
      "Stochastic Gradient Descent(32487): loss=0.5348147911448201\n",
      "Stochastic Gradient Descent(32488): loss=2.5526332263552027\n",
      "Stochastic Gradient Descent(32489): loss=9.568260814044509\n",
      "Stochastic Gradient Descent(32490): loss=0.5126099945432195\n",
      "Stochastic Gradient Descent(32491): loss=0.18711735814246314\n",
      "Stochastic Gradient Descent(32492): loss=3.247955156372695\n",
      "Stochastic Gradient Descent(32493): loss=2.1191583111330714\n",
      "Stochastic Gradient Descent(32494): loss=2.946998660266651\n",
      "Stochastic Gradient Descent(32495): loss=9.404375031990409e-05\n",
      "Stochastic Gradient Descent(32496): loss=1.5454367901191166\n",
      "Stochastic Gradient Descent(32497): loss=0.102758192025169\n",
      "Stochastic Gradient Descent(32498): loss=2.5221055877001346\n",
      "Stochastic Gradient Descent(32499): loss=1.853372471834318\n",
      "Stochastic Gradient Descent(32500): loss=0.5348551004942816\n",
      "Stochastic Gradient Descent(32501): loss=1.7452561631951111\n",
      "Stochastic Gradient Descent(32502): loss=0.19226983074630963\n",
      "Stochastic Gradient Descent(32503): loss=0.6600340846829447\n",
      "Stochastic Gradient Descent(32504): loss=11.18237880575481\n",
      "Stochastic Gradient Descent(32505): loss=3.8609131596390087\n",
      "Stochastic Gradient Descent(32506): loss=33.98937525669467\n",
      "Stochastic Gradient Descent(32507): loss=0.07374883429495747\n",
      "Stochastic Gradient Descent(32508): loss=1.4404857113356409\n",
      "Stochastic Gradient Descent(32509): loss=6.268476682226798\n",
      "Stochastic Gradient Descent(32510): loss=0.749634627783385\n",
      "Stochastic Gradient Descent(32511): loss=0.09575486557183442\n",
      "Stochastic Gradient Descent(32512): loss=0.29486026143648925\n",
      "Stochastic Gradient Descent(32513): loss=0.5230157770014104\n",
      "Stochastic Gradient Descent(32514): loss=3.548958241786468\n",
      "Stochastic Gradient Descent(32515): loss=8.24847230395342\n",
      "Stochastic Gradient Descent(32516): loss=10.316440092736965\n",
      "Stochastic Gradient Descent(32517): loss=34.05494072719145\n",
      "Stochastic Gradient Descent(32518): loss=1.0498021866230796\n",
      "Stochastic Gradient Descent(32519): loss=31.994178332568758\n",
      "Stochastic Gradient Descent(32520): loss=0.23562431661254465\n",
      "Stochastic Gradient Descent(32521): loss=4.26091270647885\n",
      "Stochastic Gradient Descent(32522): loss=5.019297832208139\n",
      "Stochastic Gradient Descent(32523): loss=3.584652297731896\n",
      "Stochastic Gradient Descent(32524): loss=14.50011087181964\n",
      "Stochastic Gradient Descent(32525): loss=1.2340937827946727\n",
      "Stochastic Gradient Descent(32526): loss=25.53197701486301\n",
      "Stochastic Gradient Descent(32527): loss=4.209397124051096\n",
      "Stochastic Gradient Descent(32528): loss=0.020057846860414365\n",
      "Stochastic Gradient Descent(32529): loss=19.741169710278132\n",
      "Stochastic Gradient Descent(32530): loss=8.71601984524579\n",
      "Stochastic Gradient Descent(32531): loss=10.349389157604378\n",
      "Stochastic Gradient Descent(32532): loss=0.0005237652186930673\n",
      "Stochastic Gradient Descent(32533): loss=4.897692460077323\n",
      "Stochastic Gradient Descent(32534): loss=1.5753625083221683\n",
      "Stochastic Gradient Descent(32535): loss=4.129085483847581\n",
      "Stochastic Gradient Descent(32536): loss=0.24011283911159867\n",
      "Stochastic Gradient Descent(32537): loss=13.877315700028799\n",
      "Stochastic Gradient Descent(32538): loss=13.546201915391208\n",
      "Stochastic Gradient Descent(32539): loss=4.268058162985053\n",
      "Stochastic Gradient Descent(32540): loss=6.079740749386399\n",
      "Stochastic Gradient Descent(32541): loss=0.04075488552103495\n",
      "Stochastic Gradient Descent(32542): loss=0.9193266756590549\n",
      "Stochastic Gradient Descent(32543): loss=2.9887808946989405\n",
      "Stochastic Gradient Descent(32544): loss=7.814372978539398\n",
      "Stochastic Gradient Descent(32545): loss=2.638046452211997\n",
      "Stochastic Gradient Descent(32546): loss=0.45991934437637066\n",
      "Stochastic Gradient Descent(32547): loss=0.30416353114487643\n",
      "Stochastic Gradient Descent(32548): loss=0.9735261742647021\n",
      "Stochastic Gradient Descent(32549): loss=0.3306636922957188\n",
      "Stochastic Gradient Descent(32550): loss=7.699159027567682\n",
      "Stochastic Gradient Descent(32551): loss=0.6675971764681365\n",
      "Stochastic Gradient Descent(32552): loss=7.311650709685258\n",
      "Stochastic Gradient Descent(32553): loss=3.870961270262091\n",
      "Stochastic Gradient Descent(32554): loss=0.2926323830707476\n",
      "Stochastic Gradient Descent(32555): loss=5.951318891622559\n",
      "Stochastic Gradient Descent(32556): loss=1.3404431653615556\n",
      "Stochastic Gradient Descent(32557): loss=0.048791627613153844\n",
      "Stochastic Gradient Descent(32558): loss=7.366140805823271\n",
      "Stochastic Gradient Descent(32559): loss=6.456803487511321\n",
      "Stochastic Gradient Descent(32560): loss=0.4978291542736403\n",
      "Stochastic Gradient Descent(32561): loss=0.07477209693812782\n",
      "Stochastic Gradient Descent(32562): loss=16.511768714980324\n",
      "Stochastic Gradient Descent(32563): loss=2.6753925939492817\n",
      "Stochastic Gradient Descent(32564): loss=0.10266819456126379\n",
      "Stochastic Gradient Descent(32565): loss=1.0918429515433705\n",
      "Stochastic Gradient Descent(32566): loss=0.5887858098510537\n",
      "Stochastic Gradient Descent(32567): loss=4.971511998509957\n",
      "Stochastic Gradient Descent(32568): loss=0.9661697196017646\n",
      "Stochastic Gradient Descent(32569): loss=8.27617398526845\n",
      "Stochastic Gradient Descent(32570): loss=5.783598270134753\n",
      "Stochastic Gradient Descent(32571): loss=19.5526184133643\n",
      "Stochastic Gradient Descent(32572): loss=10.245579589340686\n",
      "Stochastic Gradient Descent(32573): loss=6.507546162860982\n",
      "Stochastic Gradient Descent(32574): loss=44.7212513726123\n",
      "Stochastic Gradient Descent(32575): loss=11.656795187023755\n",
      "Stochastic Gradient Descent(32576): loss=1.9783611664890592\n",
      "Stochastic Gradient Descent(32577): loss=0.19134923335577825\n",
      "Stochastic Gradient Descent(32578): loss=0.1697649285619089\n",
      "Stochastic Gradient Descent(32579): loss=0.8303930088531213\n",
      "Stochastic Gradient Descent(32580): loss=0.04462147346453589\n",
      "Stochastic Gradient Descent(32581): loss=0.0032829645320953354\n",
      "Stochastic Gradient Descent(32582): loss=3.974626909193251\n",
      "Stochastic Gradient Descent(32583): loss=0.2591647005396801\n",
      "Stochastic Gradient Descent(32584): loss=2.3211906955954658\n",
      "Stochastic Gradient Descent(32585): loss=0.38084016031576834\n",
      "Stochastic Gradient Descent(32586): loss=4.902625116484098\n",
      "Stochastic Gradient Descent(32587): loss=3.207352902049289\n",
      "Stochastic Gradient Descent(32588): loss=3.395426468121298\n",
      "Stochastic Gradient Descent(32589): loss=20.072272661398888\n",
      "Stochastic Gradient Descent(32590): loss=5.032526430198108\n",
      "Stochastic Gradient Descent(32591): loss=9.387931402926098\n",
      "Stochastic Gradient Descent(32592): loss=0.7745564969357502\n",
      "Stochastic Gradient Descent(32593): loss=0.00942309833545003\n",
      "Stochastic Gradient Descent(32594): loss=17.135742852633513\n",
      "Stochastic Gradient Descent(32595): loss=2.050752192319492\n",
      "Stochastic Gradient Descent(32596): loss=8.089720454873236\n",
      "Stochastic Gradient Descent(32597): loss=0.012141417133175304\n",
      "Stochastic Gradient Descent(32598): loss=5.639672362639128\n",
      "Stochastic Gradient Descent(32599): loss=2.568843050374161\n",
      "Stochastic Gradient Descent(32600): loss=8.60139391327799\n",
      "Stochastic Gradient Descent(32601): loss=6.665419205846679\n",
      "Stochastic Gradient Descent(32602): loss=1.172495902365785\n",
      "Stochastic Gradient Descent(32603): loss=10.07430370487713\n",
      "Stochastic Gradient Descent(32604): loss=0.11539962479190075\n",
      "Stochastic Gradient Descent(32605): loss=0.5619079517283583\n",
      "Stochastic Gradient Descent(32606): loss=12.769497993596309\n",
      "Stochastic Gradient Descent(32607): loss=0.01758480231437033\n",
      "Stochastic Gradient Descent(32608): loss=1.2809452591376749\n",
      "Stochastic Gradient Descent(32609): loss=1.5735666654352791\n",
      "Stochastic Gradient Descent(32610): loss=0.019951647189743894\n",
      "Stochastic Gradient Descent(32611): loss=2.153382330445078\n",
      "Stochastic Gradient Descent(32612): loss=0.835005655284011\n",
      "Stochastic Gradient Descent(32613): loss=3.2631865898106702\n",
      "Stochastic Gradient Descent(32614): loss=1.0126754476749227\n",
      "Stochastic Gradient Descent(32615): loss=0.6260543067678862\n",
      "Stochastic Gradient Descent(32616): loss=0.02178761752026398\n",
      "Stochastic Gradient Descent(32617): loss=14.741674791834603\n",
      "Stochastic Gradient Descent(32618): loss=8.06715111816289\n",
      "Stochastic Gradient Descent(32619): loss=16.207898283414426\n",
      "Stochastic Gradient Descent(32620): loss=17.700728473894703\n",
      "Stochastic Gradient Descent(32621): loss=0.417616454846387\n",
      "Stochastic Gradient Descent(32622): loss=45.55791419870664\n",
      "Stochastic Gradient Descent(32623): loss=9.041330229162368\n",
      "Stochastic Gradient Descent(32624): loss=0.016108452958479156\n",
      "Stochastic Gradient Descent(32625): loss=4.1352427370117715\n",
      "Stochastic Gradient Descent(32626): loss=4.127589652329572\n",
      "Stochastic Gradient Descent(32627): loss=4.679182423947318\n",
      "Stochastic Gradient Descent(32628): loss=25.83770771333093\n",
      "Stochastic Gradient Descent(32629): loss=8.521453534375327\n",
      "Stochastic Gradient Descent(32630): loss=4.5598794608380935\n",
      "Stochastic Gradient Descent(32631): loss=0.9230672543146462\n",
      "Stochastic Gradient Descent(32632): loss=1.1791575061050381\n",
      "Stochastic Gradient Descent(32633): loss=5.863733926167496\n",
      "Stochastic Gradient Descent(32634): loss=7.732055272592176\n",
      "Stochastic Gradient Descent(32635): loss=16.50358264303512\n",
      "Stochastic Gradient Descent(32636): loss=0.008672655918382939\n",
      "Stochastic Gradient Descent(32637): loss=2.8793638394703\n",
      "Stochastic Gradient Descent(32638): loss=21.749203797225327\n",
      "Stochastic Gradient Descent(32639): loss=14.619916179779802\n",
      "Stochastic Gradient Descent(32640): loss=0.27082529354377777\n",
      "Stochastic Gradient Descent(32641): loss=9.496063237663016\n",
      "Stochastic Gradient Descent(32642): loss=1.4569456285849063\n",
      "Stochastic Gradient Descent(32643): loss=0.2302128397434554\n",
      "Stochastic Gradient Descent(32644): loss=3.5200877511207826\n",
      "Stochastic Gradient Descent(32645): loss=27.897558255722657\n",
      "Stochastic Gradient Descent(32646): loss=5.998992176941373\n",
      "Stochastic Gradient Descent(32647): loss=3.25060843704482\n",
      "Stochastic Gradient Descent(32648): loss=1.3148579075774514\n",
      "Stochastic Gradient Descent(32649): loss=10.08866956788979\n",
      "Stochastic Gradient Descent(32650): loss=0.567000090530342\n",
      "Stochastic Gradient Descent(32651): loss=0.10379445433944484\n",
      "Stochastic Gradient Descent(32652): loss=0.537062980289561\n",
      "Stochastic Gradient Descent(32653): loss=6.864595332916029\n",
      "Stochastic Gradient Descent(32654): loss=0.9175888612639563\n",
      "Stochastic Gradient Descent(32655): loss=1.0462588045252579\n",
      "Stochastic Gradient Descent(32656): loss=5.37758182826744\n",
      "Stochastic Gradient Descent(32657): loss=0.5365225367898736\n",
      "Stochastic Gradient Descent(32658): loss=8.501697179715725\n",
      "Stochastic Gradient Descent(32659): loss=1.6458280337995683\n",
      "Stochastic Gradient Descent(32660): loss=0.07854719489201344\n",
      "Stochastic Gradient Descent(32661): loss=0.3378049061547905\n",
      "Stochastic Gradient Descent(32662): loss=3.3396795355179485\n",
      "Stochastic Gradient Descent(32663): loss=1.679409389671967\n",
      "Stochastic Gradient Descent(32664): loss=0.6052158327843774\n",
      "Stochastic Gradient Descent(32665): loss=0.14238947829811321\n",
      "Stochastic Gradient Descent(32666): loss=5.283028316755757\n",
      "Stochastic Gradient Descent(32667): loss=0.02191293159563838\n",
      "Stochastic Gradient Descent(32668): loss=1.8254926868047803\n",
      "Stochastic Gradient Descent(32669): loss=0.8981904476098672\n",
      "Stochastic Gradient Descent(32670): loss=0.40451811574815105\n",
      "Stochastic Gradient Descent(32671): loss=15.841536518380458\n",
      "Stochastic Gradient Descent(32672): loss=3.655800191072716\n",
      "Stochastic Gradient Descent(32673): loss=24.817466896116766\n",
      "Stochastic Gradient Descent(32674): loss=0.00875561755492442\n",
      "Stochastic Gradient Descent(32675): loss=2.1696960345829157\n",
      "Stochastic Gradient Descent(32676): loss=8.345934252533832\n",
      "Stochastic Gradient Descent(32677): loss=0.19315115045193507\n",
      "Stochastic Gradient Descent(32678): loss=1.5253927161636809\n",
      "Stochastic Gradient Descent(32679): loss=0.25357014286432167\n",
      "Stochastic Gradient Descent(32680): loss=0.3878295510642587\n",
      "Stochastic Gradient Descent(32681): loss=0.35875967743222126\n",
      "Stochastic Gradient Descent(32682): loss=2.8061761037681476\n",
      "Stochastic Gradient Descent(32683): loss=0.2626471197754543\n",
      "Stochastic Gradient Descent(32684): loss=9.615843604545377\n",
      "Stochastic Gradient Descent(32685): loss=0.47574553210338294\n",
      "Stochastic Gradient Descent(32686): loss=0.1348720711143086\n",
      "Stochastic Gradient Descent(32687): loss=3.585910152410381\n",
      "Stochastic Gradient Descent(32688): loss=5.548393787088114\n",
      "Stochastic Gradient Descent(32689): loss=4.806786859494184\n",
      "Stochastic Gradient Descent(32690): loss=0.2552742975469638\n",
      "Stochastic Gradient Descent(32691): loss=5.021862330845021\n",
      "Stochastic Gradient Descent(32692): loss=0.14392132740514033\n",
      "Stochastic Gradient Descent(32693): loss=0.2510400573351703\n",
      "Stochastic Gradient Descent(32694): loss=7.246468565235063\n",
      "Stochastic Gradient Descent(32695): loss=3.5687065760576466\n",
      "Stochastic Gradient Descent(32696): loss=0.006053352298589912\n",
      "Stochastic Gradient Descent(32697): loss=0.12184660078843579\n",
      "Stochastic Gradient Descent(32698): loss=0.10219818253425661\n",
      "Stochastic Gradient Descent(32699): loss=0.17027830988784534\n",
      "Stochastic Gradient Descent(32700): loss=1.3021061045849338\n",
      "Stochastic Gradient Descent(32701): loss=8.46782056052979\n",
      "Stochastic Gradient Descent(32702): loss=6.972122720887062\n",
      "Stochastic Gradient Descent(32703): loss=2.2163255527197143\n",
      "Stochastic Gradient Descent(32704): loss=2.164938933761182\n",
      "Stochastic Gradient Descent(32705): loss=7.529848188263473\n",
      "Stochastic Gradient Descent(32706): loss=3.4763233856795885\n",
      "Stochastic Gradient Descent(32707): loss=7.746960321048105\n",
      "Stochastic Gradient Descent(32708): loss=0.4743170565198307\n",
      "Stochastic Gradient Descent(32709): loss=3.975437897302214\n",
      "Stochastic Gradient Descent(32710): loss=0.3984845504604208\n",
      "Stochastic Gradient Descent(32711): loss=0.37859037030617737\n",
      "Stochastic Gradient Descent(32712): loss=15.610398155645857\n",
      "Stochastic Gradient Descent(32713): loss=0.02217014089084954\n",
      "Stochastic Gradient Descent(32714): loss=8.872783899518074\n",
      "Stochastic Gradient Descent(32715): loss=0.3582481479725947\n",
      "Stochastic Gradient Descent(32716): loss=1.583823097275028\n",
      "Stochastic Gradient Descent(32717): loss=0.5422918379248185\n",
      "Stochastic Gradient Descent(32718): loss=1.4390400954137514\n",
      "Stochastic Gradient Descent(32719): loss=0.3541246071166139\n",
      "Stochastic Gradient Descent(32720): loss=13.541814879754769\n",
      "Stochastic Gradient Descent(32721): loss=1.7799098209517739\n",
      "Stochastic Gradient Descent(32722): loss=2.5102179098867925\n",
      "Stochastic Gradient Descent(32723): loss=1.0097612274160144\n",
      "Stochastic Gradient Descent(32724): loss=9.301378829867168\n",
      "Stochastic Gradient Descent(32725): loss=3.3728179026039657\n",
      "Stochastic Gradient Descent(32726): loss=8.326257040364146\n",
      "Stochastic Gradient Descent(32727): loss=15.704813495910013\n",
      "Stochastic Gradient Descent(32728): loss=14.763794487610884\n",
      "Stochastic Gradient Descent(32729): loss=0.34371562653392546\n",
      "Stochastic Gradient Descent(32730): loss=13.25788613247251\n",
      "Stochastic Gradient Descent(32731): loss=9.915842011642921\n",
      "Stochastic Gradient Descent(32732): loss=0.036764036483640586\n",
      "Stochastic Gradient Descent(32733): loss=17.630948924888038\n",
      "Stochastic Gradient Descent(32734): loss=0.09180231361242572\n",
      "Stochastic Gradient Descent(32735): loss=9.356033806173489\n",
      "Stochastic Gradient Descent(32736): loss=0.16840687859445413\n",
      "Stochastic Gradient Descent(32737): loss=4.045105924495702\n",
      "Stochastic Gradient Descent(32738): loss=0.3333681031596507\n",
      "Stochastic Gradient Descent(32739): loss=11.453856325655972\n",
      "Stochastic Gradient Descent(32740): loss=2.5671155159613344\n",
      "Stochastic Gradient Descent(32741): loss=0.3016968368947979\n",
      "Stochastic Gradient Descent(32742): loss=5.545576910154519\n",
      "Stochastic Gradient Descent(32743): loss=1.6775651260603741\n",
      "Stochastic Gradient Descent(32744): loss=2.0176454647366477\n",
      "Stochastic Gradient Descent(32745): loss=0.25657355400262766\n",
      "Stochastic Gradient Descent(32746): loss=0.31405145116594013\n",
      "Stochastic Gradient Descent(32747): loss=9.845696500412032\n",
      "Stochastic Gradient Descent(32748): loss=1.7981904689806751\n",
      "Stochastic Gradient Descent(32749): loss=0.8278787050969033\n",
      "Stochastic Gradient Descent(32750): loss=2.610061583652371\n",
      "Stochastic Gradient Descent(32751): loss=4.431491579407833\n",
      "Stochastic Gradient Descent(32752): loss=3.726881556178983\n",
      "Stochastic Gradient Descent(32753): loss=4.0844015939223155\n",
      "Stochastic Gradient Descent(32754): loss=3.912208468975815\n",
      "Stochastic Gradient Descent(32755): loss=2.6527030581985502\n",
      "Stochastic Gradient Descent(32756): loss=3.0892319013419165\n",
      "Stochastic Gradient Descent(32757): loss=0.5028904185233389\n",
      "Stochastic Gradient Descent(32758): loss=0.1956565411360546\n",
      "Stochastic Gradient Descent(32759): loss=10.413452574903518\n",
      "Stochastic Gradient Descent(32760): loss=7.764620291325137e-09\n",
      "Stochastic Gradient Descent(32761): loss=0.17118812845121353\n",
      "Stochastic Gradient Descent(32762): loss=1.1824959751469555\n",
      "Stochastic Gradient Descent(32763): loss=0.7803000677821602\n",
      "Stochastic Gradient Descent(32764): loss=9.988216277514473\n",
      "Stochastic Gradient Descent(32765): loss=10.224267868837241\n",
      "Stochastic Gradient Descent(32766): loss=12.190208827760898\n",
      "Stochastic Gradient Descent(32767): loss=3.536656098908735\n",
      "Stochastic Gradient Descent(32768): loss=2.7228403011116127\n",
      "Stochastic Gradient Descent(32769): loss=2.695801694008292\n",
      "Stochastic Gradient Descent(32770): loss=15.84201820624025\n",
      "Stochastic Gradient Descent(32771): loss=0.00015014178591894024\n",
      "Stochastic Gradient Descent(32772): loss=0.504984208589064\n",
      "Stochastic Gradient Descent(32773): loss=7.555984903497564\n",
      "Stochastic Gradient Descent(32774): loss=5.734641338218466\n",
      "Stochastic Gradient Descent(32775): loss=1.4617395177683898\n",
      "Stochastic Gradient Descent(32776): loss=4.130116044894627\n",
      "Stochastic Gradient Descent(32777): loss=1.5951033279502251\n",
      "Stochastic Gradient Descent(32778): loss=6.2645299604806945\n",
      "Stochastic Gradient Descent(32779): loss=0.9769191320411893\n",
      "Stochastic Gradient Descent(32780): loss=5.968205973603324\n",
      "Stochastic Gradient Descent(32781): loss=6.748030130758818\n",
      "Stochastic Gradient Descent(32782): loss=10.98187545981533\n",
      "Stochastic Gradient Descent(32783): loss=0.3696362274401821\n",
      "Stochastic Gradient Descent(32784): loss=0.21930537117698162\n",
      "Stochastic Gradient Descent(32785): loss=0.2331251490239664\n",
      "Stochastic Gradient Descent(32786): loss=0.3315691579817521\n",
      "Stochastic Gradient Descent(32787): loss=1.981906228311866\n",
      "Stochastic Gradient Descent(32788): loss=1.0658639659304434\n",
      "Stochastic Gradient Descent(32789): loss=2.4946891361958006\n",
      "Stochastic Gradient Descent(32790): loss=1.3747549197380047\n",
      "Stochastic Gradient Descent(32791): loss=11.79673756016802\n",
      "Stochastic Gradient Descent(32792): loss=2.705240829994929\n",
      "Stochastic Gradient Descent(32793): loss=2.4422564296736233\n",
      "Stochastic Gradient Descent(32794): loss=0.737918663220838\n",
      "Stochastic Gradient Descent(32795): loss=3.3547796814482123\n",
      "Stochastic Gradient Descent(32796): loss=0.8957630216552326\n",
      "Stochastic Gradient Descent(32797): loss=0.06237282223078856\n",
      "Stochastic Gradient Descent(32798): loss=7.779107182840338\n",
      "Stochastic Gradient Descent(32799): loss=16.12214523311563\n",
      "Stochastic Gradient Descent(32800): loss=0.32467552099101116\n",
      "Stochastic Gradient Descent(32801): loss=3.929426885776809\n",
      "Stochastic Gradient Descent(32802): loss=0.8175820088762805\n",
      "Stochastic Gradient Descent(32803): loss=6.028725022777465\n",
      "Stochastic Gradient Descent(32804): loss=3.73985417075532\n",
      "Stochastic Gradient Descent(32805): loss=0.00475893308154664\n",
      "Stochastic Gradient Descent(32806): loss=0.8700566051865758\n",
      "Stochastic Gradient Descent(32807): loss=6.758384157386516\n",
      "Stochastic Gradient Descent(32808): loss=0.024666317077954084\n",
      "Stochastic Gradient Descent(32809): loss=0.0027406607730571207\n",
      "Stochastic Gradient Descent(32810): loss=8.607843106950957\n",
      "Stochastic Gradient Descent(32811): loss=4.794753079485443\n",
      "Stochastic Gradient Descent(32812): loss=19.07916170845249\n",
      "Stochastic Gradient Descent(32813): loss=0.9165654578018806\n",
      "Stochastic Gradient Descent(32814): loss=0.03778880272820699\n",
      "Stochastic Gradient Descent(32815): loss=30.53417709341828\n",
      "Stochastic Gradient Descent(32816): loss=2.0004336295650456\n",
      "Stochastic Gradient Descent(32817): loss=0.34538727501605027\n",
      "Stochastic Gradient Descent(32818): loss=0.41486623469841016\n",
      "Stochastic Gradient Descent(32819): loss=4.830737849416446\n",
      "Stochastic Gradient Descent(32820): loss=0.7768850781966509\n",
      "Stochastic Gradient Descent(32821): loss=2.131335098858112\n",
      "Stochastic Gradient Descent(32822): loss=6.563231941028761\n",
      "Stochastic Gradient Descent(32823): loss=7.566082858751548\n",
      "Stochastic Gradient Descent(32824): loss=5.317903202452299\n",
      "Stochastic Gradient Descent(32825): loss=4.129435304641964\n",
      "Stochastic Gradient Descent(32826): loss=0.06712902801632525\n",
      "Stochastic Gradient Descent(32827): loss=0.009681995687935043\n",
      "Stochastic Gradient Descent(32828): loss=0.019820295375683493\n",
      "Stochastic Gradient Descent(32829): loss=1.1159731659557446\n",
      "Stochastic Gradient Descent(32830): loss=2.281999634125447\n",
      "Stochastic Gradient Descent(32831): loss=10.071676667655607\n",
      "Stochastic Gradient Descent(32832): loss=1.8020020790277538\n",
      "Stochastic Gradient Descent(32833): loss=0.010578567412133722\n",
      "Stochastic Gradient Descent(32834): loss=11.412178747210431\n",
      "Stochastic Gradient Descent(32835): loss=11.739217900710704\n",
      "Stochastic Gradient Descent(32836): loss=12.15666948172021\n",
      "Stochastic Gradient Descent(32837): loss=8.33413661338329\n",
      "Stochastic Gradient Descent(32838): loss=1.988988375042194\n",
      "Stochastic Gradient Descent(32839): loss=5.484078634686221\n",
      "Stochastic Gradient Descent(32840): loss=0.5017140737569983\n",
      "Stochastic Gradient Descent(32841): loss=2.74588333630529\n",
      "Stochastic Gradient Descent(32842): loss=0.31985770730049606\n",
      "Stochastic Gradient Descent(32843): loss=1.1182150365272767\n",
      "Stochastic Gradient Descent(32844): loss=2.155704396887334\n",
      "Stochastic Gradient Descent(32845): loss=1.0334178094640563\n",
      "Stochastic Gradient Descent(32846): loss=2.43789143134599\n",
      "Stochastic Gradient Descent(32847): loss=54.44327949889058\n",
      "Stochastic Gradient Descent(32848): loss=20.011437191371037\n",
      "Stochastic Gradient Descent(32849): loss=20.477774118230695\n",
      "Stochastic Gradient Descent(32850): loss=12.1712285797295\n",
      "Stochastic Gradient Descent(32851): loss=4.899093545127004\n",
      "Stochastic Gradient Descent(32852): loss=0.003990067218992126\n",
      "Stochastic Gradient Descent(32853): loss=1.0847413875282208\n",
      "Stochastic Gradient Descent(32854): loss=3.8548465429728354\n",
      "Stochastic Gradient Descent(32855): loss=2.838600344221295\n",
      "Stochastic Gradient Descent(32856): loss=3.043783479177869\n",
      "Stochastic Gradient Descent(32857): loss=6.360550959901905\n",
      "Stochastic Gradient Descent(32858): loss=2.1984611324054955\n",
      "Stochastic Gradient Descent(32859): loss=4.6699660110349495\n",
      "Stochastic Gradient Descent(32860): loss=12.21424620777258\n",
      "Stochastic Gradient Descent(32861): loss=1.4688274595407593\n",
      "Stochastic Gradient Descent(32862): loss=4.899011637326064\n",
      "Stochastic Gradient Descent(32863): loss=2.2501279604488746\n",
      "Stochastic Gradient Descent(32864): loss=0.014147543809305316\n",
      "Stochastic Gradient Descent(32865): loss=5.29750190407735\n",
      "Stochastic Gradient Descent(32866): loss=2.885855743893813\n",
      "Stochastic Gradient Descent(32867): loss=4.4827206826094645\n",
      "Stochastic Gradient Descent(32868): loss=2.134254079902396\n",
      "Stochastic Gradient Descent(32869): loss=4.326405128813535\n",
      "Stochastic Gradient Descent(32870): loss=2.615482849085506\n",
      "Stochastic Gradient Descent(32871): loss=13.569926257007886\n",
      "Stochastic Gradient Descent(32872): loss=0.8420715997126516\n",
      "Stochastic Gradient Descent(32873): loss=6.846904837259783\n",
      "Stochastic Gradient Descent(32874): loss=0.26103714240763337\n",
      "Stochastic Gradient Descent(32875): loss=0.7993396777130901\n",
      "Stochastic Gradient Descent(32876): loss=1.0381689229732975\n",
      "Stochastic Gradient Descent(32877): loss=0.9823793630208378\n",
      "Stochastic Gradient Descent(32878): loss=2.835170171259395\n",
      "Stochastic Gradient Descent(32879): loss=0.046224291345791306\n",
      "Stochastic Gradient Descent(32880): loss=9.623313872479157\n",
      "Stochastic Gradient Descent(32881): loss=1.9926460845980827\n",
      "Stochastic Gradient Descent(32882): loss=1.5780028284826408\n",
      "Stochastic Gradient Descent(32883): loss=9.146722472199624\n",
      "Stochastic Gradient Descent(32884): loss=0.0005480266735622538\n",
      "Stochastic Gradient Descent(32885): loss=0.6345936588623099\n",
      "Stochastic Gradient Descent(32886): loss=4.670858771903339\n",
      "Stochastic Gradient Descent(32887): loss=1.5204867552690984\n",
      "Stochastic Gradient Descent(32888): loss=3.9029547423849267\n",
      "Stochastic Gradient Descent(32889): loss=3.7944442397348266\n",
      "Stochastic Gradient Descent(32890): loss=19.662868519438277\n",
      "Stochastic Gradient Descent(32891): loss=8.122242461334643\n",
      "Stochastic Gradient Descent(32892): loss=2.490391953824301\n",
      "Stochastic Gradient Descent(32893): loss=0.7763816099678271\n",
      "Stochastic Gradient Descent(32894): loss=1.2413687613918856\n",
      "Stochastic Gradient Descent(32895): loss=0.2869260147226514\n",
      "Stochastic Gradient Descent(32896): loss=13.626643140204777\n",
      "Stochastic Gradient Descent(32897): loss=0.3998145213980786\n",
      "Stochastic Gradient Descent(32898): loss=2.9260380555187564\n",
      "Stochastic Gradient Descent(32899): loss=6.9523448232802965\n",
      "Stochastic Gradient Descent(32900): loss=4.351349237697251\n",
      "Stochastic Gradient Descent(32901): loss=0.6093373352261446\n",
      "Stochastic Gradient Descent(32902): loss=2.095157516378458\n",
      "Stochastic Gradient Descent(32903): loss=0.06894562030528398\n",
      "Stochastic Gradient Descent(32904): loss=0.06682406602353284\n",
      "Stochastic Gradient Descent(32905): loss=0.8626016735771074\n",
      "Stochastic Gradient Descent(32906): loss=0.07904352335341794\n",
      "Stochastic Gradient Descent(32907): loss=1.4494373403318437\n",
      "Stochastic Gradient Descent(32908): loss=0.24955548564142405\n",
      "Stochastic Gradient Descent(32909): loss=0.24376717055237063\n",
      "Stochastic Gradient Descent(32910): loss=0.03331383530895565\n",
      "Stochastic Gradient Descent(32911): loss=0.9399795360189017\n",
      "Stochastic Gradient Descent(32912): loss=0.6415197943635438\n",
      "Stochastic Gradient Descent(32913): loss=0.002844282206719312\n",
      "Stochastic Gradient Descent(32914): loss=3.1467116696102972\n",
      "Stochastic Gradient Descent(32915): loss=0.13650952210160766\n",
      "Stochastic Gradient Descent(32916): loss=0.5084320680312197\n",
      "Stochastic Gradient Descent(32917): loss=0.7156816669904653\n",
      "Stochastic Gradient Descent(32918): loss=0.40518296473374305\n",
      "Stochastic Gradient Descent(32919): loss=0.03540193555573728\n",
      "Stochastic Gradient Descent(32920): loss=0.24870735791059068\n",
      "Stochastic Gradient Descent(32921): loss=10.263843571251773\n",
      "Stochastic Gradient Descent(32922): loss=13.598933622576657\n",
      "Stochastic Gradient Descent(32923): loss=0.2907194101180481\n",
      "Stochastic Gradient Descent(32924): loss=0.01422314576724607\n",
      "Stochastic Gradient Descent(32925): loss=1.965625537246421\n",
      "Stochastic Gradient Descent(32926): loss=0.5163650865998072\n",
      "Stochastic Gradient Descent(32927): loss=6.665140488980927\n",
      "Stochastic Gradient Descent(32928): loss=5.946260787268631\n",
      "Stochastic Gradient Descent(32929): loss=3.0384589827799267\n",
      "Stochastic Gradient Descent(32930): loss=7.945391519049065\n",
      "Stochastic Gradient Descent(32931): loss=1.9808360601248882\n",
      "Stochastic Gradient Descent(32932): loss=0.016844898904466724\n",
      "Stochastic Gradient Descent(32933): loss=0.6939073350471731\n",
      "Stochastic Gradient Descent(32934): loss=0.659597024634883\n",
      "Stochastic Gradient Descent(32935): loss=1.1524491277970392\n",
      "Stochastic Gradient Descent(32936): loss=0.5733785608685937\n",
      "Stochastic Gradient Descent(32937): loss=0.2959555235899516\n",
      "Stochastic Gradient Descent(32938): loss=18.049814600785567\n",
      "Stochastic Gradient Descent(32939): loss=3.4092220791105032\n",
      "Stochastic Gradient Descent(32940): loss=0.20821479100730192\n",
      "Stochastic Gradient Descent(32941): loss=0.12776623558541694\n",
      "Stochastic Gradient Descent(32942): loss=16.3241256409436\n",
      "Stochastic Gradient Descent(32943): loss=0.6372083345922963\n",
      "Stochastic Gradient Descent(32944): loss=0.5716388168488541\n",
      "Stochastic Gradient Descent(32945): loss=2.273593613084546\n",
      "Stochastic Gradient Descent(32946): loss=1.744582104388048\n",
      "Stochastic Gradient Descent(32947): loss=14.478680994755935\n",
      "Stochastic Gradient Descent(32948): loss=0.0722000553939814\n",
      "Stochastic Gradient Descent(32949): loss=8.594262686072465\n",
      "Stochastic Gradient Descent(32950): loss=28.376783179811387\n",
      "Stochastic Gradient Descent(32951): loss=1.3603668425175408\n",
      "Stochastic Gradient Descent(32952): loss=17.51858155205415\n",
      "Stochastic Gradient Descent(32953): loss=9.365617563595421\n",
      "Stochastic Gradient Descent(32954): loss=6.8729745181452655\n",
      "Stochastic Gradient Descent(32955): loss=4.25347613559664\n",
      "Stochastic Gradient Descent(32956): loss=12.220391618690767\n",
      "Stochastic Gradient Descent(32957): loss=3.0452042409735114\n",
      "Stochastic Gradient Descent(32958): loss=13.962092576303357\n",
      "Stochastic Gradient Descent(32959): loss=10.593775325002067\n",
      "Stochastic Gradient Descent(32960): loss=1.709424263094847\n",
      "Stochastic Gradient Descent(32961): loss=8.556160115039118\n",
      "Stochastic Gradient Descent(32962): loss=3.777194281794594\n",
      "Stochastic Gradient Descent(32963): loss=5.4458930954355615\n",
      "Stochastic Gradient Descent(32964): loss=5.087767024989771\n",
      "Stochastic Gradient Descent(32965): loss=0.6822352396264072\n",
      "Stochastic Gradient Descent(32966): loss=0.08311774284367518\n",
      "Stochastic Gradient Descent(32967): loss=1.1712145575887183\n",
      "Stochastic Gradient Descent(32968): loss=22.440856735592256\n",
      "Stochastic Gradient Descent(32969): loss=1.505502512833751\n",
      "Stochastic Gradient Descent(32970): loss=0.6419202757229824\n",
      "Stochastic Gradient Descent(32971): loss=3.7902604965045685\n",
      "Stochastic Gradient Descent(32972): loss=2.0291100535318427\n",
      "Stochastic Gradient Descent(32973): loss=6.862384634844604\n",
      "Stochastic Gradient Descent(32974): loss=0.9634701140599379\n",
      "Stochastic Gradient Descent(32975): loss=0.2813052767320537\n",
      "Stochastic Gradient Descent(32976): loss=5.131588109951914\n",
      "Stochastic Gradient Descent(32977): loss=0.5550311523132171\n",
      "Stochastic Gradient Descent(32978): loss=1.599232775147001\n",
      "Stochastic Gradient Descent(32979): loss=0.15311500662135008\n",
      "Stochastic Gradient Descent(32980): loss=0.009833051571872925\n",
      "Stochastic Gradient Descent(32981): loss=5.665494397436367\n",
      "Stochastic Gradient Descent(32982): loss=0.3246667153061938\n",
      "Stochastic Gradient Descent(32983): loss=5.295337033023697\n",
      "Stochastic Gradient Descent(32984): loss=1.3676061362156184\n",
      "Stochastic Gradient Descent(32985): loss=13.831735196616565\n",
      "Stochastic Gradient Descent(32986): loss=0.8028723991255559\n",
      "Stochastic Gradient Descent(32987): loss=9.117036667549336\n",
      "Stochastic Gradient Descent(32988): loss=1.336606660020584\n",
      "Stochastic Gradient Descent(32989): loss=5.679576147425443\n",
      "Stochastic Gradient Descent(32990): loss=1.699225916888487\n",
      "Stochastic Gradient Descent(32991): loss=0.03035498360092401\n",
      "Stochastic Gradient Descent(32992): loss=4.9928813975038\n",
      "Stochastic Gradient Descent(32993): loss=0.5451134618494571\n",
      "Stochastic Gradient Descent(32994): loss=23.024695658147344\n",
      "Stochastic Gradient Descent(32995): loss=4.977727410768093\n",
      "Stochastic Gradient Descent(32996): loss=0.0003456994865791029\n",
      "Stochastic Gradient Descent(32997): loss=4.532382739357805\n",
      "Stochastic Gradient Descent(32998): loss=3.5544058557892377\n",
      "Stochastic Gradient Descent(32999): loss=6.105583035950776\n",
      "Stochastic Gradient Descent(33000): loss=0.2755582109857559\n",
      "Stochastic Gradient Descent(33001): loss=0.13812932685044227\n",
      "Stochastic Gradient Descent(33002): loss=10.468977477357196\n",
      "Stochastic Gradient Descent(33003): loss=0.18098499683207545\n",
      "Stochastic Gradient Descent(33004): loss=0.0037057085917858463\n",
      "Stochastic Gradient Descent(33005): loss=0.9119967063856639\n",
      "Stochastic Gradient Descent(33006): loss=3.2213826377854398\n",
      "Stochastic Gradient Descent(33007): loss=0.9316298086443648\n",
      "Stochastic Gradient Descent(33008): loss=9.612812804252615\n",
      "Stochastic Gradient Descent(33009): loss=2.5059411235771405\n",
      "Stochastic Gradient Descent(33010): loss=4.8436607421164295\n",
      "Stochastic Gradient Descent(33011): loss=0.7453596591461019\n",
      "Stochastic Gradient Descent(33012): loss=0.04391933109931803\n",
      "Stochastic Gradient Descent(33013): loss=12.833691537181052\n",
      "Stochastic Gradient Descent(33014): loss=9.052254110410457\n",
      "Stochastic Gradient Descent(33015): loss=5.427863359805927\n",
      "Stochastic Gradient Descent(33016): loss=18.270956514109276\n",
      "Stochastic Gradient Descent(33017): loss=8.921826428466444\n",
      "Stochastic Gradient Descent(33018): loss=27.59192614446045\n",
      "Stochastic Gradient Descent(33019): loss=0.3869070505557018\n",
      "Stochastic Gradient Descent(33020): loss=0.021964455170540378\n",
      "Stochastic Gradient Descent(33021): loss=2.646246527310477\n",
      "Stochastic Gradient Descent(33022): loss=32.0455011081868\n",
      "Stochastic Gradient Descent(33023): loss=0.3086251986599403\n",
      "Stochastic Gradient Descent(33024): loss=0.689872070522795\n",
      "Stochastic Gradient Descent(33025): loss=19.47593976784739\n",
      "Stochastic Gradient Descent(33026): loss=2.467132351242014\n",
      "Stochastic Gradient Descent(33027): loss=1.4791104763048637\n",
      "Stochastic Gradient Descent(33028): loss=0.17685577704452946\n",
      "Stochastic Gradient Descent(33029): loss=40.16347246536952\n",
      "Stochastic Gradient Descent(33030): loss=6.072136693381111\n",
      "Stochastic Gradient Descent(33031): loss=4.620861205675902\n",
      "Stochastic Gradient Descent(33032): loss=0.10972651828186465\n",
      "Stochastic Gradient Descent(33033): loss=4.046871668005345\n",
      "Stochastic Gradient Descent(33034): loss=2.787395765779536\n",
      "Stochastic Gradient Descent(33035): loss=3.4015714427108565\n",
      "Stochastic Gradient Descent(33036): loss=0.05115153071119669\n",
      "Stochastic Gradient Descent(33037): loss=19.31218887377801\n",
      "Stochastic Gradient Descent(33038): loss=5.950829157245882\n",
      "Stochastic Gradient Descent(33039): loss=18.65862797096534\n",
      "Stochastic Gradient Descent(33040): loss=24.18629544625129\n",
      "Stochastic Gradient Descent(33041): loss=0.22622371386704285\n",
      "Stochastic Gradient Descent(33042): loss=26.304605503417328\n",
      "Stochastic Gradient Descent(33043): loss=4.3479901782436885\n",
      "Stochastic Gradient Descent(33044): loss=3.597841549966924\n",
      "Stochastic Gradient Descent(33045): loss=0.2597185859155572\n",
      "Stochastic Gradient Descent(33046): loss=0.08841695077220099\n",
      "Stochastic Gradient Descent(33047): loss=0.27734269877470225\n",
      "Stochastic Gradient Descent(33048): loss=4.311196632766499\n",
      "Stochastic Gradient Descent(33049): loss=1.9552694388694143\n",
      "Stochastic Gradient Descent(33050): loss=1.7361745185466415\n",
      "Stochastic Gradient Descent(33051): loss=0.6551609251059383\n",
      "Stochastic Gradient Descent(33052): loss=9.675833656809807\n",
      "Stochastic Gradient Descent(33053): loss=1.911703448009676\n",
      "Stochastic Gradient Descent(33054): loss=5.39366496595462\n",
      "Stochastic Gradient Descent(33055): loss=1.026549447384963\n",
      "Stochastic Gradient Descent(33056): loss=7.540704796440874\n",
      "Stochastic Gradient Descent(33057): loss=0.19448937062572344\n",
      "Stochastic Gradient Descent(33058): loss=0.961409656467326\n",
      "Stochastic Gradient Descent(33059): loss=11.747924165988824\n",
      "Stochastic Gradient Descent(33060): loss=1.9028881957070345\n",
      "Stochastic Gradient Descent(33061): loss=3.4053289624753864\n",
      "Stochastic Gradient Descent(33062): loss=1.7454269473290562\n",
      "Stochastic Gradient Descent(33063): loss=1.5980331173325475\n",
      "Stochastic Gradient Descent(33064): loss=6.115920023411848\n",
      "Stochastic Gradient Descent(33065): loss=2.901038692086289\n",
      "Stochastic Gradient Descent(33066): loss=1.1356880038353057\n",
      "Stochastic Gradient Descent(33067): loss=7.368857350821835\n",
      "Stochastic Gradient Descent(33068): loss=0.2301360896000523\n",
      "Stochastic Gradient Descent(33069): loss=6.625722887015815\n",
      "Stochastic Gradient Descent(33070): loss=15.38384299654921\n",
      "Stochastic Gradient Descent(33071): loss=12.418382277601346\n",
      "Stochastic Gradient Descent(33072): loss=8.125727673061036\n",
      "Stochastic Gradient Descent(33073): loss=7.1118936779348845\n",
      "Stochastic Gradient Descent(33074): loss=5.286911295188894\n",
      "Stochastic Gradient Descent(33075): loss=0.9756330954057723\n",
      "Stochastic Gradient Descent(33076): loss=11.39525648725125\n",
      "Stochastic Gradient Descent(33077): loss=0.06294176258495718\n",
      "Stochastic Gradient Descent(33078): loss=25.197799835204897\n",
      "Stochastic Gradient Descent(33079): loss=0.21793068574818314\n",
      "Stochastic Gradient Descent(33080): loss=2.3330885473778094\n",
      "Stochastic Gradient Descent(33081): loss=8.321935733096828\n",
      "Stochastic Gradient Descent(33082): loss=0.023169956202872474\n",
      "Stochastic Gradient Descent(33083): loss=0.11732404575045827\n",
      "Stochastic Gradient Descent(33084): loss=0.41157207404817336\n",
      "Stochastic Gradient Descent(33085): loss=11.895120779752872\n",
      "Stochastic Gradient Descent(33086): loss=0.004007671237670942\n",
      "Stochastic Gradient Descent(33087): loss=2.9310656793582845\n",
      "Stochastic Gradient Descent(33088): loss=1.3467495471892683\n",
      "Stochastic Gradient Descent(33089): loss=1.660379367955597\n",
      "Stochastic Gradient Descent(33090): loss=5.133185665106429\n",
      "Stochastic Gradient Descent(33091): loss=1.3406243108675908\n",
      "Stochastic Gradient Descent(33092): loss=2.687910457897297\n",
      "Stochastic Gradient Descent(33093): loss=0.8620676215467686\n",
      "Stochastic Gradient Descent(33094): loss=4.98474760606991\n",
      "Stochastic Gradient Descent(33095): loss=2.5653110401204495\n",
      "Stochastic Gradient Descent(33096): loss=2.2545414916915223\n",
      "Stochastic Gradient Descent(33097): loss=2.5463913014924726\n",
      "Stochastic Gradient Descent(33098): loss=0.08508861465529297\n",
      "Stochastic Gradient Descent(33099): loss=6.95835719138026\n",
      "Stochastic Gradient Descent(33100): loss=0.16962704406444526\n",
      "Stochastic Gradient Descent(33101): loss=7.3899655044433885\n",
      "Stochastic Gradient Descent(33102): loss=0.38633845249864507\n",
      "Stochastic Gradient Descent(33103): loss=6.065581635398924\n",
      "Stochastic Gradient Descent(33104): loss=0.11436066420944732\n",
      "Stochastic Gradient Descent(33105): loss=13.829077009155204\n",
      "Stochastic Gradient Descent(33106): loss=0.5475895604957947\n",
      "Stochastic Gradient Descent(33107): loss=7.247777236674194\n",
      "Stochastic Gradient Descent(33108): loss=0.055396900809876005\n",
      "Stochastic Gradient Descent(33109): loss=8.650425869685265\n",
      "Stochastic Gradient Descent(33110): loss=5.988173252177922\n",
      "Stochastic Gradient Descent(33111): loss=0.004613717214461003\n",
      "Stochastic Gradient Descent(33112): loss=9.167154543783264\n",
      "Stochastic Gradient Descent(33113): loss=0.7242183376471204\n",
      "Stochastic Gradient Descent(33114): loss=7.94550458791029\n",
      "Stochastic Gradient Descent(33115): loss=0.5447968162310928\n",
      "Stochastic Gradient Descent(33116): loss=0.35360843194995994\n",
      "Stochastic Gradient Descent(33117): loss=5.8133527484801055\n",
      "Stochastic Gradient Descent(33118): loss=0.9970429545539089\n",
      "Stochastic Gradient Descent(33119): loss=0.10154496661728969\n",
      "Stochastic Gradient Descent(33120): loss=14.041319436576712\n",
      "Stochastic Gradient Descent(33121): loss=31.78043602923562\n",
      "Stochastic Gradient Descent(33122): loss=5.307401477019873\n",
      "Stochastic Gradient Descent(33123): loss=0.118140406257775\n",
      "Stochastic Gradient Descent(33124): loss=4.89980169971128\n",
      "Stochastic Gradient Descent(33125): loss=0.5416380749870783\n",
      "Stochastic Gradient Descent(33126): loss=0.15119227491182066\n",
      "Stochastic Gradient Descent(33127): loss=5.321661240706932\n",
      "Stochastic Gradient Descent(33128): loss=1.6427950635986515\n",
      "Stochastic Gradient Descent(33129): loss=0.09885181544871245\n",
      "Stochastic Gradient Descent(33130): loss=0.014266409893311564\n",
      "Stochastic Gradient Descent(33131): loss=4.558726243383812\n",
      "Stochastic Gradient Descent(33132): loss=0.027597706310930475\n",
      "Stochastic Gradient Descent(33133): loss=14.464845795838029\n",
      "Stochastic Gradient Descent(33134): loss=12.455112830205012\n",
      "Stochastic Gradient Descent(33135): loss=1.5310080883201556\n",
      "Stochastic Gradient Descent(33136): loss=7.096115570040076\n",
      "Stochastic Gradient Descent(33137): loss=0.5903247519672361\n",
      "Stochastic Gradient Descent(33138): loss=1.0703662758953973\n",
      "Stochastic Gradient Descent(33139): loss=0.8972623821522936\n",
      "Stochastic Gradient Descent(33140): loss=5.32024222677279\n",
      "Stochastic Gradient Descent(33141): loss=1.322860566649395\n",
      "Stochastic Gradient Descent(33142): loss=2.789287413374168\n",
      "Stochastic Gradient Descent(33143): loss=3.1856968874232097\n",
      "Stochastic Gradient Descent(33144): loss=0.037542834845630944\n",
      "Stochastic Gradient Descent(33145): loss=3.4404361816420206\n",
      "Stochastic Gradient Descent(33146): loss=6.477099180737782\n",
      "Stochastic Gradient Descent(33147): loss=3.7718786322851123\n",
      "Stochastic Gradient Descent(33148): loss=1.0245418794994625\n",
      "Stochastic Gradient Descent(33149): loss=0.07045104890665038\n",
      "Stochastic Gradient Descent(33150): loss=21.453486874644696\n",
      "Stochastic Gradient Descent(33151): loss=0.1266168989499251\n",
      "Stochastic Gradient Descent(33152): loss=5.657539485190243\n",
      "Stochastic Gradient Descent(33153): loss=3.5323171628162564\n",
      "Stochastic Gradient Descent(33154): loss=0.9808850728404408\n",
      "Stochastic Gradient Descent(33155): loss=3.1789038711816735\n",
      "Stochastic Gradient Descent(33156): loss=3.737664451432577\n",
      "Stochastic Gradient Descent(33157): loss=6.327977850998208\n",
      "Stochastic Gradient Descent(33158): loss=0.5789481894708923\n",
      "Stochastic Gradient Descent(33159): loss=1.258651880599358\n",
      "Stochastic Gradient Descent(33160): loss=0.01149694821650483\n",
      "Stochastic Gradient Descent(33161): loss=4.741796369077014\n",
      "Stochastic Gradient Descent(33162): loss=4.515176511639399\n",
      "Stochastic Gradient Descent(33163): loss=18.511108539527992\n",
      "Stochastic Gradient Descent(33164): loss=8.50233481145619\n",
      "Stochastic Gradient Descent(33165): loss=4.27091026469251\n",
      "Stochastic Gradient Descent(33166): loss=1.1230593273886722\n",
      "Stochastic Gradient Descent(33167): loss=3.2556460304671533\n",
      "Stochastic Gradient Descent(33168): loss=0.22328125910460178\n",
      "Stochastic Gradient Descent(33169): loss=1.4272801535178774\n",
      "Stochastic Gradient Descent(33170): loss=0.16153386520302848\n",
      "Stochastic Gradient Descent(33171): loss=10.215115081695728\n",
      "Stochastic Gradient Descent(33172): loss=0.08860375974817412\n",
      "Stochastic Gradient Descent(33173): loss=8.772262125204726\n",
      "Stochastic Gradient Descent(33174): loss=0.007136429468304502\n",
      "Stochastic Gradient Descent(33175): loss=4.024308813781722\n",
      "Stochastic Gradient Descent(33176): loss=1.6390456300123724\n",
      "Stochastic Gradient Descent(33177): loss=0.023673756166546763\n",
      "Stochastic Gradient Descent(33178): loss=31.055220422765487\n",
      "Stochastic Gradient Descent(33179): loss=0.041455041685129354\n",
      "Stochastic Gradient Descent(33180): loss=20.583944430788346\n",
      "Stochastic Gradient Descent(33181): loss=0.5751802698576313\n",
      "Stochastic Gradient Descent(33182): loss=7.707235939248712\n",
      "Stochastic Gradient Descent(33183): loss=19.880594086077323\n",
      "Stochastic Gradient Descent(33184): loss=3.48361376206295\n",
      "Stochastic Gradient Descent(33185): loss=0.32497830990401083\n",
      "Stochastic Gradient Descent(33186): loss=0.017889959448350257\n",
      "Stochastic Gradient Descent(33187): loss=4.442435788937053\n",
      "Stochastic Gradient Descent(33188): loss=28.14503131427818\n",
      "Stochastic Gradient Descent(33189): loss=14.838709906079274\n",
      "Stochastic Gradient Descent(33190): loss=5.817096311494166\n",
      "Stochastic Gradient Descent(33191): loss=19.051471064105193\n",
      "Stochastic Gradient Descent(33192): loss=0.03357375788111476\n",
      "Stochastic Gradient Descent(33193): loss=25.45828127480803\n",
      "Stochastic Gradient Descent(33194): loss=5.996347554423144\n",
      "Stochastic Gradient Descent(33195): loss=0.010789097513200847\n",
      "Stochastic Gradient Descent(33196): loss=4.401784893138751\n",
      "Stochastic Gradient Descent(33197): loss=2.0646218792571855\n",
      "Stochastic Gradient Descent(33198): loss=2.7529413169834775\n",
      "Stochastic Gradient Descent(33199): loss=17.339188603269495\n",
      "Stochastic Gradient Descent(33200): loss=9.517686123938352\n",
      "Stochastic Gradient Descent(33201): loss=1.5618553767878047\n",
      "Stochastic Gradient Descent(33202): loss=0.8930275352694689\n",
      "Stochastic Gradient Descent(33203): loss=0.0052922746487702975\n",
      "Stochastic Gradient Descent(33204): loss=13.840045395047667\n",
      "Stochastic Gradient Descent(33205): loss=9.986946626897394\n",
      "Stochastic Gradient Descent(33206): loss=6.792853022073201\n",
      "Stochastic Gradient Descent(33207): loss=2.462576212544098\n",
      "Stochastic Gradient Descent(33208): loss=1.180328393449627\n",
      "Stochastic Gradient Descent(33209): loss=0.0015799169456784431\n",
      "Stochastic Gradient Descent(33210): loss=0.5531761628753046\n",
      "Stochastic Gradient Descent(33211): loss=2.4876572700376176\n",
      "Stochastic Gradient Descent(33212): loss=5.617071811885716\n",
      "Stochastic Gradient Descent(33213): loss=0.17615557318940145\n",
      "Stochastic Gradient Descent(33214): loss=0.19327244745843417\n",
      "Stochastic Gradient Descent(33215): loss=0.37163551206318135\n",
      "Stochastic Gradient Descent(33216): loss=0.44244714036758814\n",
      "Stochastic Gradient Descent(33217): loss=1.3776033294233963\n",
      "Stochastic Gradient Descent(33218): loss=9.038182833777828\n",
      "Stochastic Gradient Descent(33219): loss=10.698755668310062\n",
      "Stochastic Gradient Descent(33220): loss=10.031208288935643\n",
      "Stochastic Gradient Descent(33221): loss=30.819411396737582\n",
      "Stochastic Gradient Descent(33222): loss=1.7556997743080003\n",
      "Stochastic Gradient Descent(33223): loss=0.7227290378060486\n",
      "Stochastic Gradient Descent(33224): loss=4.402760194558704\n",
      "Stochastic Gradient Descent(33225): loss=0.6870201515312361\n",
      "Stochastic Gradient Descent(33226): loss=4.8543423520584055\n",
      "Stochastic Gradient Descent(33227): loss=3.0514595581714024\n",
      "Stochastic Gradient Descent(33228): loss=5.419314953482024\n",
      "Stochastic Gradient Descent(33229): loss=0.24693668575492395\n",
      "Stochastic Gradient Descent(33230): loss=5.223370566352474\n",
      "Stochastic Gradient Descent(33231): loss=40.643110821960704\n",
      "Stochastic Gradient Descent(33232): loss=18.603809482426183\n",
      "Stochastic Gradient Descent(33233): loss=4.333546726782562\n",
      "Stochastic Gradient Descent(33234): loss=0.21287146580072414\n",
      "Stochastic Gradient Descent(33235): loss=0.278033237201237\n",
      "Stochastic Gradient Descent(33236): loss=0.6658182834350777\n",
      "Stochastic Gradient Descent(33237): loss=18.366151931916807\n",
      "Stochastic Gradient Descent(33238): loss=2.9833670641963055\n",
      "Stochastic Gradient Descent(33239): loss=4.600431827630897\n",
      "Stochastic Gradient Descent(33240): loss=15.68509365615627\n",
      "Stochastic Gradient Descent(33241): loss=0.39388618129277253\n",
      "Stochastic Gradient Descent(33242): loss=2.224482815407091\n",
      "Stochastic Gradient Descent(33243): loss=0.0024887545593251827\n",
      "Stochastic Gradient Descent(33244): loss=6.196770097818487\n",
      "Stochastic Gradient Descent(33245): loss=27.384196124322948\n",
      "Stochastic Gradient Descent(33246): loss=7.662696183728726\n",
      "Stochastic Gradient Descent(33247): loss=1.533271901314645\n",
      "Stochastic Gradient Descent(33248): loss=0.3103403418550226\n",
      "Stochastic Gradient Descent(33249): loss=11.130450808244712\n",
      "Stochastic Gradient Descent(33250): loss=2.556359230142847\n",
      "Stochastic Gradient Descent(33251): loss=1.3364118820923512\n",
      "Stochastic Gradient Descent(33252): loss=0.11752727417677349\n",
      "Stochastic Gradient Descent(33253): loss=14.657350705720875\n",
      "Stochastic Gradient Descent(33254): loss=8.532557639689298\n",
      "Stochastic Gradient Descent(33255): loss=0.0011395045465747843\n",
      "Stochastic Gradient Descent(33256): loss=0.060226604234887834\n",
      "Stochastic Gradient Descent(33257): loss=4.013951752225148\n",
      "Stochastic Gradient Descent(33258): loss=0.07576768495854641\n",
      "Stochastic Gradient Descent(33259): loss=0.27878571392377993\n",
      "Stochastic Gradient Descent(33260): loss=3.962963501701413\n",
      "Stochastic Gradient Descent(33261): loss=0.21075297889790004\n",
      "Stochastic Gradient Descent(33262): loss=2.512686940604598\n",
      "Stochastic Gradient Descent(33263): loss=11.543959980074892\n",
      "Stochastic Gradient Descent(33264): loss=10.829304192839283\n",
      "Stochastic Gradient Descent(33265): loss=0.30899879783424095\n",
      "Stochastic Gradient Descent(33266): loss=0.03072701691652971\n",
      "Stochastic Gradient Descent(33267): loss=11.655891467237419\n",
      "Stochastic Gradient Descent(33268): loss=7.751020530729034\n",
      "Stochastic Gradient Descent(33269): loss=2.8414509492163607\n",
      "Stochastic Gradient Descent(33270): loss=2.532335308832756\n",
      "Stochastic Gradient Descent(33271): loss=0.3467671193220236\n",
      "Stochastic Gradient Descent(33272): loss=17.608178916528175\n",
      "Stochastic Gradient Descent(33273): loss=0.014843906425634736\n",
      "Stochastic Gradient Descent(33274): loss=2.4566476024598063\n",
      "Stochastic Gradient Descent(33275): loss=3.23735245822794\n",
      "Stochastic Gradient Descent(33276): loss=27.199132830155932\n",
      "Stochastic Gradient Descent(33277): loss=8.663830989340992\n",
      "Stochastic Gradient Descent(33278): loss=39.918561452085484\n",
      "Stochastic Gradient Descent(33279): loss=4.954737680037257\n",
      "Stochastic Gradient Descent(33280): loss=15.88056002673518\n",
      "Stochastic Gradient Descent(33281): loss=0.2828154368431275\n",
      "Stochastic Gradient Descent(33282): loss=8.329147219823295\n",
      "Stochastic Gradient Descent(33283): loss=1.6574056023333201\n",
      "Stochastic Gradient Descent(33284): loss=0.8929782770856719\n",
      "Stochastic Gradient Descent(33285): loss=18.10874636957594\n",
      "Stochastic Gradient Descent(33286): loss=1.7616958529689608\n",
      "Stochastic Gradient Descent(33287): loss=13.57902658648538\n",
      "Stochastic Gradient Descent(33288): loss=8.346787589917085\n",
      "Stochastic Gradient Descent(33289): loss=1.9959653779076707\n",
      "Stochastic Gradient Descent(33290): loss=4.5213121992604295\n",
      "Stochastic Gradient Descent(33291): loss=11.651867732544256\n",
      "Stochastic Gradient Descent(33292): loss=0.6160259953008339\n",
      "Stochastic Gradient Descent(33293): loss=5.898390253627467\n",
      "Stochastic Gradient Descent(33294): loss=29.5876817370019\n",
      "Stochastic Gradient Descent(33295): loss=0.25351095978264887\n",
      "Stochastic Gradient Descent(33296): loss=1.5515330146653104\n",
      "Stochastic Gradient Descent(33297): loss=11.98044668450688\n",
      "Stochastic Gradient Descent(33298): loss=1.2691775911680832\n",
      "Stochastic Gradient Descent(33299): loss=22.10460214511941\n",
      "Stochastic Gradient Descent(33300): loss=0.8762622848706355\n",
      "Stochastic Gradient Descent(33301): loss=0.012206130315662261\n",
      "Stochastic Gradient Descent(33302): loss=0.11163602389772208\n",
      "Stochastic Gradient Descent(33303): loss=0.7251690179623957\n",
      "Stochastic Gradient Descent(33304): loss=26.846772136194037\n",
      "Stochastic Gradient Descent(33305): loss=8.496406749615568\n",
      "Stochastic Gradient Descent(33306): loss=17.979292668535255\n",
      "Stochastic Gradient Descent(33307): loss=0.11360574326929179\n",
      "Stochastic Gradient Descent(33308): loss=0.004339214774154098\n",
      "Stochastic Gradient Descent(33309): loss=0.21756904053258472\n",
      "Stochastic Gradient Descent(33310): loss=0.007931272051611106\n",
      "Stochastic Gradient Descent(33311): loss=2.869412164448322\n",
      "Stochastic Gradient Descent(33312): loss=4.679886705100781\n",
      "Stochastic Gradient Descent(33313): loss=1.5772312142653948\n",
      "Stochastic Gradient Descent(33314): loss=0.006623714684150992\n",
      "Stochastic Gradient Descent(33315): loss=0.34081202476811034\n",
      "Stochastic Gradient Descent(33316): loss=0.0011832870164370953\n",
      "Stochastic Gradient Descent(33317): loss=0.0014806455120654181\n",
      "Stochastic Gradient Descent(33318): loss=0.04270325735377599\n",
      "Stochastic Gradient Descent(33319): loss=0.04313292710625579\n",
      "Stochastic Gradient Descent(33320): loss=2.9209830768602787\n",
      "Stochastic Gradient Descent(33321): loss=6.794584776325614\n",
      "Stochastic Gradient Descent(33322): loss=2.343691042966651\n",
      "Stochastic Gradient Descent(33323): loss=8.371220205635657\n",
      "Stochastic Gradient Descent(33324): loss=19.39948493242388\n",
      "Stochastic Gradient Descent(33325): loss=0.12473609370265609\n",
      "Stochastic Gradient Descent(33326): loss=12.696276490501775\n",
      "Stochastic Gradient Descent(33327): loss=0.03556792484634953\n",
      "Stochastic Gradient Descent(33328): loss=4.075640493324494\n",
      "Stochastic Gradient Descent(33329): loss=1.9422766561620093\n",
      "Stochastic Gradient Descent(33330): loss=1.1824397366586283\n",
      "Stochastic Gradient Descent(33331): loss=0.0007111450216537818\n",
      "Stochastic Gradient Descent(33332): loss=9.047531611048171\n",
      "Stochastic Gradient Descent(33333): loss=4.51866028422243\n",
      "Stochastic Gradient Descent(33334): loss=12.469447612046238\n",
      "Stochastic Gradient Descent(33335): loss=0.11102728205550533\n",
      "Stochastic Gradient Descent(33336): loss=0.15997987135311798\n",
      "Stochastic Gradient Descent(33337): loss=8.414751125983669\n",
      "Stochastic Gradient Descent(33338): loss=0.33607428871731\n",
      "Stochastic Gradient Descent(33339): loss=1.2304269514928496\n",
      "Stochastic Gradient Descent(33340): loss=1.47863098410913\n",
      "Stochastic Gradient Descent(33341): loss=21.963947005412834\n",
      "Stochastic Gradient Descent(33342): loss=1.2636153804941912\n",
      "Stochastic Gradient Descent(33343): loss=2.647405479036396\n",
      "Stochastic Gradient Descent(33344): loss=9.74576895768645\n",
      "Stochastic Gradient Descent(33345): loss=3.0461508994251343\n",
      "Stochastic Gradient Descent(33346): loss=10.45547226431351\n",
      "Stochastic Gradient Descent(33347): loss=5.899569374498118\n",
      "Stochastic Gradient Descent(33348): loss=1.2721216357214318\n",
      "Stochastic Gradient Descent(33349): loss=0.5781508765426508\n",
      "Stochastic Gradient Descent(33350): loss=0.4676595776271153\n",
      "Stochastic Gradient Descent(33351): loss=0.9542299186946186\n",
      "Stochastic Gradient Descent(33352): loss=21.08917345740082\n",
      "Stochastic Gradient Descent(33353): loss=0.33281281793828965\n",
      "Stochastic Gradient Descent(33354): loss=1.1131898259599589\n",
      "Stochastic Gradient Descent(33355): loss=0.08711703742320576\n",
      "Stochastic Gradient Descent(33356): loss=9.588710350754013\n",
      "Stochastic Gradient Descent(33357): loss=0.03005706033777892\n",
      "Stochastic Gradient Descent(33358): loss=8.390023763606061\n",
      "Stochastic Gradient Descent(33359): loss=0.5270328476297701\n",
      "Stochastic Gradient Descent(33360): loss=1.2442090518546907\n",
      "Stochastic Gradient Descent(33361): loss=0.31593387531139777\n",
      "Stochastic Gradient Descent(33362): loss=1.0920224514289152\n",
      "Stochastic Gradient Descent(33363): loss=5.630458714577426\n",
      "Stochastic Gradient Descent(33364): loss=4.509409002976025\n",
      "Stochastic Gradient Descent(33365): loss=7.801323588174637\n",
      "Stochastic Gradient Descent(33366): loss=0.5048220872863972\n",
      "Stochastic Gradient Descent(33367): loss=1.3048970293197413\n",
      "Stochastic Gradient Descent(33368): loss=13.989259131228144\n",
      "Stochastic Gradient Descent(33369): loss=13.720315702848042\n",
      "Stochastic Gradient Descent(33370): loss=12.644733516181605\n",
      "Stochastic Gradient Descent(33371): loss=8.528505626887883\n",
      "Stochastic Gradient Descent(33372): loss=17.206507690720453\n",
      "Stochastic Gradient Descent(33373): loss=2.8618208176560604\n",
      "Stochastic Gradient Descent(33374): loss=5.111952236455844\n",
      "Stochastic Gradient Descent(33375): loss=4.686726196959659\n",
      "Stochastic Gradient Descent(33376): loss=1.9306809476381592\n",
      "Stochastic Gradient Descent(33377): loss=8.559937707367615\n",
      "Stochastic Gradient Descent(33378): loss=3.208036288763756\n",
      "Stochastic Gradient Descent(33379): loss=14.719986875020716\n",
      "Stochastic Gradient Descent(33380): loss=8.065280897914356\n",
      "Stochastic Gradient Descent(33381): loss=12.619554133159491\n",
      "Stochastic Gradient Descent(33382): loss=8.70109453714419\n",
      "Stochastic Gradient Descent(33383): loss=1.059505202181616\n",
      "Stochastic Gradient Descent(33384): loss=0.5980089173176246\n",
      "Stochastic Gradient Descent(33385): loss=0.0007880067991911762\n",
      "Stochastic Gradient Descent(33386): loss=0.08908678081888746\n",
      "Stochastic Gradient Descent(33387): loss=0.20873560340935696\n",
      "Stochastic Gradient Descent(33388): loss=0.6631423221013473\n",
      "Stochastic Gradient Descent(33389): loss=12.539727753836331\n",
      "Stochastic Gradient Descent(33390): loss=1.079896702651379\n",
      "Stochastic Gradient Descent(33391): loss=0.20320341488375773\n",
      "Stochastic Gradient Descent(33392): loss=0.9729976763220066\n",
      "Stochastic Gradient Descent(33393): loss=2.6519360154788414\n",
      "Stochastic Gradient Descent(33394): loss=8.792108718364254\n",
      "Stochastic Gradient Descent(33395): loss=7.8623856440402005\n",
      "Stochastic Gradient Descent(33396): loss=3.359024614961827\n",
      "Stochastic Gradient Descent(33397): loss=7.2019244318181865\n",
      "Stochastic Gradient Descent(33398): loss=1.5644107220336314\n",
      "Stochastic Gradient Descent(33399): loss=3.4347686091728993\n",
      "Stochastic Gradient Descent(33400): loss=4.210601543696973\n",
      "Stochastic Gradient Descent(33401): loss=0.0780916597006664\n",
      "Stochastic Gradient Descent(33402): loss=1.0769209440304575\n",
      "Stochastic Gradient Descent(33403): loss=4.309879307418961\n",
      "Stochastic Gradient Descent(33404): loss=0.14940452456329031\n",
      "Stochastic Gradient Descent(33405): loss=144.57326029361425\n",
      "Stochastic Gradient Descent(33406): loss=516.0046065911671\n",
      "Stochastic Gradient Descent(33407): loss=1.146467934860181\n",
      "Stochastic Gradient Descent(33408): loss=23.59805901264419\n",
      "Stochastic Gradient Descent(33409): loss=2.3005383693138137\n",
      "Stochastic Gradient Descent(33410): loss=61.25782030694602\n",
      "Stochastic Gradient Descent(33411): loss=0.1893520784302201\n",
      "Stochastic Gradient Descent(33412): loss=6.186702961854657\n",
      "Stochastic Gradient Descent(33413): loss=1.0762575562671841\n",
      "Stochastic Gradient Descent(33414): loss=1.0667104073970295\n",
      "Stochastic Gradient Descent(33415): loss=4.957295573990527\n",
      "Stochastic Gradient Descent(33416): loss=0.002247132156092294\n",
      "Stochastic Gradient Descent(33417): loss=11.259675518880819\n",
      "Stochastic Gradient Descent(33418): loss=3.593028253737407\n",
      "Stochastic Gradient Descent(33419): loss=4.701198612902637\n",
      "Stochastic Gradient Descent(33420): loss=1.0271506318194887\n",
      "Stochastic Gradient Descent(33421): loss=6.181912942766267\n",
      "Stochastic Gradient Descent(33422): loss=2.4864342242925352\n",
      "Stochastic Gradient Descent(33423): loss=2.07627636149423\n",
      "Stochastic Gradient Descent(33424): loss=0.5077862349146447\n",
      "Stochastic Gradient Descent(33425): loss=6.265305868937078\n",
      "Stochastic Gradient Descent(33426): loss=0.3301683732601166\n",
      "Stochastic Gradient Descent(33427): loss=0.6464113358758853\n",
      "Stochastic Gradient Descent(33428): loss=6.806834741544607\n",
      "Stochastic Gradient Descent(33429): loss=0.0005873384643186646\n",
      "Stochastic Gradient Descent(33430): loss=0.2963090175775321\n",
      "Stochastic Gradient Descent(33431): loss=4.22674703293134\n",
      "Stochastic Gradient Descent(33432): loss=18.311005396928802\n",
      "Stochastic Gradient Descent(33433): loss=4.750601864954251\n",
      "Stochastic Gradient Descent(33434): loss=21.661959036548815\n",
      "Stochastic Gradient Descent(33435): loss=0.09112218162863683\n",
      "Stochastic Gradient Descent(33436): loss=20.79241023509948\n",
      "Stochastic Gradient Descent(33437): loss=4.526055237571203\n",
      "Stochastic Gradient Descent(33438): loss=3.6293722824700443\n",
      "Stochastic Gradient Descent(33439): loss=4.823710677549768\n",
      "Stochastic Gradient Descent(33440): loss=0.10768915883271822\n",
      "Stochastic Gradient Descent(33441): loss=0.5604226448437929\n",
      "Stochastic Gradient Descent(33442): loss=9.256473870167666\n",
      "Stochastic Gradient Descent(33443): loss=2.232547304651258\n",
      "Stochastic Gradient Descent(33444): loss=3.1402439394390282\n",
      "Stochastic Gradient Descent(33445): loss=2.9944607847766287\n",
      "Stochastic Gradient Descent(33446): loss=0.1530952617465259\n",
      "Stochastic Gradient Descent(33447): loss=0.03639436237321251\n",
      "Stochastic Gradient Descent(33448): loss=2.0682338277937613\n",
      "Stochastic Gradient Descent(33449): loss=1.3377358153242345\n",
      "Stochastic Gradient Descent(33450): loss=4.420290771868345\n",
      "Stochastic Gradient Descent(33451): loss=1.0483354396826254\n",
      "Stochastic Gradient Descent(33452): loss=5.359665684756197\n",
      "Stochastic Gradient Descent(33453): loss=3.352379513636396\n",
      "Stochastic Gradient Descent(33454): loss=0.979437036661223\n",
      "Stochastic Gradient Descent(33455): loss=0.05070622774384914\n",
      "Stochastic Gradient Descent(33456): loss=14.965686884403356\n",
      "Stochastic Gradient Descent(33457): loss=28.211238942951294\n",
      "Stochastic Gradient Descent(33458): loss=0.3953049134864544\n",
      "Stochastic Gradient Descent(33459): loss=0.4226448843417921\n",
      "Stochastic Gradient Descent(33460): loss=12.435326173133747\n",
      "Stochastic Gradient Descent(33461): loss=7.105237009713655\n",
      "Stochastic Gradient Descent(33462): loss=2.5932941586301377\n",
      "Stochastic Gradient Descent(33463): loss=14.703441379919141\n",
      "Stochastic Gradient Descent(33464): loss=14.658938299668913\n",
      "Stochastic Gradient Descent(33465): loss=0.14679213630999186\n",
      "Stochastic Gradient Descent(33466): loss=0.7394544120229245\n",
      "Stochastic Gradient Descent(33467): loss=3.32738693593804\n",
      "Stochastic Gradient Descent(33468): loss=1.3480201597349153\n",
      "Stochastic Gradient Descent(33469): loss=4.27955095724944\n",
      "Stochastic Gradient Descent(33470): loss=18.850039253691\n",
      "Stochastic Gradient Descent(33471): loss=9.466691352115832\n",
      "Stochastic Gradient Descent(33472): loss=16.23577888249388\n",
      "Stochastic Gradient Descent(33473): loss=11.750878693621683\n",
      "Stochastic Gradient Descent(33474): loss=12.483248372928898\n",
      "Stochastic Gradient Descent(33475): loss=12.461935666812133\n",
      "Stochastic Gradient Descent(33476): loss=0.5266502784873908\n",
      "Stochastic Gradient Descent(33477): loss=1.2917654596412724\n",
      "Stochastic Gradient Descent(33478): loss=6.053443656129435\n",
      "Stochastic Gradient Descent(33479): loss=3.1524686831999293\n",
      "Stochastic Gradient Descent(33480): loss=0.000810960678667786\n",
      "Stochastic Gradient Descent(33481): loss=2.2665243156834487\n",
      "Stochastic Gradient Descent(33482): loss=0.26963453858954073\n",
      "Stochastic Gradient Descent(33483): loss=5.231621731071757\n",
      "Stochastic Gradient Descent(33484): loss=6.5428865021908384\n",
      "Stochastic Gradient Descent(33485): loss=9.720642563224024\n",
      "Stochastic Gradient Descent(33486): loss=0.6062234432462875\n",
      "Stochastic Gradient Descent(33487): loss=11.540931159255216\n",
      "Stochastic Gradient Descent(33488): loss=0.30211472761702535\n",
      "Stochastic Gradient Descent(33489): loss=0.01444315399938465\n",
      "Stochastic Gradient Descent(33490): loss=0.6855946428425247\n",
      "Stochastic Gradient Descent(33491): loss=4.674205047116437\n",
      "Stochastic Gradient Descent(33492): loss=1.7273528527684001\n",
      "Stochastic Gradient Descent(33493): loss=1.4527169176103345\n",
      "Stochastic Gradient Descent(33494): loss=0.0031992749672107553\n",
      "Stochastic Gradient Descent(33495): loss=5.200089434466288\n",
      "Stochastic Gradient Descent(33496): loss=0.24677688121139502\n",
      "Stochastic Gradient Descent(33497): loss=3.275619841203483\n",
      "Stochastic Gradient Descent(33498): loss=4.172946924327414\n",
      "Stochastic Gradient Descent(33499): loss=1.5766521157484663\n",
      "Stochastic Gradient Descent(33500): loss=4.110678874140393e-05\n",
      "Stochastic Gradient Descent(33501): loss=27.399661103064815\n",
      "Stochastic Gradient Descent(33502): loss=11.483705350468147\n",
      "Stochastic Gradient Descent(33503): loss=0.15670789328924215\n",
      "Stochastic Gradient Descent(33504): loss=0.6453200553638361\n",
      "Stochastic Gradient Descent(33505): loss=7.35959559444036\n",
      "Stochastic Gradient Descent(33506): loss=0.0010731637992833342\n",
      "Stochastic Gradient Descent(33507): loss=17.259448939147426\n",
      "Stochastic Gradient Descent(33508): loss=0.8503392491618569\n",
      "Stochastic Gradient Descent(33509): loss=4.382736278700962\n",
      "Stochastic Gradient Descent(33510): loss=1.8093690352797915\n",
      "Stochastic Gradient Descent(33511): loss=0.8702298556953741\n",
      "Stochastic Gradient Descent(33512): loss=3.306311027032232\n",
      "Stochastic Gradient Descent(33513): loss=5.741059969520578\n",
      "Stochastic Gradient Descent(33514): loss=5.203229123515034\n",
      "Stochastic Gradient Descent(33515): loss=0.10311460103129452\n",
      "Stochastic Gradient Descent(33516): loss=0.05086584230949081\n",
      "Stochastic Gradient Descent(33517): loss=1.8721994050023503\n",
      "Stochastic Gradient Descent(33518): loss=4.608753553768813\n",
      "Stochastic Gradient Descent(33519): loss=0.03498976075358462\n",
      "Stochastic Gradient Descent(33520): loss=0.2067005233103785\n",
      "Stochastic Gradient Descent(33521): loss=0.40182175219256683\n",
      "Stochastic Gradient Descent(33522): loss=1.9039528851248546\n",
      "Stochastic Gradient Descent(33523): loss=1.5789443150231428\n",
      "Stochastic Gradient Descent(33524): loss=8.8211406353986\n",
      "Stochastic Gradient Descent(33525): loss=6.060454953329084\n",
      "Stochastic Gradient Descent(33526): loss=3.1734593854880355\n",
      "Stochastic Gradient Descent(33527): loss=1.1783713937419031\n",
      "Stochastic Gradient Descent(33528): loss=1.0728200902032656\n",
      "Stochastic Gradient Descent(33529): loss=1.521823555803828\n",
      "Stochastic Gradient Descent(33530): loss=0.06999817674024554\n",
      "Stochastic Gradient Descent(33531): loss=0.9598828096384185\n",
      "Stochastic Gradient Descent(33532): loss=2.338699271376258\n",
      "Stochastic Gradient Descent(33533): loss=0.00538331079814013\n",
      "Stochastic Gradient Descent(33534): loss=1.0932656727294763\n",
      "Stochastic Gradient Descent(33535): loss=2.2168354127945173\n",
      "Stochastic Gradient Descent(33536): loss=0.4091662611736024\n",
      "Stochastic Gradient Descent(33537): loss=4.039284888778537\n",
      "Stochastic Gradient Descent(33538): loss=3.1205168340474647\n",
      "Stochastic Gradient Descent(33539): loss=0.3039048141262854\n",
      "Stochastic Gradient Descent(33540): loss=0.000917860017917003\n",
      "Stochastic Gradient Descent(33541): loss=0.18182507654453428\n",
      "Stochastic Gradient Descent(33542): loss=8.474182997495205\n",
      "Stochastic Gradient Descent(33543): loss=0.14424076888977264\n",
      "Stochastic Gradient Descent(33544): loss=0.6753780317163428\n",
      "Stochastic Gradient Descent(33545): loss=0.08015803076561978\n",
      "Stochastic Gradient Descent(33546): loss=3.0812996498471614\n",
      "Stochastic Gradient Descent(33547): loss=8.571013122043903\n",
      "Stochastic Gradient Descent(33548): loss=10.793670467913977\n",
      "Stochastic Gradient Descent(33549): loss=3.701724999141346\n",
      "Stochastic Gradient Descent(33550): loss=0.17682815323655843\n",
      "Stochastic Gradient Descent(33551): loss=4.747408448570797\n",
      "Stochastic Gradient Descent(33552): loss=8.157598521809282\n",
      "Stochastic Gradient Descent(33553): loss=3.038358688678507\n",
      "Stochastic Gradient Descent(33554): loss=0.2632341651710594\n",
      "Stochastic Gradient Descent(33555): loss=0.5149906969818403\n",
      "Stochastic Gradient Descent(33556): loss=1.5345493631499172\n",
      "Stochastic Gradient Descent(33557): loss=16.896959345499656\n",
      "Stochastic Gradient Descent(33558): loss=0.8465258919897621\n",
      "Stochastic Gradient Descent(33559): loss=8.080190661971608\n",
      "Stochastic Gradient Descent(33560): loss=3.5555440402488405\n",
      "Stochastic Gradient Descent(33561): loss=7.655696240749254\n",
      "Stochastic Gradient Descent(33562): loss=11.138587425871675\n",
      "Stochastic Gradient Descent(33563): loss=7.633910074506117e-07\n",
      "Stochastic Gradient Descent(33564): loss=6.174609403763271\n",
      "Stochastic Gradient Descent(33565): loss=19.21459379875306\n",
      "Stochastic Gradient Descent(33566): loss=56.12475298828585\n",
      "Stochastic Gradient Descent(33567): loss=0.8605001007529574\n",
      "Stochastic Gradient Descent(33568): loss=0.0181233010853503\n",
      "Stochastic Gradient Descent(33569): loss=30.812696947117228\n",
      "Stochastic Gradient Descent(33570): loss=2.6702301571447826\n",
      "Stochastic Gradient Descent(33571): loss=8.886237710504878\n",
      "Stochastic Gradient Descent(33572): loss=0.7023244372756132\n",
      "Stochastic Gradient Descent(33573): loss=3.8833398736750335\n",
      "Stochastic Gradient Descent(33574): loss=9.847728359654743\n",
      "Stochastic Gradient Descent(33575): loss=1.788175762685947\n",
      "Stochastic Gradient Descent(33576): loss=9.392016267849222\n",
      "Stochastic Gradient Descent(33577): loss=0.19454737304284725\n",
      "Stochastic Gradient Descent(33578): loss=0.399345831643866\n",
      "Stochastic Gradient Descent(33579): loss=23.446031575982246\n",
      "Stochastic Gradient Descent(33580): loss=3.7583947834479843\n",
      "Stochastic Gradient Descent(33581): loss=0.0044175589128302006\n",
      "Stochastic Gradient Descent(33582): loss=17.92794535401637\n",
      "Stochastic Gradient Descent(33583): loss=1.5492327813453444\n",
      "Stochastic Gradient Descent(33584): loss=2.922601818542443\n",
      "Stochastic Gradient Descent(33585): loss=3.0351503919429423\n",
      "Stochastic Gradient Descent(33586): loss=0.43693010535568466\n",
      "Stochastic Gradient Descent(33587): loss=8.532230375308952\n",
      "Stochastic Gradient Descent(33588): loss=1.1692379950119287\n",
      "Stochastic Gradient Descent(33589): loss=10.20669785254077\n",
      "Stochastic Gradient Descent(33590): loss=2.4045452949929422\n",
      "Stochastic Gradient Descent(33591): loss=2.364135268573574\n",
      "Stochastic Gradient Descent(33592): loss=0.028200560264650797\n",
      "Stochastic Gradient Descent(33593): loss=5.843397550810951\n",
      "Stochastic Gradient Descent(33594): loss=0.387237473517074\n",
      "Stochastic Gradient Descent(33595): loss=2.135255525757415\n",
      "Stochastic Gradient Descent(33596): loss=1.2340703688811057\n",
      "Stochastic Gradient Descent(33597): loss=13.537016331510143\n",
      "Stochastic Gradient Descent(33598): loss=4.519620858871004\n",
      "Stochastic Gradient Descent(33599): loss=2.156148007695101\n",
      "Stochastic Gradient Descent(33600): loss=3.6624374245329103\n",
      "Stochastic Gradient Descent(33601): loss=0.5059823101203279\n",
      "Stochastic Gradient Descent(33602): loss=11.571628217323358\n",
      "Stochastic Gradient Descent(33603): loss=48.33786087486503\n",
      "Stochastic Gradient Descent(33604): loss=2.0501558056799394\n",
      "Stochastic Gradient Descent(33605): loss=3.7818007921302006\n",
      "Stochastic Gradient Descent(33606): loss=17.53463861915542\n",
      "Stochastic Gradient Descent(33607): loss=1.6377367161944791\n",
      "Stochastic Gradient Descent(33608): loss=5.715631212910614\n",
      "Stochastic Gradient Descent(33609): loss=6.3422946054200855\n",
      "Stochastic Gradient Descent(33610): loss=16.267163402469702\n",
      "Stochastic Gradient Descent(33611): loss=2.5791087959535006\n",
      "Stochastic Gradient Descent(33612): loss=1.1523757905531284\n",
      "Stochastic Gradient Descent(33613): loss=2.859102865562712\n",
      "Stochastic Gradient Descent(33614): loss=4.2471513758727175\n",
      "Stochastic Gradient Descent(33615): loss=0.05455961259810093\n",
      "Stochastic Gradient Descent(33616): loss=0.7093875736440082\n",
      "Stochastic Gradient Descent(33617): loss=5.1219483736169185\n",
      "Stochastic Gradient Descent(33618): loss=1.022972707116348\n",
      "Stochastic Gradient Descent(33619): loss=4.995462657781037\n",
      "Stochastic Gradient Descent(33620): loss=1.2653705806718525\n",
      "Stochastic Gradient Descent(33621): loss=0.27345126516661455\n",
      "Stochastic Gradient Descent(33622): loss=0.2473452093672797\n",
      "Stochastic Gradient Descent(33623): loss=8.5985092376387\n",
      "Stochastic Gradient Descent(33624): loss=8.086561537381293\n",
      "Stochastic Gradient Descent(33625): loss=0.8611795609767658\n",
      "Stochastic Gradient Descent(33626): loss=1.1878652086899797\n",
      "Stochastic Gradient Descent(33627): loss=1.7668660434211312\n",
      "Stochastic Gradient Descent(33628): loss=0.24803792956893542\n",
      "Stochastic Gradient Descent(33629): loss=0.9404639233532459\n",
      "Stochastic Gradient Descent(33630): loss=3.7337274608710485\n",
      "Stochastic Gradient Descent(33631): loss=3.359616707652323\n",
      "Stochastic Gradient Descent(33632): loss=9.043937594769691\n",
      "Stochastic Gradient Descent(33633): loss=6.457698891653868\n",
      "Stochastic Gradient Descent(33634): loss=1.6559237952550774\n",
      "Stochastic Gradient Descent(33635): loss=2.8886897486582\n",
      "Stochastic Gradient Descent(33636): loss=15.30276582082583\n",
      "Stochastic Gradient Descent(33637): loss=7.0939106492875785\n",
      "Stochastic Gradient Descent(33638): loss=2.8313098653848368\n",
      "Stochastic Gradient Descent(33639): loss=0.14508133104409607\n",
      "Stochastic Gradient Descent(33640): loss=1.3468214462530028\n",
      "Stochastic Gradient Descent(33641): loss=0.3873701157438243\n",
      "Stochastic Gradient Descent(33642): loss=3.687262898091649\n",
      "Stochastic Gradient Descent(33643): loss=0.5720755295381912\n",
      "Stochastic Gradient Descent(33644): loss=0.014302387364845504\n",
      "Stochastic Gradient Descent(33645): loss=0.06992400875471567\n",
      "Stochastic Gradient Descent(33646): loss=0.7273436427302068\n",
      "Stochastic Gradient Descent(33647): loss=9.864206705146117\n",
      "Stochastic Gradient Descent(33648): loss=1.4532930697095514\n",
      "Stochastic Gradient Descent(33649): loss=2.0576009346108504\n",
      "Stochastic Gradient Descent(33650): loss=0.9225812232831828\n",
      "Stochastic Gradient Descent(33651): loss=3.544900604915602\n",
      "Stochastic Gradient Descent(33652): loss=1.6526879809300437\n",
      "Stochastic Gradient Descent(33653): loss=3.0695173389809427\n",
      "Stochastic Gradient Descent(33654): loss=0.2287754323588996\n",
      "Stochastic Gradient Descent(33655): loss=0.3898302941756231\n",
      "Stochastic Gradient Descent(33656): loss=3.742931136072443\n",
      "Stochastic Gradient Descent(33657): loss=9.791131493365453\n",
      "Stochastic Gradient Descent(33658): loss=0.0051633597509457025\n",
      "Stochastic Gradient Descent(33659): loss=0.34436619765323806\n",
      "Stochastic Gradient Descent(33660): loss=4.315894418892915\n",
      "Stochastic Gradient Descent(33661): loss=9.342923582677338\n",
      "Stochastic Gradient Descent(33662): loss=0.4658521060169203\n",
      "Stochastic Gradient Descent(33663): loss=1.0860952464026536\n",
      "Stochastic Gradient Descent(33664): loss=2.706031907382681\n",
      "Stochastic Gradient Descent(33665): loss=2.8439670436754\n",
      "Stochastic Gradient Descent(33666): loss=0.5242933158984471\n",
      "Stochastic Gradient Descent(33667): loss=6.161059217965629\n",
      "Stochastic Gradient Descent(33668): loss=3.8743178046629465\n",
      "Stochastic Gradient Descent(33669): loss=5.652947804737735\n",
      "Stochastic Gradient Descent(33670): loss=0.7407907322056756\n",
      "Stochastic Gradient Descent(33671): loss=0.13403738230026468\n",
      "Stochastic Gradient Descent(33672): loss=0.29317869311237144\n",
      "Stochastic Gradient Descent(33673): loss=0.619828538519477\n",
      "Stochastic Gradient Descent(33674): loss=0.041955957109416214\n",
      "Stochastic Gradient Descent(33675): loss=4.66970010769945\n",
      "Stochastic Gradient Descent(33676): loss=0.4388221398258692\n",
      "Stochastic Gradient Descent(33677): loss=0.2638788759513077\n",
      "Stochastic Gradient Descent(33678): loss=11.172737202416204\n",
      "Stochastic Gradient Descent(33679): loss=10.073878198866424\n",
      "Stochastic Gradient Descent(33680): loss=0.00022699280180565206\n",
      "Stochastic Gradient Descent(33681): loss=0.0399456772458001\n",
      "Stochastic Gradient Descent(33682): loss=0.20484625208082474\n",
      "Stochastic Gradient Descent(33683): loss=0.45068639298052215\n",
      "Stochastic Gradient Descent(33684): loss=0.005327912366533624\n",
      "Stochastic Gradient Descent(33685): loss=6.778466777029207\n",
      "Stochastic Gradient Descent(33686): loss=0.4744417565798024\n",
      "Stochastic Gradient Descent(33687): loss=0.0749385704339501\n",
      "Stochastic Gradient Descent(33688): loss=3.532247307852115\n",
      "Stochastic Gradient Descent(33689): loss=3.7526789020205698\n",
      "Stochastic Gradient Descent(33690): loss=0.009899708933851002\n",
      "Stochastic Gradient Descent(33691): loss=4.0505587219344825\n",
      "Stochastic Gradient Descent(33692): loss=0.006029209504618109\n",
      "Stochastic Gradient Descent(33693): loss=2.946418454312619\n",
      "Stochastic Gradient Descent(33694): loss=11.434736983691936\n",
      "Stochastic Gradient Descent(33695): loss=4.6862110774240096\n",
      "Stochastic Gradient Descent(33696): loss=13.794791323436327\n",
      "Stochastic Gradient Descent(33697): loss=0.7322146374582481\n",
      "Stochastic Gradient Descent(33698): loss=0.38448865408166216\n",
      "Stochastic Gradient Descent(33699): loss=6.114597148723144\n",
      "Stochastic Gradient Descent(33700): loss=0.24635411744673996\n",
      "Stochastic Gradient Descent(33701): loss=3.949031978805986\n",
      "Stochastic Gradient Descent(33702): loss=1.8202515762734277\n",
      "Stochastic Gradient Descent(33703): loss=0.7081872086195368\n",
      "Stochastic Gradient Descent(33704): loss=0.07764109213771057\n",
      "Stochastic Gradient Descent(33705): loss=2.3589367747997367\n",
      "Stochastic Gradient Descent(33706): loss=2.5505500420001\n",
      "Stochastic Gradient Descent(33707): loss=35.39737518653771\n",
      "Stochastic Gradient Descent(33708): loss=1.7779919684567098\n",
      "Stochastic Gradient Descent(33709): loss=0.45229888116204714\n",
      "Stochastic Gradient Descent(33710): loss=1.3691644105905172\n",
      "Stochastic Gradient Descent(33711): loss=0.5232727678548897\n",
      "Stochastic Gradient Descent(33712): loss=0.7300735315343435\n",
      "Stochastic Gradient Descent(33713): loss=5.761554499909774\n",
      "Stochastic Gradient Descent(33714): loss=0.5889747133794314\n",
      "Stochastic Gradient Descent(33715): loss=0.07479599945193231\n",
      "Stochastic Gradient Descent(33716): loss=0.09470318043429571\n",
      "Stochastic Gradient Descent(33717): loss=0.07292933921848199\n",
      "Stochastic Gradient Descent(33718): loss=0.09163055671104722\n",
      "Stochastic Gradient Descent(33719): loss=1.6062423231553824\n",
      "Stochastic Gradient Descent(33720): loss=2.249458616070394\n",
      "Stochastic Gradient Descent(33721): loss=13.160010034694823\n",
      "Stochastic Gradient Descent(33722): loss=1.1729315235481208\n",
      "Stochastic Gradient Descent(33723): loss=0.40144095752623377\n",
      "Stochastic Gradient Descent(33724): loss=1.9243868032052904\n",
      "Stochastic Gradient Descent(33725): loss=0.8625382897174334\n",
      "Stochastic Gradient Descent(33726): loss=0.5266879412895908\n",
      "Stochastic Gradient Descent(33727): loss=3.353704997507744\n",
      "Stochastic Gradient Descent(33728): loss=1.484313604317284\n",
      "Stochastic Gradient Descent(33729): loss=2.7243862037407682\n",
      "Stochastic Gradient Descent(33730): loss=7.145694784028709\n",
      "Stochastic Gradient Descent(33731): loss=0.023322219332837634\n",
      "Stochastic Gradient Descent(33732): loss=5.232770397974415\n",
      "Stochastic Gradient Descent(33733): loss=2.8082387538571494\n",
      "Stochastic Gradient Descent(33734): loss=1.012506608536722\n",
      "Stochastic Gradient Descent(33735): loss=3.3311463837365083\n",
      "Stochastic Gradient Descent(33736): loss=4.702247388455728\n",
      "Stochastic Gradient Descent(33737): loss=8.102527197599436\n",
      "Stochastic Gradient Descent(33738): loss=6.157144956829319\n",
      "Stochastic Gradient Descent(33739): loss=5.224940174067342\n",
      "Stochastic Gradient Descent(33740): loss=2.0154336853146653\n",
      "Stochastic Gradient Descent(33741): loss=0.867627168577177\n",
      "Stochastic Gradient Descent(33742): loss=0.5883653658405101\n",
      "Stochastic Gradient Descent(33743): loss=1.2883019031792509\n",
      "Stochastic Gradient Descent(33744): loss=1.4231431393112295\n",
      "Stochastic Gradient Descent(33745): loss=0.44497681948229506\n",
      "Stochastic Gradient Descent(33746): loss=0.6510497473711969\n",
      "Stochastic Gradient Descent(33747): loss=0.5991756523696504\n",
      "Stochastic Gradient Descent(33748): loss=1.245514871311088\n",
      "Stochastic Gradient Descent(33749): loss=2.5710070926627187\n",
      "Stochastic Gradient Descent(33750): loss=1.3334531421373244\n",
      "Stochastic Gradient Descent(33751): loss=1.4474500776802888\n",
      "Stochastic Gradient Descent(33752): loss=1.2618551628554744\n",
      "Stochastic Gradient Descent(33753): loss=0.038594631202127395\n",
      "Stochastic Gradient Descent(33754): loss=1.067601937357053\n",
      "Stochastic Gradient Descent(33755): loss=0.11372019457956385\n",
      "Stochastic Gradient Descent(33756): loss=12.222185798607411\n",
      "Stochastic Gradient Descent(33757): loss=9.670067676799531\n",
      "Stochastic Gradient Descent(33758): loss=2.7220740128570156\n",
      "Stochastic Gradient Descent(33759): loss=0.6785661085703508\n",
      "Stochastic Gradient Descent(33760): loss=3.7647700023319803\n",
      "Stochastic Gradient Descent(33761): loss=0.4951545541870782\n",
      "Stochastic Gradient Descent(33762): loss=1.5634171733524744\n",
      "Stochastic Gradient Descent(33763): loss=0.11823933977188182\n",
      "Stochastic Gradient Descent(33764): loss=0.34567809707866765\n",
      "Stochastic Gradient Descent(33765): loss=1.3813130320452558\n",
      "Stochastic Gradient Descent(33766): loss=0.8283817772811165\n",
      "Stochastic Gradient Descent(33767): loss=2.210853280514624\n",
      "Stochastic Gradient Descent(33768): loss=7.398825308963109\n",
      "Stochastic Gradient Descent(33769): loss=4.041743041866592\n",
      "Stochastic Gradient Descent(33770): loss=7.8772513678928435\n",
      "Stochastic Gradient Descent(33771): loss=29.536139775032876\n",
      "Stochastic Gradient Descent(33772): loss=0.3085094286172011\n",
      "Stochastic Gradient Descent(33773): loss=40.660538410615644\n",
      "Stochastic Gradient Descent(33774): loss=12.016715535683767\n",
      "Stochastic Gradient Descent(33775): loss=1.044577804816941\n",
      "Stochastic Gradient Descent(33776): loss=5.912670468006218\n",
      "Stochastic Gradient Descent(33777): loss=12.26696540056101\n",
      "Stochastic Gradient Descent(33778): loss=0.11154054378458851\n",
      "Stochastic Gradient Descent(33779): loss=22.2278314641897\n",
      "Stochastic Gradient Descent(33780): loss=2.371363650099338\n",
      "Stochastic Gradient Descent(33781): loss=0.38778765806084087\n",
      "Stochastic Gradient Descent(33782): loss=2.739421697524681\n",
      "Stochastic Gradient Descent(33783): loss=1.8707434222083699\n",
      "Stochastic Gradient Descent(33784): loss=2.2035151121651406\n",
      "Stochastic Gradient Descent(33785): loss=0.1898263345569838\n",
      "Stochastic Gradient Descent(33786): loss=0.8766316655279395\n",
      "Stochastic Gradient Descent(33787): loss=3.3178948012320886\n",
      "Stochastic Gradient Descent(33788): loss=0.9142514498807356\n",
      "Stochastic Gradient Descent(33789): loss=8.70897142916364\n",
      "Stochastic Gradient Descent(33790): loss=12.945433346261636\n",
      "Stochastic Gradient Descent(33791): loss=0.013209845422079586\n",
      "Stochastic Gradient Descent(33792): loss=0.4906532026164962\n",
      "Stochastic Gradient Descent(33793): loss=1.5923195075871728\n",
      "Stochastic Gradient Descent(33794): loss=15.549271047035818\n",
      "Stochastic Gradient Descent(33795): loss=0.8179279508818339\n",
      "Stochastic Gradient Descent(33796): loss=1.3320564780804318\n",
      "Stochastic Gradient Descent(33797): loss=6.999833014239925\n",
      "Stochastic Gradient Descent(33798): loss=0.945146658995905\n",
      "Stochastic Gradient Descent(33799): loss=0.14542960018246534\n",
      "Stochastic Gradient Descent(33800): loss=0.38421344889135667\n",
      "Stochastic Gradient Descent(33801): loss=0.022439215377376465\n",
      "Stochastic Gradient Descent(33802): loss=0.0679801467381674\n",
      "Stochastic Gradient Descent(33803): loss=2.2217840932455544\n",
      "Stochastic Gradient Descent(33804): loss=1.933288859876464\n",
      "Stochastic Gradient Descent(33805): loss=0.7533788916873072\n",
      "Stochastic Gradient Descent(33806): loss=0.2606902720069039\n",
      "Stochastic Gradient Descent(33807): loss=0.8415739620471727\n",
      "Stochastic Gradient Descent(33808): loss=23.297670838121675\n",
      "Stochastic Gradient Descent(33809): loss=2.766436505308443\n",
      "Stochastic Gradient Descent(33810): loss=2.9368821908563763\n",
      "Stochastic Gradient Descent(33811): loss=2.53067451082252\n",
      "Stochastic Gradient Descent(33812): loss=1.2257279434004087\n",
      "Stochastic Gradient Descent(33813): loss=17.60377264344227\n",
      "Stochastic Gradient Descent(33814): loss=46.674867718746896\n",
      "Stochastic Gradient Descent(33815): loss=6.92736883780132e-05\n",
      "Stochastic Gradient Descent(33816): loss=0.30377781739819476\n",
      "Stochastic Gradient Descent(33817): loss=4.666477216082251\n",
      "Stochastic Gradient Descent(33818): loss=5.619536079970312\n",
      "Stochastic Gradient Descent(33819): loss=11.890033989440154\n",
      "Stochastic Gradient Descent(33820): loss=4.441394433769962\n",
      "Stochastic Gradient Descent(33821): loss=2.7335010962950843\n",
      "Stochastic Gradient Descent(33822): loss=6.300152469894687\n",
      "Stochastic Gradient Descent(33823): loss=3.5237283731455085\n",
      "Stochastic Gradient Descent(33824): loss=6.819235943614595\n",
      "Stochastic Gradient Descent(33825): loss=2.1828804935158073\n",
      "Stochastic Gradient Descent(33826): loss=0.46511693420707295\n",
      "Stochastic Gradient Descent(33827): loss=29.74719182069984\n",
      "Stochastic Gradient Descent(33828): loss=0.09821245095889483\n",
      "Stochastic Gradient Descent(33829): loss=48.109464844618465\n",
      "Stochastic Gradient Descent(33830): loss=0.0001931602965498648\n",
      "Stochastic Gradient Descent(33831): loss=16.4328451528314\n",
      "Stochastic Gradient Descent(33832): loss=0.009290373888237616\n",
      "Stochastic Gradient Descent(33833): loss=15.618992285753375\n",
      "Stochastic Gradient Descent(33834): loss=0.6635233184560796\n",
      "Stochastic Gradient Descent(33835): loss=0.18718373261814208\n",
      "Stochastic Gradient Descent(33836): loss=0.3230391601437771\n",
      "Stochastic Gradient Descent(33837): loss=2.1282319988609535\n",
      "Stochastic Gradient Descent(33838): loss=3.425339523335835\n",
      "Stochastic Gradient Descent(33839): loss=0.2090443085806748\n",
      "Stochastic Gradient Descent(33840): loss=0.08755727549465006\n",
      "Stochastic Gradient Descent(33841): loss=5.147831923984548\n",
      "Stochastic Gradient Descent(33842): loss=0.3902747162707695\n",
      "Stochastic Gradient Descent(33843): loss=1.0199594585293132\n",
      "Stochastic Gradient Descent(33844): loss=7.08456868178229\n",
      "Stochastic Gradient Descent(33845): loss=2.364215120390499\n",
      "Stochastic Gradient Descent(33846): loss=3.943447050351808\n",
      "Stochastic Gradient Descent(33847): loss=21.257771526133165\n",
      "Stochastic Gradient Descent(33848): loss=0.2817952217676871\n",
      "Stochastic Gradient Descent(33849): loss=0.006855467081030469\n",
      "Stochastic Gradient Descent(33850): loss=1.8267797039801863\n",
      "Stochastic Gradient Descent(33851): loss=0.013411604554447636\n",
      "Stochastic Gradient Descent(33852): loss=1.9803663767910198\n",
      "Stochastic Gradient Descent(33853): loss=2.201865048505976\n",
      "Stochastic Gradient Descent(33854): loss=2.6816882891943203\n",
      "Stochastic Gradient Descent(33855): loss=0.4849760251322201\n",
      "Stochastic Gradient Descent(33856): loss=0.4575121027430849\n",
      "Stochastic Gradient Descent(33857): loss=2.211809937479949\n",
      "Stochastic Gradient Descent(33858): loss=0.01651115694656207\n",
      "Stochastic Gradient Descent(33859): loss=3.3742464080567185\n",
      "Stochastic Gradient Descent(33860): loss=2.108911121312593\n",
      "Stochastic Gradient Descent(33861): loss=14.728509395056681\n",
      "Stochastic Gradient Descent(33862): loss=30.716679449987236\n",
      "Stochastic Gradient Descent(33863): loss=0.13549232700016584\n",
      "Stochastic Gradient Descent(33864): loss=0.29426020530351876\n",
      "Stochastic Gradient Descent(33865): loss=0.011170931732452056\n",
      "Stochastic Gradient Descent(33866): loss=0.02200881049542164\n",
      "Stochastic Gradient Descent(33867): loss=0.08771832889303789\n",
      "Stochastic Gradient Descent(33868): loss=4.452324008075477\n",
      "Stochastic Gradient Descent(33869): loss=3.451644570568502\n",
      "Stochastic Gradient Descent(33870): loss=3.7623998047084073\n",
      "Stochastic Gradient Descent(33871): loss=8.063563536742732e-05\n",
      "Stochastic Gradient Descent(33872): loss=0.9486313251755766\n",
      "Stochastic Gradient Descent(33873): loss=0.744941820257454\n",
      "Stochastic Gradient Descent(33874): loss=4.205982057238128\n",
      "Stochastic Gradient Descent(33875): loss=3.8673761212762137\n",
      "Stochastic Gradient Descent(33876): loss=0.02131566429196129\n",
      "Stochastic Gradient Descent(33877): loss=18.114219836359307\n",
      "Stochastic Gradient Descent(33878): loss=7.838610842542474\n",
      "Stochastic Gradient Descent(33879): loss=1.1194446125202213\n",
      "Stochastic Gradient Descent(33880): loss=22.97387761018641\n",
      "Stochastic Gradient Descent(33881): loss=4.73807234129646\n",
      "Stochastic Gradient Descent(33882): loss=5.868484343521448\n",
      "Stochastic Gradient Descent(33883): loss=1.9410774731273666\n",
      "Stochastic Gradient Descent(33884): loss=2.2065119613537596\n",
      "Stochastic Gradient Descent(33885): loss=1.235843715258983\n",
      "Stochastic Gradient Descent(33886): loss=6.4681192561770535\n",
      "Stochastic Gradient Descent(33887): loss=1.4203870216337569\n",
      "Stochastic Gradient Descent(33888): loss=5.235829388921333\n",
      "Stochastic Gradient Descent(33889): loss=1.4758736492720992\n",
      "Stochastic Gradient Descent(33890): loss=5.619042334413892\n",
      "Stochastic Gradient Descent(33891): loss=3.593570442794144\n",
      "Stochastic Gradient Descent(33892): loss=9.477156927791446\n",
      "Stochastic Gradient Descent(33893): loss=2.843117100939173\n",
      "Stochastic Gradient Descent(33894): loss=14.324803205072522\n",
      "Stochastic Gradient Descent(33895): loss=0.07236771706946307\n",
      "Stochastic Gradient Descent(33896): loss=8.623968913423516\n",
      "Stochastic Gradient Descent(33897): loss=0.5994255661471777\n",
      "Stochastic Gradient Descent(33898): loss=1.0713191632954435\n",
      "Stochastic Gradient Descent(33899): loss=0.016695941953933122\n",
      "Stochastic Gradient Descent(33900): loss=7.03019170807652\n",
      "Stochastic Gradient Descent(33901): loss=2.7757090571981244\n",
      "Stochastic Gradient Descent(33902): loss=6.675121710713577\n",
      "Stochastic Gradient Descent(33903): loss=1.9586802705445354\n",
      "Stochastic Gradient Descent(33904): loss=1.2642851318460495\n",
      "Stochastic Gradient Descent(33905): loss=0.5812017232114539\n",
      "Stochastic Gradient Descent(33906): loss=0.9971879374565923\n",
      "Stochastic Gradient Descent(33907): loss=1.7130854117847396\n",
      "Stochastic Gradient Descent(33908): loss=9.212928077520152\n",
      "Stochastic Gradient Descent(33909): loss=4.840990893440619\n",
      "Stochastic Gradient Descent(33910): loss=0.48565445514970823\n",
      "Stochastic Gradient Descent(33911): loss=1.004261842944574\n",
      "Stochastic Gradient Descent(33912): loss=0.10467863648261476\n",
      "Stochastic Gradient Descent(33913): loss=1.1456597359256093\n",
      "Stochastic Gradient Descent(33914): loss=0.5963893287187865\n",
      "Stochastic Gradient Descent(33915): loss=0.4134169702407266\n",
      "Stochastic Gradient Descent(33916): loss=0.9701679859734175\n",
      "Stochastic Gradient Descent(33917): loss=0.6469526418925345\n",
      "Stochastic Gradient Descent(33918): loss=2.3350001849417765\n",
      "Stochastic Gradient Descent(33919): loss=5.224556204169021\n",
      "Stochastic Gradient Descent(33920): loss=0.004559857401606429\n",
      "Stochastic Gradient Descent(33921): loss=0.1409827492572997\n",
      "Stochastic Gradient Descent(33922): loss=4.747363876811066\n",
      "Stochastic Gradient Descent(33923): loss=12.584706695462959\n",
      "Stochastic Gradient Descent(33924): loss=1.632139014138555\n",
      "Stochastic Gradient Descent(33925): loss=50.103287539108294\n",
      "Stochastic Gradient Descent(33926): loss=3.0724359607413723\n",
      "Stochastic Gradient Descent(33927): loss=0.6211856726533042\n",
      "Stochastic Gradient Descent(33928): loss=0.5324054449975137\n",
      "Stochastic Gradient Descent(33929): loss=1.1243066700639048\n",
      "Stochastic Gradient Descent(33930): loss=3.7109261485424505\n",
      "Stochastic Gradient Descent(33931): loss=8.263634585596169\n",
      "Stochastic Gradient Descent(33932): loss=2.8335436602485133\n",
      "Stochastic Gradient Descent(33933): loss=6.274799732964306\n",
      "Stochastic Gradient Descent(33934): loss=2.131274719714877\n",
      "Stochastic Gradient Descent(33935): loss=5.911640034672131\n",
      "Stochastic Gradient Descent(33936): loss=28.65124155023451\n",
      "Stochastic Gradient Descent(33937): loss=0.011277816938880301\n",
      "Stochastic Gradient Descent(33938): loss=1.6081227118196728\n",
      "Stochastic Gradient Descent(33939): loss=22.232898512394012\n",
      "Stochastic Gradient Descent(33940): loss=1.7850893424032777\n",
      "Stochastic Gradient Descent(33941): loss=17.171162862846185\n",
      "Stochastic Gradient Descent(33942): loss=1.7226177069905046\n",
      "Stochastic Gradient Descent(33943): loss=2.7327396814736056\n",
      "Stochastic Gradient Descent(33944): loss=17.045930740450846\n",
      "Stochastic Gradient Descent(33945): loss=9.921890403593158\n",
      "Stochastic Gradient Descent(33946): loss=1.981564748508039\n",
      "Stochastic Gradient Descent(33947): loss=0.20181737192847768\n",
      "Stochastic Gradient Descent(33948): loss=10.369184375577861\n",
      "Stochastic Gradient Descent(33949): loss=0.6122505132777072\n",
      "Stochastic Gradient Descent(33950): loss=0.025353633287521545\n",
      "Stochastic Gradient Descent(33951): loss=0.3023512188831903\n",
      "Stochastic Gradient Descent(33952): loss=2.178890690431895\n",
      "Stochastic Gradient Descent(33953): loss=2.2965442778618725\n",
      "Stochastic Gradient Descent(33954): loss=0.0011542744710600632\n",
      "Stochastic Gradient Descent(33955): loss=1.924050144882909\n",
      "Stochastic Gradient Descent(33956): loss=2.0654584373535494\n",
      "Stochastic Gradient Descent(33957): loss=0.40235559578332686\n",
      "Stochastic Gradient Descent(33958): loss=0.10315503286382692\n",
      "Stochastic Gradient Descent(33959): loss=3.487085877663209\n",
      "Stochastic Gradient Descent(33960): loss=0.44222503754732423\n",
      "Stochastic Gradient Descent(33961): loss=10.637290605056036\n",
      "Stochastic Gradient Descent(33962): loss=1.501827497554943\n",
      "Stochastic Gradient Descent(33963): loss=3.7976082210701585\n",
      "Stochastic Gradient Descent(33964): loss=13.821655022089455\n",
      "Stochastic Gradient Descent(33965): loss=0.8182002940849891\n",
      "Stochastic Gradient Descent(33966): loss=0.5719347599097583\n",
      "Stochastic Gradient Descent(33967): loss=1.3613074308114035\n",
      "Stochastic Gradient Descent(33968): loss=0.02931136052049442\n",
      "Stochastic Gradient Descent(33969): loss=2.2309123658755783\n",
      "Stochastic Gradient Descent(33970): loss=0.22136605009457022\n",
      "Stochastic Gradient Descent(33971): loss=0.6560190417437185\n",
      "Stochastic Gradient Descent(33972): loss=0.24749766277047883\n",
      "Stochastic Gradient Descent(33973): loss=0.8486097697690407\n",
      "Stochastic Gradient Descent(33974): loss=16.31678077393689\n",
      "Stochastic Gradient Descent(33975): loss=5.757215474632047\n",
      "Stochastic Gradient Descent(33976): loss=9.387739592415802\n",
      "Stochastic Gradient Descent(33977): loss=9.308632113955975\n",
      "Stochastic Gradient Descent(33978): loss=2.6146514749778844\n",
      "Stochastic Gradient Descent(33979): loss=0.1576864152246029\n",
      "Stochastic Gradient Descent(33980): loss=1.131438270041158\n",
      "Stochastic Gradient Descent(33981): loss=0.22168683704013975\n",
      "Stochastic Gradient Descent(33982): loss=1.7487030776063295\n",
      "Stochastic Gradient Descent(33983): loss=13.331326974008824\n",
      "Stochastic Gradient Descent(33984): loss=2.4426709684980796\n",
      "Stochastic Gradient Descent(33985): loss=3.4758882855503828\n",
      "Stochastic Gradient Descent(33986): loss=2.4621853023736566\n",
      "Stochastic Gradient Descent(33987): loss=0.3999226304581802\n",
      "Stochastic Gradient Descent(33988): loss=25.64809863301636\n",
      "Stochastic Gradient Descent(33989): loss=0.44712667622005586\n",
      "Stochastic Gradient Descent(33990): loss=4.294170194665676\n",
      "Stochastic Gradient Descent(33991): loss=3.5377499505724863\n",
      "Stochastic Gradient Descent(33992): loss=0.04930176044250874\n",
      "Stochastic Gradient Descent(33993): loss=0.9320584036956777\n",
      "Stochastic Gradient Descent(33994): loss=3.3397024747058444\n",
      "Stochastic Gradient Descent(33995): loss=17.82461100479498\n",
      "Stochastic Gradient Descent(33996): loss=4.8632827051131295\n",
      "Stochastic Gradient Descent(33997): loss=6.687655099053355\n",
      "Stochastic Gradient Descent(33998): loss=38.269503499120404\n",
      "Stochastic Gradient Descent(33999): loss=19.04560989037036\n",
      "Stochastic Gradient Descent(34000): loss=18.522098103123913\n",
      "Stochastic Gradient Descent(34001): loss=0.0006092607805631943\n",
      "Stochastic Gradient Descent(34002): loss=14.794974093885743\n",
      "Stochastic Gradient Descent(34003): loss=0.5783605180706357\n",
      "Stochastic Gradient Descent(34004): loss=0.965395985572368\n",
      "Stochastic Gradient Descent(34005): loss=8.135760916265639\n",
      "Stochastic Gradient Descent(34006): loss=0.4002624101896409\n",
      "Stochastic Gradient Descent(34007): loss=0.7007255908615246\n",
      "Stochastic Gradient Descent(34008): loss=0.6886157659464079\n",
      "Stochastic Gradient Descent(34009): loss=1.3575176847067387\n",
      "Stochastic Gradient Descent(34010): loss=20.860408353010627\n",
      "Stochastic Gradient Descent(34011): loss=3.0658463162837304\n",
      "Stochastic Gradient Descent(34012): loss=0.40421117684968255\n",
      "Stochastic Gradient Descent(34013): loss=5.566034384131568\n",
      "Stochastic Gradient Descent(34014): loss=13.703939137487838\n",
      "Stochastic Gradient Descent(34015): loss=3.894301506772873\n",
      "Stochastic Gradient Descent(34016): loss=1.3400042462516657\n",
      "Stochastic Gradient Descent(34017): loss=0.060649194747831016\n",
      "Stochastic Gradient Descent(34018): loss=1.1015094422001093\n",
      "Stochastic Gradient Descent(34019): loss=16.16356551509604\n",
      "Stochastic Gradient Descent(34020): loss=1.2322218889410987\n",
      "Stochastic Gradient Descent(34021): loss=0.12357870181411339\n",
      "Stochastic Gradient Descent(34022): loss=1.980246028686757\n",
      "Stochastic Gradient Descent(34023): loss=6.4071745833380405\n",
      "Stochastic Gradient Descent(34024): loss=0.2572073202361916\n",
      "Stochastic Gradient Descent(34025): loss=0.5243421787817248\n",
      "Stochastic Gradient Descent(34026): loss=0.2996949979374981\n",
      "Stochastic Gradient Descent(34027): loss=2.6372564342213952e-05\n",
      "Stochastic Gradient Descent(34028): loss=23.449195455475767\n",
      "Stochastic Gradient Descent(34029): loss=13.295900286834282\n",
      "Stochastic Gradient Descent(34030): loss=1.0480927434893414\n",
      "Stochastic Gradient Descent(34031): loss=19.014597926860468\n",
      "Stochastic Gradient Descent(34032): loss=2.5547231713873813\n",
      "Stochastic Gradient Descent(34033): loss=0.5538548768578628\n",
      "Stochastic Gradient Descent(34034): loss=13.377555121021576\n",
      "Stochastic Gradient Descent(34035): loss=0.06887799459595963\n",
      "Stochastic Gradient Descent(34036): loss=2.8571100211316325\n",
      "Stochastic Gradient Descent(34037): loss=3.2918157333198215\n",
      "Stochastic Gradient Descent(34038): loss=1.086032198429048\n",
      "Stochastic Gradient Descent(34039): loss=4.457375249923388\n",
      "Stochastic Gradient Descent(34040): loss=7.894698052276979\n",
      "Stochastic Gradient Descent(34041): loss=0.0053244359536134085\n",
      "Stochastic Gradient Descent(34042): loss=0.729418206345691\n",
      "Stochastic Gradient Descent(34043): loss=0.42532790786617647\n",
      "Stochastic Gradient Descent(34044): loss=17.83005971048526\n",
      "Stochastic Gradient Descent(34045): loss=15.118774972558754\n",
      "Stochastic Gradient Descent(34046): loss=0.12009289199496875\n",
      "Stochastic Gradient Descent(34047): loss=4.05308068913139\n",
      "Stochastic Gradient Descent(34048): loss=0.11178106292532793\n",
      "Stochastic Gradient Descent(34049): loss=1.2720262872805137\n",
      "Stochastic Gradient Descent(34050): loss=0.8204311214176561\n",
      "Stochastic Gradient Descent(34051): loss=8.788061199972216\n",
      "Stochastic Gradient Descent(34052): loss=0.03380833376994808\n",
      "Stochastic Gradient Descent(34053): loss=5.389553624058185\n",
      "Stochastic Gradient Descent(34054): loss=0.1469043705290357\n",
      "Stochastic Gradient Descent(34055): loss=2.59138291297977\n",
      "Stochastic Gradient Descent(34056): loss=0.0010334521581708469\n",
      "Stochastic Gradient Descent(34057): loss=0.0015863924921313652\n",
      "Stochastic Gradient Descent(34058): loss=1.449665941074581\n",
      "Stochastic Gradient Descent(34059): loss=2.264643386994196\n",
      "Stochastic Gradient Descent(34060): loss=9.652222256121153\n",
      "Stochastic Gradient Descent(34061): loss=4.81639031094312\n",
      "Stochastic Gradient Descent(34062): loss=0.7787528581898865\n",
      "Stochastic Gradient Descent(34063): loss=6.124584183535343\n",
      "Stochastic Gradient Descent(34064): loss=1.2364690589649767\n",
      "Stochastic Gradient Descent(34065): loss=3.7224825773414305\n",
      "Stochastic Gradient Descent(34066): loss=0.08985202256880438\n",
      "Stochastic Gradient Descent(34067): loss=0.4664355710096596\n",
      "Stochastic Gradient Descent(34068): loss=5.392352428149979\n",
      "Stochastic Gradient Descent(34069): loss=0.7087215440549699\n",
      "Stochastic Gradient Descent(34070): loss=3.471987403618146\n",
      "Stochastic Gradient Descent(34071): loss=1.3477774252558017\n",
      "Stochastic Gradient Descent(34072): loss=0.23465737850342513\n",
      "Stochastic Gradient Descent(34073): loss=5.559282709376857\n",
      "Stochastic Gradient Descent(34074): loss=1.08582552312516\n",
      "Stochastic Gradient Descent(34075): loss=0.1383182425000503\n",
      "Stochastic Gradient Descent(34076): loss=7.791962554650955\n",
      "Stochastic Gradient Descent(34077): loss=2.7235257271603106\n",
      "Stochastic Gradient Descent(34078): loss=8.208188157054634\n",
      "Stochastic Gradient Descent(34079): loss=4.607444626967439\n",
      "Stochastic Gradient Descent(34080): loss=3.6205728773625685\n",
      "Stochastic Gradient Descent(34081): loss=2.8642523814298677\n",
      "Stochastic Gradient Descent(34082): loss=9.127781978807668\n",
      "Stochastic Gradient Descent(34083): loss=9.526695658864409\n",
      "Stochastic Gradient Descent(34084): loss=14.885390896500205\n",
      "Stochastic Gradient Descent(34085): loss=2.7774014864664798\n",
      "Stochastic Gradient Descent(34086): loss=1.2888355120485337\n",
      "Stochastic Gradient Descent(34087): loss=0.4521556289997267\n",
      "Stochastic Gradient Descent(34088): loss=2.3192081861052514\n",
      "Stochastic Gradient Descent(34089): loss=4.562187372854562\n",
      "Stochastic Gradient Descent(34090): loss=4.30124402156734\n",
      "Stochastic Gradient Descent(34091): loss=4.885394255017373\n",
      "Stochastic Gradient Descent(34092): loss=1.094715387368319\n",
      "Stochastic Gradient Descent(34093): loss=0.2890770884738596\n",
      "Stochastic Gradient Descent(34094): loss=2.075662808111508\n",
      "Stochastic Gradient Descent(34095): loss=0.3167052594243962\n",
      "Stochastic Gradient Descent(34096): loss=0.36401438786826895\n",
      "Stochastic Gradient Descent(34097): loss=1.2715787048260059\n",
      "Stochastic Gradient Descent(34098): loss=8.097326132436715\n",
      "Stochastic Gradient Descent(34099): loss=2.5258367123700745\n",
      "Stochastic Gradient Descent(34100): loss=0.8687763889279133\n",
      "Stochastic Gradient Descent(34101): loss=6.141636594024135\n",
      "Stochastic Gradient Descent(34102): loss=17.858505470780276\n",
      "Stochastic Gradient Descent(34103): loss=11.962103390671663\n",
      "Stochastic Gradient Descent(34104): loss=14.586693005512608\n",
      "Stochastic Gradient Descent(34105): loss=14.018911130425867\n",
      "Stochastic Gradient Descent(34106): loss=4.441579050698128\n",
      "Stochastic Gradient Descent(34107): loss=1.6677993198303718\n",
      "Stochastic Gradient Descent(34108): loss=0.006531614562265694\n",
      "Stochastic Gradient Descent(34109): loss=2.493305132174812\n",
      "Stochastic Gradient Descent(34110): loss=1.697736919737282\n",
      "Stochastic Gradient Descent(34111): loss=3.2215655082495096\n",
      "Stochastic Gradient Descent(34112): loss=0.7181795250039746\n",
      "Stochastic Gradient Descent(34113): loss=1.1885829781306576\n",
      "Stochastic Gradient Descent(34114): loss=2.0749251759487053\n",
      "Stochastic Gradient Descent(34115): loss=1.7767472046785453\n",
      "Stochastic Gradient Descent(34116): loss=6.176094864299315\n",
      "Stochastic Gradient Descent(34117): loss=10.943493556594907\n",
      "Stochastic Gradient Descent(34118): loss=7.889095393798071\n",
      "Stochastic Gradient Descent(34119): loss=0.0549962079042406\n",
      "Stochastic Gradient Descent(34120): loss=2.987606791058951\n",
      "Stochastic Gradient Descent(34121): loss=1.6877120115182407\n",
      "Stochastic Gradient Descent(34122): loss=1.5628827789496138\n",
      "Stochastic Gradient Descent(34123): loss=0.27061410470828534\n",
      "Stochastic Gradient Descent(34124): loss=8.77179128634627\n",
      "Stochastic Gradient Descent(34125): loss=6.431615916961563\n",
      "Stochastic Gradient Descent(34126): loss=2.322492798276315\n",
      "Stochastic Gradient Descent(34127): loss=0.258701510126037\n",
      "Stochastic Gradient Descent(34128): loss=0.3127238576104513\n",
      "Stochastic Gradient Descent(34129): loss=1.71758154286217\n",
      "Stochastic Gradient Descent(34130): loss=1.89289009347116\n",
      "Stochastic Gradient Descent(34131): loss=10.801798506283243\n",
      "Stochastic Gradient Descent(34132): loss=0.687979539670326\n",
      "Stochastic Gradient Descent(34133): loss=5.095680487156742\n",
      "Stochastic Gradient Descent(34134): loss=4.693025658968924\n",
      "Stochastic Gradient Descent(34135): loss=6.990619121058133\n",
      "Stochastic Gradient Descent(34136): loss=0.008063533887785478\n",
      "Stochastic Gradient Descent(34137): loss=1.6745837656791205\n",
      "Stochastic Gradient Descent(34138): loss=0.4471429450228396\n",
      "Stochastic Gradient Descent(34139): loss=1.3692345431928734\n",
      "Stochastic Gradient Descent(34140): loss=0.007557184350828181\n",
      "Stochastic Gradient Descent(34141): loss=0.20360871880999223\n",
      "Stochastic Gradient Descent(34142): loss=1.3356184836798748\n",
      "Stochastic Gradient Descent(34143): loss=0.009136758958583738\n",
      "Stochastic Gradient Descent(34144): loss=0.7710143574722301\n",
      "Stochastic Gradient Descent(34145): loss=0.1435942547581766\n",
      "Stochastic Gradient Descent(34146): loss=17.47146609828707\n",
      "Stochastic Gradient Descent(34147): loss=7.658812705034231\n",
      "Stochastic Gradient Descent(34148): loss=5.409844865460912\n",
      "Stochastic Gradient Descent(34149): loss=1.5189382348196994\n",
      "Stochastic Gradient Descent(34150): loss=2.335371026267335\n",
      "Stochastic Gradient Descent(34151): loss=3.433950241729454\n",
      "Stochastic Gradient Descent(34152): loss=0.6667797426142517\n",
      "Stochastic Gradient Descent(34153): loss=5.34897485842059\n",
      "Stochastic Gradient Descent(34154): loss=3.22691322587604\n",
      "Stochastic Gradient Descent(34155): loss=0.006903100378880155\n",
      "Stochastic Gradient Descent(34156): loss=0.2833074369024293\n",
      "Stochastic Gradient Descent(34157): loss=0.6787063088200481\n",
      "Stochastic Gradient Descent(34158): loss=0.20390049356136003\n",
      "Stochastic Gradient Descent(34159): loss=0.08428545666004948\n",
      "Stochastic Gradient Descent(34160): loss=7.526950473423414\n",
      "Stochastic Gradient Descent(34161): loss=4.8595789534834815\n",
      "Stochastic Gradient Descent(34162): loss=12.326634559116409\n",
      "Stochastic Gradient Descent(34163): loss=0.630214087124343\n",
      "Stochastic Gradient Descent(34164): loss=1.6082058528967098\n",
      "Stochastic Gradient Descent(34165): loss=0.02494113804410317\n",
      "Stochastic Gradient Descent(34166): loss=39.05631927871642\n",
      "Stochastic Gradient Descent(34167): loss=0.022991458486383065\n",
      "Stochastic Gradient Descent(34168): loss=0.198094607773589\n",
      "Stochastic Gradient Descent(34169): loss=0.8146699366753865\n",
      "Stochastic Gradient Descent(34170): loss=14.342786428160442\n",
      "Stochastic Gradient Descent(34171): loss=5.614857752393366\n",
      "Stochastic Gradient Descent(34172): loss=12.849385460111163\n",
      "Stochastic Gradient Descent(34173): loss=0.11362134113767472\n",
      "Stochastic Gradient Descent(34174): loss=0.8422061642509129\n",
      "Stochastic Gradient Descent(34175): loss=0.25814065064118047\n",
      "Stochastic Gradient Descent(34176): loss=2.5491717834300265\n",
      "Stochastic Gradient Descent(34177): loss=16.207933688755535\n",
      "Stochastic Gradient Descent(34178): loss=27.487754999477005\n",
      "Stochastic Gradient Descent(34179): loss=0.12723658766731263\n",
      "Stochastic Gradient Descent(34180): loss=0.009709179118018513\n",
      "Stochastic Gradient Descent(34181): loss=0.008139122695334738\n",
      "Stochastic Gradient Descent(34182): loss=0.26199728585903925\n",
      "Stochastic Gradient Descent(34183): loss=2.435389090703126\n",
      "Stochastic Gradient Descent(34184): loss=0.19168160440162357\n",
      "Stochastic Gradient Descent(34185): loss=3.092186573998916\n",
      "Stochastic Gradient Descent(34186): loss=0.15240205863625805\n",
      "Stochastic Gradient Descent(34187): loss=0.4083044172040972\n",
      "Stochastic Gradient Descent(34188): loss=7.312926839926849\n",
      "Stochastic Gradient Descent(34189): loss=3.473974787507493\n",
      "Stochastic Gradient Descent(34190): loss=5.823571346178469\n",
      "Stochastic Gradient Descent(34191): loss=0.15658544876586913\n",
      "Stochastic Gradient Descent(34192): loss=0.7855952564854547\n",
      "Stochastic Gradient Descent(34193): loss=0.08519671135326497\n",
      "Stochastic Gradient Descent(34194): loss=0.10007540548385509\n",
      "Stochastic Gradient Descent(34195): loss=2.021265922395607\n",
      "Stochastic Gradient Descent(34196): loss=1.2518790125998875\n",
      "Stochastic Gradient Descent(34197): loss=2.0777767640896925\n",
      "Stochastic Gradient Descent(34198): loss=1.90971310257027\n",
      "Stochastic Gradient Descent(34199): loss=2.0792136553939704\n",
      "Stochastic Gradient Descent(34200): loss=16.45065497374977\n",
      "Stochastic Gradient Descent(34201): loss=2.396436039275253\n",
      "Stochastic Gradient Descent(34202): loss=1.3753844612971193\n",
      "Stochastic Gradient Descent(34203): loss=2.3422202059572443\n",
      "Stochastic Gradient Descent(34204): loss=0.04757894352592368\n",
      "Stochastic Gradient Descent(34205): loss=0.8995047285624631\n",
      "Stochastic Gradient Descent(34206): loss=2.433880671900667\n",
      "Stochastic Gradient Descent(34207): loss=2.6643037567634438\n",
      "Stochastic Gradient Descent(34208): loss=2.65628348171184\n",
      "Stochastic Gradient Descent(34209): loss=0.09245620342534712\n",
      "Stochastic Gradient Descent(34210): loss=1.7857403959941065\n",
      "Stochastic Gradient Descent(34211): loss=2.733410226004842\n",
      "Stochastic Gradient Descent(34212): loss=1.9704652872622195\n",
      "Stochastic Gradient Descent(34213): loss=0.2630341902915545\n",
      "Stochastic Gradient Descent(34214): loss=39.737372396660774\n",
      "Stochastic Gradient Descent(34215): loss=2.1805534454862756\n",
      "Stochastic Gradient Descent(34216): loss=11.572067135507515\n",
      "Stochastic Gradient Descent(34217): loss=0.12272572057954635\n",
      "Stochastic Gradient Descent(34218): loss=3.725875502605099\n",
      "Stochastic Gradient Descent(34219): loss=0.2958877666196443\n",
      "Stochastic Gradient Descent(34220): loss=19.802926122081377\n",
      "Stochastic Gradient Descent(34221): loss=2.9776490454830054\n",
      "Stochastic Gradient Descent(34222): loss=7.258060400674949\n",
      "Stochastic Gradient Descent(34223): loss=2.831458248347051\n",
      "Stochastic Gradient Descent(34224): loss=5.440154746555813\n",
      "Stochastic Gradient Descent(34225): loss=0.002767994477777289\n",
      "Stochastic Gradient Descent(34226): loss=0.5868714765591957\n",
      "Stochastic Gradient Descent(34227): loss=3.4688841409237283\n",
      "Stochastic Gradient Descent(34228): loss=10.367421028406785\n",
      "Stochastic Gradient Descent(34229): loss=21.383916109211462\n",
      "Stochastic Gradient Descent(34230): loss=2.6849628899433955\n",
      "Stochastic Gradient Descent(34231): loss=10.93378415880781\n",
      "Stochastic Gradient Descent(34232): loss=0.5732471640614925\n",
      "Stochastic Gradient Descent(34233): loss=0.15416164395522253\n",
      "Stochastic Gradient Descent(34234): loss=1.2929411629297094\n",
      "Stochastic Gradient Descent(34235): loss=1.6254022980365235\n",
      "Stochastic Gradient Descent(34236): loss=6.651769396058487\n",
      "Stochastic Gradient Descent(34237): loss=7.381448514749623\n",
      "Stochastic Gradient Descent(34238): loss=1.9922402850726626\n",
      "Stochastic Gradient Descent(34239): loss=0.7047166927605483\n",
      "Stochastic Gradient Descent(34240): loss=4.059636116604053\n",
      "Stochastic Gradient Descent(34241): loss=11.207743963695185\n",
      "Stochastic Gradient Descent(34242): loss=10.604982178915888\n",
      "Stochastic Gradient Descent(34243): loss=8.975679944990135\n",
      "Stochastic Gradient Descent(34244): loss=0.3379634718548227\n",
      "Stochastic Gradient Descent(34245): loss=0.6566862229789842\n",
      "Stochastic Gradient Descent(34246): loss=0.7902838938940832\n",
      "Stochastic Gradient Descent(34247): loss=0.5988515373960476\n",
      "Stochastic Gradient Descent(34248): loss=4.2154805077429485\n",
      "Stochastic Gradient Descent(34249): loss=0.03726419331026986\n",
      "Stochastic Gradient Descent(34250): loss=8.62597484749973\n",
      "Stochastic Gradient Descent(34251): loss=1.4070273875515011\n",
      "Stochastic Gradient Descent(34252): loss=0.25974931123700046\n",
      "Stochastic Gradient Descent(34253): loss=5.725455273958104\n",
      "Stochastic Gradient Descent(34254): loss=0.41991678887279527\n",
      "Stochastic Gradient Descent(34255): loss=10.378124266764198\n",
      "Stochastic Gradient Descent(34256): loss=0.21732883659433205\n",
      "Stochastic Gradient Descent(34257): loss=0.12803906879245744\n",
      "Stochastic Gradient Descent(34258): loss=0.5630973864832578\n",
      "Stochastic Gradient Descent(34259): loss=0.07258522685943153\n",
      "Stochastic Gradient Descent(34260): loss=0.06959845215366008\n",
      "Stochastic Gradient Descent(34261): loss=9.722091483134847\n",
      "Stochastic Gradient Descent(34262): loss=1.1641918053555054\n",
      "Stochastic Gradient Descent(34263): loss=3.8646245890666977\n",
      "Stochastic Gradient Descent(34264): loss=0.3695640781253195\n",
      "Stochastic Gradient Descent(34265): loss=2.892704821904802\n",
      "Stochastic Gradient Descent(34266): loss=7.4524776862426965\n",
      "Stochastic Gradient Descent(34267): loss=1.2545228323029949\n",
      "Stochastic Gradient Descent(34268): loss=0.08986343856115074\n",
      "Stochastic Gradient Descent(34269): loss=0.012863144619212785\n",
      "Stochastic Gradient Descent(34270): loss=4.149956860457252\n",
      "Stochastic Gradient Descent(34271): loss=0.09502433978733424\n",
      "Stochastic Gradient Descent(34272): loss=3.4096121755818407\n",
      "Stochastic Gradient Descent(34273): loss=0.05141574788384626\n",
      "Stochastic Gradient Descent(34274): loss=0.12814291050163526\n",
      "Stochastic Gradient Descent(34275): loss=1.137833253018737\n",
      "Stochastic Gradient Descent(34276): loss=17.73298375782971\n",
      "Stochastic Gradient Descent(34277): loss=23.051403940782897\n",
      "Stochastic Gradient Descent(34278): loss=2.4480387177040748\n",
      "Stochastic Gradient Descent(34279): loss=8.022249254100892\n",
      "Stochastic Gradient Descent(34280): loss=0.1656253348290625\n",
      "Stochastic Gradient Descent(34281): loss=29.122221059060625\n",
      "Stochastic Gradient Descent(34282): loss=0.26556594018087665\n",
      "Stochastic Gradient Descent(34283): loss=0.003951493692170243\n",
      "Stochastic Gradient Descent(34284): loss=0.574287773072734\n",
      "Stochastic Gradient Descent(34285): loss=1.7915388400891963\n",
      "Stochastic Gradient Descent(34286): loss=1.0292152574391946\n",
      "Stochastic Gradient Descent(34287): loss=36.520402709205506\n",
      "Stochastic Gradient Descent(34288): loss=38.23878898108006\n",
      "Stochastic Gradient Descent(34289): loss=13.270427168176813\n",
      "Stochastic Gradient Descent(34290): loss=0.21430162371076442\n",
      "Stochastic Gradient Descent(34291): loss=0.3278912890866648\n",
      "Stochastic Gradient Descent(34292): loss=2.4468961097213993\n",
      "Stochastic Gradient Descent(34293): loss=1.3247932740716168\n",
      "Stochastic Gradient Descent(34294): loss=0.1526897553413847\n",
      "Stochastic Gradient Descent(34295): loss=0.10758576131324911\n",
      "Stochastic Gradient Descent(34296): loss=0.22582598723613434\n",
      "Stochastic Gradient Descent(34297): loss=2.8329550304719904\n",
      "Stochastic Gradient Descent(34298): loss=5.100655478242943\n",
      "Stochastic Gradient Descent(34299): loss=5.318314996632325\n",
      "Stochastic Gradient Descent(34300): loss=1.1852745548510883\n",
      "Stochastic Gradient Descent(34301): loss=3.8398147279411723\n",
      "Stochastic Gradient Descent(34302): loss=2.5351967354793294\n",
      "Stochastic Gradient Descent(34303): loss=6.888667147680846\n",
      "Stochastic Gradient Descent(34304): loss=0.1918546417227815\n",
      "Stochastic Gradient Descent(34305): loss=1.3293070139530914\n",
      "Stochastic Gradient Descent(34306): loss=50.068278021779115\n",
      "Stochastic Gradient Descent(34307): loss=5.024667853155743\n",
      "Stochastic Gradient Descent(34308): loss=53.95343897412435\n",
      "Stochastic Gradient Descent(34309): loss=4.054690375583254\n",
      "Stochastic Gradient Descent(34310): loss=0.5475684149320688\n",
      "Stochastic Gradient Descent(34311): loss=0.0012217647895902556\n",
      "Stochastic Gradient Descent(34312): loss=8.431052646711027\n",
      "Stochastic Gradient Descent(34313): loss=1.6709960107809723\n",
      "Stochastic Gradient Descent(34314): loss=8.934315687199357\n",
      "Stochastic Gradient Descent(34315): loss=28.901198138073298\n",
      "Stochastic Gradient Descent(34316): loss=27.954771444717814\n",
      "Stochastic Gradient Descent(34317): loss=0.19731499818600742\n",
      "Stochastic Gradient Descent(34318): loss=5.043001514056945\n",
      "Stochastic Gradient Descent(34319): loss=7.3372533865810405\n",
      "Stochastic Gradient Descent(34320): loss=6.145734086749624\n",
      "Stochastic Gradient Descent(34321): loss=0.7263295791382229\n",
      "Stochastic Gradient Descent(34322): loss=0.42292122184755226\n",
      "Stochastic Gradient Descent(34323): loss=4.802035477174328\n",
      "Stochastic Gradient Descent(34324): loss=1.442789716001269\n",
      "Stochastic Gradient Descent(34325): loss=0.3790541028244593\n",
      "Stochastic Gradient Descent(34326): loss=5.807492979072559\n",
      "Stochastic Gradient Descent(34327): loss=5.541599822183942\n",
      "Stochastic Gradient Descent(34328): loss=0.1733427048215807\n",
      "Stochastic Gradient Descent(34329): loss=0.11613313251614177\n",
      "Stochastic Gradient Descent(34330): loss=2.0897808934083604\n",
      "Stochastic Gradient Descent(34331): loss=1.387729273569816\n",
      "Stochastic Gradient Descent(34332): loss=5.226100589774242\n",
      "Stochastic Gradient Descent(34333): loss=4.99127552528573\n",
      "Stochastic Gradient Descent(34334): loss=0.9313796604415131\n",
      "Stochastic Gradient Descent(34335): loss=5.053557943013412\n",
      "Stochastic Gradient Descent(34336): loss=1.3226276606876946\n",
      "Stochastic Gradient Descent(34337): loss=0.06190745835557876\n",
      "Stochastic Gradient Descent(34338): loss=0.011614878903830402\n",
      "Stochastic Gradient Descent(34339): loss=13.386480631612166\n",
      "Stochastic Gradient Descent(34340): loss=2.360156184236251\n",
      "Stochastic Gradient Descent(34341): loss=2.0753967055709626\n",
      "Stochastic Gradient Descent(34342): loss=0.1777587046924035\n",
      "Stochastic Gradient Descent(34343): loss=11.23572943344842\n",
      "Stochastic Gradient Descent(34344): loss=0.3144931059917393\n",
      "Stochastic Gradient Descent(34345): loss=0.14698837494487077\n",
      "Stochastic Gradient Descent(34346): loss=39.87720101299588\n",
      "Stochastic Gradient Descent(34347): loss=11.41928008677541\n",
      "Stochastic Gradient Descent(34348): loss=1.3828050103879674\n",
      "Stochastic Gradient Descent(34349): loss=1.5057921419295037\n",
      "Stochastic Gradient Descent(34350): loss=0.5632903111858165\n",
      "Stochastic Gradient Descent(34351): loss=0.09278300834218553\n",
      "Stochastic Gradient Descent(34352): loss=0.20212245312494279\n",
      "Stochastic Gradient Descent(34353): loss=22.10238752204599\n",
      "Stochastic Gradient Descent(34354): loss=0.04994484693316453\n",
      "Stochastic Gradient Descent(34355): loss=0.6063834901178085\n",
      "Stochastic Gradient Descent(34356): loss=0.25338941491612443\n",
      "Stochastic Gradient Descent(34357): loss=0.9313814086573436\n",
      "Stochastic Gradient Descent(34358): loss=9.019579904571264\n",
      "Stochastic Gradient Descent(34359): loss=1.112964195032695\n",
      "Stochastic Gradient Descent(34360): loss=4.2571450661371255\n",
      "Stochastic Gradient Descent(34361): loss=5.449913796029134\n",
      "Stochastic Gradient Descent(34362): loss=2.175566423841534\n",
      "Stochastic Gradient Descent(34363): loss=3.7121457621864447\n",
      "Stochastic Gradient Descent(34364): loss=5.55468842029924\n",
      "Stochastic Gradient Descent(34365): loss=4.53811741718477\n",
      "Stochastic Gradient Descent(34366): loss=6.136482878846216\n",
      "Stochastic Gradient Descent(34367): loss=3.1418272672001084\n",
      "Stochastic Gradient Descent(34368): loss=0.33031807722798934\n",
      "Stochastic Gradient Descent(34369): loss=8.987316339541387\n",
      "Stochastic Gradient Descent(34370): loss=0.11638928913748645\n",
      "Stochastic Gradient Descent(34371): loss=0.9984521498959916\n",
      "Stochastic Gradient Descent(34372): loss=5.724443974642252\n",
      "Stochastic Gradient Descent(34373): loss=0.015762634326907148\n",
      "Stochastic Gradient Descent(34374): loss=0.9553509606056737\n",
      "Stochastic Gradient Descent(34375): loss=22.93727534145555\n",
      "Stochastic Gradient Descent(34376): loss=0.15816276392624465\n",
      "Stochastic Gradient Descent(34377): loss=0.5170508407580731\n",
      "Stochastic Gradient Descent(34378): loss=0.7165209255153694\n",
      "Stochastic Gradient Descent(34379): loss=10.468321641880763\n",
      "Stochastic Gradient Descent(34380): loss=1.6676129998255924\n",
      "Stochastic Gradient Descent(34381): loss=4.578228249197048\n",
      "Stochastic Gradient Descent(34382): loss=1.2093126130344158\n",
      "Stochastic Gradient Descent(34383): loss=13.421216137943317\n",
      "Stochastic Gradient Descent(34384): loss=3.280291048381145\n",
      "Stochastic Gradient Descent(34385): loss=0.0004889706818706383\n",
      "Stochastic Gradient Descent(34386): loss=3.520168353807403\n",
      "Stochastic Gradient Descent(34387): loss=0.2491924193107871\n",
      "Stochastic Gradient Descent(34388): loss=1.8543284166461056\n",
      "Stochastic Gradient Descent(34389): loss=0.1200113179588718\n",
      "Stochastic Gradient Descent(34390): loss=2.3441724889702056\n",
      "Stochastic Gradient Descent(34391): loss=0.359569812999976\n",
      "Stochastic Gradient Descent(34392): loss=1.2824939728841664\n",
      "Stochastic Gradient Descent(34393): loss=7.614738182435941\n",
      "Stochastic Gradient Descent(34394): loss=0.43407493410900305\n",
      "Stochastic Gradient Descent(34395): loss=0.7235339406674092\n",
      "Stochastic Gradient Descent(34396): loss=3.578324987217825\n",
      "Stochastic Gradient Descent(34397): loss=1.3606895882457044\n",
      "Stochastic Gradient Descent(34398): loss=0.0006859825193858871\n",
      "Stochastic Gradient Descent(34399): loss=0.06939387873427912\n",
      "Stochastic Gradient Descent(34400): loss=7.296580578235427\n",
      "Stochastic Gradient Descent(34401): loss=4.445724820111461\n",
      "Stochastic Gradient Descent(34402): loss=9.7517019296062\n",
      "Stochastic Gradient Descent(34403): loss=1.635081437834604\n",
      "Stochastic Gradient Descent(34404): loss=6.529806886332625\n",
      "Stochastic Gradient Descent(34405): loss=0.636857547444392\n",
      "Stochastic Gradient Descent(34406): loss=8.119459074001705\n",
      "Stochastic Gradient Descent(34407): loss=16.925811370862657\n",
      "Stochastic Gradient Descent(34408): loss=3.747188401522521\n",
      "Stochastic Gradient Descent(34409): loss=1.6885301429574564\n",
      "Stochastic Gradient Descent(34410): loss=0.6302315135930555\n",
      "Stochastic Gradient Descent(34411): loss=11.353214970154456\n",
      "Stochastic Gradient Descent(34412): loss=1.4671111564153836\n",
      "Stochastic Gradient Descent(34413): loss=0.009669852664246062\n",
      "Stochastic Gradient Descent(34414): loss=20.724680656539835\n",
      "Stochastic Gradient Descent(34415): loss=12.42746294724468\n",
      "Stochastic Gradient Descent(34416): loss=31.313078496787938\n",
      "Stochastic Gradient Descent(34417): loss=0.4459179810211987\n",
      "Stochastic Gradient Descent(34418): loss=0.2927802329613479\n",
      "Stochastic Gradient Descent(34419): loss=0.5245724370077572\n",
      "Stochastic Gradient Descent(34420): loss=0.6300124797022225\n",
      "Stochastic Gradient Descent(34421): loss=0.5517804192613115\n",
      "Stochastic Gradient Descent(34422): loss=0.483600007315319\n",
      "Stochastic Gradient Descent(34423): loss=1.9564330049518275\n",
      "Stochastic Gradient Descent(34424): loss=11.265679352951775\n",
      "Stochastic Gradient Descent(34425): loss=1.427910115517205\n",
      "Stochastic Gradient Descent(34426): loss=0.6031323146886679\n",
      "Stochastic Gradient Descent(34427): loss=8.884041503103177\n",
      "Stochastic Gradient Descent(34428): loss=10.029514596513929\n",
      "Stochastic Gradient Descent(34429): loss=0.307895946265659\n",
      "Stochastic Gradient Descent(34430): loss=0.8118829807467534\n",
      "Stochastic Gradient Descent(34431): loss=1.1590227677816758\n",
      "Stochastic Gradient Descent(34432): loss=3.8931465576986284\n",
      "Stochastic Gradient Descent(34433): loss=0.9416667361083637\n",
      "Stochastic Gradient Descent(34434): loss=8.218776131389642\n",
      "Stochastic Gradient Descent(34435): loss=4.685340254078734\n",
      "Stochastic Gradient Descent(34436): loss=3.8799842660991026\n",
      "Stochastic Gradient Descent(34437): loss=0.04948057646782677\n",
      "Stochastic Gradient Descent(34438): loss=0.5877852218809937\n",
      "Stochastic Gradient Descent(34439): loss=1.9691913401170882\n",
      "Stochastic Gradient Descent(34440): loss=1.7758159095650476\n",
      "Stochastic Gradient Descent(34441): loss=0.42923306014535206\n",
      "Stochastic Gradient Descent(34442): loss=3.69951842804727\n",
      "Stochastic Gradient Descent(34443): loss=0.16689341433966332\n",
      "Stochastic Gradient Descent(34444): loss=2.0269691547740263\n",
      "Stochastic Gradient Descent(34445): loss=6.267698087161162\n",
      "Stochastic Gradient Descent(34446): loss=4.559692684087755\n",
      "Stochastic Gradient Descent(34447): loss=1.159389289974257\n",
      "Stochastic Gradient Descent(34448): loss=0.8979887006746123\n",
      "Stochastic Gradient Descent(34449): loss=6.5409212025746175\n",
      "Stochastic Gradient Descent(34450): loss=0.3537703104213855\n",
      "Stochastic Gradient Descent(34451): loss=1.5817604035849653\n",
      "Stochastic Gradient Descent(34452): loss=1.4173822912728078\n",
      "Stochastic Gradient Descent(34453): loss=10.803950326684983\n",
      "Stochastic Gradient Descent(34454): loss=18.14521741526368\n",
      "Stochastic Gradient Descent(34455): loss=0.5550126199308769\n",
      "Stochastic Gradient Descent(34456): loss=0.06770573845560428\n",
      "Stochastic Gradient Descent(34457): loss=1.2157914086037025\n",
      "Stochastic Gradient Descent(34458): loss=0.26552519301033795\n",
      "Stochastic Gradient Descent(34459): loss=3.949793876500732\n",
      "Stochastic Gradient Descent(34460): loss=0.002292859021450423\n",
      "Stochastic Gradient Descent(34461): loss=0.041863037833109996\n",
      "Stochastic Gradient Descent(34462): loss=1.3738938234678622\n",
      "Stochastic Gradient Descent(34463): loss=0.10505476561796286\n",
      "Stochastic Gradient Descent(34464): loss=3.4592946547595407\n",
      "Stochastic Gradient Descent(34465): loss=0.04315801872478728\n",
      "Stochastic Gradient Descent(34466): loss=7.14701593021323\n",
      "Stochastic Gradient Descent(34467): loss=5.029860865804607\n",
      "Stochastic Gradient Descent(34468): loss=1.5096162241636963\n",
      "Stochastic Gradient Descent(34469): loss=6.577701074442082\n",
      "Stochastic Gradient Descent(34470): loss=2.3368696581472044\n",
      "Stochastic Gradient Descent(34471): loss=2.4403102384228803\n",
      "Stochastic Gradient Descent(34472): loss=2.3844138700126547\n",
      "Stochastic Gradient Descent(34473): loss=1.462932328720501\n",
      "Stochastic Gradient Descent(34474): loss=7.412848050160649\n",
      "Stochastic Gradient Descent(34475): loss=0.1712353821378654\n",
      "Stochastic Gradient Descent(34476): loss=7.728960182929933\n",
      "Stochastic Gradient Descent(34477): loss=1.546588390929882\n",
      "Stochastic Gradient Descent(34478): loss=0.3325567822311124\n",
      "Stochastic Gradient Descent(34479): loss=1.077107241105888\n",
      "Stochastic Gradient Descent(34480): loss=2.8049930562136005\n",
      "Stochastic Gradient Descent(34481): loss=0.03955688602268076\n",
      "Stochastic Gradient Descent(34482): loss=0.7858063128196968\n",
      "Stochastic Gradient Descent(34483): loss=2.0358516264476085\n",
      "Stochastic Gradient Descent(34484): loss=3.071569382062936\n",
      "Stochastic Gradient Descent(34485): loss=0.03525560054870609\n",
      "Stochastic Gradient Descent(34486): loss=1.1353404584157525\n",
      "Stochastic Gradient Descent(34487): loss=12.28289399405313\n",
      "Stochastic Gradient Descent(34488): loss=3.4644281617198436\n",
      "Stochastic Gradient Descent(34489): loss=0.6466148899298659\n",
      "Stochastic Gradient Descent(34490): loss=0.018397670964345962\n",
      "Stochastic Gradient Descent(34491): loss=7.658776785964647\n",
      "Stochastic Gradient Descent(34492): loss=3.789582995219918\n",
      "Stochastic Gradient Descent(34493): loss=13.54841892248274\n",
      "Stochastic Gradient Descent(34494): loss=12.84800267513969\n",
      "Stochastic Gradient Descent(34495): loss=4.632108016895939\n",
      "Stochastic Gradient Descent(34496): loss=4.635061740713921\n",
      "Stochastic Gradient Descent(34497): loss=2.0004797179293368\n",
      "Stochastic Gradient Descent(34498): loss=0.021227753216604042\n",
      "Stochastic Gradient Descent(34499): loss=1.8682887659247036\n",
      "Stochastic Gradient Descent(34500): loss=0.6422882490582028\n",
      "Stochastic Gradient Descent(34501): loss=1.183282109584685\n",
      "Stochastic Gradient Descent(34502): loss=7.955892996686529\n",
      "Stochastic Gradient Descent(34503): loss=1.5600705119723237\n",
      "Stochastic Gradient Descent(34504): loss=0.3368031106981941\n",
      "Stochastic Gradient Descent(34505): loss=0.040690502122338365\n",
      "Stochastic Gradient Descent(34506): loss=10.344059229007495\n",
      "Stochastic Gradient Descent(34507): loss=20.16258666116811\n",
      "Stochastic Gradient Descent(34508): loss=2.872315336198903\n",
      "Stochastic Gradient Descent(34509): loss=2.7044867204880942\n",
      "Stochastic Gradient Descent(34510): loss=3.1199239741808587\n",
      "Stochastic Gradient Descent(34511): loss=0.10015089384003686\n",
      "Stochastic Gradient Descent(34512): loss=0.3900301787290352\n",
      "Stochastic Gradient Descent(34513): loss=13.840146713498429\n",
      "Stochastic Gradient Descent(34514): loss=0.7937173693069876\n",
      "Stochastic Gradient Descent(34515): loss=23.366828092755238\n",
      "Stochastic Gradient Descent(34516): loss=1.5266667257521414\n",
      "Stochastic Gradient Descent(34517): loss=0.8437273584225057\n",
      "Stochastic Gradient Descent(34518): loss=2.4171927318201343\n",
      "Stochastic Gradient Descent(34519): loss=0.042619341471921694\n",
      "Stochastic Gradient Descent(34520): loss=4.609303303629257\n",
      "Stochastic Gradient Descent(34521): loss=0.15690391514735422\n",
      "Stochastic Gradient Descent(34522): loss=1.4888007443447742\n",
      "Stochastic Gradient Descent(34523): loss=7.787857271398336\n",
      "Stochastic Gradient Descent(34524): loss=0.30769826130751515\n",
      "Stochastic Gradient Descent(34525): loss=0.01924485247810974\n",
      "Stochastic Gradient Descent(34526): loss=0.38141165924622134\n",
      "Stochastic Gradient Descent(34527): loss=0.018587746126282258\n",
      "Stochastic Gradient Descent(34528): loss=0.7394816690220855\n",
      "Stochastic Gradient Descent(34529): loss=0.8250512133285495\n",
      "Stochastic Gradient Descent(34530): loss=0.04625432310999079\n",
      "Stochastic Gradient Descent(34531): loss=2.2830428395374316\n",
      "Stochastic Gradient Descent(34532): loss=0.5163149482980738\n",
      "Stochastic Gradient Descent(34533): loss=0.6950982841553052\n",
      "Stochastic Gradient Descent(34534): loss=5.492011309932176\n",
      "Stochastic Gradient Descent(34535): loss=1.1822958996951587\n",
      "Stochastic Gradient Descent(34536): loss=1.7339857416384499\n",
      "Stochastic Gradient Descent(34537): loss=0.0012545329637344705\n",
      "Stochastic Gradient Descent(34538): loss=1.5648819863657053\n",
      "Stochastic Gradient Descent(34539): loss=1.1241360673487377\n",
      "Stochastic Gradient Descent(34540): loss=0.2530971596850118\n",
      "Stochastic Gradient Descent(34541): loss=6.996018383984403\n",
      "Stochastic Gradient Descent(34542): loss=3.065446666879759\n",
      "Stochastic Gradient Descent(34543): loss=12.607183927984662\n",
      "Stochastic Gradient Descent(34544): loss=7.853025871480114\n",
      "Stochastic Gradient Descent(34545): loss=0.035309669213429956\n",
      "Stochastic Gradient Descent(34546): loss=0.007698712137262861\n",
      "Stochastic Gradient Descent(34547): loss=0.1289340273684197\n",
      "Stochastic Gradient Descent(34548): loss=3.0769423110719822\n",
      "Stochastic Gradient Descent(34549): loss=4.257416615834763\n",
      "Stochastic Gradient Descent(34550): loss=7.791123476886394\n",
      "Stochastic Gradient Descent(34551): loss=1.0854118363420446\n",
      "Stochastic Gradient Descent(34552): loss=24.55384357267527\n",
      "Stochastic Gradient Descent(34553): loss=51.98910270406864\n",
      "Stochastic Gradient Descent(34554): loss=0.00012820134349241665\n",
      "Stochastic Gradient Descent(34555): loss=6.080618678776936\n",
      "Stochastic Gradient Descent(34556): loss=4.461333729064381\n",
      "Stochastic Gradient Descent(34557): loss=1.7441882565601048\n",
      "Stochastic Gradient Descent(34558): loss=0.5603633252139877\n",
      "Stochastic Gradient Descent(34559): loss=27.11844246200907\n",
      "Stochastic Gradient Descent(34560): loss=0.049677064259651287\n",
      "Stochastic Gradient Descent(34561): loss=10.963287126863705\n",
      "Stochastic Gradient Descent(34562): loss=8.272228914443966\n",
      "Stochastic Gradient Descent(34563): loss=24.336559010937513\n",
      "Stochastic Gradient Descent(34564): loss=11.991248498097425\n",
      "Stochastic Gradient Descent(34565): loss=0.1813748702590424\n",
      "Stochastic Gradient Descent(34566): loss=0.2845470313001351\n",
      "Stochastic Gradient Descent(34567): loss=2.9548373243815194\n",
      "Stochastic Gradient Descent(34568): loss=1.022909552942188\n",
      "Stochastic Gradient Descent(34569): loss=1.0326442625697774\n",
      "Stochastic Gradient Descent(34570): loss=0.4369590841730446\n",
      "Stochastic Gradient Descent(34571): loss=1.3037829158279874\n",
      "Stochastic Gradient Descent(34572): loss=9.85082664750881\n",
      "Stochastic Gradient Descent(34573): loss=5.851768649472555\n",
      "Stochastic Gradient Descent(34574): loss=1.130192667256735\n",
      "Stochastic Gradient Descent(34575): loss=0.05346549996750319\n",
      "Stochastic Gradient Descent(34576): loss=0.10216735559678704\n",
      "Stochastic Gradient Descent(34577): loss=8.389068863541585\n",
      "Stochastic Gradient Descent(34578): loss=0.0022415480004267\n",
      "Stochastic Gradient Descent(34579): loss=10.795969973698925\n",
      "Stochastic Gradient Descent(34580): loss=1.8691510286619326\n",
      "Stochastic Gradient Descent(34581): loss=12.197758723561673\n",
      "Stochastic Gradient Descent(34582): loss=0.449196326947189\n",
      "Stochastic Gradient Descent(34583): loss=4.066854179312417\n",
      "Stochastic Gradient Descent(34584): loss=5.367867527001623\n",
      "Stochastic Gradient Descent(34585): loss=0.0010774300722469954\n",
      "Stochastic Gradient Descent(34586): loss=15.443835209274628\n",
      "Stochastic Gradient Descent(34587): loss=2.2098017008702837\n",
      "Stochastic Gradient Descent(34588): loss=1.3817664115206407\n",
      "Stochastic Gradient Descent(34589): loss=1.0825889583166477\n",
      "Stochastic Gradient Descent(34590): loss=0.029632970891451133\n",
      "Stochastic Gradient Descent(34591): loss=0.07829239372107773\n",
      "Stochastic Gradient Descent(34592): loss=0.1961035119088859\n",
      "Stochastic Gradient Descent(34593): loss=2.364911566913337\n",
      "Stochastic Gradient Descent(34594): loss=1.4191008980154456\n",
      "Stochastic Gradient Descent(34595): loss=0.264179837788216\n",
      "Stochastic Gradient Descent(34596): loss=0.7235246580717849\n",
      "Stochastic Gradient Descent(34597): loss=1.774010599583182\n",
      "Stochastic Gradient Descent(34598): loss=12.612053491484135\n",
      "Stochastic Gradient Descent(34599): loss=4.22332806865061\n",
      "Stochastic Gradient Descent(34600): loss=6.341120768978431\n",
      "Stochastic Gradient Descent(34601): loss=15.252256376782077\n",
      "Stochastic Gradient Descent(34602): loss=1.368920289336138\n",
      "Stochastic Gradient Descent(34603): loss=8.96183005540865\n",
      "Stochastic Gradient Descent(34604): loss=0.7720874187193124\n",
      "Stochastic Gradient Descent(34605): loss=0.6468314906223671\n",
      "Stochastic Gradient Descent(34606): loss=3.712443879428762\n",
      "Stochastic Gradient Descent(34607): loss=0.011291891014198311\n",
      "Stochastic Gradient Descent(34608): loss=0.0483297341523722\n",
      "Stochastic Gradient Descent(34609): loss=0.13314679442148641\n",
      "Stochastic Gradient Descent(34610): loss=1.5590699728594857\n",
      "Stochastic Gradient Descent(34611): loss=0.8404976885691713\n",
      "Stochastic Gradient Descent(34612): loss=25.403832534964188\n",
      "Stochastic Gradient Descent(34613): loss=0.5488485631927379\n",
      "Stochastic Gradient Descent(34614): loss=0.4309299898639656\n",
      "Stochastic Gradient Descent(34615): loss=0.36008861471728176\n",
      "Stochastic Gradient Descent(34616): loss=0.2404325873804943\n",
      "Stochastic Gradient Descent(34617): loss=20.343224464693918\n",
      "Stochastic Gradient Descent(34618): loss=6.932966872247096\n",
      "Stochastic Gradient Descent(34619): loss=0.38303040626754326\n",
      "Stochastic Gradient Descent(34620): loss=9.98693303707121\n",
      "Stochastic Gradient Descent(34621): loss=0.12238994089403604\n",
      "Stochastic Gradient Descent(34622): loss=0.21657190674965132\n",
      "Stochastic Gradient Descent(34623): loss=5.104573768451198\n",
      "Stochastic Gradient Descent(34624): loss=0.5063692190364674\n",
      "Stochastic Gradient Descent(34625): loss=0.7211739505563961\n",
      "Stochastic Gradient Descent(34626): loss=8.087371469642376\n",
      "Stochastic Gradient Descent(34627): loss=1.442494336019496\n",
      "Stochastic Gradient Descent(34628): loss=24.97751018378128\n",
      "Stochastic Gradient Descent(34629): loss=3.1406773779726938\n",
      "Stochastic Gradient Descent(34630): loss=2.992246250947395\n",
      "Stochastic Gradient Descent(34631): loss=4.738333286482031\n",
      "Stochastic Gradient Descent(34632): loss=1.1161434069559284\n",
      "Stochastic Gradient Descent(34633): loss=0.05155474296753508\n",
      "Stochastic Gradient Descent(34634): loss=1.968382765274094\n",
      "Stochastic Gradient Descent(34635): loss=3.048881615637747\n",
      "Stochastic Gradient Descent(34636): loss=27.537948012718715\n",
      "Stochastic Gradient Descent(34637): loss=0.7729503224883855\n",
      "Stochastic Gradient Descent(34638): loss=5.516098948940153\n",
      "Stochastic Gradient Descent(34639): loss=0.5100664235127461\n",
      "Stochastic Gradient Descent(34640): loss=0.7507777976411107\n",
      "Stochastic Gradient Descent(34641): loss=0.30413005982990654\n",
      "Stochastic Gradient Descent(34642): loss=1.100649416789256\n",
      "Stochastic Gradient Descent(34643): loss=2.9795754059465436\n",
      "Stochastic Gradient Descent(34644): loss=0.5772649885255005\n",
      "Stochastic Gradient Descent(34645): loss=2.9682699722304915\n",
      "Stochastic Gradient Descent(34646): loss=0.30093797280148527\n",
      "Stochastic Gradient Descent(34647): loss=0.5105822113769605\n",
      "Stochastic Gradient Descent(34648): loss=0.2727763723633711\n",
      "Stochastic Gradient Descent(34649): loss=0.4378606639143795\n",
      "Stochastic Gradient Descent(34650): loss=2.455398690407349\n",
      "Stochastic Gradient Descent(34651): loss=2.9541565733708444\n",
      "Stochastic Gradient Descent(34652): loss=3.7689383303371264\n",
      "Stochastic Gradient Descent(34653): loss=1.5175070739346244\n",
      "Stochastic Gradient Descent(34654): loss=0.0751929699172719\n",
      "Stochastic Gradient Descent(34655): loss=5.656725133630731\n",
      "Stochastic Gradient Descent(34656): loss=4.316206667300178\n",
      "Stochastic Gradient Descent(34657): loss=1.8676601275804179\n",
      "Stochastic Gradient Descent(34658): loss=16.075602900194983\n",
      "Stochastic Gradient Descent(34659): loss=8.650870846711879\n",
      "Stochastic Gradient Descent(34660): loss=0.00444425943378166\n",
      "Stochastic Gradient Descent(34661): loss=0.5145885398836082\n",
      "Stochastic Gradient Descent(34662): loss=7.563863687927849e-05\n",
      "Stochastic Gradient Descent(34663): loss=1.23986456200139\n",
      "Stochastic Gradient Descent(34664): loss=0.04286458138123433\n",
      "Stochastic Gradient Descent(34665): loss=6.017483603382321\n",
      "Stochastic Gradient Descent(34666): loss=0.9537403596075014\n",
      "Stochastic Gradient Descent(34667): loss=0.6919205649404764\n",
      "Stochastic Gradient Descent(34668): loss=5.365479037015918\n",
      "Stochastic Gradient Descent(34669): loss=0.25612731494889146\n",
      "Stochastic Gradient Descent(34670): loss=4.771781684552956\n",
      "Stochastic Gradient Descent(34671): loss=0.8894986117942011\n",
      "Stochastic Gradient Descent(34672): loss=0.29262607828345377\n",
      "Stochastic Gradient Descent(34673): loss=6.675373932187716\n",
      "Stochastic Gradient Descent(34674): loss=2.286872026609333\n",
      "Stochastic Gradient Descent(34675): loss=5.6792804213319386\n",
      "Stochastic Gradient Descent(34676): loss=0.5221574229223279\n",
      "Stochastic Gradient Descent(34677): loss=12.447331504432658\n",
      "Stochastic Gradient Descent(34678): loss=1.9281860981832342\n",
      "Stochastic Gradient Descent(34679): loss=9.042145578469793\n",
      "Stochastic Gradient Descent(34680): loss=0.19524841135385274\n",
      "Stochastic Gradient Descent(34681): loss=0.14242760774553326\n",
      "Stochastic Gradient Descent(34682): loss=5.36882940716268\n",
      "Stochastic Gradient Descent(34683): loss=0.9745139630397387\n",
      "Stochastic Gradient Descent(34684): loss=16.920713803340426\n",
      "Stochastic Gradient Descent(34685): loss=2.128422622362647\n",
      "Stochastic Gradient Descent(34686): loss=3.3968886690613624\n",
      "Stochastic Gradient Descent(34687): loss=0.9503548376730431\n",
      "Stochastic Gradient Descent(34688): loss=1.668824563334561\n",
      "Stochastic Gradient Descent(34689): loss=3.964985371944409\n",
      "Stochastic Gradient Descent(34690): loss=16.73583506787773\n",
      "Stochastic Gradient Descent(34691): loss=11.234723871324167\n",
      "Stochastic Gradient Descent(34692): loss=7.227776004729167\n",
      "Stochastic Gradient Descent(34693): loss=3.5389204914338057\n",
      "Stochastic Gradient Descent(34694): loss=0.5496527041298627\n",
      "Stochastic Gradient Descent(34695): loss=1.150231910173427\n",
      "Stochastic Gradient Descent(34696): loss=5.254843720621448\n",
      "Stochastic Gradient Descent(34697): loss=0.30614778087638256\n",
      "Stochastic Gradient Descent(34698): loss=10.165722688564482\n",
      "Stochastic Gradient Descent(34699): loss=10.072472931074904\n",
      "Stochastic Gradient Descent(34700): loss=17.007832563853583\n",
      "Stochastic Gradient Descent(34701): loss=0.5768447256765784\n",
      "Stochastic Gradient Descent(34702): loss=4.920250274634155\n",
      "Stochastic Gradient Descent(34703): loss=6.856471888951987\n",
      "Stochastic Gradient Descent(34704): loss=0.8742063328081109\n",
      "Stochastic Gradient Descent(34705): loss=4.3732248363621\n",
      "Stochastic Gradient Descent(34706): loss=6.538824636071439\n",
      "Stochastic Gradient Descent(34707): loss=3.2663281628187293\n",
      "Stochastic Gradient Descent(34708): loss=3.324606160999517\n",
      "Stochastic Gradient Descent(34709): loss=0.4188836589703247\n",
      "Stochastic Gradient Descent(34710): loss=0.021735406202279987\n",
      "Stochastic Gradient Descent(34711): loss=3.304337665745369\n",
      "Stochastic Gradient Descent(34712): loss=8.085367975024958\n",
      "Stochastic Gradient Descent(34713): loss=0.002952280653522862\n",
      "Stochastic Gradient Descent(34714): loss=5.833538829122851\n",
      "Stochastic Gradient Descent(34715): loss=28.539230031440983\n",
      "Stochastic Gradient Descent(34716): loss=33.097715326923385\n",
      "Stochastic Gradient Descent(34717): loss=0.8805389912187541\n",
      "Stochastic Gradient Descent(34718): loss=3.1337571298043074\n",
      "Stochastic Gradient Descent(34719): loss=0.09127421135736605\n",
      "Stochastic Gradient Descent(34720): loss=0.10747653869256735\n",
      "Stochastic Gradient Descent(34721): loss=1.2741901117108845\n",
      "Stochastic Gradient Descent(34722): loss=5.951933161171917\n",
      "Stochastic Gradient Descent(34723): loss=4.908608376309942\n",
      "Stochastic Gradient Descent(34724): loss=1.1624436337086934\n",
      "Stochastic Gradient Descent(34725): loss=12.62645191239817\n",
      "Stochastic Gradient Descent(34726): loss=0.27594970059104895\n",
      "Stochastic Gradient Descent(34727): loss=13.94753520968515\n",
      "Stochastic Gradient Descent(34728): loss=0.8503841654639177\n",
      "Stochastic Gradient Descent(34729): loss=20.679661765119956\n",
      "Stochastic Gradient Descent(34730): loss=2.2915035137133235\n",
      "Stochastic Gradient Descent(34731): loss=2.3555119340215267\n",
      "Stochastic Gradient Descent(34732): loss=1.9070387996473592\n",
      "Stochastic Gradient Descent(34733): loss=0.7074115007487408\n",
      "Stochastic Gradient Descent(34734): loss=2.020895354243584\n",
      "Stochastic Gradient Descent(34735): loss=7.870811183572312\n",
      "Stochastic Gradient Descent(34736): loss=1.6616174753703028\n",
      "Stochastic Gradient Descent(34737): loss=4.5978427494419725\n",
      "Stochastic Gradient Descent(34738): loss=1.845588451210479\n",
      "Stochastic Gradient Descent(34739): loss=7.602359635269532\n",
      "Stochastic Gradient Descent(34740): loss=0.29374276284484785\n",
      "Stochastic Gradient Descent(34741): loss=1.12469886088282\n",
      "Stochastic Gradient Descent(34742): loss=0.5990330848188649\n",
      "Stochastic Gradient Descent(34743): loss=5.696539491713832\n",
      "Stochastic Gradient Descent(34744): loss=32.370668166805714\n",
      "Stochastic Gradient Descent(34745): loss=4.926183465415425\n",
      "Stochastic Gradient Descent(34746): loss=0.06902878339498929\n",
      "Stochastic Gradient Descent(34747): loss=0.23820217111348738\n",
      "Stochastic Gradient Descent(34748): loss=0.4379071058983939\n",
      "Stochastic Gradient Descent(34749): loss=12.229553833712819\n",
      "Stochastic Gradient Descent(34750): loss=0.028167475663592862\n",
      "Stochastic Gradient Descent(34751): loss=1.2226452685361662\n",
      "Stochastic Gradient Descent(34752): loss=1.7170171684082538\n",
      "Stochastic Gradient Descent(34753): loss=0.25668274288835186\n",
      "Stochastic Gradient Descent(34754): loss=7.485861452479241\n",
      "Stochastic Gradient Descent(34755): loss=0.18725308974982463\n",
      "Stochastic Gradient Descent(34756): loss=0.42927343372372895\n",
      "Stochastic Gradient Descent(34757): loss=1.4034881961775967\n",
      "Stochastic Gradient Descent(34758): loss=0.0513157969789143\n",
      "Stochastic Gradient Descent(34759): loss=0.18927396702812185\n",
      "Stochastic Gradient Descent(34760): loss=0.00011797605213552708\n",
      "Stochastic Gradient Descent(34761): loss=0.0014559275463365518\n",
      "Stochastic Gradient Descent(34762): loss=0.6508368803680747\n",
      "Stochastic Gradient Descent(34763): loss=5.211319787371203\n",
      "Stochastic Gradient Descent(34764): loss=1.8080818938504415\n",
      "Stochastic Gradient Descent(34765): loss=0.0016658854687892552\n",
      "Stochastic Gradient Descent(34766): loss=0.7830532442334343\n",
      "Stochastic Gradient Descent(34767): loss=0.40421877118478033\n",
      "Stochastic Gradient Descent(34768): loss=2.9178363863072354\n",
      "Stochastic Gradient Descent(34769): loss=1.2317883625208512\n",
      "Stochastic Gradient Descent(34770): loss=17.56663080286713\n",
      "Stochastic Gradient Descent(34771): loss=5.67084727176649\n",
      "Stochastic Gradient Descent(34772): loss=0.02361743804836397\n",
      "Stochastic Gradient Descent(34773): loss=0.32629671397737026\n",
      "Stochastic Gradient Descent(34774): loss=5.445075277306352\n",
      "Stochastic Gradient Descent(34775): loss=6.3283018873408805\n",
      "Stochastic Gradient Descent(34776): loss=4.298018887085402\n",
      "Stochastic Gradient Descent(34777): loss=0.38276184110075623\n",
      "Stochastic Gradient Descent(34778): loss=5.643864503512141\n",
      "Stochastic Gradient Descent(34779): loss=0.00022804814656752728\n",
      "Stochastic Gradient Descent(34780): loss=0.01111953556832979\n",
      "Stochastic Gradient Descent(34781): loss=0.09190516404580527\n",
      "Stochastic Gradient Descent(34782): loss=0.2744339013290345\n",
      "Stochastic Gradient Descent(34783): loss=3.3101646051143208\n",
      "Stochastic Gradient Descent(34784): loss=8.529777483430834\n",
      "Stochastic Gradient Descent(34785): loss=0.43281880085658003\n",
      "Stochastic Gradient Descent(34786): loss=22.740894534571122\n",
      "Stochastic Gradient Descent(34787): loss=0.0550276174452655\n",
      "Stochastic Gradient Descent(34788): loss=0.07160501461679811\n",
      "Stochastic Gradient Descent(34789): loss=0.8408342724478969\n",
      "Stochastic Gradient Descent(34790): loss=34.26626471507635\n",
      "Stochastic Gradient Descent(34791): loss=15.927655710217536\n",
      "Stochastic Gradient Descent(34792): loss=0.4176124977001712\n",
      "Stochastic Gradient Descent(34793): loss=1.301244800596751\n",
      "Stochastic Gradient Descent(34794): loss=14.978138978340102\n",
      "Stochastic Gradient Descent(34795): loss=56.41503995200071\n",
      "Stochastic Gradient Descent(34796): loss=3.5156239161108465\n",
      "Stochastic Gradient Descent(34797): loss=0.51834050006099\n",
      "Stochastic Gradient Descent(34798): loss=8.24449779696553\n",
      "Stochastic Gradient Descent(34799): loss=11.140620411470731\n",
      "Stochastic Gradient Descent(34800): loss=0.8070615793961816\n",
      "Stochastic Gradient Descent(34801): loss=6.984874897066774\n",
      "Stochastic Gradient Descent(34802): loss=0.0028985115052194894\n",
      "Stochastic Gradient Descent(34803): loss=5.6520068916560975\n",
      "Stochastic Gradient Descent(34804): loss=0.22563026148483373\n",
      "Stochastic Gradient Descent(34805): loss=0.6551831956789053\n",
      "Stochastic Gradient Descent(34806): loss=6.008592069801427\n",
      "Stochastic Gradient Descent(34807): loss=3.0450771085961246\n",
      "Stochastic Gradient Descent(34808): loss=1.4012633934739083\n",
      "Stochastic Gradient Descent(34809): loss=0.9321440518276263\n",
      "Stochastic Gradient Descent(34810): loss=1.3407025163164208\n",
      "Stochastic Gradient Descent(34811): loss=0.4515833387321208\n",
      "Stochastic Gradient Descent(34812): loss=8.067703128399488\n",
      "Stochastic Gradient Descent(34813): loss=0.03950626460673727\n",
      "Stochastic Gradient Descent(34814): loss=7.322990121016482\n",
      "Stochastic Gradient Descent(34815): loss=0.10897790542854498\n",
      "Stochastic Gradient Descent(34816): loss=1.1559710594691452\n",
      "Stochastic Gradient Descent(34817): loss=5.308878116725087\n",
      "Stochastic Gradient Descent(34818): loss=13.986232733068483\n",
      "Stochastic Gradient Descent(34819): loss=0.1259902923300631\n",
      "Stochastic Gradient Descent(34820): loss=0.3726109402577898\n",
      "Stochastic Gradient Descent(34821): loss=2.3514219947674744\n",
      "Stochastic Gradient Descent(34822): loss=0.3653477245145529\n",
      "Stochastic Gradient Descent(34823): loss=6.2301793766587785\n",
      "Stochastic Gradient Descent(34824): loss=0.26296379293078226\n",
      "Stochastic Gradient Descent(34825): loss=0.5953248450235095\n",
      "Stochastic Gradient Descent(34826): loss=7.202744449197334\n",
      "Stochastic Gradient Descent(34827): loss=18.144435793750503\n",
      "Stochastic Gradient Descent(34828): loss=12.984356913197011\n",
      "Stochastic Gradient Descent(34829): loss=7.102144454397546\n",
      "Stochastic Gradient Descent(34830): loss=0.15330326767846425\n",
      "Stochastic Gradient Descent(34831): loss=1.4233925288819484\n",
      "Stochastic Gradient Descent(34832): loss=1.5835993806811093\n",
      "Stochastic Gradient Descent(34833): loss=12.796602291258225\n",
      "Stochastic Gradient Descent(34834): loss=0.2580035029237292\n",
      "Stochastic Gradient Descent(34835): loss=0.040430669460105595\n",
      "Stochastic Gradient Descent(34836): loss=8.032622060303027\n",
      "Stochastic Gradient Descent(34837): loss=0.0906128019933258\n",
      "Stochastic Gradient Descent(34838): loss=4.86015086913196\n",
      "Stochastic Gradient Descent(34839): loss=0.0919145025270116\n",
      "Stochastic Gradient Descent(34840): loss=0.14884167216172162\n",
      "Stochastic Gradient Descent(34841): loss=1.4098440430994135\n",
      "Stochastic Gradient Descent(34842): loss=0.1806404010232809\n",
      "Stochastic Gradient Descent(34843): loss=2.175478388389967\n",
      "Stochastic Gradient Descent(34844): loss=1.368236313276252\n",
      "Stochastic Gradient Descent(34845): loss=0.19498289625404705\n",
      "Stochastic Gradient Descent(34846): loss=0.7148583056620251\n",
      "Stochastic Gradient Descent(34847): loss=9.480485902958838\n",
      "Stochastic Gradient Descent(34848): loss=0.04347204191783099\n",
      "Stochastic Gradient Descent(34849): loss=0.835564978361822\n",
      "Stochastic Gradient Descent(34850): loss=10.381971271477251\n",
      "Stochastic Gradient Descent(34851): loss=4.609192136321761\n",
      "Stochastic Gradient Descent(34852): loss=0.29869965168767504\n",
      "Stochastic Gradient Descent(34853): loss=2.1834266159638065\n",
      "Stochastic Gradient Descent(34854): loss=2.464010095514182\n",
      "Stochastic Gradient Descent(34855): loss=2.425901008118898\n",
      "Stochastic Gradient Descent(34856): loss=4.094935860081623\n",
      "Stochastic Gradient Descent(34857): loss=0.3015103840185427\n",
      "Stochastic Gradient Descent(34858): loss=7.415075961217192\n",
      "Stochastic Gradient Descent(34859): loss=0.8552989078233891\n",
      "Stochastic Gradient Descent(34860): loss=1.342432327167298\n",
      "Stochastic Gradient Descent(34861): loss=0.7129817276699827\n",
      "Stochastic Gradient Descent(34862): loss=0.7283395598316956\n",
      "Stochastic Gradient Descent(34863): loss=0.009332382879149814\n",
      "Stochastic Gradient Descent(34864): loss=8.189650351647204\n",
      "Stochastic Gradient Descent(34865): loss=4.319567457190176\n",
      "Stochastic Gradient Descent(34866): loss=2.3687348326674824\n",
      "Stochastic Gradient Descent(34867): loss=0.3093258149588953\n",
      "Stochastic Gradient Descent(34868): loss=5.627243071297046\n",
      "Stochastic Gradient Descent(34869): loss=6.097064201467451\n",
      "Stochastic Gradient Descent(34870): loss=1.618318545024195\n",
      "Stochastic Gradient Descent(34871): loss=4.645900163783138\n",
      "Stochastic Gradient Descent(34872): loss=0.139130664131689\n",
      "Stochastic Gradient Descent(34873): loss=2.7477712071730256\n",
      "Stochastic Gradient Descent(34874): loss=0.24241829085217695\n",
      "Stochastic Gradient Descent(34875): loss=1.5236718170139665\n",
      "Stochastic Gradient Descent(34876): loss=2.6049824751689066\n",
      "Stochastic Gradient Descent(34877): loss=13.071954858504695\n",
      "Stochastic Gradient Descent(34878): loss=5.228351082052549\n",
      "Stochastic Gradient Descent(34879): loss=7.198794260360047\n",
      "Stochastic Gradient Descent(34880): loss=0.24996000703754612\n",
      "Stochastic Gradient Descent(34881): loss=0.12333440056590732\n",
      "Stochastic Gradient Descent(34882): loss=10.825129562918427\n",
      "Stochastic Gradient Descent(34883): loss=2.1044379151183574\n",
      "Stochastic Gradient Descent(34884): loss=0.21963496346328565\n",
      "Stochastic Gradient Descent(34885): loss=1.1994037706193226\n",
      "Stochastic Gradient Descent(34886): loss=5.2312053740663975\n",
      "Stochastic Gradient Descent(34887): loss=0.4914647847145946\n",
      "Stochastic Gradient Descent(34888): loss=1.4691221529667509\n",
      "Stochastic Gradient Descent(34889): loss=10.620950090352412\n",
      "Stochastic Gradient Descent(34890): loss=2.0402521892465884\n",
      "Stochastic Gradient Descent(34891): loss=4.054506337045358\n",
      "Stochastic Gradient Descent(34892): loss=1.2781678681679414\n",
      "Stochastic Gradient Descent(34893): loss=3.5949910163645398\n",
      "Stochastic Gradient Descent(34894): loss=0.35850201957847916\n",
      "Stochastic Gradient Descent(34895): loss=0.03532971517252567\n",
      "Stochastic Gradient Descent(34896): loss=1.4286199930834513\n",
      "Stochastic Gradient Descent(34897): loss=9.662021756673697\n",
      "Stochastic Gradient Descent(34898): loss=0.1577547947576751\n",
      "Stochastic Gradient Descent(34899): loss=0.0388005447306083\n",
      "Stochastic Gradient Descent(34900): loss=1.3847957147564047\n",
      "Stochastic Gradient Descent(34901): loss=0.12333806132521156\n",
      "Stochastic Gradient Descent(34902): loss=3.5217500510679094\n",
      "Stochastic Gradient Descent(34903): loss=1.7910254173783071\n",
      "Stochastic Gradient Descent(34904): loss=0.8861181406919653\n",
      "Stochastic Gradient Descent(34905): loss=1.1347896744233177\n",
      "Stochastic Gradient Descent(34906): loss=0.011530289247827135\n",
      "Stochastic Gradient Descent(34907): loss=0.20305973433915195\n",
      "Stochastic Gradient Descent(34908): loss=5.153052486092509\n",
      "Stochastic Gradient Descent(34909): loss=12.808027413612459\n",
      "Stochastic Gradient Descent(34910): loss=0.0007065633523185447\n",
      "Stochastic Gradient Descent(34911): loss=0.8766396903706908\n",
      "Stochastic Gradient Descent(34912): loss=10.104624418324159\n",
      "Stochastic Gradient Descent(34913): loss=0.6392708353637239\n",
      "Stochastic Gradient Descent(34914): loss=8.580415740288904\n",
      "Stochastic Gradient Descent(34915): loss=0.12384828738007474\n",
      "Stochastic Gradient Descent(34916): loss=1.95465917007306\n",
      "Stochastic Gradient Descent(34917): loss=0.8283214856829493\n",
      "Stochastic Gradient Descent(34918): loss=0.5551003982795156\n",
      "Stochastic Gradient Descent(34919): loss=9.787723309178856\n",
      "Stochastic Gradient Descent(34920): loss=0.9676621709441835\n",
      "Stochastic Gradient Descent(34921): loss=3.897194307635559\n",
      "Stochastic Gradient Descent(34922): loss=1.115845198048912\n",
      "Stochastic Gradient Descent(34923): loss=14.141862712634872\n",
      "Stochastic Gradient Descent(34924): loss=10.897728644966746\n",
      "Stochastic Gradient Descent(34925): loss=0.107490383671919\n",
      "Stochastic Gradient Descent(34926): loss=10.414144007794228\n",
      "Stochastic Gradient Descent(34927): loss=3.4288197015856534\n",
      "Stochastic Gradient Descent(34928): loss=3.6973359925946725\n",
      "Stochastic Gradient Descent(34929): loss=3.5692807328244194\n",
      "Stochastic Gradient Descent(34930): loss=1.323471217606864\n",
      "Stochastic Gradient Descent(34931): loss=2.454009478701308\n",
      "Stochastic Gradient Descent(34932): loss=0.22994143343993492\n",
      "Stochastic Gradient Descent(34933): loss=1.2559996798959547\n",
      "Stochastic Gradient Descent(34934): loss=8.764423979341537\n",
      "Stochastic Gradient Descent(34935): loss=1.7154865874517382\n",
      "Stochastic Gradient Descent(34936): loss=0.24239597820102451\n",
      "Stochastic Gradient Descent(34937): loss=0.24246773335642163\n",
      "Stochastic Gradient Descent(34938): loss=16.5192624595438\n",
      "Stochastic Gradient Descent(34939): loss=0.8070586029564922\n",
      "Stochastic Gradient Descent(34940): loss=80.78459663958033\n",
      "Stochastic Gradient Descent(34941): loss=1.6333019995859683\n",
      "Stochastic Gradient Descent(34942): loss=3.5467313839390266\n",
      "Stochastic Gradient Descent(34943): loss=3.8527611707479514\n",
      "Stochastic Gradient Descent(34944): loss=4.829268034005847\n",
      "Stochastic Gradient Descent(34945): loss=9.335530466128581\n",
      "Stochastic Gradient Descent(34946): loss=0.3060521746416346\n",
      "Stochastic Gradient Descent(34947): loss=4.24481465080859e-06\n",
      "Stochastic Gradient Descent(34948): loss=4.137062906114498\n",
      "Stochastic Gradient Descent(34949): loss=0.0025464170331932295\n",
      "Stochastic Gradient Descent(34950): loss=3.4338977227887373\n",
      "Stochastic Gradient Descent(34951): loss=18.089780629947157\n",
      "Stochastic Gradient Descent(34952): loss=1.7915573533994973\n",
      "Stochastic Gradient Descent(34953): loss=0.3660066532314998\n",
      "Stochastic Gradient Descent(34954): loss=0.06724977302416923\n",
      "Stochastic Gradient Descent(34955): loss=14.526929437939385\n",
      "Stochastic Gradient Descent(34956): loss=8.213370397659714\n",
      "Stochastic Gradient Descent(34957): loss=8.086984867100611\n",
      "Stochastic Gradient Descent(34958): loss=1.3153367871296917\n",
      "Stochastic Gradient Descent(34959): loss=2.3268124457746726\n",
      "Stochastic Gradient Descent(34960): loss=3.0312660615385933\n",
      "Stochastic Gradient Descent(34961): loss=0.397350959679437\n",
      "Stochastic Gradient Descent(34962): loss=11.082777900376547\n",
      "Stochastic Gradient Descent(34963): loss=0.05473571795312104\n",
      "Stochastic Gradient Descent(34964): loss=0.43345131023863676\n",
      "Stochastic Gradient Descent(34965): loss=1.7326749039682556\n",
      "Stochastic Gradient Descent(34966): loss=0.2111757081867946\n",
      "Stochastic Gradient Descent(34967): loss=1.028577449245005\n",
      "Stochastic Gradient Descent(34968): loss=24.96420666913227\n",
      "Stochastic Gradient Descent(34969): loss=1.8346190694630322\n",
      "Stochastic Gradient Descent(34970): loss=0.6346058177712411\n",
      "Stochastic Gradient Descent(34971): loss=0.6149300391898932\n",
      "Stochastic Gradient Descent(34972): loss=8.09644088303327\n",
      "Stochastic Gradient Descent(34973): loss=8.768005761983664\n",
      "Stochastic Gradient Descent(34974): loss=5.314796245497153\n",
      "Stochastic Gradient Descent(34975): loss=17.906756633895444\n",
      "Stochastic Gradient Descent(34976): loss=0.0782324374575458\n",
      "Stochastic Gradient Descent(34977): loss=3.336558982837465\n",
      "Stochastic Gradient Descent(34978): loss=1.6933263294195562\n",
      "Stochastic Gradient Descent(34979): loss=3.14943109027048\n",
      "Stochastic Gradient Descent(34980): loss=11.94852069177037\n",
      "Stochastic Gradient Descent(34981): loss=2.45347203449016\n",
      "Stochastic Gradient Descent(34982): loss=1.8229887607188382\n",
      "Stochastic Gradient Descent(34983): loss=0.07171984343290255\n",
      "Stochastic Gradient Descent(34984): loss=0.1000784188811238\n",
      "Stochastic Gradient Descent(34985): loss=6.863340023238515\n",
      "Stochastic Gradient Descent(34986): loss=0.32076561760808026\n",
      "Stochastic Gradient Descent(34987): loss=4.395659504401986\n",
      "Stochastic Gradient Descent(34988): loss=5.005650620574766\n",
      "Stochastic Gradient Descent(34989): loss=0.5630778694204333\n",
      "Stochastic Gradient Descent(34990): loss=0.5656794659600447\n",
      "Stochastic Gradient Descent(34991): loss=0.7809303022507691\n",
      "Stochastic Gradient Descent(34992): loss=4.3489238518455515\n",
      "Stochastic Gradient Descent(34993): loss=1.864535307037882\n",
      "Stochastic Gradient Descent(34994): loss=6.982407214343224\n",
      "Stochastic Gradient Descent(34995): loss=6.405162859785861\n",
      "Stochastic Gradient Descent(34996): loss=0.6995655350460446\n",
      "Stochastic Gradient Descent(34997): loss=0.3316261980727254\n",
      "Stochastic Gradient Descent(34998): loss=0.8421547166830785\n",
      "Stochastic Gradient Descent(34999): loss=1.5842071255570502\n",
      "Stochastic Gradient Descent(35000): loss=19.48351207027172\n",
      "Stochastic Gradient Descent(35001): loss=14.194659937941461\n",
      "Stochastic Gradient Descent(35002): loss=0.2993809919523034\n",
      "Stochastic Gradient Descent(35003): loss=1.4151750161294612\n",
      "Stochastic Gradient Descent(35004): loss=1.098306228838252\n",
      "Stochastic Gradient Descent(35005): loss=3.3987015649951644\n",
      "Stochastic Gradient Descent(35006): loss=5.7910839651679735\n",
      "Stochastic Gradient Descent(35007): loss=4.4746409148353905\n",
      "Stochastic Gradient Descent(35008): loss=1.2449490875406375\n",
      "Stochastic Gradient Descent(35009): loss=0.39139910305066516\n",
      "Stochastic Gradient Descent(35010): loss=21.117265240204024\n",
      "Stochastic Gradient Descent(35011): loss=0.3863522817745362\n",
      "Stochastic Gradient Descent(35012): loss=2.9078199983717434\n",
      "Stochastic Gradient Descent(35013): loss=0.20781919487492995\n",
      "Stochastic Gradient Descent(35014): loss=1.6601886857692736\n",
      "Stochastic Gradient Descent(35015): loss=45.19284355090818\n",
      "Stochastic Gradient Descent(35016): loss=1.578906588586254\n",
      "Stochastic Gradient Descent(35017): loss=1.3591783293602726\n",
      "Stochastic Gradient Descent(35018): loss=3.4193569900910923\n",
      "Stochastic Gradient Descent(35019): loss=0.2959114116196716\n",
      "Stochastic Gradient Descent(35020): loss=14.487867339882639\n",
      "Stochastic Gradient Descent(35021): loss=0.6795367367960341\n",
      "Stochastic Gradient Descent(35022): loss=1.6587212555589685\n",
      "Stochastic Gradient Descent(35023): loss=3.04463116080449\n",
      "Stochastic Gradient Descent(35024): loss=3.3845529966067525\n",
      "Stochastic Gradient Descent(35025): loss=0.015116565242554595\n",
      "Stochastic Gradient Descent(35026): loss=0.472455743223415\n",
      "Stochastic Gradient Descent(35027): loss=0.5392327735938275\n",
      "Stochastic Gradient Descent(35028): loss=1.6796164326940493\n",
      "Stochastic Gradient Descent(35029): loss=1.8964245200917391\n",
      "Stochastic Gradient Descent(35030): loss=8.02215051651591\n",
      "Stochastic Gradient Descent(35031): loss=1.3656403721686738\n",
      "Stochastic Gradient Descent(35032): loss=0.10362098858065026\n",
      "Stochastic Gradient Descent(35033): loss=2.0980475741948594\n",
      "Stochastic Gradient Descent(35034): loss=0.4053112488848976\n",
      "Stochastic Gradient Descent(35035): loss=1.2034195844993811\n",
      "Stochastic Gradient Descent(35036): loss=4.800711385249386\n",
      "Stochastic Gradient Descent(35037): loss=11.437572856875207\n",
      "Stochastic Gradient Descent(35038): loss=1.5323188973029773\n",
      "Stochastic Gradient Descent(35039): loss=23.258114663186856\n",
      "Stochastic Gradient Descent(35040): loss=0.05220561867364894\n",
      "Stochastic Gradient Descent(35041): loss=3.5318438229395683\n",
      "Stochastic Gradient Descent(35042): loss=1.8859982176185803\n",
      "Stochastic Gradient Descent(35043): loss=0.0004579044362343299\n",
      "Stochastic Gradient Descent(35044): loss=0.023097703109552753\n",
      "Stochastic Gradient Descent(35045): loss=0.29794128082449156\n",
      "Stochastic Gradient Descent(35046): loss=2.657287344302142\n",
      "Stochastic Gradient Descent(35047): loss=11.75232517004613\n",
      "Stochastic Gradient Descent(35048): loss=2.582255644374664\n",
      "Stochastic Gradient Descent(35049): loss=9.867060658394458\n",
      "Stochastic Gradient Descent(35050): loss=2.890831236374158\n",
      "Stochastic Gradient Descent(35051): loss=2.738628727312121\n",
      "Stochastic Gradient Descent(35052): loss=11.615166134259553\n",
      "Stochastic Gradient Descent(35053): loss=8.10264834501314\n",
      "Stochastic Gradient Descent(35054): loss=0.7834475709990868\n",
      "Stochastic Gradient Descent(35055): loss=0.4506433824628572\n",
      "Stochastic Gradient Descent(35056): loss=12.731021039056191\n",
      "Stochastic Gradient Descent(35057): loss=0.40982970611142944\n",
      "Stochastic Gradient Descent(35058): loss=3.604512106654087\n",
      "Stochastic Gradient Descent(35059): loss=2.1976441603321097\n",
      "Stochastic Gradient Descent(35060): loss=0.4442244368912563\n",
      "Stochastic Gradient Descent(35061): loss=5.076544327719178\n",
      "Stochastic Gradient Descent(35062): loss=0.007884751561643996\n",
      "Stochastic Gradient Descent(35063): loss=4.485489077824917\n",
      "Stochastic Gradient Descent(35064): loss=24.46932924774972\n",
      "Stochastic Gradient Descent(35065): loss=4.582721285087217\n",
      "Stochastic Gradient Descent(35066): loss=0.3145956412809802\n",
      "Stochastic Gradient Descent(35067): loss=0.03572486540357448\n",
      "Stochastic Gradient Descent(35068): loss=0.9527356593436976\n",
      "Stochastic Gradient Descent(35069): loss=7.249358507740932\n",
      "Stochastic Gradient Descent(35070): loss=1.2781411651035655\n",
      "Stochastic Gradient Descent(35071): loss=3.5300461123192224\n",
      "Stochastic Gradient Descent(35072): loss=1.8669387761591\n",
      "Stochastic Gradient Descent(35073): loss=11.305483880879963\n",
      "Stochastic Gradient Descent(35074): loss=7.2074874647290805\n",
      "Stochastic Gradient Descent(35075): loss=8.293679714499035\n",
      "Stochastic Gradient Descent(35076): loss=19.4144998498853\n",
      "Stochastic Gradient Descent(35077): loss=2.8966783134817637\n",
      "Stochastic Gradient Descent(35078): loss=9.1857938033614\n",
      "Stochastic Gradient Descent(35079): loss=4.247286767334121\n",
      "Stochastic Gradient Descent(35080): loss=5.50986092106691\n",
      "Stochastic Gradient Descent(35081): loss=14.231224071958295\n",
      "Stochastic Gradient Descent(35082): loss=2.697107370285205\n",
      "Stochastic Gradient Descent(35083): loss=0.9032210162781402\n",
      "Stochastic Gradient Descent(35084): loss=0.3495626558428983\n",
      "Stochastic Gradient Descent(35085): loss=0.7901138265503711\n",
      "Stochastic Gradient Descent(35086): loss=0.9570965850661032\n",
      "Stochastic Gradient Descent(35087): loss=1.0951963622280012\n",
      "Stochastic Gradient Descent(35088): loss=5.5704093395149075\n",
      "Stochastic Gradient Descent(35089): loss=0.6266900979784655\n",
      "Stochastic Gradient Descent(35090): loss=0.44597864745240057\n",
      "Stochastic Gradient Descent(35091): loss=4.996012392229541\n",
      "Stochastic Gradient Descent(35092): loss=0.0011100282768962786\n",
      "Stochastic Gradient Descent(35093): loss=2.271840525010875\n",
      "Stochastic Gradient Descent(35094): loss=0.01318228664543927\n",
      "Stochastic Gradient Descent(35095): loss=8.431332811055524\n",
      "Stochastic Gradient Descent(35096): loss=3.9401393936905915\n",
      "Stochastic Gradient Descent(35097): loss=0.992726306395822\n",
      "Stochastic Gradient Descent(35098): loss=0.0054281336456005495\n",
      "Stochastic Gradient Descent(35099): loss=1.0900188296303246\n",
      "Stochastic Gradient Descent(35100): loss=0.31067843288816266\n",
      "Stochastic Gradient Descent(35101): loss=4.855287630339072\n",
      "Stochastic Gradient Descent(35102): loss=0.010215933860509016\n",
      "Stochastic Gradient Descent(35103): loss=0.023773402133191214\n",
      "Stochastic Gradient Descent(35104): loss=0.6632000385782613\n",
      "Stochastic Gradient Descent(35105): loss=0.9081948584233325\n",
      "Stochastic Gradient Descent(35106): loss=0.9468489906119351\n",
      "Stochastic Gradient Descent(35107): loss=0.5567048101184918\n",
      "Stochastic Gradient Descent(35108): loss=7.635779914959399\n",
      "Stochastic Gradient Descent(35109): loss=5.747042143451205\n",
      "Stochastic Gradient Descent(35110): loss=7.246285244899848\n",
      "Stochastic Gradient Descent(35111): loss=2.0918686684040275\n",
      "Stochastic Gradient Descent(35112): loss=6.642685156627024\n",
      "Stochastic Gradient Descent(35113): loss=2.537406491742608\n",
      "Stochastic Gradient Descent(35114): loss=1.7287087741865825\n",
      "Stochastic Gradient Descent(35115): loss=1.5215361101085079\n",
      "Stochastic Gradient Descent(35116): loss=0.8401372093671428\n",
      "Stochastic Gradient Descent(35117): loss=0.4952921005138462\n",
      "Stochastic Gradient Descent(35118): loss=4.237951513754365\n",
      "Stochastic Gradient Descent(35119): loss=1.2712958771838099\n",
      "Stochastic Gradient Descent(35120): loss=0.9646978479735883\n",
      "Stochastic Gradient Descent(35121): loss=38.94231706676985\n",
      "Stochastic Gradient Descent(35122): loss=6.288028269752524\n",
      "Stochastic Gradient Descent(35123): loss=1.3652251687144827\n",
      "Stochastic Gradient Descent(35124): loss=6.582954506176239\n",
      "Stochastic Gradient Descent(35125): loss=0.005502984932245437\n",
      "Stochastic Gradient Descent(35126): loss=0.05615762004626933\n",
      "Stochastic Gradient Descent(35127): loss=2.835063621310761\n",
      "Stochastic Gradient Descent(35128): loss=1.7132862457466287\n",
      "Stochastic Gradient Descent(35129): loss=1.6003191895917197\n",
      "Stochastic Gradient Descent(35130): loss=0.3366386921289743\n",
      "Stochastic Gradient Descent(35131): loss=0.7040347535493955\n",
      "Stochastic Gradient Descent(35132): loss=1.7273849357044144\n",
      "Stochastic Gradient Descent(35133): loss=0.5159819940942464\n",
      "Stochastic Gradient Descent(35134): loss=0.09596196132677047\n",
      "Stochastic Gradient Descent(35135): loss=3.4597750582643747\n",
      "Stochastic Gradient Descent(35136): loss=4.219573232448257\n",
      "Stochastic Gradient Descent(35137): loss=4.641499156063538\n",
      "Stochastic Gradient Descent(35138): loss=0.37942315356856154\n",
      "Stochastic Gradient Descent(35139): loss=0.00017950128125480966\n",
      "Stochastic Gradient Descent(35140): loss=0.8783720907182949\n",
      "Stochastic Gradient Descent(35141): loss=0.7746317116286551\n",
      "Stochastic Gradient Descent(35142): loss=1.362463803828515\n",
      "Stochastic Gradient Descent(35143): loss=3.8649597729288807\n",
      "Stochastic Gradient Descent(35144): loss=3.595865869671672\n",
      "Stochastic Gradient Descent(35145): loss=0.25654576320045636\n",
      "Stochastic Gradient Descent(35146): loss=1.025828828301552\n",
      "Stochastic Gradient Descent(35147): loss=0.35071010324199114\n",
      "Stochastic Gradient Descent(35148): loss=3.3086667220426857\n",
      "Stochastic Gradient Descent(35149): loss=0.4133138116317341\n",
      "Stochastic Gradient Descent(35150): loss=11.436387974064916\n",
      "Stochastic Gradient Descent(35151): loss=2.167239074962777\n",
      "Stochastic Gradient Descent(35152): loss=0.01193282952484062\n",
      "Stochastic Gradient Descent(35153): loss=6.6124311342527395\n",
      "Stochastic Gradient Descent(35154): loss=0.3378603813884661\n",
      "Stochastic Gradient Descent(35155): loss=9.305911977061282\n",
      "Stochastic Gradient Descent(35156): loss=1.635564455864503\n",
      "Stochastic Gradient Descent(35157): loss=6.062655117380573\n",
      "Stochastic Gradient Descent(35158): loss=0.8343849077203054\n",
      "Stochastic Gradient Descent(35159): loss=4.436011385907921\n",
      "Stochastic Gradient Descent(35160): loss=5.455065549700926\n",
      "Stochastic Gradient Descent(35161): loss=0.9093581236093043\n",
      "Stochastic Gradient Descent(35162): loss=1.9352308368671403\n",
      "Stochastic Gradient Descent(35163): loss=0.005438541310054807\n",
      "Stochastic Gradient Descent(35164): loss=1.5152602971630496\n",
      "Stochastic Gradient Descent(35165): loss=7.752807072055938\n",
      "Stochastic Gradient Descent(35166): loss=13.49093581529744\n",
      "Stochastic Gradient Descent(35167): loss=0.5254092145882756\n",
      "Stochastic Gradient Descent(35168): loss=0.4078742840513548\n",
      "Stochastic Gradient Descent(35169): loss=0.000994881138715167\n",
      "Stochastic Gradient Descent(35170): loss=2.2426777266631452\n",
      "Stochastic Gradient Descent(35171): loss=3.9963304563003876\n",
      "Stochastic Gradient Descent(35172): loss=1.3206899430624088\n",
      "Stochastic Gradient Descent(35173): loss=2.529563701093235\n",
      "Stochastic Gradient Descent(35174): loss=2.694839601565391\n",
      "Stochastic Gradient Descent(35175): loss=0.3593106805741571\n",
      "Stochastic Gradient Descent(35176): loss=0.7086538844764723\n",
      "Stochastic Gradient Descent(35177): loss=1.689982942017395\n",
      "Stochastic Gradient Descent(35178): loss=2.729414422544062\n",
      "Stochastic Gradient Descent(35179): loss=34.705199483375104\n",
      "Stochastic Gradient Descent(35180): loss=11.27889494078826\n",
      "Stochastic Gradient Descent(35181): loss=20.65580064800406\n",
      "Stochastic Gradient Descent(35182): loss=5.095864462589568\n",
      "Stochastic Gradient Descent(35183): loss=0.0572839204983607\n",
      "Stochastic Gradient Descent(35184): loss=1.4171156212639484\n",
      "Stochastic Gradient Descent(35185): loss=0.3852445861041906\n",
      "Stochastic Gradient Descent(35186): loss=0.00227553608725837\n",
      "Stochastic Gradient Descent(35187): loss=4.68604184356196\n",
      "Stochastic Gradient Descent(35188): loss=0.300445237062437\n",
      "Stochastic Gradient Descent(35189): loss=23.832327343963787\n",
      "Stochastic Gradient Descent(35190): loss=7.914033085868948\n",
      "Stochastic Gradient Descent(35191): loss=17.500219723183758\n",
      "Stochastic Gradient Descent(35192): loss=2.293569657443251\n",
      "Stochastic Gradient Descent(35193): loss=0.7393327683838684\n",
      "Stochastic Gradient Descent(35194): loss=0.3072503865793565\n",
      "Stochastic Gradient Descent(35195): loss=8.420517947323912\n",
      "Stochastic Gradient Descent(35196): loss=0.22437736579390205\n",
      "Stochastic Gradient Descent(35197): loss=0.48504244476443303\n",
      "Stochastic Gradient Descent(35198): loss=2.274467419466525\n",
      "Stochastic Gradient Descent(35199): loss=10.05751553479218\n",
      "Stochastic Gradient Descent(35200): loss=14.031588147364712\n",
      "Stochastic Gradient Descent(35201): loss=23.70729668035108\n",
      "Stochastic Gradient Descent(35202): loss=0.001322215920723794\n",
      "Stochastic Gradient Descent(35203): loss=0.2447313719159755\n",
      "Stochastic Gradient Descent(35204): loss=0.001554261136607214\n",
      "Stochastic Gradient Descent(35205): loss=6.346785331577575\n",
      "Stochastic Gradient Descent(35206): loss=3.073065747290308\n",
      "Stochastic Gradient Descent(35207): loss=9.122500845867105\n",
      "Stochastic Gradient Descent(35208): loss=3.5458321490303413\n",
      "Stochastic Gradient Descent(35209): loss=3.5839457478038392\n",
      "Stochastic Gradient Descent(35210): loss=1.9353462878232073\n",
      "Stochastic Gradient Descent(35211): loss=1.483801039796811\n",
      "Stochastic Gradient Descent(35212): loss=0.20894008422578159\n",
      "Stochastic Gradient Descent(35213): loss=0.2943217599295549\n",
      "Stochastic Gradient Descent(35214): loss=5.590648530399766\n",
      "Stochastic Gradient Descent(35215): loss=11.622507774536286\n",
      "Stochastic Gradient Descent(35216): loss=10.601770735322328\n",
      "Stochastic Gradient Descent(35217): loss=0.28743672276691273\n",
      "Stochastic Gradient Descent(35218): loss=10.693589663899942\n",
      "Stochastic Gradient Descent(35219): loss=10.264875441539703\n",
      "Stochastic Gradient Descent(35220): loss=21.52563442878937\n",
      "Stochastic Gradient Descent(35221): loss=0.12170010686247616\n",
      "Stochastic Gradient Descent(35222): loss=0.6383799283992023\n",
      "Stochastic Gradient Descent(35223): loss=4.486264496785213\n",
      "Stochastic Gradient Descent(35224): loss=4.647895365143997\n",
      "Stochastic Gradient Descent(35225): loss=9.428916454425908\n",
      "Stochastic Gradient Descent(35226): loss=6.022369302816995\n",
      "Stochastic Gradient Descent(35227): loss=9.712698380338125\n",
      "Stochastic Gradient Descent(35228): loss=0.15337377401747385\n",
      "Stochastic Gradient Descent(35229): loss=1.609023812336629\n",
      "Stochastic Gradient Descent(35230): loss=1.535670904748961\n",
      "Stochastic Gradient Descent(35231): loss=0.05587034113169143\n",
      "Stochastic Gradient Descent(35232): loss=0.3635400275724163\n",
      "Stochastic Gradient Descent(35233): loss=0.044207019488441975\n",
      "Stochastic Gradient Descent(35234): loss=2.3680476982215097\n",
      "Stochastic Gradient Descent(35235): loss=0.17040041835445685\n",
      "Stochastic Gradient Descent(35236): loss=1.461667211061411\n",
      "Stochastic Gradient Descent(35237): loss=1.1682600141090438\n",
      "Stochastic Gradient Descent(35238): loss=6.1436404615987135\n",
      "Stochastic Gradient Descent(35239): loss=3.9781842347936545\n",
      "Stochastic Gradient Descent(35240): loss=3.05194807363984\n",
      "Stochastic Gradient Descent(35241): loss=0.18122675423883453\n",
      "Stochastic Gradient Descent(35242): loss=11.206721264249383\n",
      "Stochastic Gradient Descent(35243): loss=32.2165012382604\n",
      "Stochastic Gradient Descent(35244): loss=2.389535252341461\n",
      "Stochastic Gradient Descent(35245): loss=6.775208466144281\n",
      "Stochastic Gradient Descent(35246): loss=2.1783407651872615\n",
      "Stochastic Gradient Descent(35247): loss=1.1529193354922174\n",
      "Stochastic Gradient Descent(35248): loss=0.07718083587337722\n",
      "Stochastic Gradient Descent(35249): loss=11.367119244265135\n",
      "Stochastic Gradient Descent(35250): loss=14.420414984100843\n",
      "Stochastic Gradient Descent(35251): loss=0.17161231819217568\n",
      "Stochastic Gradient Descent(35252): loss=14.472939994612586\n",
      "Stochastic Gradient Descent(35253): loss=20.152132631964598\n",
      "Stochastic Gradient Descent(35254): loss=0.11190507743096482\n",
      "Stochastic Gradient Descent(35255): loss=0.00972089856697661\n",
      "Stochastic Gradient Descent(35256): loss=6.809693460108058\n",
      "Stochastic Gradient Descent(35257): loss=0.00039726400243713067\n",
      "Stochastic Gradient Descent(35258): loss=0.9767201833467473\n",
      "Stochastic Gradient Descent(35259): loss=1.652162033614187\n",
      "Stochastic Gradient Descent(35260): loss=0.28917296059813474\n",
      "Stochastic Gradient Descent(35261): loss=3.727950516710064\n",
      "Stochastic Gradient Descent(35262): loss=6.430040019446567\n",
      "Stochastic Gradient Descent(35263): loss=0.38397489458225276\n",
      "Stochastic Gradient Descent(35264): loss=1.440948577483768\n",
      "Stochastic Gradient Descent(35265): loss=3.173248972597609\n",
      "Stochastic Gradient Descent(35266): loss=6.164713085066499\n",
      "Stochastic Gradient Descent(35267): loss=3.860103084451984\n",
      "Stochastic Gradient Descent(35268): loss=0.000278188062568088\n",
      "Stochastic Gradient Descent(35269): loss=10.121325898848795\n",
      "Stochastic Gradient Descent(35270): loss=3.0612076113080624\n",
      "Stochastic Gradient Descent(35271): loss=0.6552394197566763\n",
      "Stochastic Gradient Descent(35272): loss=0.7183169441216751\n",
      "Stochastic Gradient Descent(35273): loss=1.3865122541878743\n",
      "Stochastic Gradient Descent(35274): loss=9.48989811760593\n",
      "Stochastic Gradient Descent(35275): loss=5.478031013592875\n",
      "Stochastic Gradient Descent(35276): loss=1.7810190996981927\n",
      "Stochastic Gradient Descent(35277): loss=2.4214166657634495\n",
      "Stochastic Gradient Descent(35278): loss=0.572358400846513\n",
      "Stochastic Gradient Descent(35279): loss=4.617004443904905\n",
      "Stochastic Gradient Descent(35280): loss=12.79849018014112\n",
      "Stochastic Gradient Descent(35281): loss=1.1166212085413705\n",
      "Stochastic Gradient Descent(35282): loss=6.796640200115149\n",
      "Stochastic Gradient Descent(35283): loss=0.16887294283945642\n",
      "Stochastic Gradient Descent(35284): loss=4.14069685643832\n",
      "Stochastic Gradient Descent(35285): loss=4.722232038587633\n",
      "Stochastic Gradient Descent(35286): loss=0.3604746693365028\n",
      "Stochastic Gradient Descent(35287): loss=2.454954209553719\n",
      "Stochastic Gradient Descent(35288): loss=11.513578987275412\n",
      "Stochastic Gradient Descent(35289): loss=1.6899620852239021\n",
      "Stochastic Gradient Descent(35290): loss=4.589988798925028\n",
      "Stochastic Gradient Descent(35291): loss=1.453688069340749\n",
      "Stochastic Gradient Descent(35292): loss=1.7094282750870378\n",
      "Stochastic Gradient Descent(35293): loss=5.73991023772519\n",
      "Stochastic Gradient Descent(35294): loss=3.000185054450422\n",
      "Stochastic Gradient Descent(35295): loss=1.5870530953812911\n",
      "Stochastic Gradient Descent(35296): loss=3.183725966710819\n",
      "Stochastic Gradient Descent(35297): loss=7.125504169740027\n",
      "Stochastic Gradient Descent(35298): loss=1.9754978522913573\n",
      "Stochastic Gradient Descent(35299): loss=1.7957207650381795\n",
      "Stochastic Gradient Descent(35300): loss=1.3809954541821539\n",
      "Stochastic Gradient Descent(35301): loss=3.3048870553690866\n",
      "Stochastic Gradient Descent(35302): loss=0.14268272416834585\n",
      "Stochastic Gradient Descent(35303): loss=0.33355405354218437\n",
      "Stochastic Gradient Descent(35304): loss=0.2949091442273568\n",
      "Stochastic Gradient Descent(35305): loss=1.404575899776562\n",
      "Stochastic Gradient Descent(35306): loss=10.781105602254526\n",
      "Stochastic Gradient Descent(35307): loss=0.40190186056019095\n",
      "Stochastic Gradient Descent(35308): loss=4.821771132536768\n",
      "Stochastic Gradient Descent(35309): loss=4.173393655735379\n",
      "Stochastic Gradient Descent(35310): loss=1.0446179131566542\n",
      "Stochastic Gradient Descent(35311): loss=4.683297813217698\n",
      "Stochastic Gradient Descent(35312): loss=0.5187332111391382\n",
      "Stochastic Gradient Descent(35313): loss=0.7381531524383266\n",
      "Stochastic Gradient Descent(35314): loss=0.29760922476494767\n",
      "Stochastic Gradient Descent(35315): loss=5.60633856241686\n",
      "Stochastic Gradient Descent(35316): loss=0.007604777663154069\n",
      "Stochastic Gradient Descent(35317): loss=0.19845677667409822\n",
      "Stochastic Gradient Descent(35318): loss=2.018897846442664\n",
      "Stochastic Gradient Descent(35319): loss=0.6915309656747948\n",
      "Stochastic Gradient Descent(35320): loss=0.05714771221846294\n",
      "Stochastic Gradient Descent(35321): loss=0.003899650261569706\n",
      "Stochastic Gradient Descent(35322): loss=0.21174399632787888\n",
      "Stochastic Gradient Descent(35323): loss=1.857119344994591\n",
      "Stochastic Gradient Descent(35324): loss=3.0014591406587465\n",
      "Stochastic Gradient Descent(35325): loss=0.10261815884367734\n",
      "Stochastic Gradient Descent(35326): loss=0.0009301053633189206\n",
      "Stochastic Gradient Descent(35327): loss=1.3606228835873346\n",
      "Stochastic Gradient Descent(35328): loss=5.851312294963746\n",
      "Stochastic Gradient Descent(35329): loss=0.20629761823900744\n",
      "Stochastic Gradient Descent(35330): loss=0.22754256891681424\n",
      "Stochastic Gradient Descent(35331): loss=5.275015970712117\n",
      "Stochastic Gradient Descent(35332): loss=0.08125553110998825\n",
      "Stochastic Gradient Descent(35333): loss=16.474066872377854\n",
      "Stochastic Gradient Descent(35334): loss=2.5597658398962286\n",
      "Stochastic Gradient Descent(35335): loss=0.22798087784379192\n",
      "Stochastic Gradient Descent(35336): loss=0.733114529219249\n",
      "Stochastic Gradient Descent(35337): loss=4.375384233628671\n",
      "Stochastic Gradient Descent(35338): loss=0.9451654420649173\n",
      "Stochastic Gradient Descent(35339): loss=7.55973420632415\n",
      "Stochastic Gradient Descent(35340): loss=5.840230423711368\n",
      "Stochastic Gradient Descent(35341): loss=2.480719751389671\n",
      "Stochastic Gradient Descent(35342): loss=0.23284398138052967\n",
      "Stochastic Gradient Descent(35343): loss=1.518325960455805\n",
      "Stochastic Gradient Descent(35344): loss=0.5726024033369045\n",
      "Stochastic Gradient Descent(35345): loss=0.01157193910393583\n",
      "Stochastic Gradient Descent(35346): loss=1.4368503721781813\n",
      "Stochastic Gradient Descent(35347): loss=1.8109432374342052\n",
      "Stochastic Gradient Descent(35348): loss=13.244518235820495\n",
      "Stochastic Gradient Descent(35349): loss=0.16901080980195093\n",
      "Stochastic Gradient Descent(35350): loss=14.47085689129983\n",
      "Stochastic Gradient Descent(35351): loss=1.117394984273567\n",
      "Stochastic Gradient Descent(35352): loss=2.654500684992553\n",
      "Stochastic Gradient Descent(35353): loss=2.5282054320952576\n",
      "Stochastic Gradient Descent(35354): loss=0.6264611126653293\n",
      "Stochastic Gradient Descent(35355): loss=1.1690079055966327\n",
      "Stochastic Gradient Descent(35356): loss=0.019289497759305173\n",
      "Stochastic Gradient Descent(35357): loss=2.986448729750111\n",
      "Stochastic Gradient Descent(35358): loss=4.3286230625366295\n",
      "Stochastic Gradient Descent(35359): loss=0.0057335869450603275\n",
      "Stochastic Gradient Descent(35360): loss=1.2762527409707227\n",
      "Stochastic Gradient Descent(35361): loss=0.6459219661861751\n",
      "Stochastic Gradient Descent(35362): loss=0.0007267884494076851\n",
      "Stochastic Gradient Descent(35363): loss=10.108521790390835\n",
      "Stochastic Gradient Descent(35364): loss=0.04783894740210306\n",
      "Stochastic Gradient Descent(35365): loss=2.0519250806154434\n",
      "Stochastic Gradient Descent(35366): loss=4.4701529290343185\n",
      "Stochastic Gradient Descent(35367): loss=0.01110390929685279\n",
      "Stochastic Gradient Descent(35368): loss=3.2962243315576853\n",
      "Stochastic Gradient Descent(35369): loss=4.580104249177885\n",
      "Stochastic Gradient Descent(35370): loss=1.8879393682125793\n",
      "Stochastic Gradient Descent(35371): loss=2.7988192034104316\n",
      "Stochastic Gradient Descent(35372): loss=0.0006905549859856843\n",
      "Stochastic Gradient Descent(35373): loss=3.075863815758013\n",
      "Stochastic Gradient Descent(35374): loss=4.571388670881392\n",
      "Stochastic Gradient Descent(35375): loss=0.28357746481142554\n",
      "Stochastic Gradient Descent(35376): loss=20.382083972948752\n",
      "Stochastic Gradient Descent(35377): loss=0.7632070266022062\n",
      "Stochastic Gradient Descent(35378): loss=3.9619958328127076\n",
      "Stochastic Gradient Descent(35379): loss=0.31728904645283085\n",
      "Stochastic Gradient Descent(35380): loss=0.43516031538416117\n",
      "Stochastic Gradient Descent(35381): loss=1.2338320828096498\n",
      "Stochastic Gradient Descent(35382): loss=44.4826553289303\n",
      "Stochastic Gradient Descent(35383): loss=1.1841608948293245\n",
      "Stochastic Gradient Descent(35384): loss=23.7593621950615\n",
      "Stochastic Gradient Descent(35385): loss=10.250193496968567\n",
      "Stochastic Gradient Descent(35386): loss=1.6062181317013222\n",
      "Stochastic Gradient Descent(35387): loss=7.59360743173289\n",
      "Stochastic Gradient Descent(35388): loss=2.7993721154449243\n",
      "Stochastic Gradient Descent(35389): loss=17.5512983664154\n",
      "Stochastic Gradient Descent(35390): loss=5.265456034784999\n",
      "Stochastic Gradient Descent(35391): loss=13.867100080465216\n",
      "Stochastic Gradient Descent(35392): loss=1.104522578164684\n",
      "Stochastic Gradient Descent(35393): loss=74.30060759633065\n",
      "Stochastic Gradient Descent(35394): loss=2.7515070804024093\n",
      "Stochastic Gradient Descent(35395): loss=0.6595451690802122\n",
      "Stochastic Gradient Descent(35396): loss=3.045988664993904\n",
      "Stochastic Gradient Descent(35397): loss=7.7732023505928005\n",
      "Stochastic Gradient Descent(35398): loss=18.801292747624206\n",
      "Stochastic Gradient Descent(35399): loss=10.21998671930462\n",
      "Stochastic Gradient Descent(35400): loss=8.162568160352594\n",
      "Stochastic Gradient Descent(35401): loss=1.8621522915880286\n",
      "Stochastic Gradient Descent(35402): loss=0.5868158906402512\n",
      "Stochastic Gradient Descent(35403): loss=2.7430880588819235\n",
      "Stochastic Gradient Descent(35404): loss=62.64589192560861\n",
      "Stochastic Gradient Descent(35405): loss=4.206129112796704\n",
      "Stochastic Gradient Descent(35406): loss=0.34782284176239164\n",
      "Stochastic Gradient Descent(35407): loss=1.2223523954260223\n",
      "Stochastic Gradient Descent(35408): loss=5.7714129832057015\n",
      "Stochastic Gradient Descent(35409): loss=21.718990238454477\n",
      "Stochastic Gradient Descent(35410): loss=3.503715394192267\n",
      "Stochastic Gradient Descent(35411): loss=0.562432624761228\n",
      "Stochastic Gradient Descent(35412): loss=0.4408431534110529\n",
      "Stochastic Gradient Descent(35413): loss=4.8388567356711825\n",
      "Stochastic Gradient Descent(35414): loss=0.28438196567737173\n",
      "Stochastic Gradient Descent(35415): loss=0.37993661101776494\n",
      "Stochastic Gradient Descent(35416): loss=9.522612800373699\n",
      "Stochastic Gradient Descent(35417): loss=7.492808056460864\n",
      "Stochastic Gradient Descent(35418): loss=0.7101587492963204\n",
      "Stochastic Gradient Descent(35419): loss=5.379902713786922\n",
      "Stochastic Gradient Descent(35420): loss=0.5155787063499024\n",
      "Stochastic Gradient Descent(35421): loss=1.3556165906972506\n",
      "Stochastic Gradient Descent(35422): loss=0.9525961539715899\n",
      "Stochastic Gradient Descent(35423): loss=8.186493383470308\n",
      "Stochastic Gradient Descent(35424): loss=0.013469793819560748\n",
      "Stochastic Gradient Descent(35425): loss=3.9098915196835176\n",
      "Stochastic Gradient Descent(35426): loss=0.018030724038787863\n",
      "Stochastic Gradient Descent(35427): loss=0.6408333127545278\n",
      "Stochastic Gradient Descent(35428): loss=1.3866704295091201\n",
      "Stochastic Gradient Descent(35429): loss=0.316947815436543\n",
      "Stochastic Gradient Descent(35430): loss=0.023378900736936783\n",
      "Stochastic Gradient Descent(35431): loss=8.788982383624303\n",
      "Stochastic Gradient Descent(35432): loss=10.171157555873096\n",
      "Stochastic Gradient Descent(35433): loss=7.412703615394894\n",
      "Stochastic Gradient Descent(35434): loss=2.25045833369195\n",
      "Stochastic Gradient Descent(35435): loss=2.0948421425189254\n",
      "Stochastic Gradient Descent(35436): loss=3.0638872425962385\n",
      "Stochastic Gradient Descent(35437): loss=0.14936336846907503\n",
      "Stochastic Gradient Descent(35438): loss=1.5034052925879762\n",
      "Stochastic Gradient Descent(35439): loss=0.9964948101532836\n",
      "Stochastic Gradient Descent(35440): loss=0.5290672612083711\n",
      "Stochastic Gradient Descent(35441): loss=0.6004247242761402\n",
      "Stochastic Gradient Descent(35442): loss=2.687151404733934\n",
      "Stochastic Gradient Descent(35443): loss=57.79596659665051\n",
      "Stochastic Gradient Descent(35444): loss=0.7231749386917597\n",
      "Stochastic Gradient Descent(35445): loss=10.691507290201471\n",
      "Stochastic Gradient Descent(35446): loss=10.564069279635069\n",
      "Stochastic Gradient Descent(35447): loss=0.8653827283255945\n",
      "Stochastic Gradient Descent(35448): loss=6.778656021972237\n",
      "Stochastic Gradient Descent(35449): loss=0.5967146062054987\n",
      "Stochastic Gradient Descent(35450): loss=0.8693266471874136\n",
      "Stochastic Gradient Descent(35451): loss=8.518802795959932\n",
      "Stochastic Gradient Descent(35452): loss=1.3505160284906732\n",
      "Stochastic Gradient Descent(35453): loss=1.239688216109683\n",
      "Stochastic Gradient Descent(35454): loss=0.6888546219566763\n",
      "Stochastic Gradient Descent(35455): loss=1.0315990804445683\n",
      "Stochastic Gradient Descent(35456): loss=5.940063254528052\n",
      "Stochastic Gradient Descent(35457): loss=0.11128195545209713\n",
      "Stochastic Gradient Descent(35458): loss=2.2438042927828437\n",
      "Stochastic Gradient Descent(35459): loss=2.6151762400217535\n",
      "Stochastic Gradient Descent(35460): loss=0.03464896967182843\n",
      "Stochastic Gradient Descent(35461): loss=16.650279929128974\n",
      "Stochastic Gradient Descent(35462): loss=0.15878705161724865\n",
      "Stochastic Gradient Descent(35463): loss=8.088939246689888\n",
      "Stochastic Gradient Descent(35464): loss=6.611814590349518\n",
      "Stochastic Gradient Descent(35465): loss=7.66178478935251\n",
      "Stochastic Gradient Descent(35466): loss=1.400844258912077\n",
      "Stochastic Gradient Descent(35467): loss=0.3118900990100897\n",
      "Stochastic Gradient Descent(35468): loss=1.965497485125814\n",
      "Stochastic Gradient Descent(35469): loss=2.2981522634857954\n",
      "Stochastic Gradient Descent(35470): loss=28.897359778217165\n",
      "Stochastic Gradient Descent(35471): loss=10.595360739457602\n",
      "Stochastic Gradient Descent(35472): loss=0.2858978083249775\n",
      "Stochastic Gradient Descent(35473): loss=37.2696608491919\n",
      "Stochastic Gradient Descent(35474): loss=3.0279786379394946\n",
      "Stochastic Gradient Descent(35475): loss=3.1297818694211754\n",
      "Stochastic Gradient Descent(35476): loss=2.1185124860956956\n",
      "Stochastic Gradient Descent(35477): loss=6.91184780983753\n",
      "Stochastic Gradient Descent(35478): loss=4.170647197555486\n",
      "Stochastic Gradient Descent(35479): loss=0.16273675850124789\n",
      "Stochastic Gradient Descent(35480): loss=15.99251111926707\n",
      "Stochastic Gradient Descent(35481): loss=15.274013890285485\n",
      "Stochastic Gradient Descent(35482): loss=0.6761230322677054\n",
      "Stochastic Gradient Descent(35483): loss=3.455637840390371\n",
      "Stochastic Gradient Descent(35484): loss=2.2117401432532193\n",
      "Stochastic Gradient Descent(35485): loss=1.059974808714686\n",
      "Stochastic Gradient Descent(35486): loss=13.475398442576221\n",
      "Stochastic Gradient Descent(35487): loss=0.09013938659820965\n",
      "Stochastic Gradient Descent(35488): loss=7.997377224194816\n",
      "Stochastic Gradient Descent(35489): loss=8.620276983541874\n",
      "Stochastic Gradient Descent(35490): loss=3.851109710262727\n",
      "Stochastic Gradient Descent(35491): loss=25.11246972197372\n",
      "Stochastic Gradient Descent(35492): loss=36.02862347068501\n",
      "Stochastic Gradient Descent(35493): loss=24.679021533504876\n",
      "Stochastic Gradient Descent(35494): loss=6.877207164870123\n",
      "Stochastic Gradient Descent(35495): loss=3.7537689293009153\n",
      "Stochastic Gradient Descent(35496): loss=1.6523888600310528\n",
      "Stochastic Gradient Descent(35497): loss=11.319670379200863\n",
      "Stochastic Gradient Descent(35498): loss=0.6305626580327994\n",
      "Stochastic Gradient Descent(35499): loss=2.7949014924554203\n",
      "Stochastic Gradient Descent(35500): loss=28.194739691724568\n",
      "Stochastic Gradient Descent(35501): loss=3.488060887080785\n",
      "Stochastic Gradient Descent(35502): loss=4.823014962872539\n",
      "Stochastic Gradient Descent(35503): loss=1.4436842897905637\n",
      "Stochastic Gradient Descent(35504): loss=0.4929284518144422\n",
      "Stochastic Gradient Descent(35505): loss=4.353553923294713\n",
      "Stochastic Gradient Descent(35506): loss=0.06693669196595466\n",
      "Stochastic Gradient Descent(35507): loss=0.042717485595346064\n",
      "Stochastic Gradient Descent(35508): loss=1.8650788352189853\n",
      "Stochastic Gradient Descent(35509): loss=0.6424023292947684\n",
      "Stochastic Gradient Descent(35510): loss=44.12775213569734\n",
      "Stochastic Gradient Descent(35511): loss=0.5160625038022348\n",
      "Stochastic Gradient Descent(35512): loss=0.05345115020485884\n",
      "Stochastic Gradient Descent(35513): loss=2.437705308464221\n",
      "Stochastic Gradient Descent(35514): loss=0.8731858142981213\n",
      "Stochastic Gradient Descent(35515): loss=1.2794916663712048\n",
      "Stochastic Gradient Descent(35516): loss=1.305888929101865\n",
      "Stochastic Gradient Descent(35517): loss=0.03548362659787708\n",
      "Stochastic Gradient Descent(35518): loss=0.3090403272552972\n",
      "Stochastic Gradient Descent(35519): loss=0.08883709787459473\n",
      "Stochastic Gradient Descent(35520): loss=0.1696633355514913\n",
      "Stochastic Gradient Descent(35521): loss=0.06142241604778631\n",
      "Stochastic Gradient Descent(35522): loss=5.613706718684165\n",
      "Stochastic Gradient Descent(35523): loss=5.372286689467957\n",
      "Stochastic Gradient Descent(35524): loss=0.02951406017045463\n",
      "Stochastic Gradient Descent(35525): loss=5.574104349944522\n",
      "Stochastic Gradient Descent(35526): loss=1.3217510813720477\n",
      "Stochastic Gradient Descent(35527): loss=5.694303332488058\n",
      "Stochastic Gradient Descent(35528): loss=7.099704575760257\n",
      "Stochastic Gradient Descent(35529): loss=1.1458408551149715\n",
      "Stochastic Gradient Descent(35530): loss=3.2141463729145667\n",
      "Stochastic Gradient Descent(35531): loss=2.0316239282411335\n",
      "Stochastic Gradient Descent(35532): loss=15.182417418278577\n",
      "Stochastic Gradient Descent(35533): loss=3.6368112705671085\n",
      "Stochastic Gradient Descent(35534): loss=0.6296838669593943\n",
      "Stochastic Gradient Descent(35535): loss=1.999955684464745\n",
      "Stochastic Gradient Descent(35536): loss=5.811397641883455\n",
      "Stochastic Gradient Descent(35537): loss=3.1351294073447677\n",
      "Stochastic Gradient Descent(35538): loss=4.579402510135073\n",
      "Stochastic Gradient Descent(35539): loss=1.421756279072635\n",
      "Stochastic Gradient Descent(35540): loss=6.263110078558428\n",
      "Stochastic Gradient Descent(35541): loss=2.636719307642435\n",
      "Stochastic Gradient Descent(35542): loss=0.28508470461343893\n",
      "Stochastic Gradient Descent(35543): loss=14.6715078878032\n",
      "Stochastic Gradient Descent(35544): loss=0.9538664350037113\n",
      "Stochastic Gradient Descent(35545): loss=0.2937742952241873\n",
      "Stochastic Gradient Descent(35546): loss=1.2777920551183142\n",
      "Stochastic Gradient Descent(35547): loss=2.284183533301288\n",
      "Stochastic Gradient Descent(35548): loss=5.08726232042305\n",
      "Stochastic Gradient Descent(35549): loss=6.879848506333958\n",
      "Stochastic Gradient Descent(35550): loss=2.1779686549071307\n",
      "Stochastic Gradient Descent(35551): loss=5.128934003197012\n",
      "Stochastic Gradient Descent(35552): loss=1.634487513035518\n",
      "Stochastic Gradient Descent(35553): loss=8.440276809659588\n",
      "Stochastic Gradient Descent(35554): loss=14.508459247584472\n",
      "Stochastic Gradient Descent(35555): loss=0.3854345280646438\n",
      "Stochastic Gradient Descent(35556): loss=3.0672597889207065\n",
      "Stochastic Gradient Descent(35557): loss=26.33958707990122\n",
      "Stochastic Gradient Descent(35558): loss=1.9285601851323977\n",
      "Stochastic Gradient Descent(35559): loss=2.301705133053251\n",
      "Stochastic Gradient Descent(35560): loss=4.35816060758882\n",
      "Stochastic Gradient Descent(35561): loss=3.327017316525176\n",
      "Stochastic Gradient Descent(35562): loss=0.09446440538429719\n",
      "Stochastic Gradient Descent(35563): loss=3.103839030006427\n",
      "Stochastic Gradient Descent(35564): loss=0.7087755049473441\n",
      "Stochastic Gradient Descent(35565): loss=28.913114031264662\n",
      "Stochastic Gradient Descent(35566): loss=0.838470678515235\n",
      "Stochastic Gradient Descent(35567): loss=1.2593660235911588\n",
      "Stochastic Gradient Descent(35568): loss=3.1815123563915413\n",
      "Stochastic Gradient Descent(35569): loss=10.082437613895712\n",
      "Stochastic Gradient Descent(35570): loss=6.3162018748095665\n",
      "Stochastic Gradient Descent(35571): loss=5.75535720232496\n",
      "Stochastic Gradient Descent(35572): loss=0.0192084124038673\n",
      "Stochastic Gradient Descent(35573): loss=2.027831593491835\n",
      "Stochastic Gradient Descent(35574): loss=2.086376499275392\n",
      "Stochastic Gradient Descent(35575): loss=3.0696121871441138\n",
      "Stochastic Gradient Descent(35576): loss=0.04722867602064596\n",
      "Stochastic Gradient Descent(35577): loss=9.812773714157434\n",
      "Stochastic Gradient Descent(35578): loss=8.221191735636246\n",
      "Stochastic Gradient Descent(35579): loss=3.8887210917857016\n",
      "Stochastic Gradient Descent(35580): loss=1.9535789749250139\n",
      "Stochastic Gradient Descent(35581): loss=21.130576446165975\n",
      "Stochastic Gradient Descent(35582): loss=0.08192226094040622\n",
      "Stochastic Gradient Descent(35583): loss=0.021156626676455476\n",
      "Stochastic Gradient Descent(35584): loss=1.0185604064781586\n",
      "Stochastic Gradient Descent(35585): loss=11.620142262221027\n",
      "Stochastic Gradient Descent(35586): loss=13.25510936721151\n",
      "Stochastic Gradient Descent(35587): loss=2.803985286861793\n",
      "Stochastic Gradient Descent(35588): loss=2.687727863542974\n",
      "Stochastic Gradient Descent(35589): loss=10.738852895036375\n",
      "Stochastic Gradient Descent(35590): loss=0.029762024531708904\n",
      "Stochastic Gradient Descent(35591): loss=0.029955248898361594\n",
      "Stochastic Gradient Descent(35592): loss=11.18973041789665\n",
      "Stochastic Gradient Descent(35593): loss=3.86036106366382\n",
      "Stochastic Gradient Descent(35594): loss=0.7606160054779505\n",
      "Stochastic Gradient Descent(35595): loss=0.003176377160485078\n",
      "Stochastic Gradient Descent(35596): loss=0.08211469028417068\n",
      "Stochastic Gradient Descent(35597): loss=18.94843272336384\n",
      "Stochastic Gradient Descent(35598): loss=1.947576116551296\n",
      "Stochastic Gradient Descent(35599): loss=0.7011874235229154\n",
      "Stochastic Gradient Descent(35600): loss=3.03345782768677\n",
      "Stochastic Gradient Descent(35601): loss=1.9638602088667576\n",
      "Stochastic Gradient Descent(35602): loss=0.667149543050758\n",
      "Stochastic Gradient Descent(35603): loss=4.930292389977456\n",
      "Stochastic Gradient Descent(35604): loss=0.2010871871575304\n",
      "Stochastic Gradient Descent(35605): loss=6.424834168192771\n",
      "Stochastic Gradient Descent(35606): loss=9.7309924314457\n",
      "Stochastic Gradient Descent(35607): loss=0.523627508966117\n",
      "Stochastic Gradient Descent(35608): loss=1.5982833754354002\n",
      "Stochastic Gradient Descent(35609): loss=4.821183478965605\n",
      "Stochastic Gradient Descent(35610): loss=0.45811928738698177\n",
      "Stochastic Gradient Descent(35611): loss=8.907746680546342\n",
      "Stochastic Gradient Descent(35612): loss=2.5129165786306937\n",
      "Stochastic Gradient Descent(35613): loss=1.4674209842370924\n",
      "Stochastic Gradient Descent(35614): loss=0.00014734750413728092\n",
      "Stochastic Gradient Descent(35615): loss=2.0642010550009107\n",
      "Stochastic Gradient Descent(35616): loss=1.0427293851275024\n",
      "Stochastic Gradient Descent(35617): loss=3.104355796006368\n",
      "Stochastic Gradient Descent(35618): loss=25.634968165259284\n",
      "Stochastic Gradient Descent(35619): loss=0.4268215388937794\n",
      "Stochastic Gradient Descent(35620): loss=6.586180885757069\n",
      "Stochastic Gradient Descent(35621): loss=4.722211978254331\n",
      "Stochastic Gradient Descent(35622): loss=2.411381837692094\n",
      "Stochastic Gradient Descent(35623): loss=6.111732364078379\n",
      "Stochastic Gradient Descent(35624): loss=3.1134313503358073\n",
      "Stochastic Gradient Descent(35625): loss=5.972723117479011\n",
      "Stochastic Gradient Descent(35626): loss=0.036275759487996315\n",
      "Stochastic Gradient Descent(35627): loss=2.8535208290556113\n",
      "Stochastic Gradient Descent(35628): loss=0.2287061614382041\n",
      "Stochastic Gradient Descent(35629): loss=11.795438273225978\n",
      "Stochastic Gradient Descent(35630): loss=0.1594002738406958\n",
      "Stochastic Gradient Descent(35631): loss=59.291830418681116\n",
      "Stochastic Gradient Descent(35632): loss=1.9974123463307514\n",
      "Stochastic Gradient Descent(35633): loss=0.02096341620895434\n",
      "Stochastic Gradient Descent(35634): loss=7.024954569747488\n",
      "Stochastic Gradient Descent(35635): loss=0.06352908275043341\n",
      "Stochastic Gradient Descent(35636): loss=0.0037078606494231315\n",
      "Stochastic Gradient Descent(35637): loss=2.6613404750820244\n",
      "Stochastic Gradient Descent(35638): loss=0.7049322163641027\n",
      "Stochastic Gradient Descent(35639): loss=1.3143384863070289\n",
      "Stochastic Gradient Descent(35640): loss=0.04096659864135483\n",
      "Stochastic Gradient Descent(35641): loss=0.28655845734839736\n",
      "Stochastic Gradient Descent(35642): loss=0.6546773301393128\n",
      "Stochastic Gradient Descent(35643): loss=11.246302429069354\n",
      "Stochastic Gradient Descent(35644): loss=0.1627713350936571\n",
      "Stochastic Gradient Descent(35645): loss=3.4611784092190754\n",
      "Stochastic Gradient Descent(35646): loss=0.4329292222460779\n",
      "Stochastic Gradient Descent(35647): loss=3.4709312110962145\n",
      "Stochastic Gradient Descent(35648): loss=3.8135498130024823\n",
      "Stochastic Gradient Descent(35649): loss=4.246828239770371\n",
      "Stochastic Gradient Descent(35650): loss=1.1028988461875013\n",
      "Stochastic Gradient Descent(35651): loss=0.09240908509506526\n",
      "Stochastic Gradient Descent(35652): loss=1.1107725752185647\n",
      "Stochastic Gradient Descent(35653): loss=3.7014966188232674\n",
      "Stochastic Gradient Descent(35654): loss=0.4405934554796255\n",
      "Stochastic Gradient Descent(35655): loss=2.0036434917834556\n",
      "Stochastic Gradient Descent(35656): loss=2.7568935845573725\n",
      "Stochastic Gradient Descent(35657): loss=8.078646054917577\n",
      "Stochastic Gradient Descent(35658): loss=0.04331465743356629\n",
      "Stochastic Gradient Descent(35659): loss=1.8741558090041526\n",
      "Stochastic Gradient Descent(35660): loss=19.86064198499341\n",
      "Stochastic Gradient Descent(35661): loss=2.2427384842334757\n",
      "Stochastic Gradient Descent(35662): loss=5.363479953147686\n",
      "Stochastic Gradient Descent(35663): loss=8.516857611686902\n",
      "Stochastic Gradient Descent(35664): loss=6.432383098569931\n",
      "Stochastic Gradient Descent(35665): loss=2.419884506853395\n",
      "Stochastic Gradient Descent(35666): loss=1.3221701488817144\n",
      "Stochastic Gradient Descent(35667): loss=3.5234666845607627\n",
      "Stochastic Gradient Descent(35668): loss=0.2934303490448337\n",
      "Stochastic Gradient Descent(35669): loss=0.14520212899814358\n",
      "Stochastic Gradient Descent(35670): loss=8.987565245859736\n",
      "Stochastic Gradient Descent(35671): loss=16.319260501890923\n",
      "Stochastic Gradient Descent(35672): loss=0.15350351860804456\n",
      "Stochastic Gradient Descent(35673): loss=0.8326391148041183\n",
      "Stochastic Gradient Descent(35674): loss=19.901072087840596\n",
      "Stochastic Gradient Descent(35675): loss=0.6039121047239329\n",
      "Stochastic Gradient Descent(35676): loss=0.525885646291158\n",
      "Stochastic Gradient Descent(35677): loss=10.965136928270248\n",
      "Stochastic Gradient Descent(35678): loss=0.16244458317247157\n",
      "Stochastic Gradient Descent(35679): loss=14.627810395820411\n",
      "Stochastic Gradient Descent(35680): loss=1.43527231845076\n",
      "Stochastic Gradient Descent(35681): loss=3.341582978993649\n",
      "Stochastic Gradient Descent(35682): loss=6.959170248073839\n",
      "Stochastic Gradient Descent(35683): loss=5.456308536757083\n",
      "Stochastic Gradient Descent(35684): loss=0.28368971474121574\n",
      "Stochastic Gradient Descent(35685): loss=1.400548585746472\n",
      "Stochastic Gradient Descent(35686): loss=0.09812716534205046\n",
      "Stochastic Gradient Descent(35687): loss=0.00908191871200661\n",
      "Stochastic Gradient Descent(35688): loss=10.76561008648382\n",
      "Stochastic Gradient Descent(35689): loss=21.993106714498566\n",
      "Stochastic Gradient Descent(35690): loss=0.5549196059524266\n",
      "Stochastic Gradient Descent(35691): loss=1.2067263336089495\n",
      "Stochastic Gradient Descent(35692): loss=8.16639756039021\n",
      "Stochastic Gradient Descent(35693): loss=8.343304355157093\n",
      "Stochastic Gradient Descent(35694): loss=0.056858604676518744\n",
      "Stochastic Gradient Descent(35695): loss=1.3282150002689397\n",
      "Stochastic Gradient Descent(35696): loss=7.234866915844687\n",
      "Stochastic Gradient Descent(35697): loss=1.8476914561792466\n",
      "Stochastic Gradient Descent(35698): loss=5.171816238993389\n",
      "Stochastic Gradient Descent(35699): loss=1.1194219638875729\n",
      "Stochastic Gradient Descent(35700): loss=2.58604775750896\n",
      "Stochastic Gradient Descent(35701): loss=0.3613276179750837\n",
      "Stochastic Gradient Descent(35702): loss=0.722789012167307\n",
      "Stochastic Gradient Descent(35703): loss=3.8449693886876437\n",
      "Stochastic Gradient Descent(35704): loss=6.623237095076891\n",
      "Stochastic Gradient Descent(35705): loss=5.967159941928093\n",
      "Stochastic Gradient Descent(35706): loss=1.6172669907241621\n",
      "Stochastic Gradient Descent(35707): loss=13.862084440470449\n",
      "Stochastic Gradient Descent(35708): loss=1.049499861274939\n",
      "Stochastic Gradient Descent(35709): loss=0.08280484212890127\n",
      "Stochastic Gradient Descent(35710): loss=3.899125634136661\n",
      "Stochastic Gradient Descent(35711): loss=0.005485243634573016\n",
      "Stochastic Gradient Descent(35712): loss=7.520705480864711\n",
      "Stochastic Gradient Descent(35713): loss=1.0218735787050628\n",
      "Stochastic Gradient Descent(35714): loss=0.3192056841466178\n",
      "Stochastic Gradient Descent(35715): loss=0.20736738989746847\n",
      "Stochastic Gradient Descent(35716): loss=3.018136730733054\n",
      "Stochastic Gradient Descent(35717): loss=5.422639295712019\n",
      "Stochastic Gradient Descent(35718): loss=0.33236269985373923\n",
      "Stochastic Gradient Descent(35719): loss=0.7454983252346695\n",
      "Stochastic Gradient Descent(35720): loss=0.0027615060796103725\n",
      "Stochastic Gradient Descent(35721): loss=3.790512261594895e-06\n",
      "Stochastic Gradient Descent(35722): loss=6.7983207701171295\n",
      "Stochastic Gradient Descent(35723): loss=7.967497532033416\n",
      "Stochastic Gradient Descent(35724): loss=0.16246769841555145\n",
      "Stochastic Gradient Descent(35725): loss=9.028218773315144\n",
      "Stochastic Gradient Descent(35726): loss=0.5773735601831621\n",
      "Stochastic Gradient Descent(35727): loss=0.21976870692550096\n",
      "Stochastic Gradient Descent(35728): loss=11.251206323432493\n",
      "Stochastic Gradient Descent(35729): loss=11.93181890671936\n",
      "Stochastic Gradient Descent(35730): loss=4.40919663852026\n",
      "Stochastic Gradient Descent(35731): loss=0.5756611674439917\n",
      "Stochastic Gradient Descent(35732): loss=7.159798569316855\n",
      "Stochastic Gradient Descent(35733): loss=0.1623670628574616\n",
      "Stochastic Gradient Descent(35734): loss=0.007136071991312984\n",
      "Stochastic Gradient Descent(35735): loss=0.18485888877528492\n",
      "Stochastic Gradient Descent(35736): loss=0.02520244076314531\n",
      "Stochastic Gradient Descent(35737): loss=3.3968301896425594\n",
      "Stochastic Gradient Descent(35738): loss=0.12576216595216252\n",
      "Stochastic Gradient Descent(35739): loss=1.6644408783716331\n",
      "Stochastic Gradient Descent(35740): loss=3.3486210718158618\n",
      "Stochastic Gradient Descent(35741): loss=0.03928574237285607\n",
      "Stochastic Gradient Descent(35742): loss=1.8215179304396198\n",
      "Stochastic Gradient Descent(35743): loss=4.17247560783001\n",
      "Stochastic Gradient Descent(35744): loss=0.0018628177722491916\n",
      "Stochastic Gradient Descent(35745): loss=8.989039363881275\n",
      "Stochastic Gradient Descent(35746): loss=0.9131370643218888\n",
      "Stochastic Gradient Descent(35747): loss=0.4539674804136707\n",
      "Stochastic Gradient Descent(35748): loss=0.828109472050658\n",
      "Stochastic Gradient Descent(35749): loss=1.9617940493260093\n",
      "Stochastic Gradient Descent(35750): loss=0.14254018876577895\n",
      "Stochastic Gradient Descent(35751): loss=0.8292096229299104\n",
      "Stochastic Gradient Descent(35752): loss=3.499803184173874\n",
      "Stochastic Gradient Descent(35753): loss=0.6201317455228604\n",
      "Stochastic Gradient Descent(35754): loss=6.234109958021891\n",
      "Stochastic Gradient Descent(35755): loss=1.0944716487909396\n",
      "Stochastic Gradient Descent(35756): loss=7.262380761188136\n",
      "Stochastic Gradient Descent(35757): loss=1.0268776082229532\n",
      "Stochastic Gradient Descent(35758): loss=13.497038812314353\n",
      "Stochastic Gradient Descent(35759): loss=6.002298036378411\n",
      "Stochastic Gradient Descent(35760): loss=0.8381645182227262\n",
      "Stochastic Gradient Descent(35761): loss=0.07175538783470668\n",
      "Stochastic Gradient Descent(35762): loss=0.6421608403228037\n",
      "Stochastic Gradient Descent(35763): loss=0.02915935845673171\n",
      "Stochastic Gradient Descent(35764): loss=2.3243264151599687\n",
      "Stochastic Gradient Descent(35765): loss=0.1020743053402759\n",
      "Stochastic Gradient Descent(35766): loss=0.0011812658475909836\n",
      "Stochastic Gradient Descent(35767): loss=1.793763208929731\n",
      "Stochastic Gradient Descent(35768): loss=14.864988495185347\n",
      "Stochastic Gradient Descent(35769): loss=6.08037692920438\n",
      "Stochastic Gradient Descent(35770): loss=0.0026623916528410027\n",
      "Stochastic Gradient Descent(35771): loss=1.3486311175178427\n",
      "Stochastic Gradient Descent(35772): loss=0.09052341115055278\n",
      "Stochastic Gradient Descent(35773): loss=0.5109174583598537\n",
      "Stochastic Gradient Descent(35774): loss=22.18475625578939\n",
      "Stochastic Gradient Descent(35775): loss=0.9992152764797206\n",
      "Stochastic Gradient Descent(35776): loss=0.2969915477546098\n",
      "Stochastic Gradient Descent(35777): loss=0.01373415119954814\n",
      "Stochastic Gradient Descent(35778): loss=0.5241290481610353\n",
      "Stochastic Gradient Descent(35779): loss=1.1906982213320019\n",
      "Stochastic Gradient Descent(35780): loss=1.7988458195796169\n",
      "Stochastic Gradient Descent(35781): loss=2.5001964647751382\n",
      "Stochastic Gradient Descent(35782): loss=3.9250152156330937\n",
      "Stochastic Gradient Descent(35783): loss=6.404216511432264\n",
      "Stochastic Gradient Descent(35784): loss=4.0867014715834955\n",
      "Stochastic Gradient Descent(35785): loss=8.473112020657746\n",
      "Stochastic Gradient Descent(35786): loss=0.5410469113328367\n",
      "Stochastic Gradient Descent(35787): loss=1.2544854074448286\n",
      "Stochastic Gradient Descent(35788): loss=13.968334696431599\n",
      "Stochastic Gradient Descent(35789): loss=3.558382324960875\n",
      "Stochastic Gradient Descent(35790): loss=0.10267281343264179\n",
      "Stochastic Gradient Descent(35791): loss=0.6251750194847546\n",
      "Stochastic Gradient Descent(35792): loss=1.1318506311122998\n",
      "Stochastic Gradient Descent(35793): loss=2.558856234211955\n",
      "Stochastic Gradient Descent(35794): loss=0.7154578333011431\n",
      "Stochastic Gradient Descent(35795): loss=2.4078734651700335\n",
      "Stochastic Gradient Descent(35796): loss=15.502791121453667\n",
      "Stochastic Gradient Descent(35797): loss=2.514786068449151\n",
      "Stochastic Gradient Descent(35798): loss=1.1602630915070662\n",
      "Stochastic Gradient Descent(35799): loss=2.609543212259788\n",
      "Stochastic Gradient Descent(35800): loss=1.4607265038979595\n",
      "Stochastic Gradient Descent(35801): loss=1.7957614517134897\n",
      "Stochastic Gradient Descent(35802): loss=0.1152970131851334\n",
      "Stochastic Gradient Descent(35803): loss=0.37632564192082935\n",
      "Stochastic Gradient Descent(35804): loss=0.04904292890238363\n",
      "Stochastic Gradient Descent(35805): loss=22.424687671783538\n",
      "Stochastic Gradient Descent(35806): loss=32.1111032976179\n",
      "Stochastic Gradient Descent(35807): loss=4.461377094734116\n",
      "Stochastic Gradient Descent(35808): loss=8.574979999599936\n",
      "Stochastic Gradient Descent(35809): loss=0.15227559375370758\n",
      "Stochastic Gradient Descent(35810): loss=3.0903209848300905\n",
      "Stochastic Gradient Descent(35811): loss=2.4678177992650427\n",
      "Stochastic Gradient Descent(35812): loss=6.490767464158196\n",
      "Stochastic Gradient Descent(35813): loss=2.1756278001283387\n",
      "Stochastic Gradient Descent(35814): loss=2.160781079540767\n",
      "Stochastic Gradient Descent(35815): loss=10.196961685980229\n",
      "Stochastic Gradient Descent(35816): loss=26.427672031507974\n",
      "Stochastic Gradient Descent(35817): loss=3.6144847831483085\n",
      "Stochastic Gradient Descent(35818): loss=1.5672749589950943\n",
      "Stochastic Gradient Descent(35819): loss=0.9791726683301335\n",
      "Stochastic Gradient Descent(35820): loss=3.2247913500437027\n",
      "Stochastic Gradient Descent(35821): loss=4.511768716523484\n",
      "Stochastic Gradient Descent(35822): loss=0.8098152010847303\n",
      "Stochastic Gradient Descent(35823): loss=0.569887786249428\n",
      "Stochastic Gradient Descent(35824): loss=1.3685977925923403\n",
      "Stochastic Gradient Descent(35825): loss=14.22814419498606\n",
      "Stochastic Gradient Descent(35826): loss=0.2705248469964344\n",
      "Stochastic Gradient Descent(35827): loss=11.125838487903977\n",
      "Stochastic Gradient Descent(35828): loss=0.45735578641150915\n",
      "Stochastic Gradient Descent(35829): loss=82.40193761635923\n",
      "Stochastic Gradient Descent(35830): loss=5.364371660717264\n",
      "Stochastic Gradient Descent(35831): loss=0.3263636477014756\n",
      "Stochastic Gradient Descent(35832): loss=25.99701517967236\n",
      "Stochastic Gradient Descent(35833): loss=8.679142671587488\n",
      "Stochastic Gradient Descent(35834): loss=17.4284627312173\n",
      "Stochastic Gradient Descent(35835): loss=3.0179435396978302\n",
      "Stochastic Gradient Descent(35836): loss=1.476941490679885\n",
      "Stochastic Gradient Descent(35837): loss=2.2587639008775113\n",
      "Stochastic Gradient Descent(35838): loss=15.933598602992394\n",
      "Stochastic Gradient Descent(35839): loss=0.4279360932556361\n",
      "Stochastic Gradient Descent(35840): loss=37.71582040062254\n",
      "Stochastic Gradient Descent(35841): loss=0.0008171926408380992\n",
      "Stochastic Gradient Descent(35842): loss=17.35922941191843\n",
      "Stochastic Gradient Descent(35843): loss=0.26157414190814493\n",
      "Stochastic Gradient Descent(35844): loss=0.057300976304622615\n",
      "Stochastic Gradient Descent(35845): loss=1.278872691982058\n",
      "Stochastic Gradient Descent(35846): loss=5.433437920705145\n",
      "Stochastic Gradient Descent(35847): loss=4.290505824363245\n",
      "Stochastic Gradient Descent(35848): loss=0.0261082326607164\n",
      "Stochastic Gradient Descent(35849): loss=0.17634444345073766\n",
      "Stochastic Gradient Descent(35850): loss=1.4549219890209164\n",
      "Stochastic Gradient Descent(35851): loss=3.310420617717743\n",
      "Stochastic Gradient Descent(35852): loss=5.592330000849449\n",
      "Stochastic Gradient Descent(35853): loss=0.10463494722423013\n",
      "Stochastic Gradient Descent(35854): loss=1.3505640894633877\n",
      "Stochastic Gradient Descent(35855): loss=1.6251348908341645\n",
      "Stochastic Gradient Descent(35856): loss=0.4742112991232374\n",
      "Stochastic Gradient Descent(35857): loss=7.003201668887377\n",
      "Stochastic Gradient Descent(35858): loss=14.15271411969584\n",
      "Stochastic Gradient Descent(35859): loss=2.48348986383301\n",
      "Stochastic Gradient Descent(35860): loss=4.543994611345163\n",
      "Stochastic Gradient Descent(35861): loss=1.1792390300692144\n",
      "Stochastic Gradient Descent(35862): loss=8.328796106039237\n",
      "Stochastic Gradient Descent(35863): loss=2.9271106645922575\n",
      "Stochastic Gradient Descent(35864): loss=2.1441004687986243\n",
      "Stochastic Gradient Descent(35865): loss=2.135863375983943\n",
      "Stochastic Gradient Descent(35866): loss=5.912109172112897e-05\n",
      "Stochastic Gradient Descent(35867): loss=5.053622244432517\n",
      "Stochastic Gradient Descent(35868): loss=0.09470322848854887\n",
      "Stochastic Gradient Descent(35869): loss=3.2626075417860716\n",
      "Stochastic Gradient Descent(35870): loss=2.388681733391894\n",
      "Stochastic Gradient Descent(35871): loss=0.7886693338319319\n",
      "Stochastic Gradient Descent(35872): loss=2.157909594627998\n",
      "Stochastic Gradient Descent(35873): loss=5.562733491173167\n",
      "Stochastic Gradient Descent(35874): loss=0.877644639862749\n",
      "Stochastic Gradient Descent(35875): loss=0.24436303580421262\n",
      "Stochastic Gradient Descent(35876): loss=1.4809181890460499\n",
      "Stochastic Gradient Descent(35877): loss=0.2392396790827858\n",
      "Stochastic Gradient Descent(35878): loss=0.8999389339231351\n",
      "Stochastic Gradient Descent(35879): loss=5.906859193819561\n",
      "Stochastic Gradient Descent(35880): loss=16.881023481995427\n",
      "Stochastic Gradient Descent(35881): loss=1.3120982199380564\n",
      "Stochastic Gradient Descent(35882): loss=0.01806187257469123\n",
      "Stochastic Gradient Descent(35883): loss=0.4461747885432015\n",
      "Stochastic Gradient Descent(35884): loss=3.500067685998503\n",
      "Stochastic Gradient Descent(35885): loss=15.644398111462221\n",
      "Stochastic Gradient Descent(35886): loss=4.5132519428594176\n",
      "Stochastic Gradient Descent(35887): loss=1.839140917776885\n",
      "Stochastic Gradient Descent(35888): loss=0.5866804512699604\n",
      "Stochastic Gradient Descent(35889): loss=4.494642758635527\n",
      "Stochastic Gradient Descent(35890): loss=16.628137689493677\n",
      "Stochastic Gradient Descent(35891): loss=2.3360738488604125\n",
      "Stochastic Gradient Descent(35892): loss=15.623754410038698\n",
      "Stochastic Gradient Descent(35893): loss=2.2434208547463235\n",
      "Stochastic Gradient Descent(35894): loss=0.3036785730861384\n",
      "Stochastic Gradient Descent(35895): loss=3.5578490667512\n",
      "Stochastic Gradient Descent(35896): loss=33.50730700448845\n",
      "Stochastic Gradient Descent(35897): loss=6.676003060851293\n",
      "Stochastic Gradient Descent(35898): loss=0.01277916499142774\n",
      "Stochastic Gradient Descent(35899): loss=2.087880625482322\n",
      "Stochastic Gradient Descent(35900): loss=3.1871352453268846\n",
      "Stochastic Gradient Descent(35901): loss=11.115009606503463\n",
      "Stochastic Gradient Descent(35902): loss=11.087845407404581\n",
      "Stochastic Gradient Descent(35903): loss=0.03334155163345196\n",
      "Stochastic Gradient Descent(35904): loss=1.4032682476466085\n",
      "Stochastic Gradient Descent(35905): loss=1.2644510632726091\n",
      "Stochastic Gradient Descent(35906): loss=11.050701155933545\n",
      "Stochastic Gradient Descent(35907): loss=5.658161792493341\n",
      "Stochastic Gradient Descent(35908): loss=1.8246340784676445\n",
      "Stochastic Gradient Descent(35909): loss=0.16566240717440878\n",
      "Stochastic Gradient Descent(35910): loss=1.088389486616891\n",
      "Stochastic Gradient Descent(35911): loss=1.3471556560196722\n",
      "Stochastic Gradient Descent(35912): loss=2.9339416478904674\n",
      "Stochastic Gradient Descent(35913): loss=7.90802963339944\n",
      "Stochastic Gradient Descent(35914): loss=3.1964620713112453\n",
      "Stochastic Gradient Descent(35915): loss=6.437969066371633\n",
      "Stochastic Gradient Descent(35916): loss=2.288544387661931\n",
      "Stochastic Gradient Descent(35917): loss=3.5963379059337166\n",
      "Stochastic Gradient Descent(35918): loss=0.6059400719889578\n",
      "Stochastic Gradient Descent(35919): loss=7.101314895491148\n",
      "Stochastic Gradient Descent(35920): loss=2.296271901187072\n",
      "Stochastic Gradient Descent(35921): loss=0.34479201526800946\n",
      "Stochastic Gradient Descent(35922): loss=0.017693044431061344\n",
      "Stochastic Gradient Descent(35923): loss=2.549723499666019\n",
      "Stochastic Gradient Descent(35924): loss=0.15784367339843366\n",
      "Stochastic Gradient Descent(35925): loss=1.3670482802066395\n",
      "Stochastic Gradient Descent(35926): loss=0.00014948382026826624\n",
      "Stochastic Gradient Descent(35927): loss=1.5051463755285261\n",
      "Stochastic Gradient Descent(35928): loss=4.279680502481073\n",
      "Stochastic Gradient Descent(35929): loss=1.8620096604001832\n",
      "Stochastic Gradient Descent(35930): loss=0.25486696734930375\n",
      "Stochastic Gradient Descent(35931): loss=0.27862798349962004\n",
      "Stochastic Gradient Descent(35932): loss=6.238937553763989\n",
      "Stochastic Gradient Descent(35933): loss=4.110183672405694\n",
      "Stochastic Gradient Descent(35934): loss=1.7296752751042868\n",
      "Stochastic Gradient Descent(35935): loss=2.450826481051535\n",
      "Stochastic Gradient Descent(35936): loss=17.252996270147488\n",
      "Stochastic Gradient Descent(35937): loss=0.9701649326378244\n",
      "Stochastic Gradient Descent(35938): loss=3.729237107604931\n",
      "Stochastic Gradient Descent(35939): loss=0.23297892832751557\n",
      "Stochastic Gradient Descent(35940): loss=2.645019550466319\n",
      "Stochastic Gradient Descent(35941): loss=0.013348811528557289\n",
      "Stochastic Gradient Descent(35942): loss=0.009205965319168929\n",
      "Stochastic Gradient Descent(35943): loss=4.606034120848182\n",
      "Stochastic Gradient Descent(35944): loss=0.8223122557819219\n",
      "Stochastic Gradient Descent(35945): loss=0.13395319881350365\n",
      "Stochastic Gradient Descent(35946): loss=0.2054104518653915\n",
      "Stochastic Gradient Descent(35947): loss=2.6321116437834897\n",
      "Stochastic Gradient Descent(35948): loss=1.8077242083608636\n",
      "Stochastic Gradient Descent(35949): loss=0.4366927021687681\n",
      "Stochastic Gradient Descent(35950): loss=3.115257799132001\n",
      "Stochastic Gradient Descent(35951): loss=0.6279263960840358\n",
      "Stochastic Gradient Descent(35952): loss=14.83166456704168\n",
      "Stochastic Gradient Descent(35953): loss=2.037929047702996\n",
      "Stochastic Gradient Descent(35954): loss=0.14004567018601935\n",
      "Stochastic Gradient Descent(35955): loss=0.2758116165904043\n",
      "Stochastic Gradient Descent(35956): loss=0.08035860029156443\n",
      "Stochastic Gradient Descent(35957): loss=10.249841171806636\n",
      "Stochastic Gradient Descent(35958): loss=0.04956459896280948\n",
      "Stochastic Gradient Descent(35959): loss=0.0835256501729789\n",
      "Stochastic Gradient Descent(35960): loss=5.225587452536653\n",
      "Stochastic Gradient Descent(35961): loss=3.189365353940425\n",
      "Stochastic Gradient Descent(35962): loss=12.881021739975257\n",
      "Stochastic Gradient Descent(35963): loss=0.2159425924976529\n",
      "Stochastic Gradient Descent(35964): loss=0.18813247251804052\n",
      "Stochastic Gradient Descent(35965): loss=1.7232637657325953\n",
      "Stochastic Gradient Descent(35966): loss=35.320031280406475\n",
      "Stochastic Gradient Descent(35967): loss=14.297511266151284\n",
      "Stochastic Gradient Descent(35968): loss=2.430795285423583\n",
      "Stochastic Gradient Descent(35969): loss=0.9402256516570734\n",
      "Stochastic Gradient Descent(35970): loss=0.12574328313908353\n",
      "Stochastic Gradient Descent(35971): loss=2.4174455836096893\n",
      "Stochastic Gradient Descent(35972): loss=4.80815790500769\n",
      "Stochastic Gradient Descent(35973): loss=0.21425059353258413\n",
      "Stochastic Gradient Descent(35974): loss=4.783707099788182\n",
      "Stochastic Gradient Descent(35975): loss=17.09780449660499\n",
      "Stochastic Gradient Descent(35976): loss=0.2793843248257603\n",
      "Stochastic Gradient Descent(35977): loss=0.3249823165260941\n",
      "Stochastic Gradient Descent(35978): loss=0.027512803698608927\n",
      "Stochastic Gradient Descent(35979): loss=1.0158505764310173\n",
      "Stochastic Gradient Descent(35980): loss=0.20999853081694242\n",
      "Stochastic Gradient Descent(35981): loss=1.295537340848639\n",
      "Stochastic Gradient Descent(35982): loss=1.709750076800544\n",
      "Stochastic Gradient Descent(35983): loss=7.614978883806916\n",
      "Stochastic Gradient Descent(35984): loss=2.1021275577037275\n",
      "Stochastic Gradient Descent(35985): loss=2.086389625455154\n",
      "Stochastic Gradient Descent(35986): loss=1.0083538662355291\n",
      "Stochastic Gradient Descent(35987): loss=0.5549074451802746\n",
      "Stochastic Gradient Descent(35988): loss=19.18528786096002\n",
      "Stochastic Gradient Descent(35989): loss=1.1974005516477377\n",
      "Stochastic Gradient Descent(35990): loss=0.2799851955593206\n",
      "Stochastic Gradient Descent(35991): loss=20.80067698290842\n",
      "Stochastic Gradient Descent(35992): loss=3.3180742621811503\n",
      "Stochastic Gradient Descent(35993): loss=0.004059879883493958\n",
      "Stochastic Gradient Descent(35994): loss=0.7042868729509586\n",
      "Stochastic Gradient Descent(35995): loss=6.023869578013725\n",
      "Stochastic Gradient Descent(35996): loss=7.353429131218414\n",
      "Stochastic Gradient Descent(35997): loss=7.005215948419638\n",
      "Stochastic Gradient Descent(35998): loss=3.655867232307601\n",
      "Stochastic Gradient Descent(35999): loss=0.2502335796525907\n",
      "Stochastic Gradient Descent(36000): loss=15.99149270786705\n",
      "Stochastic Gradient Descent(36001): loss=2.461307392728402\n",
      "Stochastic Gradient Descent(36002): loss=0.06054890415261913\n",
      "Stochastic Gradient Descent(36003): loss=0.6321197566233159\n",
      "Stochastic Gradient Descent(36004): loss=1.2600023639258946\n",
      "Stochastic Gradient Descent(36005): loss=3.734012158647148\n",
      "Stochastic Gradient Descent(36006): loss=4.987246179630766\n",
      "Stochastic Gradient Descent(36007): loss=18.67396553655537\n",
      "Stochastic Gradient Descent(36008): loss=0.3182689860375465\n",
      "Stochastic Gradient Descent(36009): loss=0.03756947902417714\n",
      "Stochastic Gradient Descent(36010): loss=0.26198022657382153\n",
      "Stochastic Gradient Descent(36011): loss=5.584617053857123\n",
      "Stochastic Gradient Descent(36012): loss=0.23823862681223226\n",
      "Stochastic Gradient Descent(36013): loss=0.6143906036049073\n",
      "Stochastic Gradient Descent(36014): loss=5.324211966985788\n",
      "Stochastic Gradient Descent(36015): loss=5.206367777204806\n",
      "Stochastic Gradient Descent(36016): loss=4.609107930244287\n",
      "Stochastic Gradient Descent(36017): loss=2.240600007452381\n",
      "Stochastic Gradient Descent(36018): loss=5.935843526726866\n",
      "Stochastic Gradient Descent(36019): loss=1.7996811594864488\n",
      "Stochastic Gradient Descent(36020): loss=0.0901644811102058\n",
      "Stochastic Gradient Descent(36021): loss=0.14263647287748754\n",
      "Stochastic Gradient Descent(36022): loss=2.1675451286937273\n",
      "Stochastic Gradient Descent(36023): loss=1.4302322500795843\n",
      "Stochastic Gradient Descent(36024): loss=3.5432174589110828\n",
      "Stochastic Gradient Descent(36025): loss=1.8216221598334565\n",
      "Stochastic Gradient Descent(36026): loss=5.054173462583092\n",
      "Stochastic Gradient Descent(36027): loss=0.7937087785362953\n",
      "Stochastic Gradient Descent(36028): loss=0.2463115573978684\n",
      "Stochastic Gradient Descent(36029): loss=0.13562300419599987\n",
      "Stochastic Gradient Descent(36030): loss=0.1284369715787574\n",
      "Stochastic Gradient Descent(36031): loss=1.1902770600183443\n",
      "Stochastic Gradient Descent(36032): loss=1.8006822821522002\n",
      "Stochastic Gradient Descent(36033): loss=1.5581313171876894\n",
      "Stochastic Gradient Descent(36034): loss=1.8514891991875742\n",
      "Stochastic Gradient Descent(36035): loss=7.523843763634368\n",
      "Stochastic Gradient Descent(36036): loss=4.315521907748187\n",
      "Stochastic Gradient Descent(36037): loss=4.3637490656308335\n",
      "Stochastic Gradient Descent(36038): loss=1.15168445459552\n",
      "Stochastic Gradient Descent(36039): loss=2.4244010707596537\n",
      "Stochastic Gradient Descent(36040): loss=6.283589016930946\n",
      "Stochastic Gradient Descent(36041): loss=1.0151252333202934\n",
      "Stochastic Gradient Descent(36042): loss=0.9293929441827525\n",
      "Stochastic Gradient Descent(36043): loss=2.025577839310412\n",
      "Stochastic Gradient Descent(36044): loss=0.9097864752756933\n",
      "Stochastic Gradient Descent(36045): loss=1.5864277646955895\n",
      "Stochastic Gradient Descent(36046): loss=0.2765612161719511\n",
      "Stochastic Gradient Descent(36047): loss=0.03697442728604213\n",
      "Stochastic Gradient Descent(36048): loss=2.103452677588073\n",
      "Stochastic Gradient Descent(36049): loss=1.125011658194706\n",
      "Stochastic Gradient Descent(36050): loss=2.7573656409161784\n",
      "Stochastic Gradient Descent(36051): loss=0.2253343503700953\n",
      "Stochastic Gradient Descent(36052): loss=0.2563744086237342\n",
      "Stochastic Gradient Descent(36053): loss=0.04503211315388484\n",
      "Stochastic Gradient Descent(36054): loss=0.11291862850222706\n",
      "Stochastic Gradient Descent(36055): loss=1.1762759891398085\n",
      "Stochastic Gradient Descent(36056): loss=1.2081835234500984\n",
      "Stochastic Gradient Descent(36057): loss=6.039902404773505\n",
      "Stochastic Gradient Descent(36058): loss=7.503746038897407\n",
      "Stochastic Gradient Descent(36059): loss=2.90298174242515\n",
      "Stochastic Gradient Descent(36060): loss=0.08426656529867026\n",
      "Stochastic Gradient Descent(36061): loss=0.49723497006458434\n",
      "Stochastic Gradient Descent(36062): loss=1.308454785828374\n",
      "Stochastic Gradient Descent(36063): loss=0.005854059212012314\n",
      "Stochastic Gradient Descent(36064): loss=13.380142846607436\n",
      "Stochastic Gradient Descent(36065): loss=1.9168716523618363\n",
      "Stochastic Gradient Descent(36066): loss=0.6624007638053713\n",
      "Stochastic Gradient Descent(36067): loss=0.3042632197949747\n",
      "Stochastic Gradient Descent(36068): loss=1.261575160122971\n",
      "Stochastic Gradient Descent(36069): loss=20.65759693474688\n",
      "Stochastic Gradient Descent(36070): loss=0.3861516727863199\n",
      "Stochastic Gradient Descent(36071): loss=2.9122330156366836\n",
      "Stochastic Gradient Descent(36072): loss=4.60861261468214\n",
      "Stochastic Gradient Descent(36073): loss=1.6708110972595636\n",
      "Stochastic Gradient Descent(36074): loss=0.9510557190180899\n",
      "Stochastic Gradient Descent(36075): loss=1.9857325071161027\n",
      "Stochastic Gradient Descent(36076): loss=0.0025209952079049817\n",
      "Stochastic Gradient Descent(36077): loss=1.5246661327434405\n",
      "Stochastic Gradient Descent(36078): loss=1.175408042332021\n",
      "Stochastic Gradient Descent(36079): loss=6.495697862560591\n",
      "Stochastic Gradient Descent(36080): loss=19.808339637221394\n",
      "Stochastic Gradient Descent(36081): loss=3.1025093281698903\n",
      "Stochastic Gradient Descent(36082): loss=3.2699881226427787\n",
      "Stochastic Gradient Descent(36083): loss=0.07571350669270883\n",
      "Stochastic Gradient Descent(36084): loss=35.25249503078241\n",
      "Stochastic Gradient Descent(36085): loss=0.0006556524270616023\n",
      "Stochastic Gradient Descent(36086): loss=2.4939163853401256\n",
      "Stochastic Gradient Descent(36087): loss=13.268679911584341\n",
      "Stochastic Gradient Descent(36088): loss=8.738433364032451\n",
      "Stochastic Gradient Descent(36089): loss=29.62630947105938\n",
      "Stochastic Gradient Descent(36090): loss=28.524627160318943\n",
      "Stochastic Gradient Descent(36091): loss=9.178658684118844\n",
      "Stochastic Gradient Descent(36092): loss=1.4539791566169957\n",
      "Stochastic Gradient Descent(36093): loss=3.2154856352475605\n",
      "Stochastic Gradient Descent(36094): loss=0.15536768771032378\n",
      "Stochastic Gradient Descent(36095): loss=0.3286992906412712\n",
      "Stochastic Gradient Descent(36096): loss=3.556721227918972\n",
      "Stochastic Gradient Descent(36097): loss=1.5589704028874414\n",
      "Stochastic Gradient Descent(36098): loss=0.20668956546541553\n",
      "Stochastic Gradient Descent(36099): loss=0.5114271527817126\n",
      "Stochastic Gradient Descent(36100): loss=0.1240007400450151\n",
      "Stochastic Gradient Descent(36101): loss=23.91102438281535\n",
      "Stochastic Gradient Descent(36102): loss=9.936452781330965\n",
      "Stochastic Gradient Descent(36103): loss=3.3768796657623636\n",
      "Stochastic Gradient Descent(36104): loss=0.5345823083432509\n",
      "Stochastic Gradient Descent(36105): loss=7.161589780010247\n",
      "Stochastic Gradient Descent(36106): loss=7.839153715773725\n",
      "Stochastic Gradient Descent(36107): loss=0.008224190802208275\n",
      "Stochastic Gradient Descent(36108): loss=12.78290174155449\n",
      "Stochastic Gradient Descent(36109): loss=2.4297040424762484\n",
      "Stochastic Gradient Descent(36110): loss=1.4742492123616389\n",
      "Stochastic Gradient Descent(36111): loss=0.8677446523536556\n",
      "Stochastic Gradient Descent(36112): loss=2.481560903560936\n",
      "Stochastic Gradient Descent(36113): loss=2.4221991643405976\n",
      "Stochastic Gradient Descent(36114): loss=1.5290166692862919\n",
      "Stochastic Gradient Descent(36115): loss=0.02363039316265888\n",
      "Stochastic Gradient Descent(36116): loss=1.0196040537865718\n",
      "Stochastic Gradient Descent(36117): loss=0.021174971426694868\n",
      "Stochastic Gradient Descent(36118): loss=4.783444336027495\n",
      "Stochastic Gradient Descent(36119): loss=3.4725918707388947\n",
      "Stochastic Gradient Descent(36120): loss=0.12357293065553261\n",
      "Stochastic Gradient Descent(36121): loss=0.06559158794869696\n",
      "Stochastic Gradient Descent(36122): loss=0.8910523017570416\n",
      "Stochastic Gradient Descent(36123): loss=0.012430509330370314\n",
      "Stochastic Gradient Descent(36124): loss=0.18402668337159558\n",
      "Stochastic Gradient Descent(36125): loss=0.2258858921840462\n",
      "Stochastic Gradient Descent(36126): loss=2.6535986314054547\n",
      "Stochastic Gradient Descent(36127): loss=3.2402112193921195\n",
      "Stochastic Gradient Descent(36128): loss=0.41830713960034777\n",
      "Stochastic Gradient Descent(36129): loss=0.5661454346159231\n",
      "Stochastic Gradient Descent(36130): loss=0.9247749031074997\n",
      "Stochastic Gradient Descent(36131): loss=0.038700354947258445\n",
      "Stochastic Gradient Descent(36132): loss=1.6483107974953446e-05\n",
      "Stochastic Gradient Descent(36133): loss=1.0377769991384274\n",
      "Stochastic Gradient Descent(36134): loss=1.084408339723558\n",
      "Stochastic Gradient Descent(36135): loss=11.164647057077719\n",
      "Stochastic Gradient Descent(36136): loss=2.3708779300817557\n",
      "Stochastic Gradient Descent(36137): loss=2.490198424177386\n",
      "Stochastic Gradient Descent(36138): loss=0.7748231830160583\n",
      "Stochastic Gradient Descent(36139): loss=8.77636035349207\n",
      "Stochastic Gradient Descent(36140): loss=2.3786353289504065\n",
      "Stochastic Gradient Descent(36141): loss=6.98176752531953\n",
      "Stochastic Gradient Descent(36142): loss=3.587897767541768\n",
      "Stochastic Gradient Descent(36143): loss=7.1237124052849925\n",
      "Stochastic Gradient Descent(36144): loss=0.6851571942277259\n",
      "Stochastic Gradient Descent(36145): loss=8.114647809375299\n",
      "Stochastic Gradient Descent(36146): loss=6.863788943023795\n",
      "Stochastic Gradient Descent(36147): loss=10.387471568080782\n",
      "Stochastic Gradient Descent(36148): loss=1.7920138450382752\n",
      "Stochastic Gradient Descent(36149): loss=0.02824820038416974\n",
      "Stochastic Gradient Descent(36150): loss=2.4696045517806846\n",
      "Stochastic Gradient Descent(36151): loss=1.5615543511959578\n",
      "Stochastic Gradient Descent(36152): loss=2.8877476076889326\n",
      "Stochastic Gradient Descent(36153): loss=7.905926899607335\n",
      "Stochastic Gradient Descent(36154): loss=0.7125031411410632\n",
      "Stochastic Gradient Descent(36155): loss=0.21820064087006324\n",
      "Stochastic Gradient Descent(36156): loss=8.869287350016737\n",
      "Stochastic Gradient Descent(36157): loss=0.39337362676760457\n",
      "Stochastic Gradient Descent(36158): loss=0.2543442380354144\n",
      "Stochastic Gradient Descent(36159): loss=0.16373575964109705\n",
      "Stochastic Gradient Descent(36160): loss=0.02300138069370754\n",
      "Stochastic Gradient Descent(36161): loss=0.697902024826594\n",
      "Stochastic Gradient Descent(36162): loss=6.834181028763358\n",
      "Stochastic Gradient Descent(36163): loss=11.452069830268979\n",
      "Stochastic Gradient Descent(36164): loss=0.04121507888560094\n",
      "Stochastic Gradient Descent(36165): loss=13.706881159311736\n",
      "Stochastic Gradient Descent(36166): loss=0.10258937466685134\n",
      "Stochastic Gradient Descent(36167): loss=6.733174355647678\n",
      "Stochastic Gradient Descent(36168): loss=3.996685021356806\n",
      "Stochastic Gradient Descent(36169): loss=1.0620180255046412\n",
      "Stochastic Gradient Descent(36170): loss=13.42593853933936\n",
      "Stochastic Gradient Descent(36171): loss=10.950040177276184\n",
      "Stochastic Gradient Descent(36172): loss=0.056449417546670336\n",
      "Stochastic Gradient Descent(36173): loss=2.2907527195274673\n",
      "Stochastic Gradient Descent(36174): loss=0.07558859950917006\n",
      "Stochastic Gradient Descent(36175): loss=1.820151778864626\n",
      "Stochastic Gradient Descent(36176): loss=11.79835366902141\n",
      "Stochastic Gradient Descent(36177): loss=10.682827863342848\n",
      "Stochastic Gradient Descent(36178): loss=9.72364068925699\n",
      "Stochastic Gradient Descent(36179): loss=3.9946882073897596\n",
      "Stochastic Gradient Descent(36180): loss=0.5633683652220713\n",
      "Stochastic Gradient Descent(36181): loss=1.5470085578936927\n",
      "Stochastic Gradient Descent(36182): loss=17.403806666157678\n",
      "Stochastic Gradient Descent(36183): loss=7.184887946856951\n",
      "Stochastic Gradient Descent(36184): loss=4.5068719596576265\n",
      "Stochastic Gradient Descent(36185): loss=3.7795389655522396\n",
      "Stochastic Gradient Descent(36186): loss=0.010025847785239366\n",
      "Stochastic Gradient Descent(36187): loss=0.6582430925485467\n",
      "Stochastic Gradient Descent(36188): loss=4.935638455214271\n",
      "Stochastic Gradient Descent(36189): loss=0.25293554144142116\n",
      "Stochastic Gradient Descent(36190): loss=1.311770602031699\n",
      "Stochastic Gradient Descent(36191): loss=5.9084099416486024\n",
      "Stochastic Gradient Descent(36192): loss=3.1177788445497576\n",
      "Stochastic Gradient Descent(36193): loss=1.1900571369566613\n",
      "Stochastic Gradient Descent(36194): loss=0.40425220054973815\n",
      "Stochastic Gradient Descent(36195): loss=1.4244279145087613\n",
      "Stochastic Gradient Descent(36196): loss=2.3407617241466885\n",
      "Stochastic Gradient Descent(36197): loss=0.429686395144235\n",
      "Stochastic Gradient Descent(36198): loss=2.2989477442656683\n",
      "Stochastic Gradient Descent(36199): loss=6.307114217099436\n",
      "Stochastic Gradient Descent(36200): loss=0.011567759835581315\n",
      "Stochastic Gradient Descent(36201): loss=0.6730742825086131\n",
      "Stochastic Gradient Descent(36202): loss=4.19193200127141\n",
      "Stochastic Gradient Descent(36203): loss=4.7010184130704165\n",
      "Stochastic Gradient Descent(36204): loss=1.9344858547352322\n",
      "Stochastic Gradient Descent(36205): loss=10.574982726240803\n",
      "Stochastic Gradient Descent(36206): loss=6.0136146393112355\n",
      "Stochastic Gradient Descent(36207): loss=1.9367415168465256\n",
      "Stochastic Gradient Descent(36208): loss=5.57084986797972\n",
      "Stochastic Gradient Descent(36209): loss=5.1256802853253225\n",
      "Stochastic Gradient Descent(36210): loss=1.818552342082756\n",
      "Stochastic Gradient Descent(36211): loss=29.295344755610937\n",
      "Stochastic Gradient Descent(36212): loss=0.0005686075660205803\n",
      "Stochastic Gradient Descent(36213): loss=0.6486677538538369\n",
      "Stochastic Gradient Descent(36214): loss=0.7814383886406335\n",
      "Stochastic Gradient Descent(36215): loss=9.31469632353062\n",
      "Stochastic Gradient Descent(36216): loss=1.8300046383287267\n",
      "Stochastic Gradient Descent(36217): loss=3.967074634084468\n",
      "Stochastic Gradient Descent(36218): loss=6.339806550700108\n",
      "Stochastic Gradient Descent(36219): loss=0.29230669529835\n",
      "Stochastic Gradient Descent(36220): loss=2.311622885113453\n",
      "Stochastic Gradient Descent(36221): loss=1.5352004727784005\n",
      "Stochastic Gradient Descent(36222): loss=8.195173150986424\n",
      "Stochastic Gradient Descent(36223): loss=1.698491311107526\n",
      "Stochastic Gradient Descent(36224): loss=1.047932565794655\n",
      "Stochastic Gradient Descent(36225): loss=2.878975929982668\n",
      "Stochastic Gradient Descent(36226): loss=0.13432933955472282\n",
      "Stochastic Gradient Descent(36227): loss=0.6697439638164816\n",
      "Stochastic Gradient Descent(36228): loss=0.2724199641433942\n",
      "Stochastic Gradient Descent(36229): loss=5.149669249538552\n",
      "Stochastic Gradient Descent(36230): loss=4.728510016133244\n",
      "Stochastic Gradient Descent(36231): loss=1.3692447770540863\n",
      "Stochastic Gradient Descent(36232): loss=0.32679491313744224\n",
      "Stochastic Gradient Descent(36233): loss=0.9934198685579132\n",
      "Stochastic Gradient Descent(36234): loss=9.946082847658012\n",
      "Stochastic Gradient Descent(36235): loss=13.591300739797102\n",
      "Stochastic Gradient Descent(36236): loss=0.06092035920234611\n",
      "Stochastic Gradient Descent(36237): loss=0.15296085359549985\n",
      "Stochastic Gradient Descent(36238): loss=9.157325627601175\n",
      "Stochastic Gradient Descent(36239): loss=0.007617716343075492\n",
      "Stochastic Gradient Descent(36240): loss=0.5323318535002497\n",
      "Stochastic Gradient Descent(36241): loss=8.2686607570056\n",
      "Stochastic Gradient Descent(36242): loss=0.05586509428858934\n",
      "Stochastic Gradient Descent(36243): loss=0.35302294584682303\n",
      "Stochastic Gradient Descent(36244): loss=0.11182951342684802\n",
      "Stochastic Gradient Descent(36245): loss=6.308061929367866\n",
      "Stochastic Gradient Descent(36246): loss=7.926255171684263\n",
      "Stochastic Gradient Descent(36247): loss=0.589355159978336\n",
      "Stochastic Gradient Descent(36248): loss=11.538600417803824\n",
      "Stochastic Gradient Descent(36249): loss=0.061418014489254484\n",
      "Stochastic Gradient Descent(36250): loss=2.143202205470615\n",
      "Stochastic Gradient Descent(36251): loss=5.53351004434867\n",
      "Stochastic Gradient Descent(36252): loss=0.0009519297281647152\n",
      "Stochastic Gradient Descent(36253): loss=0.5788020215307177\n",
      "Stochastic Gradient Descent(36254): loss=24.443101785575774\n",
      "Stochastic Gradient Descent(36255): loss=89.7009417471856\n",
      "Stochastic Gradient Descent(36256): loss=103.5026553661682\n",
      "Stochastic Gradient Descent(36257): loss=0.8536050249843473\n",
      "Stochastic Gradient Descent(36258): loss=1.2790494840037119\n",
      "Stochastic Gradient Descent(36259): loss=5.795127384528841\n",
      "Stochastic Gradient Descent(36260): loss=0.04457905387418778\n",
      "Stochastic Gradient Descent(36261): loss=15.479126893921201\n",
      "Stochastic Gradient Descent(36262): loss=0.13113228320243223\n",
      "Stochastic Gradient Descent(36263): loss=18.507324287437992\n",
      "Stochastic Gradient Descent(36264): loss=0.5786711330886805\n",
      "Stochastic Gradient Descent(36265): loss=18.290009456017863\n",
      "Stochastic Gradient Descent(36266): loss=1.77963216584773\n",
      "Stochastic Gradient Descent(36267): loss=0.6481819032516639\n",
      "Stochastic Gradient Descent(36268): loss=4.974452534057537\n",
      "Stochastic Gradient Descent(36269): loss=0.4722596180882405\n",
      "Stochastic Gradient Descent(36270): loss=0.03303936697828168\n",
      "Stochastic Gradient Descent(36271): loss=17.24434050367403\n",
      "Stochastic Gradient Descent(36272): loss=18.37073972795283\n",
      "Stochastic Gradient Descent(36273): loss=3.3007408357552515\n",
      "Stochastic Gradient Descent(36274): loss=33.84884164107659\n",
      "Stochastic Gradient Descent(36275): loss=4.650523471781881\n",
      "Stochastic Gradient Descent(36276): loss=7.826712475514565\n",
      "Stochastic Gradient Descent(36277): loss=0.3094797229339073\n",
      "Stochastic Gradient Descent(36278): loss=5.062402208988058\n",
      "Stochastic Gradient Descent(36279): loss=0.0007264096852950002\n",
      "Stochastic Gradient Descent(36280): loss=0.7489054252678354\n",
      "Stochastic Gradient Descent(36281): loss=0.19046048041718489\n",
      "Stochastic Gradient Descent(36282): loss=0.1350847704320052\n",
      "Stochastic Gradient Descent(36283): loss=2.561737488719109\n",
      "Stochastic Gradient Descent(36284): loss=2.672089429506821\n",
      "Stochastic Gradient Descent(36285): loss=4.346060743146377\n",
      "Stochastic Gradient Descent(36286): loss=0.0004512677982092171\n",
      "Stochastic Gradient Descent(36287): loss=14.352374387088458\n",
      "Stochastic Gradient Descent(36288): loss=5.653993518139073\n",
      "Stochastic Gradient Descent(36289): loss=0.03997364790953372\n",
      "Stochastic Gradient Descent(36290): loss=3.6953530453569914\n",
      "Stochastic Gradient Descent(36291): loss=4.189580188213918\n",
      "Stochastic Gradient Descent(36292): loss=1.9416922388903601\n",
      "Stochastic Gradient Descent(36293): loss=0.25359557241053526\n",
      "Stochastic Gradient Descent(36294): loss=7.3919972969031855\n",
      "Stochastic Gradient Descent(36295): loss=0.7466090155930247\n",
      "Stochastic Gradient Descent(36296): loss=1.6515833157048336\n",
      "Stochastic Gradient Descent(36297): loss=0.5044808155982259\n",
      "Stochastic Gradient Descent(36298): loss=1.5681827617517863\n",
      "Stochastic Gradient Descent(36299): loss=8.242251191020685\n",
      "Stochastic Gradient Descent(36300): loss=2.9079399873514986\n",
      "Stochastic Gradient Descent(36301): loss=11.78445008968723\n",
      "Stochastic Gradient Descent(36302): loss=6.303923429589041\n",
      "Stochastic Gradient Descent(36303): loss=0.1571787373752345\n",
      "Stochastic Gradient Descent(36304): loss=0.16290573867867122\n",
      "Stochastic Gradient Descent(36305): loss=0.34819315233890374\n",
      "Stochastic Gradient Descent(36306): loss=1.5270448009506739\n",
      "Stochastic Gradient Descent(36307): loss=2.070446778141522\n",
      "Stochastic Gradient Descent(36308): loss=11.161724802589628\n",
      "Stochastic Gradient Descent(36309): loss=0.008408100439887158\n",
      "Stochastic Gradient Descent(36310): loss=12.656621948788107\n",
      "Stochastic Gradient Descent(36311): loss=0.0024941582194640886\n",
      "Stochastic Gradient Descent(36312): loss=2.2858742215706433\n",
      "Stochastic Gradient Descent(36313): loss=0.04727392547347674\n",
      "Stochastic Gradient Descent(36314): loss=0.3185958920709744\n",
      "Stochastic Gradient Descent(36315): loss=30.588899616743003\n",
      "Stochastic Gradient Descent(36316): loss=0.06087430533965303\n",
      "Stochastic Gradient Descent(36317): loss=0.007898169679783967\n",
      "Stochastic Gradient Descent(36318): loss=1.6331231415351608\n",
      "Stochastic Gradient Descent(36319): loss=0.13961913122961628\n",
      "Stochastic Gradient Descent(36320): loss=1.4087752833267333\n",
      "Stochastic Gradient Descent(36321): loss=0.6231190396113517\n",
      "Stochastic Gradient Descent(36322): loss=6.319991302366826\n",
      "Stochastic Gradient Descent(36323): loss=1.9094035580074364\n",
      "Stochastic Gradient Descent(36324): loss=0.07332691439624765\n",
      "Stochastic Gradient Descent(36325): loss=28.33674488633807\n",
      "Stochastic Gradient Descent(36326): loss=0.004983065297359108\n",
      "Stochastic Gradient Descent(36327): loss=5.085994573540412\n",
      "Stochastic Gradient Descent(36328): loss=1.7388994942672635\n",
      "Stochastic Gradient Descent(36329): loss=2.6785983807309957\n",
      "Stochastic Gradient Descent(36330): loss=1.0374297307797096\n",
      "Stochastic Gradient Descent(36331): loss=0.5975868038624613\n",
      "Stochastic Gradient Descent(36332): loss=3.2791087922871176\n",
      "Stochastic Gradient Descent(36333): loss=1.0562221876023468\n",
      "Stochastic Gradient Descent(36334): loss=7.404006145851856\n",
      "Stochastic Gradient Descent(36335): loss=3.8142714681737\n",
      "Stochastic Gradient Descent(36336): loss=1.0544314549075127e-05\n",
      "Stochastic Gradient Descent(36337): loss=0.1511996468842716\n",
      "Stochastic Gradient Descent(36338): loss=10.492090528974392\n",
      "Stochastic Gradient Descent(36339): loss=2.580200887473522\n",
      "Stochastic Gradient Descent(36340): loss=1.882180866126416\n",
      "Stochastic Gradient Descent(36341): loss=5.139152568740003\n",
      "Stochastic Gradient Descent(36342): loss=10.726080914921669\n",
      "Stochastic Gradient Descent(36343): loss=3.373288568621983\n",
      "Stochastic Gradient Descent(36344): loss=6.385788370176938\n",
      "Stochastic Gradient Descent(36345): loss=1.3373409433162364\n",
      "Stochastic Gradient Descent(36346): loss=0.0010677704168358965\n",
      "Stochastic Gradient Descent(36347): loss=0.5170827567122175\n",
      "Stochastic Gradient Descent(36348): loss=5.888757087090991\n",
      "Stochastic Gradient Descent(36349): loss=11.365362027095587\n",
      "Stochastic Gradient Descent(36350): loss=0.17964946156475672\n",
      "Stochastic Gradient Descent(36351): loss=0.26668251696597717\n",
      "Stochastic Gradient Descent(36352): loss=2.538563685302184\n",
      "Stochastic Gradient Descent(36353): loss=6.3040683167612075\n",
      "Stochastic Gradient Descent(36354): loss=4.315180510679856\n",
      "Stochastic Gradient Descent(36355): loss=4.565297075171396\n",
      "Stochastic Gradient Descent(36356): loss=0.01477213174251231\n",
      "Stochastic Gradient Descent(36357): loss=1.2357138633385054\n",
      "Stochastic Gradient Descent(36358): loss=0.379687866017699\n",
      "Stochastic Gradient Descent(36359): loss=0.2467009522957966\n",
      "Stochastic Gradient Descent(36360): loss=0.01766620872498478\n",
      "Stochastic Gradient Descent(36361): loss=7.800484346349022\n",
      "Stochastic Gradient Descent(36362): loss=5.571750317936817\n",
      "Stochastic Gradient Descent(36363): loss=7.298302664128976\n",
      "Stochastic Gradient Descent(36364): loss=0.4680242544823887\n",
      "Stochastic Gradient Descent(36365): loss=0.8809647167777985\n",
      "Stochastic Gradient Descent(36366): loss=1.9958892973883195\n",
      "Stochastic Gradient Descent(36367): loss=3.063751445957563\n",
      "Stochastic Gradient Descent(36368): loss=0.981463611096543\n",
      "Stochastic Gradient Descent(36369): loss=13.741435038211595\n",
      "Stochastic Gradient Descent(36370): loss=2.342995021317108\n",
      "Stochastic Gradient Descent(36371): loss=0.023046850661216752\n",
      "Stochastic Gradient Descent(36372): loss=1.966552428343039\n",
      "Stochastic Gradient Descent(36373): loss=11.245022655762964\n",
      "Stochastic Gradient Descent(36374): loss=0.4772211249577318\n",
      "Stochastic Gradient Descent(36375): loss=11.035426045443725\n",
      "Stochastic Gradient Descent(36376): loss=0.07614294892329065\n",
      "Stochastic Gradient Descent(36377): loss=2.062589761194055\n",
      "Stochastic Gradient Descent(36378): loss=2.0572964219416003\n",
      "Stochastic Gradient Descent(36379): loss=2.1272032154714533\n",
      "Stochastic Gradient Descent(36380): loss=1.052820869368269\n",
      "Stochastic Gradient Descent(36381): loss=0.0005497213009273599\n",
      "Stochastic Gradient Descent(36382): loss=0.9943932036845199\n",
      "Stochastic Gradient Descent(36383): loss=0.4451257888806449\n",
      "Stochastic Gradient Descent(36384): loss=0.012838458453441339\n",
      "Stochastic Gradient Descent(36385): loss=0.4945170322336948\n",
      "Stochastic Gradient Descent(36386): loss=5.009525410155646\n",
      "Stochastic Gradient Descent(36387): loss=1.3445953476866597\n",
      "Stochastic Gradient Descent(36388): loss=24.952233278331946\n",
      "Stochastic Gradient Descent(36389): loss=16.26641715963894\n",
      "Stochastic Gradient Descent(36390): loss=6.611370167061004\n",
      "Stochastic Gradient Descent(36391): loss=4.291262696724079\n",
      "Stochastic Gradient Descent(36392): loss=0.28813523505497995\n",
      "Stochastic Gradient Descent(36393): loss=5.18637426378336\n",
      "Stochastic Gradient Descent(36394): loss=2.7077528170536276\n",
      "Stochastic Gradient Descent(36395): loss=0.5023577429048872\n",
      "Stochastic Gradient Descent(36396): loss=2.3406524987360195\n",
      "Stochastic Gradient Descent(36397): loss=2.2877939093047166\n",
      "Stochastic Gradient Descent(36398): loss=5.438582179722902\n",
      "Stochastic Gradient Descent(36399): loss=1.878029568668152\n",
      "Stochastic Gradient Descent(36400): loss=0.0053235699853706195\n",
      "Stochastic Gradient Descent(36401): loss=3.4288966566839614\n",
      "Stochastic Gradient Descent(36402): loss=0.4998410286042014\n",
      "Stochastic Gradient Descent(36403): loss=4.252102710640261\n",
      "Stochastic Gradient Descent(36404): loss=6.473307934092459\n",
      "Stochastic Gradient Descent(36405): loss=1.4232448766736951\n",
      "Stochastic Gradient Descent(36406): loss=0.6077001000570361\n",
      "Stochastic Gradient Descent(36407): loss=1.7573804533435906\n",
      "Stochastic Gradient Descent(36408): loss=0.00010205798877912839\n",
      "Stochastic Gradient Descent(36409): loss=8.992266552991572\n",
      "Stochastic Gradient Descent(36410): loss=0.8982115295659017\n",
      "Stochastic Gradient Descent(36411): loss=1.194166278159817\n",
      "Stochastic Gradient Descent(36412): loss=0.32242992859114017\n",
      "Stochastic Gradient Descent(36413): loss=0.1110237829938719\n",
      "Stochastic Gradient Descent(36414): loss=9.930017487783964\n",
      "Stochastic Gradient Descent(36415): loss=0.13245296104066392\n",
      "Stochastic Gradient Descent(36416): loss=12.272750795091422\n",
      "Stochastic Gradient Descent(36417): loss=1.5521212855041884\n",
      "Stochastic Gradient Descent(36418): loss=0.026605065164183726\n",
      "Stochastic Gradient Descent(36419): loss=4.22198271233538\n",
      "Stochastic Gradient Descent(36420): loss=2.3547585124674515\n",
      "Stochastic Gradient Descent(36421): loss=5.199950003927452\n",
      "Stochastic Gradient Descent(36422): loss=1.9114734452109692\n",
      "Stochastic Gradient Descent(36423): loss=1.4963025944714534\n",
      "Stochastic Gradient Descent(36424): loss=0.3299765645979161\n",
      "Stochastic Gradient Descent(36425): loss=2.1674721768588663\n",
      "Stochastic Gradient Descent(36426): loss=0.45383060241768003\n",
      "Stochastic Gradient Descent(36427): loss=0.9326899618741705\n",
      "Stochastic Gradient Descent(36428): loss=0.03189613579670812\n",
      "Stochastic Gradient Descent(36429): loss=6.579647907990013\n",
      "Stochastic Gradient Descent(36430): loss=0.14843016406798523\n",
      "Stochastic Gradient Descent(36431): loss=2.028560690630105\n",
      "Stochastic Gradient Descent(36432): loss=5.120929626724197\n",
      "Stochastic Gradient Descent(36433): loss=3.4250127985742975\n",
      "Stochastic Gradient Descent(36434): loss=10.740315798692222\n",
      "Stochastic Gradient Descent(36435): loss=0.026487866579769256\n",
      "Stochastic Gradient Descent(36436): loss=16.89994345595775\n",
      "Stochastic Gradient Descent(36437): loss=4.9822987039547675\n",
      "Stochastic Gradient Descent(36438): loss=6.094862695407155\n",
      "Stochastic Gradient Descent(36439): loss=4.653524395779019\n",
      "Stochastic Gradient Descent(36440): loss=0.39687284732543493\n",
      "Stochastic Gradient Descent(36441): loss=1.8758797809293355\n",
      "Stochastic Gradient Descent(36442): loss=0.19576655091053435\n",
      "Stochastic Gradient Descent(36443): loss=0.006241491877929607\n",
      "Stochastic Gradient Descent(36444): loss=0.6932976186829924\n",
      "Stochastic Gradient Descent(36445): loss=0.263626527486306\n",
      "Stochastic Gradient Descent(36446): loss=8.99277725757907\n",
      "Stochastic Gradient Descent(36447): loss=11.598801532099193\n",
      "Stochastic Gradient Descent(36448): loss=3.687917281216442\n",
      "Stochastic Gradient Descent(36449): loss=2.949120077361064\n",
      "Stochastic Gradient Descent(36450): loss=3.2428653016559483\n",
      "Stochastic Gradient Descent(36451): loss=3.1650240790287274\n",
      "Stochastic Gradient Descent(36452): loss=5.420349864275027\n",
      "Stochastic Gradient Descent(36453): loss=12.921623128132119\n",
      "Stochastic Gradient Descent(36454): loss=0.006965763084066041\n",
      "Stochastic Gradient Descent(36455): loss=2.6453070598282884\n",
      "Stochastic Gradient Descent(36456): loss=1.2007964666487119\n",
      "Stochastic Gradient Descent(36457): loss=3.7620400877264126\n",
      "Stochastic Gradient Descent(36458): loss=2.822115147471561\n",
      "Stochastic Gradient Descent(36459): loss=0.48082782572421295\n",
      "Stochastic Gradient Descent(36460): loss=0.04255061976483519\n",
      "Stochastic Gradient Descent(36461): loss=0.0009398587041143365\n",
      "Stochastic Gradient Descent(36462): loss=9.060428622263785\n",
      "Stochastic Gradient Descent(36463): loss=1.122775130345059\n",
      "Stochastic Gradient Descent(36464): loss=8.422217262443684\n",
      "Stochastic Gradient Descent(36465): loss=3.6451381450554123\n",
      "Stochastic Gradient Descent(36466): loss=0.3876939949536382\n",
      "Stochastic Gradient Descent(36467): loss=1.2515447602202658\n",
      "Stochastic Gradient Descent(36468): loss=2.3672386203609697\n",
      "Stochastic Gradient Descent(36469): loss=6.580328774146968\n",
      "Stochastic Gradient Descent(36470): loss=17.613003925769963\n",
      "Stochastic Gradient Descent(36471): loss=0.09620010939834929\n",
      "Stochastic Gradient Descent(36472): loss=4.078130618426157\n",
      "Stochastic Gradient Descent(36473): loss=0.6037918491727594\n",
      "Stochastic Gradient Descent(36474): loss=5.786063213397088\n",
      "Stochastic Gradient Descent(36475): loss=0.15553191564113245\n",
      "Stochastic Gradient Descent(36476): loss=7.725006065665875\n",
      "Stochastic Gradient Descent(36477): loss=0.6115530447754003\n",
      "Stochastic Gradient Descent(36478): loss=0.2274771029197931\n",
      "Stochastic Gradient Descent(36479): loss=0.5487718912725074\n",
      "Stochastic Gradient Descent(36480): loss=7.634651134076041\n",
      "Stochastic Gradient Descent(36481): loss=1.4607335364484015\n",
      "Stochastic Gradient Descent(36482): loss=5.051266777121641\n",
      "Stochastic Gradient Descent(36483): loss=10.696711248404926\n",
      "Stochastic Gradient Descent(36484): loss=0.42941905221260646\n",
      "Stochastic Gradient Descent(36485): loss=0.5998742819284265\n",
      "Stochastic Gradient Descent(36486): loss=0.29133735761226603\n",
      "Stochastic Gradient Descent(36487): loss=0.3672912940041902\n",
      "Stochastic Gradient Descent(36488): loss=0.15180489625027047\n",
      "Stochastic Gradient Descent(36489): loss=1.3108328023641767\n",
      "Stochastic Gradient Descent(36490): loss=0.20504144767807905\n",
      "Stochastic Gradient Descent(36491): loss=7.061436338527501\n",
      "Stochastic Gradient Descent(36492): loss=1.2478546347144086\n",
      "Stochastic Gradient Descent(36493): loss=0.0028241103996730654\n",
      "Stochastic Gradient Descent(36494): loss=2.9389675294375555\n",
      "Stochastic Gradient Descent(36495): loss=1.5262580417048215\n",
      "Stochastic Gradient Descent(36496): loss=1.039148274042241\n",
      "Stochastic Gradient Descent(36497): loss=0.004363474310235654\n",
      "Stochastic Gradient Descent(36498): loss=0.04280879432893461\n",
      "Stochastic Gradient Descent(36499): loss=0.003222400653809204\n",
      "Stochastic Gradient Descent(36500): loss=0.43751233978957454\n",
      "Stochastic Gradient Descent(36501): loss=0.0029233427191640193\n",
      "Stochastic Gradient Descent(36502): loss=1.9602492645441902\n",
      "Stochastic Gradient Descent(36503): loss=8.0676676751683\n",
      "Stochastic Gradient Descent(36504): loss=21.59817848605841\n",
      "Stochastic Gradient Descent(36505): loss=0.49775390186957397\n",
      "Stochastic Gradient Descent(36506): loss=11.460344757555953\n",
      "Stochastic Gradient Descent(36507): loss=0.9029342561261856\n",
      "Stochastic Gradient Descent(36508): loss=0.24921439193579945\n",
      "Stochastic Gradient Descent(36509): loss=0.3812591926606109\n",
      "Stochastic Gradient Descent(36510): loss=1.1439610745670064\n",
      "Stochastic Gradient Descent(36511): loss=15.347483312316362\n",
      "Stochastic Gradient Descent(36512): loss=0.6316766832276275\n",
      "Stochastic Gradient Descent(36513): loss=4.97073517903954\n",
      "Stochastic Gradient Descent(36514): loss=1.0660424882211432\n",
      "Stochastic Gradient Descent(36515): loss=5.9947100511105855\n",
      "Stochastic Gradient Descent(36516): loss=0.533309887407282\n",
      "Stochastic Gradient Descent(36517): loss=3.010764356080447\n",
      "Stochastic Gradient Descent(36518): loss=0.6409805799106579\n",
      "Stochastic Gradient Descent(36519): loss=5.656645715610672\n",
      "Stochastic Gradient Descent(36520): loss=1.7695223065873424\n",
      "Stochastic Gradient Descent(36521): loss=0.2704413549629007\n",
      "Stochastic Gradient Descent(36522): loss=0.22858215186127437\n",
      "Stochastic Gradient Descent(36523): loss=3.3974404326255274\n",
      "Stochastic Gradient Descent(36524): loss=7.41295717678834\n",
      "Stochastic Gradient Descent(36525): loss=0.00028278375682222717\n",
      "Stochastic Gradient Descent(36526): loss=7.098205712690117\n",
      "Stochastic Gradient Descent(36527): loss=0.016786055623286857\n",
      "Stochastic Gradient Descent(36528): loss=5.374398851657902\n",
      "Stochastic Gradient Descent(36529): loss=4.118800528565625\n",
      "Stochastic Gradient Descent(36530): loss=5.492362348426707\n",
      "Stochastic Gradient Descent(36531): loss=8.210161575240539\n",
      "Stochastic Gradient Descent(36532): loss=2.8475806868866123\n",
      "Stochastic Gradient Descent(36533): loss=6.122760039559218\n",
      "Stochastic Gradient Descent(36534): loss=5.529945147674123\n",
      "Stochastic Gradient Descent(36535): loss=0.6776058660478436\n",
      "Stochastic Gradient Descent(36536): loss=0.6989856986910046\n",
      "Stochastic Gradient Descent(36537): loss=0.12867519269810182\n",
      "Stochastic Gradient Descent(36538): loss=16.103058354246713\n",
      "Stochastic Gradient Descent(36539): loss=1.2197070030252017\n",
      "Stochastic Gradient Descent(36540): loss=0.004773225789345226\n",
      "Stochastic Gradient Descent(36541): loss=1.0493979474177308\n",
      "Stochastic Gradient Descent(36542): loss=0.380360112279263\n",
      "Stochastic Gradient Descent(36543): loss=2.162505198313475\n",
      "Stochastic Gradient Descent(36544): loss=1.6225025900638148\n",
      "Stochastic Gradient Descent(36545): loss=2.1078272091743204\n",
      "Stochastic Gradient Descent(36546): loss=10.662722083606907\n",
      "Stochastic Gradient Descent(36547): loss=0.05024652106370285\n",
      "Stochastic Gradient Descent(36548): loss=8.814897160346044\n",
      "Stochastic Gradient Descent(36549): loss=0.00029493659856368466\n",
      "Stochastic Gradient Descent(36550): loss=1.2232355753178978\n",
      "Stochastic Gradient Descent(36551): loss=0.015468989391306151\n",
      "Stochastic Gradient Descent(36552): loss=2.4075792676729173\n",
      "Stochastic Gradient Descent(36553): loss=0.5269869617954263\n",
      "Stochastic Gradient Descent(36554): loss=11.110474336410523\n",
      "Stochastic Gradient Descent(36555): loss=1.6493800040565572\n",
      "Stochastic Gradient Descent(36556): loss=4.395797923972974\n",
      "Stochastic Gradient Descent(36557): loss=2.7877228543338752\n",
      "Stochastic Gradient Descent(36558): loss=2.4084145746197234\n",
      "Stochastic Gradient Descent(36559): loss=13.56220872163798\n",
      "Stochastic Gradient Descent(36560): loss=0.07875693676186059\n",
      "Stochastic Gradient Descent(36561): loss=8.895361845586395\n",
      "Stochastic Gradient Descent(36562): loss=3.9615048039754597\n",
      "Stochastic Gradient Descent(36563): loss=3.180063366165683\n",
      "Stochastic Gradient Descent(36564): loss=1.0120624843790493\n",
      "Stochastic Gradient Descent(36565): loss=5.067913745514986\n",
      "Stochastic Gradient Descent(36566): loss=0.09289689547097156\n",
      "Stochastic Gradient Descent(36567): loss=5.6795338047004185\n",
      "Stochastic Gradient Descent(36568): loss=10.755292236156116\n",
      "Stochastic Gradient Descent(36569): loss=0.5273093414876766\n",
      "Stochastic Gradient Descent(36570): loss=6.330420011606479\n",
      "Stochastic Gradient Descent(36571): loss=1.1150551333039629\n",
      "Stochastic Gradient Descent(36572): loss=0.018766524408270088\n",
      "Stochastic Gradient Descent(36573): loss=1.820673656916721\n",
      "Stochastic Gradient Descent(36574): loss=0.21904209042919134\n",
      "Stochastic Gradient Descent(36575): loss=0.0013258399743918245\n",
      "Stochastic Gradient Descent(36576): loss=3.913787706878476\n",
      "Stochastic Gradient Descent(36577): loss=2.074182734033253\n",
      "Stochastic Gradient Descent(36578): loss=0.19235675182096865\n",
      "Stochastic Gradient Descent(36579): loss=1.1362219836157075\n",
      "Stochastic Gradient Descent(36580): loss=5.2930547478755905\n",
      "Stochastic Gradient Descent(36581): loss=4.312884519373162\n",
      "Stochastic Gradient Descent(36582): loss=0.5680256133134924\n",
      "Stochastic Gradient Descent(36583): loss=1.5757911832967273\n",
      "Stochastic Gradient Descent(36584): loss=0.8118011215434785\n",
      "Stochastic Gradient Descent(36585): loss=0.6179143917979711\n",
      "Stochastic Gradient Descent(36586): loss=11.271770642284421\n",
      "Stochastic Gradient Descent(36587): loss=0.19015923539510546\n",
      "Stochastic Gradient Descent(36588): loss=6.158616023420761\n",
      "Stochastic Gradient Descent(36589): loss=0.008862450540692523\n",
      "Stochastic Gradient Descent(36590): loss=0.5056797275665207\n",
      "Stochastic Gradient Descent(36591): loss=0.022502756448710567\n",
      "Stochastic Gradient Descent(36592): loss=2.7287009401345466\n",
      "Stochastic Gradient Descent(36593): loss=2.632625553191671\n",
      "Stochastic Gradient Descent(36594): loss=0.24665212107948056\n",
      "Stochastic Gradient Descent(36595): loss=0.0755168426587604\n",
      "Stochastic Gradient Descent(36596): loss=0.6325523828609717\n",
      "Stochastic Gradient Descent(36597): loss=0.6903358855802308\n",
      "Stochastic Gradient Descent(36598): loss=13.635518064882492\n",
      "Stochastic Gradient Descent(36599): loss=1.140989018294121\n",
      "Stochastic Gradient Descent(36600): loss=2.628840084255912\n",
      "Stochastic Gradient Descent(36601): loss=0.010877966958158732\n",
      "Stochastic Gradient Descent(36602): loss=0.4370525747577224\n",
      "Stochastic Gradient Descent(36603): loss=1.918454514588408e-06\n",
      "Stochastic Gradient Descent(36604): loss=6.703853188261855\n",
      "Stochastic Gradient Descent(36605): loss=0.3337520214692471\n",
      "Stochastic Gradient Descent(36606): loss=0.3948438665809568\n",
      "Stochastic Gradient Descent(36607): loss=0.4916548402213643\n",
      "Stochastic Gradient Descent(36608): loss=0.5482179136384644\n",
      "Stochastic Gradient Descent(36609): loss=3.2574051183812465\n",
      "Stochastic Gradient Descent(36610): loss=7.5697854889089005\n",
      "Stochastic Gradient Descent(36611): loss=0.9236343011243155\n",
      "Stochastic Gradient Descent(36612): loss=0.16747227991164435\n",
      "Stochastic Gradient Descent(36613): loss=3.400007943856572\n",
      "Stochastic Gradient Descent(36614): loss=1.726102628405296\n",
      "Stochastic Gradient Descent(36615): loss=3.3416876014256247\n",
      "Stochastic Gradient Descent(36616): loss=11.286672131794385\n",
      "Stochastic Gradient Descent(36617): loss=6.150294592742106\n",
      "Stochastic Gradient Descent(36618): loss=20.038928135686863\n",
      "Stochastic Gradient Descent(36619): loss=4.53913543512063\n",
      "Stochastic Gradient Descent(36620): loss=0.40794580939052477\n",
      "Stochastic Gradient Descent(36621): loss=2.8256845557607964\n",
      "Stochastic Gradient Descent(36622): loss=12.079468023403988\n",
      "Stochastic Gradient Descent(36623): loss=2.235409335776369\n",
      "Stochastic Gradient Descent(36624): loss=0.03254662330429839\n",
      "Stochastic Gradient Descent(36625): loss=0.18172668824161245\n",
      "Stochastic Gradient Descent(36626): loss=0.06821198472555175\n",
      "Stochastic Gradient Descent(36627): loss=9.921829986359315\n",
      "Stochastic Gradient Descent(36628): loss=8.780577037457984\n",
      "Stochastic Gradient Descent(36629): loss=0.7044616609247341\n",
      "Stochastic Gradient Descent(36630): loss=2.412484839796152\n",
      "Stochastic Gradient Descent(36631): loss=0.034302445682095686\n",
      "Stochastic Gradient Descent(36632): loss=2.7390342930730567\n",
      "Stochastic Gradient Descent(36633): loss=0.05858228295674431\n",
      "Stochastic Gradient Descent(36634): loss=2.709272209486207\n",
      "Stochastic Gradient Descent(36635): loss=0.002031960013787568\n",
      "Stochastic Gradient Descent(36636): loss=0.6482338457779774\n",
      "Stochastic Gradient Descent(36637): loss=0.0034678435243922904\n",
      "Stochastic Gradient Descent(36638): loss=1.3646810515416927\n",
      "Stochastic Gradient Descent(36639): loss=10.663227542807611\n",
      "Stochastic Gradient Descent(36640): loss=0.4391776899376726\n",
      "Stochastic Gradient Descent(36641): loss=8.464989607788802\n",
      "Stochastic Gradient Descent(36642): loss=1.0191833381776878\n",
      "Stochastic Gradient Descent(36643): loss=0.18089454899761095\n",
      "Stochastic Gradient Descent(36644): loss=1.2959134737507874\n",
      "Stochastic Gradient Descent(36645): loss=0.019856872098056484\n",
      "Stochastic Gradient Descent(36646): loss=3.0537130762233784\n",
      "Stochastic Gradient Descent(36647): loss=7.353691039883539\n",
      "Stochastic Gradient Descent(36648): loss=0.7242146593452059\n",
      "Stochastic Gradient Descent(36649): loss=0.1299741746406182\n",
      "Stochastic Gradient Descent(36650): loss=1.7628316033636706\n",
      "Stochastic Gradient Descent(36651): loss=13.272853907147429\n",
      "Stochastic Gradient Descent(36652): loss=4.052278554818779\n",
      "Stochastic Gradient Descent(36653): loss=1.0513172717835244\n",
      "Stochastic Gradient Descent(36654): loss=9.609067681112608\n",
      "Stochastic Gradient Descent(36655): loss=4.083053097326356\n",
      "Stochastic Gradient Descent(36656): loss=6.941708970218702\n",
      "Stochastic Gradient Descent(36657): loss=0.6013906093751349\n",
      "Stochastic Gradient Descent(36658): loss=9.99222906030993\n",
      "Stochastic Gradient Descent(36659): loss=1.43388270306677\n",
      "Stochastic Gradient Descent(36660): loss=4.008952995867129\n",
      "Stochastic Gradient Descent(36661): loss=17.86066802617127\n",
      "Stochastic Gradient Descent(36662): loss=4.265929617297215\n",
      "Stochastic Gradient Descent(36663): loss=25.00201604223267\n",
      "Stochastic Gradient Descent(36664): loss=4.599611932861707\n",
      "Stochastic Gradient Descent(36665): loss=17.063843726824746\n",
      "Stochastic Gradient Descent(36666): loss=3.720608651089819\n",
      "Stochastic Gradient Descent(36667): loss=18.234526312192102\n",
      "Stochastic Gradient Descent(36668): loss=133.76979447929773\n",
      "Stochastic Gradient Descent(36669): loss=49.47980566551666\n",
      "Stochastic Gradient Descent(36670): loss=16.4654566057792\n",
      "Stochastic Gradient Descent(36671): loss=46.58720506715907\n",
      "Stochastic Gradient Descent(36672): loss=0.8222589586249676\n",
      "Stochastic Gradient Descent(36673): loss=38.041213131885605\n",
      "Stochastic Gradient Descent(36674): loss=0.1438655500310063\n",
      "Stochastic Gradient Descent(36675): loss=0.06905809411881604\n",
      "Stochastic Gradient Descent(36676): loss=8.228770491743012\n",
      "Stochastic Gradient Descent(36677): loss=0.8409244666573317\n",
      "Stochastic Gradient Descent(36678): loss=11.126976408741369\n",
      "Stochastic Gradient Descent(36679): loss=0.12615038251400776\n",
      "Stochastic Gradient Descent(36680): loss=0.05713639887983628\n",
      "Stochastic Gradient Descent(36681): loss=8.138384287415386\n",
      "Stochastic Gradient Descent(36682): loss=1.2156028999043107\n",
      "Stochastic Gradient Descent(36683): loss=0.034678305311973814\n",
      "Stochastic Gradient Descent(36684): loss=0.0027795467802635538\n",
      "Stochastic Gradient Descent(36685): loss=2.8598989671240482\n",
      "Stochastic Gradient Descent(36686): loss=0.04357009750100316\n",
      "Stochastic Gradient Descent(36687): loss=0.14687128227673835\n",
      "Stochastic Gradient Descent(36688): loss=11.810305476632807\n",
      "Stochastic Gradient Descent(36689): loss=1.8633367492419104\n",
      "Stochastic Gradient Descent(36690): loss=1.9160595824015745\n",
      "Stochastic Gradient Descent(36691): loss=2.2869840104345087\n",
      "Stochastic Gradient Descent(36692): loss=0.20459481535556165\n",
      "Stochastic Gradient Descent(36693): loss=0.0036992977521770154\n",
      "Stochastic Gradient Descent(36694): loss=0.31185349917729294\n",
      "Stochastic Gradient Descent(36695): loss=0.004716698657962449\n",
      "Stochastic Gradient Descent(36696): loss=13.588973706293293\n",
      "Stochastic Gradient Descent(36697): loss=25.69546387364524\n",
      "Stochastic Gradient Descent(36698): loss=1.4658828770121783\n",
      "Stochastic Gradient Descent(36699): loss=2.3844708726854598\n",
      "Stochastic Gradient Descent(36700): loss=0.7919073985584737\n",
      "Stochastic Gradient Descent(36701): loss=2.881586275163544\n",
      "Stochastic Gradient Descent(36702): loss=10.906630248053311\n",
      "Stochastic Gradient Descent(36703): loss=3.4673318029345714\n",
      "Stochastic Gradient Descent(36704): loss=2.616702830258218\n",
      "Stochastic Gradient Descent(36705): loss=0.16377075414549142\n",
      "Stochastic Gradient Descent(36706): loss=0.07937012528576627\n",
      "Stochastic Gradient Descent(36707): loss=0.4327088968107654\n",
      "Stochastic Gradient Descent(36708): loss=3.066318177928195\n",
      "Stochastic Gradient Descent(36709): loss=17.448208921442475\n",
      "Stochastic Gradient Descent(36710): loss=13.899251236311231\n",
      "Stochastic Gradient Descent(36711): loss=0.19597856559531776\n",
      "Stochastic Gradient Descent(36712): loss=0.3382899030795857\n",
      "Stochastic Gradient Descent(36713): loss=1.4538315945558065\n",
      "Stochastic Gradient Descent(36714): loss=0.12377133438260148\n",
      "Stochastic Gradient Descent(36715): loss=1.5164972513381165\n",
      "Stochastic Gradient Descent(36716): loss=1.9497508686914486\n",
      "Stochastic Gradient Descent(36717): loss=0.4833911929157326\n",
      "Stochastic Gradient Descent(36718): loss=1.2726316830790834\n",
      "Stochastic Gradient Descent(36719): loss=6.96299816283665\n",
      "Stochastic Gradient Descent(36720): loss=0.3647192334957906\n",
      "Stochastic Gradient Descent(36721): loss=2.735671950982882\n",
      "Stochastic Gradient Descent(36722): loss=3.274290282300991\n",
      "Stochastic Gradient Descent(36723): loss=13.091369221209828\n",
      "Stochastic Gradient Descent(36724): loss=5.45205994841488\n",
      "Stochastic Gradient Descent(36725): loss=0.9066954552941474\n",
      "Stochastic Gradient Descent(36726): loss=1.5522273020264823\n",
      "Stochastic Gradient Descent(36727): loss=5.976009291729357\n",
      "Stochastic Gradient Descent(36728): loss=0.3251111031520787\n",
      "Stochastic Gradient Descent(36729): loss=0.6638551125256258\n",
      "Stochastic Gradient Descent(36730): loss=0.002568234314876296\n",
      "Stochastic Gradient Descent(36731): loss=8.293871141814805\n",
      "Stochastic Gradient Descent(36732): loss=0.2678589224269397\n",
      "Stochastic Gradient Descent(36733): loss=0.043060998555761704\n",
      "Stochastic Gradient Descent(36734): loss=0.28571753253606946\n",
      "Stochastic Gradient Descent(36735): loss=11.530917767715103\n",
      "Stochastic Gradient Descent(36736): loss=0.8784734784423811\n",
      "Stochastic Gradient Descent(36737): loss=2.3389581446680685\n",
      "Stochastic Gradient Descent(36738): loss=1.3286053861262104\n",
      "Stochastic Gradient Descent(36739): loss=0.5813879051089811\n",
      "Stochastic Gradient Descent(36740): loss=0.44183091999572705\n",
      "Stochastic Gradient Descent(36741): loss=0.009888679339183167\n",
      "Stochastic Gradient Descent(36742): loss=8.178672594825807\n",
      "Stochastic Gradient Descent(36743): loss=9.120799276236331\n",
      "Stochastic Gradient Descent(36744): loss=5.199200718533663\n",
      "Stochastic Gradient Descent(36745): loss=13.722436001180371\n",
      "Stochastic Gradient Descent(36746): loss=0.21776194424376347\n",
      "Stochastic Gradient Descent(36747): loss=0.16941916743677018\n",
      "Stochastic Gradient Descent(36748): loss=1.9907429970122994\n",
      "Stochastic Gradient Descent(36749): loss=2.0140483475555744\n",
      "Stochastic Gradient Descent(36750): loss=2.2878129921749215\n",
      "Stochastic Gradient Descent(36751): loss=0.16241256607854498\n",
      "Stochastic Gradient Descent(36752): loss=6.6016739366430555\n",
      "Stochastic Gradient Descent(36753): loss=6.033963296585357\n",
      "Stochastic Gradient Descent(36754): loss=32.31968381792636\n",
      "Stochastic Gradient Descent(36755): loss=3.8036613689160372\n",
      "Stochastic Gradient Descent(36756): loss=3.0680530808076596\n",
      "Stochastic Gradient Descent(36757): loss=4.489134620864286\n",
      "Stochastic Gradient Descent(36758): loss=1.0308397555611393\n",
      "Stochastic Gradient Descent(36759): loss=0.011162670984247388\n",
      "Stochastic Gradient Descent(36760): loss=0.9333760363360974\n",
      "Stochastic Gradient Descent(36761): loss=0.062460671726991004\n",
      "Stochastic Gradient Descent(36762): loss=7.883168003001028\n",
      "Stochastic Gradient Descent(36763): loss=0.003491731226709937\n",
      "Stochastic Gradient Descent(36764): loss=1.4820188473023983\n",
      "Stochastic Gradient Descent(36765): loss=6.713971316609514\n",
      "Stochastic Gradient Descent(36766): loss=20.662560191886115\n",
      "Stochastic Gradient Descent(36767): loss=0.5912506098920051\n",
      "Stochastic Gradient Descent(36768): loss=0.2422186724458877\n",
      "Stochastic Gradient Descent(36769): loss=3.805135623898005\n",
      "Stochastic Gradient Descent(36770): loss=10.317185701041355\n",
      "Stochastic Gradient Descent(36771): loss=6.240515030215925\n",
      "Stochastic Gradient Descent(36772): loss=0.4153142031660094\n",
      "Stochastic Gradient Descent(36773): loss=19.168840786739235\n",
      "Stochastic Gradient Descent(36774): loss=3.314748380290323\n",
      "Stochastic Gradient Descent(36775): loss=0.090991070825975\n",
      "Stochastic Gradient Descent(36776): loss=0.7289487519389622\n",
      "Stochastic Gradient Descent(36777): loss=2.608634551660184\n",
      "Stochastic Gradient Descent(36778): loss=0.13806200401861388\n",
      "Stochastic Gradient Descent(36779): loss=0.6306260591320811\n",
      "Stochastic Gradient Descent(36780): loss=11.68806147969809\n",
      "Stochastic Gradient Descent(36781): loss=0.008100766620445288\n",
      "Stochastic Gradient Descent(36782): loss=2.583250800515176\n",
      "Stochastic Gradient Descent(36783): loss=7.277184024176927\n",
      "Stochastic Gradient Descent(36784): loss=6.80273742563252\n",
      "Stochastic Gradient Descent(36785): loss=0.032002092817979305\n",
      "Stochastic Gradient Descent(36786): loss=2.4629945610204484\n",
      "Stochastic Gradient Descent(36787): loss=1.023232475658481\n",
      "Stochastic Gradient Descent(36788): loss=1.1490658804040996\n",
      "Stochastic Gradient Descent(36789): loss=3.5165892570773156\n",
      "Stochastic Gradient Descent(36790): loss=41.70763218952282\n",
      "Stochastic Gradient Descent(36791): loss=9.284152056617879\n",
      "Stochastic Gradient Descent(36792): loss=137.414681918558\n",
      "Stochastic Gradient Descent(36793): loss=2.2815331661546847\n",
      "Stochastic Gradient Descent(36794): loss=34.5814560251964\n",
      "Stochastic Gradient Descent(36795): loss=4.38740840677459\n",
      "Stochastic Gradient Descent(36796): loss=32.06922851400785\n",
      "Stochastic Gradient Descent(36797): loss=0.1116978912149923\n",
      "Stochastic Gradient Descent(36798): loss=0.9056912639397825\n",
      "Stochastic Gradient Descent(36799): loss=0.7985534324365446\n",
      "Stochastic Gradient Descent(36800): loss=0.026798784483249013\n",
      "Stochastic Gradient Descent(36801): loss=1.5248578374016903\n",
      "Stochastic Gradient Descent(36802): loss=0.06006737719447746\n",
      "Stochastic Gradient Descent(36803): loss=0.00039005359984311275\n",
      "Stochastic Gradient Descent(36804): loss=0.021580595640833262\n",
      "Stochastic Gradient Descent(36805): loss=0.9716871336572902\n",
      "Stochastic Gradient Descent(36806): loss=0.12006011795205272\n",
      "Stochastic Gradient Descent(36807): loss=0.041111161799280406\n",
      "Stochastic Gradient Descent(36808): loss=3.5205906758823304\n",
      "Stochastic Gradient Descent(36809): loss=1.5874690824835787\n",
      "Stochastic Gradient Descent(36810): loss=4.489265429477939\n",
      "Stochastic Gradient Descent(36811): loss=0.2615469072300586\n",
      "Stochastic Gradient Descent(36812): loss=5.113747016745332\n",
      "Stochastic Gradient Descent(36813): loss=0.07930537050337573\n",
      "Stochastic Gradient Descent(36814): loss=2.7342880278016684\n",
      "Stochastic Gradient Descent(36815): loss=2.663620708475537\n",
      "Stochastic Gradient Descent(36816): loss=0.20075985478124775\n",
      "Stochastic Gradient Descent(36817): loss=2.7023487936394672\n",
      "Stochastic Gradient Descent(36818): loss=2.2930315838234323\n",
      "Stochastic Gradient Descent(36819): loss=4.191506212289536\n",
      "Stochastic Gradient Descent(36820): loss=3.4599638844951075\n",
      "Stochastic Gradient Descent(36821): loss=3.6529690205705285\n",
      "Stochastic Gradient Descent(36822): loss=1.6442278629355285\n",
      "Stochastic Gradient Descent(36823): loss=2.1254082353046093\n",
      "Stochastic Gradient Descent(36824): loss=7.140840851679342\n",
      "Stochastic Gradient Descent(36825): loss=3.3656036828551867\n",
      "Stochastic Gradient Descent(36826): loss=1.3470088512872729\n",
      "Stochastic Gradient Descent(36827): loss=0.8049811200014142\n",
      "Stochastic Gradient Descent(36828): loss=2.188430472911039\n",
      "Stochastic Gradient Descent(36829): loss=9.05472367554679\n",
      "Stochastic Gradient Descent(36830): loss=0.06609776404501372\n",
      "Stochastic Gradient Descent(36831): loss=8.0236926138484\n",
      "Stochastic Gradient Descent(36832): loss=4.174292448007639\n",
      "Stochastic Gradient Descent(36833): loss=0.08991864884914352\n",
      "Stochastic Gradient Descent(36834): loss=4.5431941197146015\n",
      "Stochastic Gradient Descent(36835): loss=0.05056920677863591\n",
      "Stochastic Gradient Descent(36836): loss=2.555231359949804\n",
      "Stochastic Gradient Descent(36837): loss=2.472305540643677\n",
      "Stochastic Gradient Descent(36838): loss=0.626181711669687\n",
      "Stochastic Gradient Descent(36839): loss=2.9335502567864022\n",
      "Stochastic Gradient Descent(36840): loss=1.6569382007475462\n",
      "Stochastic Gradient Descent(36841): loss=19.383577007878586\n",
      "Stochastic Gradient Descent(36842): loss=4.023742157316896\n",
      "Stochastic Gradient Descent(36843): loss=1.2231830014224474\n",
      "Stochastic Gradient Descent(36844): loss=18.376047305795808\n",
      "Stochastic Gradient Descent(36845): loss=0.4107775815246058\n",
      "Stochastic Gradient Descent(36846): loss=0.12122908865110436\n",
      "Stochastic Gradient Descent(36847): loss=7.017796595327724\n",
      "Stochastic Gradient Descent(36848): loss=9.674427535452873\n",
      "Stochastic Gradient Descent(36849): loss=0.23621951672480407\n",
      "Stochastic Gradient Descent(36850): loss=6.788750836656688\n",
      "Stochastic Gradient Descent(36851): loss=0.006018486826489556\n",
      "Stochastic Gradient Descent(36852): loss=0.03441456284353256\n",
      "Stochastic Gradient Descent(36853): loss=1.018484180990315\n",
      "Stochastic Gradient Descent(36854): loss=3.792500125739788\n",
      "Stochastic Gradient Descent(36855): loss=4.5914542301379075\n",
      "Stochastic Gradient Descent(36856): loss=1.522495040307908\n",
      "Stochastic Gradient Descent(36857): loss=0.023377094063689142\n",
      "Stochastic Gradient Descent(36858): loss=0.08872611144781363\n",
      "Stochastic Gradient Descent(36859): loss=0.25381438275937573\n",
      "Stochastic Gradient Descent(36860): loss=6.689222564818875\n",
      "Stochastic Gradient Descent(36861): loss=3.6367908905668247\n",
      "Stochastic Gradient Descent(36862): loss=12.091494571731015\n",
      "Stochastic Gradient Descent(36863): loss=2.7833804967062403\n",
      "Stochastic Gradient Descent(36864): loss=1.0969859994147053\n",
      "Stochastic Gradient Descent(36865): loss=8.88791527036227\n",
      "Stochastic Gradient Descent(36866): loss=2.034406580324485\n",
      "Stochastic Gradient Descent(36867): loss=0.159642906584487\n",
      "Stochastic Gradient Descent(36868): loss=0.007967087585602191\n",
      "Stochastic Gradient Descent(36869): loss=1.7442031969121672\n",
      "Stochastic Gradient Descent(36870): loss=41.24805978711619\n",
      "Stochastic Gradient Descent(36871): loss=2.3546348609958727\n",
      "Stochastic Gradient Descent(36872): loss=0.7734823002119975\n",
      "Stochastic Gradient Descent(36873): loss=0.0047377571233494\n",
      "Stochastic Gradient Descent(36874): loss=0.012582717325685047\n",
      "Stochastic Gradient Descent(36875): loss=6.149127756627731\n",
      "Stochastic Gradient Descent(36876): loss=8.65398766329729\n",
      "Stochastic Gradient Descent(36877): loss=11.043679704651254\n",
      "Stochastic Gradient Descent(36878): loss=18.264796541183188\n",
      "Stochastic Gradient Descent(36879): loss=0.76222557744771\n",
      "Stochastic Gradient Descent(36880): loss=0.8025413894059515\n",
      "Stochastic Gradient Descent(36881): loss=0.217583852788397\n",
      "Stochastic Gradient Descent(36882): loss=0.17173429112021885\n",
      "Stochastic Gradient Descent(36883): loss=3.7142996104683834\n",
      "Stochastic Gradient Descent(36884): loss=4.140979578256642\n",
      "Stochastic Gradient Descent(36885): loss=4.71584149319504\n",
      "Stochastic Gradient Descent(36886): loss=0.050875581837250716\n",
      "Stochastic Gradient Descent(36887): loss=1.5264531555778051\n",
      "Stochastic Gradient Descent(36888): loss=0.043953109847104245\n",
      "Stochastic Gradient Descent(36889): loss=0.5806320704904593\n",
      "Stochastic Gradient Descent(36890): loss=3.9780787920591067\n",
      "Stochastic Gradient Descent(36891): loss=6.72414000755854\n",
      "Stochastic Gradient Descent(36892): loss=11.202612536482823\n",
      "Stochastic Gradient Descent(36893): loss=2.3611516662574843\n",
      "Stochastic Gradient Descent(36894): loss=2.060166709725036\n",
      "Stochastic Gradient Descent(36895): loss=1.0806463122532397\n",
      "Stochastic Gradient Descent(36896): loss=0.13523966647410732\n",
      "Stochastic Gradient Descent(36897): loss=3.7761000681411443\n",
      "Stochastic Gradient Descent(36898): loss=0.2627885012038108\n",
      "Stochastic Gradient Descent(36899): loss=5.8390602974911126\n",
      "Stochastic Gradient Descent(36900): loss=0.5847680580880512\n",
      "Stochastic Gradient Descent(36901): loss=4.609550111721508\n",
      "Stochastic Gradient Descent(36902): loss=11.14849906335975\n",
      "Stochastic Gradient Descent(36903): loss=0.6817966639895418\n",
      "Stochastic Gradient Descent(36904): loss=6.889136037805496\n",
      "Stochastic Gradient Descent(36905): loss=2.151461192998803\n",
      "Stochastic Gradient Descent(36906): loss=0.6044124383661704\n",
      "Stochastic Gradient Descent(36907): loss=0.04219092773186059\n",
      "Stochastic Gradient Descent(36908): loss=1.095953462426499\n",
      "Stochastic Gradient Descent(36909): loss=17.727897422103105\n",
      "Stochastic Gradient Descent(36910): loss=0.702292732368271\n",
      "Stochastic Gradient Descent(36911): loss=1.9109776900063211\n",
      "Stochastic Gradient Descent(36912): loss=0.030707827319116927\n",
      "Stochastic Gradient Descent(36913): loss=15.034457489939227\n",
      "Stochastic Gradient Descent(36914): loss=0.43149425936213565\n",
      "Stochastic Gradient Descent(36915): loss=2.486865938871565\n",
      "Stochastic Gradient Descent(36916): loss=2.6975239854956206\n",
      "Stochastic Gradient Descent(36917): loss=1.3186165934555598\n",
      "Stochastic Gradient Descent(36918): loss=1.574032407246465\n",
      "Stochastic Gradient Descent(36919): loss=3.365471317281867\n",
      "Stochastic Gradient Descent(36920): loss=0.2091150236022861\n",
      "Stochastic Gradient Descent(36921): loss=1.4484155667366976\n",
      "Stochastic Gradient Descent(36922): loss=0.20471611983910837\n",
      "Stochastic Gradient Descent(36923): loss=7.3124059666078205\n",
      "Stochastic Gradient Descent(36924): loss=8.028011647226549\n",
      "Stochastic Gradient Descent(36925): loss=0.652071062106059\n",
      "Stochastic Gradient Descent(36926): loss=0.01108361038697443\n",
      "Stochastic Gradient Descent(36927): loss=0.13115762420002433\n",
      "Stochastic Gradient Descent(36928): loss=8.874077985464924\n",
      "Stochastic Gradient Descent(36929): loss=0.02454830868573031\n",
      "Stochastic Gradient Descent(36930): loss=0.0020063249482284756\n",
      "Stochastic Gradient Descent(36931): loss=1.0335425990571396\n",
      "Stochastic Gradient Descent(36932): loss=0.3027363436583259\n",
      "Stochastic Gradient Descent(36933): loss=1.5090488260624504\n",
      "Stochastic Gradient Descent(36934): loss=0.13317851486903767\n",
      "Stochastic Gradient Descent(36935): loss=2.9814069244688146\n",
      "Stochastic Gradient Descent(36936): loss=0.08455095614931908\n",
      "Stochastic Gradient Descent(36937): loss=4.246460826311682\n",
      "Stochastic Gradient Descent(36938): loss=0.04458060600940869\n",
      "Stochastic Gradient Descent(36939): loss=0.02906122489809577\n",
      "Stochastic Gradient Descent(36940): loss=0.012231935389671867\n",
      "Stochastic Gradient Descent(36941): loss=17.587004258845965\n",
      "Stochastic Gradient Descent(36942): loss=8.26925139949322\n",
      "Stochastic Gradient Descent(36943): loss=5.899725890659647\n",
      "Stochastic Gradient Descent(36944): loss=0.24856332273160137\n",
      "Stochastic Gradient Descent(36945): loss=1.5800101886843991\n",
      "Stochastic Gradient Descent(36946): loss=0.0012646012083993578\n",
      "Stochastic Gradient Descent(36947): loss=7.77109638429215\n",
      "Stochastic Gradient Descent(36948): loss=12.384987265194354\n",
      "Stochastic Gradient Descent(36949): loss=0.31746609783347207\n",
      "Stochastic Gradient Descent(36950): loss=0.711407485591039\n",
      "Stochastic Gradient Descent(36951): loss=13.515603586127089\n",
      "Stochastic Gradient Descent(36952): loss=32.04823436379667\n",
      "Stochastic Gradient Descent(36953): loss=0.07012467444798191\n",
      "Stochastic Gradient Descent(36954): loss=2.6599092260598054\n",
      "Stochastic Gradient Descent(36955): loss=0.932065063102842\n",
      "Stochastic Gradient Descent(36956): loss=0.7159431055901428\n",
      "Stochastic Gradient Descent(36957): loss=3.9277739302961243\n",
      "Stochastic Gradient Descent(36958): loss=51.23612942493348\n",
      "Stochastic Gradient Descent(36959): loss=0.7961897848687645\n",
      "Stochastic Gradient Descent(36960): loss=2.8618198194497304\n",
      "Stochastic Gradient Descent(36961): loss=14.432205051492053\n",
      "Stochastic Gradient Descent(36962): loss=0.8276080312989659\n",
      "Stochastic Gradient Descent(36963): loss=0.5537686419342749\n",
      "Stochastic Gradient Descent(36964): loss=0.48771990796714926\n",
      "Stochastic Gradient Descent(36965): loss=0.011961133834398108\n",
      "Stochastic Gradient Descent(36966): loss=0.9831101077187343\n",
      "Stochastic Gradient Descent(36967): loss=5.1559974766494365\n",
      "Stochastic Gradient Descent(36968): loss=2.146352211900082\n",
      "Stochastic Gradient Descent(36969): loss=5.2176206445244135\n",
      "Stochastic Gradient Descent(36970): loss=3.8209180713535034\n",
      "Stochastic Gradient Descent(36971): loss=3.1700635876560486\n",
      "Stochastic Gradient Descent(36972): loss=41.54218954933969\n",
      "Stochastic Gradient Descent(36973): loss=1.044292563438168\n",
      "Stochastic Gradient Descent(36974): loss=0.33898839644938533\n",
      "Stochastic Gradient Descent(36975): loss=0.6949053044805018\n",
      "Stochastic Gradient Descent(36976): loss=2.504465543805049\n",
      "Stochastic Gradient Descent(36977): loss=1.9672497564146485\n",
      "Stochastic Gradient Descent(36978): loss=8.355298705923968\n",
      "Stochastic Gradient Descent(36979): loss=18.253486561526355\n",
      "Stochastic Gradient Descent(36980): loss=0.04777152661908491\n",
      "Stochastic Gradient Descent(36981): loss=1.3415791473699958\n",
      "Stochastic Gradient Descent(36982): loss=27.25783658957572\n",
      "Stochastic Gradient Descent(36983): loss=16.43900230657819\n",
      "Stochastic Gradient Descent(36984): loss=4.848663940399825\n",
      "Stochastic Gradient Descent(36985): loss=1.053418339179679\n",
      "Stochastic Gradient Descent(36986): loss=9.565557555959055\n",
      "Stochastic Gradient Descent(36987): loss=0.09017103643831656\n",
      "Stochastic Gradient Descent(36988): loss=1.399030641185426\n",
      "Stochastic Gradient Descent(36989): loss=1.0750230891742043\n",
      "Stochastic Gradient Descent(36990): loss=14.892717733173383\n",
      "Stochastic Gradient Descent(36991): loss=7.859783174739441\n",
      "Stochastic Gradient Descent(36992): loss=1.4207028651938627\n",
      "Stochastic Gradient Descent(36993): loss=12.941697746565772\n",
      "Stochastic Gradient Descent(36994): loss=0.017018459772097847\n",
      "Stochastic Gradient Descent(36995): loss=10.758943239087769\n",
      "Stochastic Gradient Descent(36996): loss=0.0018532291264508468\n",
      "Stochastic Gradient Descent(36997): loss=0.10287710791386581\n",
      "Stochastic Gradient Descent(36998): loss=0.007503183217770499\n",
      "Stochastic Gradient Descent(36999): loss=3.609443717008272\n",
      "Stochastic Gradient Descent(37000): loss=4.742674282832448\n",
      "Stochastic Gradient Descent(37001): loss=34.93899664874194\n",
      "Stochastic Gradient Descent(37002): loss=7.41487029347055\n",
      "Stochastic Gradient Descent(37003): loss=9.227459024295548\n",
      "Stochastic Gradient Descent(37004): loss=8.648376041593542\n",
      "Stochastic Gradient Descent(37005): loss=0.028856691383133147\n",
      "Stochastic Gradient Descent(37006): loss=0.752839519653102\n",
      "Stochastic Gradient Descent(37007): loss=0.23664977455393657\n",
      "Stochastic Gradient Descent(37008): loss=28.3538660646169\n",
      "Stochastic Gradient Descent(37009): loss=8.312456844131546\n",
      "Stochastic Gradient Descent(37010): loss=0.5823511162385154\n",
      "Stochastic Gradient Descent(37011): loss=0.30506341481281035\n",
      "Stochastic Gradient Descent(37012): loss=2.2389906604483993\n",
      "Stochastic Gradient Descent(37013): loss=0.05422836876990299\n",
      "Stochastic Gradient Descent(37014): loss=0.1509722524150238\n",
      "Stochastic Gradient Descent(37015): loss=6.605535516070177\n",
      "Stochastic Gradient Descent(37016): loss=0.06235196413702559\n",
      "Stochastic Gradient Descent(37017): loss=8.825599680983283\n",
      "Stochastic Gradient Descent(37018): loss=2.3313085654327854\n",
      "Stochastic Gradient Descent(37019): loss=1.4158300846294467\n",
      "Stochastic Gradient Descent(37020): loss=4.062292905183643\n",
      "Stochastic Gradient Descent(37021): loss=0.0003174320695631519\n",
      "Stochastic Gradient Descent(37022): loss=0.5435516850640205\n",
      "Stochastic Gradient Descent(37023): loss=4.660412491293745\n",
      "Stochastic Gradient Descent(37024): loss=4.592504133340084\n",
      "Stochastic Gradient Descent(37025): loss=12.711348246456707\n",
      "Stochastic Gradient Descent(37026): loss=2.9465030634645073\n",
      "Stochastic Gradient Descent(37027): loss=2.8789829144082315\n",
      "Stochastic Gradient Descent(37028): loss=15.426919785708868\n",
      "Stochastic Gradient Descent(37029): loss=0.26475637242838995\n",
      "Stochastic Gradient Descent(37030): loss=1.7253482086398402\n",
      "Stochastic Gradient Descent(37031): loss=2.6287858502276964\n",
      "Stochastic Gradient Descent(37032): loss=2.875833024079217\n",
      "Stochastic Gradient Descent(37033): loss=0.6315637200110954\n",
      "Stochastic Gradient Descent(37034): loss=0.31056337545865814\n",
      "Stochastic Gradient Descent(37035): loss=1.154039540538062\n",
      "Stochastic Gradient Descent(37036): loss=7.3802121430806205\n",
      "Stochastic Gradient Descent(37037): loss=0.11821704799080955\n",
      "Stochastic Gradient Descent(37038): loss=3.2786587911827816\n",
      "Stochastic Gradient Descent(37039): loss=4.977863654846308\n",
      "Stochastic Gradient Descent(37040): loss=0.0009519558195821741\n",
      "Stochastic Gradient Descent(37041): loss=8.087641224626301\n",
      "Stochastic Gradient Descent(37042): loss=0.026747369808295174\n",
      "Stochastic Gradient Descent(37043): loss=0.9198668341719024\n",
      "Stochastic Gradient Descent(37044): loss=0.08780642319999207\n",
      "Stochastic Gradient Descent(37045): loss=0.18403761022291282\n",
      "Stochastic Gradient Descent(37046): loss=15.293053441148892\n",
      "Stochastic Gradient Descent(37047): loss=5.60468619056276\n",
      "Stochastic Gradient Descent(37048): loss=0.015283026699559173\n",
      "Stochastic Gradient Descent(37049): loss=3.2116794859523288\n",
      "Stochastic Gradient Descent(37050): loss=24.17672653177806\n",
      "Stochastic Gradient Descent(37051): loss=12.380153908130747\n",
      "Stochastic Gradient Descent(37052): loss=1.2373719968260002\n",
      "Stochastic Gradient Descent(37053): loss=3.5020048722614723\n",
      "Stochastic Gradient Descent(37054): loss=0.09185425431013118\n",
      "Stochastic Gradient Descent(37055): loss=1.5634977398642174\n",
      "Stochastic Gradient Descent(37056): loss=1.4226640665992816\n",
      "Stochastic Gradient Descent(37057): loss=3.2860268842543516\n",
      "Stochastic Gradient Descent(37058): loss=4.349182953429734\n",
      "Stochastic Gradient Descent(37059): loss=5.357259471966985\n",
      "Stochastic Gradient Descent(37060): loss=0.7549340762030714\n",
      "Stochastic Gradient Descent(37061): loss=0.31754863857869653\n",
      "Stochastic Gradient Descent(37062): loss=27.167522820090202\n",
      "Stochastic Gradient Descent(37063): loss=17.816633188222077\n",
      "Stochastic Gradient Descent(37064): loss=0.02601149542420531\n",
      "Stochastic Gradient Descent(37065): loss=1.8094262438244284\n",
      "Stochastic Gradient Descent(37066): loss=1.0219676854194053\n",
      "Stochastic Gradient Descent(37067): loss=2.011909743498975\n",
      "Stochastic Gradient Descent(37068): loss=0.0007374494558778845\n",
      "Stochastic Gradient Descent(37069): loss=12.666050561621162\n",
      "Stochastic Gradient Descent(37070): loss=0.6506121459316608\n",
      "Stochastic Gradient Descent(37071): loss=8.327683190047074\n",
      "Stochastic Gradient Descent(37072): loss=11.346908299720393\n",
      "Stochastic Gradient Descent(37073): loss=109.83117989436391\n",
      "Stochastic Gradient Descent(37074): loss=22.986474928868894\n",
      "Stochastic Gradient Descent(37075): loss=46.19571191753133\n",
      "Stochastic Gradient Descent(37076): loss=156.43326034479546\n",
      "Stochastic Gradient Descent(37077): loss=0.05882454149285996\n",
      "Stochastic Gradient Descent(37078): loss=0.826504320341435\n",
      "Stochastic Gradient Descent(37079): loss=20.0166970779648\n",
      "Stochastic Gradient Descent(37080): loss=0.07060574361888473\n",
      "Stochastic Gradient Descent(37081): loss=0.32343756545054353\n",
      "Stochastic Gradient Descent(37082): loss=22.29925016812477\n",
      "Stochastic Gradient Descent(37083): loss=0.0038503522325835987\n",
      "Stochastic Gradient Descent(37084): loss=2.416749211873456\n",
      "Stochastic Gradient Descent(37085): loss=0.055204878089741546\n",
      "Stochastic Gradient Descent(37086): loss=0.019761791836770373\n",
      "Stochastic Gradient Descent(37087): loss=3.7677320020062686\n",
      "Stochastic Gradient Descent(37088): loss=4.514068741155179\n",
      "Stochastic Gradient Descent(37089): loss=5.9887374545585015\n",
      "Stochastic Gradient Descent(37090): loss=1.645640598829604\n",
      "Stochastic Gradient Descent(37091): loss=1.0087657689342207\n",
      "Stochastic Gradient Descent(37092): loss=2.6331165378847707\n",
      "Stochastic Gradient Descent(37093): loss=0.4163478348458743\n",
      "Stochastic Gradient Descent(37094): loss=1.177280508266546\n",
      "Stochastic Gradient Descent(37095): loss=0.44061105458677813\n",
      "Stochastic Gradient Descent(37096): loss=0.6323874010878595\n",
      "Stochastic Gradient Descent(37097): loss=3.008954331206666\n",
      "Stochastic Gradient Descent(37098): loss=0.21137844408206927\n",
      "Stochastic Gradient Descent(37099): loss=2.019388334686143\n",
      "Stochastic Gradient Descent(37100): loss=0.06926792935388221\n",
      "Stochastic Gradient Descent(37101): loss=0.2161344989852275\n",
      "Stochastic Gradient Descent(37102): loss=0.001733645936880694\n",
      "Stochastic Gradient Descent(37103): loss=0.32071416407914033\n",
      "Stochastic Gradient Descent(37104): loss=2.9359759128178577\n",
      "Stochastic Gradient Descent(37105): loss=0.5865424098249172\n",
      "Stochastic Gradient Descent(37106): loss=1.3447597562740936\n",
      "Stochastic Gradient Descent(37107): loss=0.0009157699915065585\n",
      "Stochastic Gradient Descent(37108): loss=9.53330652832702\n",
      "Stochastic Gradient Descent(37109): loss=5.121767010153918\n",
      "Stochastic Gradient Descent(37110): loss=0.0026783048731213706\n",
      "Stochastic Gradient Descent(37111): loss=1.6044184907821724\n",
      "Stochastic Gradient Descent(37112): loss=2.39610095678098\n",
      "Stochastic Gradient Descent(37113): loss=1.2491649095421093\n",
      "Stochastic Gradient Descent(37114): loss=6.943478304847744\n",
      "Stochastic Gradient Descent(37115): loss=3.968156611027421\n",
      "Stochastic Gradient Descent(37116): loss=37.065584125511975\n",
      "Stochastic Gradient Descent(37117): loss=0.0861809753489434\n",
      "Stochastic Gradient Descent(37118): loss=0.9361436172663785\n",
      "Stochastic Gradient Descent(37119): loss=0.0006764807745103971\n",
      "Stochastic Gradient Descent(37120): loss=21.861130942989742\n",
      "Stochastic Gradient Descent(37121): loss=51.858540293441266\n",
      "Stochastic Gradient Descent(37122): loss=3.8871061851393724\n",
      "Stochastic Gradient Descent(37123): loss=9.361612475626737\n",
      "Stochastic Gradient Descent(37124): loss=3.5422281188318654\n",
      "Stochastic Gradient Descent(37125): loss=0.30671695767532053\n",
      "Stochastic Gradient Descent(37126): loss=2.7625524039091585\n",
      "Stochastic Gradient Descent(37127): loss=34.30535086626617\n",
      "Stochastic Gradient Descent(37128): loss=16.030539527576018\n",
      "Stochastic Gradient Descent(37129): loss=7.559325729971232\n",
      "Stochastic Gradient Descent(37130): loss=12.195299336060176\n",
      "Stochastic Gradient Descent(37131): loss=3.44915108834043\n",
      "Stochastic Gradient Descent(37132): loss=11.930538247857477\n",
      "Stochastic Gradient Descent(37133): loss=6.5819530168056675\n",
      "Stochastic Gradient Descent(37134): loss=5.783399368520036\n",
      "Stochastic Gradient Descent(37135): loss=7.325508843743534\n",
      "Stochastic Gradient Descent(37136): loss=15.808968019844599\n",
      "Stochastic Gradient Descent(37137): loss=5.465749349773983\n",
      "Stochastic Gradient Descent(37138): loss=10.90825620327858\n",
      "Stochastic Gradient Descent(37139): loss=13.998595436862082\n",
      "Stochastic Gradient Descent(37140): loss=6.40284443644588\n",
      "Stochastic Gradient Descent(37141): loss=23.678610165531076\n",
      "Stochastic Gradient Descent(37142): loss=1.9482439647075627\n",
      "Stochastic Gradient Descent(37143): loss=0.6392240036433324\n",
      "Stochastic Gradient Descent(37144): loss=0.9802861361646085\n",
      "Stochastic Gradient Descent(37145): loss=4.762944956517424\n",
      "Stochastic Gradient Descent(37146): loss=1.0135424738924814\n",
      "Stochastic Gradient Descent(37147): loss=11.486874753066099\n",
      "Stochastic Gradient Descent(37148): loss=1.849608923672883\n",
      "Stochastic Gradient Descent(37149): loss=1.0546795651640546\n",
      "Stochastic Gradient Descent(37150): loss=0.018012995223708576\n",
      "Stochastic Gradient Descent(37151): loss=1.0990049038753018\n",
      "Stochastic Gradient Descent(37152): loss=1.7840504335486507\n",
      "Stochastic Gradient Descent(37153): loss=2.2964650201149004\n",
      "Stochastic Gradient Descent(37154): loss=3.5053329366695136\n",
      "Stochastic Gradient Descent(37155): loss=0.0317297509560558\n",
      "Stochastic Gradient Descent(37156): loss=0.14880385253432682\n",
      "Stochastic Gradient Descent(37157): loss=2.573531356051683\n",
      "Stochastic Gradient Descent(37158): loss=0.14229105344865245\n",
      "Stochastic Gradient Descent(37159): loss=1.4504266126133118\n",
      "Stochastic Gradient Descent(37160): loss=0.07119548184186784\n",
      "Stochastic Gradient Descent(37161): loss=0.10417776187773865\n",
      "Stochastic Gradient Descent(37162): loss=5.582147868768775\n",
      "Stochastic Gradient Descent(37163): loss=0.030730336905999016\n",
      "Stochastic Gradient Descent(37164): loss=5.296021794641484\n",
      "Stochastic Gradient Descent(37165): loss=1.0545523653031794\n",
      "Stochastic Gradient Descent(37166): loss=0.5594044532171426\n",
      "Stochastic Gradient Descent(37167): loss=5.129848747561663\n",
      "Stochastic Gradient Descent(37168): loss=4.800638773700169\n",
      "Stochastic Gradient Descent(37169): loss=6.270813511310918\n",
      "Stochastic Gradient Descent(37170): loss=0.056913171949501386\n",
      "Stochastic Gradient Descent(37171): loss=3.1526863086262886\n",
      "Stochastic Gradient Descent(37172): loss=2.7312546876099324\n",
      "Stochastic Gradient Descent(37173): loss=0.7363613031606165\n",
      "Stochastic Gradient Descent(37174): loss=0.1268441287667599\n",
      "Stochastic Gradient Descent(37175): loss=5.2373361454910485\n",
      "Stochastic Gradient Descent(37176): loss=0.36802955889478867\n",
      "Stochastic Gradient Descent(37177): loss=0.11973129874835017\n",
      "Stochastic Gradient Descent(37178): loss=0.22822566028876\n",
      "Stochastic Gradient Descent(37179): loss=0.8201192137114455\n",
      "Stochastic Gradient Descent(37180): loss=2.4037722035150275\n",
      "Stochastic Gradient Descent(37181): loss=0.1655240529606702\n",
      "Stochastic Gradient Descent(37182): loss=3.3028027953272425\n",
      "Stochastic Gradient Descent(37183): loss=1.3165310433427055\n",
      "Stochastic Gradient Descent(37184): loss=1.4223991654424226\n",
      "Stochastic Gradient Descent(37185): loss=1.4121657687738594\n",
      "Stochastic Gradient Descent(37186): loss=0.41093539807547164\n",
      "Stochastic Gradient Descent(37187): loss=0.09389504051377595\n",
      "Stochastic Gradient Descent(37188): loss=0.039193254829063336\n",
      "Stochastic Gradient Descent(37189): loss=0.04898258848209605\n",
      "Stochastic Gradient Descent(37190): loss=0.06382215808115181\n",
      "Stochastic Gradient Descent(37191): loss=0.25500303677086306\n",
      "Stochastic Gradient Descent(37192): loss=0.0002992497505075606\n",
      "Stochastic Gradient Descent(37193): loss=0.6508438074837762\n",
      "Stochastic Gradient Descent(37194): loss=7.467002918345695\n",
      "Stochastic Gradient Descent(37195): loss=6.787398925469847\n",
      "Stochastic Gradient Descent(37196): loss=1.0038428786033775\n",
      "Stochastic Gradient Descent(37197): loss=2.312890638516784\n",
      "Stochastic Gradient Descent(37198): loss=7.212582627428101\n",
      "Stochastic Gradient Descent(37199): loss=0.10059778823421296\n",
      "Stochastic Gradient Descent(37200): loss=0.2543251963561768\n",
      "Stochastic Gradient Descent(37201): loss=4.039250153106579\n",
      "Stochastic Gradient Descent(37202): loss=2.0440230717534984\n",
      "Stochastic Gradient Descent(37203): loss=0.26983614055313176\n",
      "Stochastic Gradient Descent(37204): loss=0.5702930388188993\n",
      "Stochastic Gradient Descent(37205): loss=0.053052708610511555\n",
      "Stochastic Gradient Descent(37206): loss=0.18865614246355825\n",
      "Stochastic Gradient Descent(37207): loss=4.365580389320711\n",
      "Stochastic Gradient Descent(37208): loss=0.18521948527299395\n",
      "Stochastic Gradient Descent(37209): loss=0.40729453537791227\n",
      "Stochastic Gradient Descent(37210): loss=0.20333249804718972\n",
      "Stochastic Gradient Descent(37211): loss=0.41079842268434913\n",
      "Stochastic Gradient Descent(37212): loss=7.009677474050734\n",
      "Stochastic Gradient Descent(37213): loss=0.3510738516546014\n",
      "Stochastic Gradient Descent(37214): loss=2.6102153149784577\n",
      "Stochastic Gradient Descent(37215): loss=7.9720779256921395\n",
      "Stochastic Gradient Descent(37216): loss=0.006949655351897728\n",
      "Stochastic Gradient Descent(37217): loss=8.263549688926334\n",
      "Stochastic Gradient Descent(37218): loss=4.6161091575014055\n",
      "Stochastic Gradient Descent(37219): loss=1.0725534990451397\n",
      "Stochastic Gradient Descent(37220): loss=1.2014466912860648\n",
      "Stochastic Gradient Descent(37221): loss=0.09557081404948933\n",
      "Stochastic Gradient Descent(37222): loss=0.7642690065121287\n",
      "Stochastic Gradient Descent(37223): loss=0.5164304937397123\n",
      "Stochastic Gradient Descent(37224): loss=0.30405242746653727\n",
      "Stochastic Gradient Descent(37225): loss=0.9585831113402626\n",
      "Stochastic Gradient Descent(37226): loss=0.0009001453832692166\n",
      "Stochastic Gradient Descent(37227): loss=9.986771870964423\n",
      "Stochastic Gradient Descent(37228): loss=0.21621000091713696\n",
      "Stochastic Gradient Descent(37229): loss=0.1687322919346934\n",
      "Stochastic Gradient Descent(37230): loss=8.639934727816716\n",
      "Stochastic Gradient Descent(37231): loss=0.6110591726047568\n",
      "Stochastic Gradient Descent(37232): loss=1.0985830252287976\n",
      "Stochastic Gradient Descent(37233): loss=2.9502250261604526\n",
      "Stochastic Gradient Descent(37234): loss=1.6376998813947736\n",
      "Stochastic Gradient Descent(37235): loss=1.8627160948435844\n",
      "Stochastic Gradient Descent(37236): loss=0.6738933973950902\n",
      "Stochastic Gradient Descent(37237): loss=1.6866167583048415\n",
      "Stochastic Gradient Descent(37238): loss=3.6235597633037626\n",
      "Stochastic Gradient Descent(37239): loss=0.07087891103178591\n",
      "Stochastic Gradient Descent(37240): loss=1.2531939952627014\n",
      "Stochastic Gradient Descent(37241): loss=0.009356268914781673\n",
      "Stochastic Gradient Descent(37242): loss=1.0641695380141971\n",
      "Stochastic Gradient Descent(37243): loss=7.879008636695731\n",
      "Stochastic Gradient Descent(37244): loss=12.33077015142956\n",
      "Stochastic Gradient Descent(37245): loss=0.6552825855297101\n",
      "Stochastic Gradient Descent(37246): loss=0.8163429789666805\n",
      "Stochastic Gradient Descent(37247): loss=12.7435682870552\n",
      "Stochastic Gradient Descent(37248): loss=1.1675987944667063\n",
      "Stochastic Gradient Descent(37249): loss=14.08974818520817\n",
      "Stochastic Gradient Descent(37250): loss=20.838606002028452\n",
      "Stochastic Gradient Descent(37251): loss=1.1325925270190063\n",
      "Stochastic Gradient Descent(37252): loss=25.43730967969235\n",
      "Stochastic Gradient Descent(37253): loss=8.630991508473853\n",
      "Stochastic Gradient Descent(37254): loss=1.5632243561833974\n",
      "Stochastic Gradient Descent(37255): loss=0.5231141609622512\n",
      "Stochastic Gradient Descent(37256): loss=0.0005324538948074652\n",
      "Stochastic Gradient Descent(37257): loss=0.48446678778077257\n",
      "Stochastic Gradient Descent(37258): loss=0.03337047537107408\n",
      "Stochastic Gradient Descent(37259): loss=0.1602413454404815\n",
      "Stochastic Gradient Descent(37260): loss=2.355190201575893\n",
      "Stochastic Gradient Descent(37261): loss=4.65762104431154\n",
      "Stochastic Gradient Descent(37262): loss=0.5197771701736579\n",
      "Stochastic Gradient Descent(37263): loss=8.912963793862385\n",
      "Stochastic Gradient Descent(37264): loss=0.3514204532368527\n",
      "Stochastic Gradient Descent(37265): loss=1.4692146376620043\n",
      "Stochastic Gradient Descent(37266): loss=0.5038605575542319\n",
      "Stochastic Gradient Descent(37267): loss=5.122490444082492\n",
      "Stochastic Gradient Descent(37268): loss=0.4599879691993538\n",
      "Stochastic Gradient Descent(37269): loss=0.15018884243337138\n",
      "Stochastic Gradient Descent(37270): loss=0.19792279919975397\n",
      "Stochastic Gradient Descent(37271): loss=6.346153646966718\n",
      "Stochastic Gradient Descent(37272): loss=4.80504155795245\n",
      "Stochastic Gradient Descent(37273): loss=0.09878747861912071\n",
      "Stochastic Gradient Descent(37274): loss=2.5845630238881117\n",
      "Stochastic Gradient Descent(37275): loss=4.350050782764261\n",
      "Stochastic Gradient Descent(37276): loss=0.3263963786821473\n",
      "Stochastic Gradient Descent(37277): loss=0.8227459158629058\n",
      "Stochastic Gradient Descent(37278): loss=1.372082636318261\n",
      "Stochastic Gradient Descent(37279): loss=1.8330826885190004\n",
      "Stochastic Gradient Descent(37280): loss=0.7135172656855875\n",
      "Stochastic Gradient Descent(37281): loss=11.896545219984267\n",
      "Stochastic Gradient Descent(37282): loss=7.099371232807775e-06\n",
      "Stochastic Gradient Descent(37283): loss=5.336385262459633\n",
      "Stochastic Gradient Descent(37284): loss=6.891895607292844\n",
      "Stochastic Gradient Descent(37285): loss=6.448573173164764\n",
      "Stochastic Gradient Descent(37286): loss=3.4274622440653952\n",
      "Stochastic Gradient Descent(37287): loss=1.752238679292683\n",
      "Stochastic Gradient Descent(37288): loss=6.328976236196969\n",
      "Stochastic Gradient Descent(37289): loss=2.212268248237589\n",
      "Stochastic Gradient Descent(37290): loss=3.2265853248236116\n",
      "Stochastic Gradient Descent(37291): loss=4.587005795010486\n",
      "Stochastic Gradient Descent(37292): loss=0.01301538915346427\n",
      "Stochastic Gradient Descent(37293): loss=0.05050827664070591\n",
      "Stochastic Gradient Descent(37294): loss=2.580284832184789\n",
      "Stochastic Gradient Descent(37295): loss=8.455270719946308\n",
      "Stochastic Gradient Descent(37296): loss=0.0222422337719962\n",
      "Stochastic Gradient Descent(37297): loss=2.4591540350459824\n",
      "Stochastic Gradient Descent(37298): loss=9.265797274540033\n",
      "Stochastic Gradient Descent(37299): loss=0.09510087325925813\n",
      "Stochastic Gradient Descent(37300): loss=24.573128506121368\n",
      "Stochastic Gradient Descent(37301): loss=9.046742236236355\n",
      "Stochastic Gradient Descent(37302): loss=6.728516866142218\n",
      "Stochastic Gradient Descent(37303): loss=5.813540309325662\n",
      "Stochastic Gradient Descent(37304): loss=3.0973625611721665\n",
      "Stochastic Gradient Descent(37305): loss=8.22345972603073\n",
      "Stochastic Gradient Descent(37306): loss=2.401174940965628\n",
      "Stochastic Gradient Descent(37307): loss=0.01370276051056375\n",
      "Stochastic Gradient Descent(37308): loss=1.276859814854315\n",
      "Stochastic Gradient Descent(37309): loss=5.71983405506939\n",
      "Stochastic Gradient Descent(37310): loss=20.20131784788599\n",
      "Stochastic Gradient Descent(37311): loss=1.5020305224190793\n",
      "Stochastic Gradient Descent(37312): loss=3.1827325745315553\n",
      "Stochastic Gradient Descent(37313): loss=1.7185441263074928\n",
      "Stochastic Gradient Descent(37314): loss=2.988544233398145\n",
      "Stochastic Gradient Descent(37315): loss=0.06509633905699214\n",
      "Stochastic Gradient Descent(37316): loss=1.8875274598340392\n",
      "Stochastic Gradient Descent(37317): loss=8.952656319193652\n",
      "Stochastic Gradient Descent(37318): loss=0.028639287626151224\n",
      "Stochastic Gradient Descent(37319): loss=1.286717860141947\n",
      "Stochastic Gradient Descent(37320): loss=0.060675243422866385\n",
      "Stochastic Gradient Descent(37321): loss=2.5441923467004774\n",
      "Stochastic Gradient Descent(37322): loss=0.14950503690780934\n",
      "Stochastic Gradient Descent(37323): loss=7.018737079822343\n",
      "Stochastic Gradient Descent(37324): loss=12.530013292373376\n",
      "Stochastic Gradient Descent(37325): loss=7.5848485510044865\n",
      "Stochastic Gradient Descent(37326): loss=1.7473307144534844\n",
      "Stochastic Gradient Descent(37327): loss=16.42526735915322\n",
      "Stochastic Gradient Descent(37328): loss=1.7072526128499563\n",
      "Stochastic Gradient Descent(37329): loss=2.8870239430563323\n",
      "Stochastic Gradient Descent(37330): loss=2.014072059134691\n",
      "Stochastic Gradient Descent(37331): loss=10.64713664673265\n",
      "Stochastic Gradient Descent(37332): loss=0.22180410034600817\n",
      "Stochastic Gradient Descent(37333): loss=1.5446546111689543\n",
      "Stochastic Gradient Descent(37334): loss=0.5472085340834691\n",
      "Stochastic Gradient Descent(37335): loss=6.242400775259324\n",
      "Stochastic Gradient Descent(37336): loss=7.185781458433745\n",
      "Stochastic Gradient Descent(37337): loss=0.16480454725200536\n",
      "Stochastic Gradient Descent(37338): loss=4.087122772872124\n",
      "Stochastic Gradient Descent(37339): loss=6.171494010190213\n",
      "Stochastic Gradient Descent(37340): loss=0.9191514283179217\n",
      "Stochastic Gradient Descent(37341): loss=0.09642056710807483\n",
      "Stochastic Gradient Descent(37342): loss=1.6231724067496631\n",
      "Stochastic Gradient Descent(37343): loss=14.05655252063055\n",
      "Stochastic Gradient Descent(37344): loss=1.5257711787667443\n",
      "Stochastic Gradient Descent(37345): loss=1.4669893395517644\n",
      "Stochastic Gradient Descent(37346): loss=1.942383741688637\n",
      "Stochastic Gradient Descent(37347): loss=0.20167301684812\n",
      "Stochastic Gradient Descent(37348): loss=1.4173214059729136\n",
      "Stochastic Gradient Descent(37349): loss=0.020268490472301965\n",
      "Stochastic Gradient Descent(37350): loss=1.9231146827205263\n",
      "Stochastic Gradient Descent(37351): loss=8.350724020751425\n",
      "Stochastic Gradient Descent(37352): loss=9.801521066366286\n",
      "Stochastic Gradient Descent(37353): loss=4.445784338603166\n",
      "Stochastic Gradient Descent(37354): loss=0.7309344476371031\n",
      "Stochastic Gradient Descent(37355): loss=14.360741228188374\n",
      "Stochastic Gradient Descent(37356): loss=1.784716626710086\n",
      "Stochastic Gradient Descent(37357): loss=0.03455244275025101\n",
      "Stochastic Gradient Descent(37358): loss=1.0122890049867677\n",
      "Stochastic Gradient Descent(37359): loss=1.0316420072909673\n",
      "Stochastic Gradient Descent(37360): loss=9.559542744569445\n",
      "Stochastic Gradient Descent(37361): loss=0.0008449971741034254\n",
      "Stochastic Gradient Descent(37362): loss=0.1571302461138932\n",
      "Stochastic Gradient Descent(37363): loss=2.2850839715985223\n",
      "Stochastic Gradient Descent(37364): loss=0.0004424178715469413\n",
      "Stochastic Gradient Descent(37365): loss=6.516880449638412\n",
      "Stochastic Gradient Descent(37366): loss=0.2823850615605086\n",
      "Stochastic Gradient Descent(37367): loss=1.3967105847793828\n",
      "Stochastic Gradient Descent(37368): loss=11.896761618142675\n",
      "Stochastic Gradient Descent(37369): loss=0.5809667206634402\n",
      "Stochastic Gradient Descent(37370): loss=4.996118467057443\n",
      "Stochastic Gradient Descent(37371): loss=0.7265426055217533\n",
      "Stochastic Gradient Descent(37372): loss=1.6628799487109263\n",
      "Stochastic Gradient Descent(37373): loss=0.15918265288429528\n",
      "Stochastic Gradient Descent(37374): loss=14.977834214145055\n",
      "Stochastic Gradient Descent(37375): loss=2.286528339776454\n",
      "Stochastic Gradient Descent(37376): loss=0.0320967732493677\n",
      "Stochastic Gradient Descent(37377): loss=0.26762640153308725\n",
      "Stochastic Gradient Descent(37378): loss=1.704771203505115\n",
      "Stochastic Gradient Descent(37379): loss=3.7936765713593834\n",
      "Stochastic Gradient Descent(37380): loss=3.1576619873681544\n",
      "Stochastic Gradient Descent(37381): loss=24.134760944992127\n",
      "Stochastic Gradient Descent(37382): loss=30.265794881444045\n",
      "Stochastic Gradient Descent(37383): loss=34.20796444771369\n",
      "Stochastic Gradient Descent(37384): loss=0.09662319533694579\n",
      "Stochastic Gradient Descent(37385): loss=0.36327008665334687\n",
      "Stochastic Gradient Descent(37386): loss=12.124848985467649\n",
      "Stochastic Gradient Descent(37387): loss=3.0331196438934707\n",
      "Stochastic Gradient Descent(37388): loss=1.2724974051611035\n",
      "Stochastic Gradient Descent(37389): loss=11.180642101719098\n",
      "Stochastic Gradient Descent(37390): loss=0.012653846165493234\n",
      "Stochastic Gradient Descent(37391): loss=2.563600342270461\n",
      "Stochastic Gradient Descent(37392): loss=0.6328335325450931\n",
      "Stochastic Gradient Descent(37393): loss=6.143091600353108\n",
      "Stochastic Gradient Descent(37394): loss=0.014732050781877264\n",
      "Stochastic Gradient Descent(37395): loss=3.864120860385147\n",
      "Stochastic Gradient Descent(37396): loss=2.9292774893449396\n",
      "Stochastic Gradient Descent(37397): loss=2.739630603589922\n",
      "Stochastic Gradient Descent(37398): loss=0.21830074533764574\n",
      "Stochastic Gradient Descent(37399): loss=16.256493947592425\n",
      "Stochastic Gradient Descent(37400): loss=9.475008298024578\n",
      "Stochastic Gradient Descent(37401): loss=1.60161816070378\n",
      "Stochastic Gradient Descent(37402): loss=1.6044711604099793\n",
      "Stochastic Gradient Descent(37403): loss=4.328154853414122\n",
      "Stochastic Gradient Descent(37404): loss=8.86130543839479\n",
      "Stochastic Gradient Descent(37405): loss=0.7723871276562014\n",
      "Stochastic Gradient Descent(37406): loss=1.8347500268355768\n",
      "Stochastic Gradient Descent(37407): loss=7.095860357530176\n",
      "Stochastic Gradient Descent(37408): loss=0.3285307095442505\n",
      "Stochastic Gradient Descent(37409): loss=6.2434379648740475\n",
      "Stochastic Gradient Descent(37410): loss=3.7821754145803927\n",
      "Stochastic Gradient Descent(37411): loss=22.158304920790055\n",
      "Stochastic Gradient Descent(37412): loss=7.275376590861654\n",
      "Stochastic Gradient Descent(37413): loss=0.3812625357196448\n",
      "Stochastic Gradient Descent(37414): loss=1.5443786737232037\n",
      "Stochastic Gradient Descent(37415): loss=3.2739713396835244\n",
      "Stochastic Gradient Descent(37416): loss=7.126575301839933\n",
      "Stochastic Gradient Descent(37417): loss=9.534227719867438\n",
      "Stochastic Gradient Descent(37418): loss=12.868252064155746\n",
      "Stochastic Gradient Descent(37419): loss=0.01699146986052915\n",
      "Stochastic Gradient Descent(37420): loss=0.2915775194841177\n",
      "Stochastic Gradient Descent(37421): loss=0.19021245046176388\n",
      "Stochastic Gradient Descent(37422): loss=4.03863144349384\n",
      "Stochastic Gradient Descent(37423): loss=11.204273989320003\n",
      "Stochastic Gradient Descent(37424): loss=8.690461279473384\n",
      "Stochastic Gradient Descent(37425): loss=9.613223753836111\n",
      "Stochastic Gradient Descent(37426): loss=4.652549258256615\n",
      "Stochastic Gradient Descent(37427): loss=9.144202380135312\n",
      "Stochastic Gradient Descent(37428): loss=7.2239664164534565\n",
      "Stochastic Gradient Descent(37429): loss=1.3221972889082356\n",
      "Stochastic Gradient Descent(37430): loss=1.9730248069178757\n",
      "Stochastic Gradient Descent(37431): loss=0.0507030526855454\n",
      "Stochastic Gradient Descent(37432): loss=0.28721207458877535\n",
      "Stochastic Gradient Descent(37433): loss=1.64254152418193\n",
      "Stochastic Gradient Descent(37434): loss=5.777554849390241\n",
      "Stochastic Gradient Descent(37435): loss=23.75781906169923\n",
      "Stochastic Gradient Descent(37436): loss=8.661360692679684\n",
      "Stochastic Gradient Descent(37437): loss=2.125801340702921\n",
      "Stochastic Gradient Descent(37438): loss=1.9625265280978093\n",
      "Stochastic Gradient Descent(37439): loss=0.2998187383118571\n",
      "Stochastic Gradient Descent(37440): loss=0.01246581288019992\n",
      "Stochastic Gradient Descent(37441): loss=4.675555367485415\n",
      "Stochastic Gradient Descent(37442): loss=0.8494830203788406\n",
      "Stochastic Gradient Descent(37443): loss=0.9189764306066345\n",
      "Stochastic Gradient Descent(37444): loss=0.48399990548098676\n",
      "Stochastic Gradient Descent(37445): loss=19.17811337809088\n",
      "Stochastic Gradient Descent(37446): loss=11.241087026076118\n",
      "Stochastic Gradient Descent(37447): loss=0.845755353222981\n",
      "Stochastic Gradient Descent(37448): loss=9.727615955072565\n",
      "Stochastic Gradient Descent(37449): loss=0.7954450086147109\n",
      "Stochastic Gradient Descent(37450): loss=1.230312105360766\n",
      "Stochastic Gradient Descent(37451): loss=15.764018686596794\n",
      "Stochastic Gradient Descent(37452): loss=5.297874070428775\n",
      "Stochastic Gradient Descent(37453): loss=12.144056156770336\n",
      "Stochastic Gradient Descent(37454): loss=4.315937881640288\n",
      "Stochastic Gradient Descent(37455): loss=0.461265929173643\n",
      "Stochastic Gradient Descent(37456): loss=3.7160676885668034\n",
      "Stochastic Gradient Descent(37457): loss=3.323109214545897\n",
      "Stochastic Gradient Descent(37458): loss=0.10036625469078628\n",
      "Stochastic Gradient Descent(37459): loss=0.37954590399267873\n",
      "Stochastic Gradient Descent(37460): loss=0.3803075202650491\n",
      "Stochastic Gradient Descent(37461): loss=14.910927069766723\n",
      "Stochastic Gradient Descent(37462): loss=9.366601684625717\n",
      "Stochastic Gradient Descent(37463): loss=2.20851569735896\n",
      "Stochastic Gradient Descent(37464): loss=10.976530588571771\n",
      "Stochastic Gradient Descent(37465): loss=3.054767316980967\n",
      "Stochastic Gradient Descent(37466): loss=5.82535636076294\n",
      "Stochastic Gradient Descent(37467): loss=2.610516757434202\n",
      "Stochastic Gradient Descent(37468): loss=2.2387843201514595\n",
      "Stochastic Gradient Descent(37469): loss=0.3945989449059334\n",
      "Stochastic Gradient Descent(37470): loss=7.578389898275401\n",
      "Stochastic Gradient Descent(37471): loss=0.864516834470884\n",
      "Stochastic Gradient Descent(37472): loss=6.462645578865813\n",
      "Stochastic Gradient Descent(37473): loss=1.7831645595062215\n",
      "Stochastic Gradient Descent(37474): loss=4.111452102225284\n",
      "Stochastic Gradient Descent(37475): loss=0.027952020923835905\n",
      "Stochastic Gradient Descent(37476): loss=0.24318780002647444\n",
      "Stochastic Gradient Descent(37477): loss=1.6501579611016617\n",
      "Stochastic Gradient Descent(37478): loss=13.166085961146523\n",
      "Stochastic Gradient Descent(37479): loss=5.4162426811661435\n",
      "Stochastic Gradient Descent(37480): loss=5.1866393587579284e-05\n",
      "Stochastic Gradient Descent(37481): loss=4.3972301740351165\n",
      "Stochastic Gradient Descent(37482): loss=1.345914955165375\n",
      "Stochastic Gradient Descent(37483): loss=0.8414317333517892\n",
      "Stochastic Gradient Descent(37484): loss=1.2671445820518337\n",
      "Stochastic Gradient Descent(37485): loss=1.0398632013447173\n",
      "Stochastic Gradient Descent(37486): loss=0.07946526129788718\n",
      "Stochastic Gradient Descent(37487): loss=2.0366514099419755\n",
      "Stochastic Gradient Descent(37488): loss=28.03058758740728\n",
      "Stochastic Gradient Descent(37489): loss=2.6888626692279542\n",
      "Stochastic Gradient Descent(37490): loss=0.5691603729099011\n",
      "Stochastic Gradient Descent(37491): loss=0.8581581786404608\n",
      "Stochastic Gradient Descent(37492): loss=6.382353114676158\n",
      "Stochastic Gradient Descent(37493): loss=0.4850252560127887\n",
      "Stochastic Gradient Descent(37494): loss=0.2041388231680143\n",
      "Stochastic Gradient Descent(37495): loss=4.636013066790609\n",
      "Stochastic Gradient Descent(37496): loss=0.399544355307538\n",
      "Stochastic Gradient Descent(37497): loss=0.4842033721232219\n",
      "Stochastic Gradient Descent(37498): loss=0.6466108086593924\n",
      "Stochastic Gradient Descent(37499): loss=3.142359911753198\n",
      "Stochastic Gradient Descent(37500): loss=3.4183034581768377\n",
      "Stochastic Gradient Descent(37501): loss=0.010137864579841827\n",
      "Stochastic Gradient Descent(37502): loss=7.375530097853153\n",
      "Stochastic Gradient Descent(37503): loss=1.8576005594264617\n",
      "Stochastic Gradient Descent(37504): loss=0.6598087107364756\n",
      "Stochastic Gradient Descent(37505): loss=8.754863509002835e-06\n",
      "Stochastic Gradient Descent(37506): loss=0.2887300871284027\n",
      "Stochastic Gradient Descent(37507): loss=11.87208176501552\n",
      "Stochastic Gradient Descent(37508): loss=3.5050420773996507\n",
      "Stochastic Gradient Descent(37509): loss=7.873110622549925\n",
      "Stochastic Gradient Descent(37510): loss=8.827035247568958\n",
      "Stochastic Gradient Descent(37511): loss=10.621029967846965\n",
      "Stochastic Gradient Descent(37512): loss=0.8621687636005404\n",
      "Stochastic Gradient Descent(37513): loss=0.7800338977838503\n",
      "Stochastic Gradient Descent(37514): loss=1.9665579725666944\n",
      "Stochastic Gradient Descent(37515): loss=6.221652216654129\n",
      "Stochastic Gradient Descent(37516): loss=25.980182526955222\n",
      "Stochastic Gradient Descent(37517): loss=1.2007875099294991\n",
      "Stochastic Gradient Descent(37518): loss=6.761319853833107\n",
      "Stochastic Gradient Descent(37519): loss=21.339077235135644\n",
      "Stochastic Gradient Descent(37520): loss=0.06296299078980733\n",
      "Stochastic Gradient Descent(37521): loss=9.807775697925871\n",
      "Stochastic Gradient Descent(37522): loss=15.200217864172902\n",
      "Stochastic Gradient Descent(37523): loss=1.1646073803254102\n",
      "Stochastic Gradient Descent(37524): loss=0.3627409855028216\n",
      "Stochastic Gradient Descent(37525): loss=7.375317063776186\n",
      "Stochastic Gradient Descent(37526): loss=4.191272008541625\n",
      "Stochastic Gradient Descent(37527): loss=3.4980993892110934\n",
      "Stochastic Gradient Descent(37528): loss=0.10920582686994545\n",
      "Stochastic Gradient Descent(37529): loss=0.12074629340023774\n",
      "Stochastic Gradient Descent(37530): loss=4.449089292862975\n",
      "Stochastic Gradient Descent(37531): loss=1.9188805883888305\n",
      "Stochastic Gradient Descent(37532): loss=2.809982647101778\n",
      "Stochastic Gradient Descent(37533): loss=0.2555438504501928\n",
      "Stochastic Gradient Descent(37534): loss=2.4019310372950593\n",
      "Stochastic Gradient Descent(37535): loss=5.948662525367231\n",
      "Stochastic Gradient Descent(37536): loss=1.5311452257767086\n",
      "Stochastic Gradient Descent(37537): loss=7.549019677115004\n",
      "Stochastic Gradient Descent(37538): loss=0.31412652544572717\n",
      "Stochastic Gradient Descent(37539): loss=0.5727522795544011\n",
      "Stochastic Gradient Descent(37540): loss=3.377912070838848\n",
      "Stochastic Gradient Descent(37541): loss=0.004063853086162578\n",
      "Stochastic Gradient Descent(37542): loss=3.144962558271527\n",
      "Stochastic Gradient Descent(37543): loss=7.226344483769571\n",
      "Stochastic Gradient Descent(37544): loss=0.4452317714482887\n",
      "Stochastic Gradient Descent(37545): loss=1.9792862369847346\n",
      "Stochastic Gradient Descent(37546): loss=8.053701212795655\n",
      "Stochastic Gradient Descent(37547): loss=0.01461502200512518\n",
      "Stochastic Gradient Descent(37548): loss=6.622675566742935\n",
      "Stochastic Gradient Descent(37549): loss=8.039704235953236\n",
      "Stochastic Gradient Descent(37550): loss=1.088848847531929\n",
      "Stochastic Gradient Descent(37551): loss=1.7578284205717678\n",
      "Stochastic Gradient Descent(37552): loss=0.4431463183140328\n",
      "Stochastic Gradient Descent(37553): loss=9.068250090010624\n",
      "Stochastic Gradient Descent(37554): loss=2.0363570106094855\n",
      "Stochastic Gradient Descent(37555): loss=0.8372157679396942\n",
      "Stochastic Gradient Descent(37556): loss=8.116287795018836\n",
      "Stochastic Gradient Descent(37557): loss=0.0014937105298406888\n",
      "Stochastic Gradient Descent(37558): loss=0.012620333711383448\n",
      "Stochastic Gradient Descent(37559): loss=3.68967862374641\n",
      "Stochastic Gradient Descent(37560): loss=0.632041941853694\n",
      "Stochastic Gradient Descent(37561): loss=0.7290112121405704\n",
      "Stochastic Gradient Descent(37562): loss=0.47691303085673487\n",
      "Stochastic Gradient Descent(37563): loss=0.3989760782209193\n",
      "Stochastic Gradient Descent(37564): loss=1.276279044938605\n",
      "Stochastic Gradient Descent(37565): loss=0.06981890110200689\n",
      "Stochastic Gradient Descent(37566): loss=0.09288521465518702\n",
      "Stochastic Gradient Descent(37567): loss=0.4986705110352901\n",
      "Stochastic Gradient Descent(37568): loss=3.327237093072491\n",
      "Stochastic Gradient Descent(37569): loss=17.711452469448197\n",
      "Stochastic Gradient Descent(37570): loss=2.836933523033904\n",
      "Stochastic Gradient Descent(37571): loss=7.516077700116068\n",
      "Stochastic Gradient Descent(37572): loss=0.38853671975828435\n",
      "Stochastic Gradient Descent(37573): loss=2.7583783713946066\n",
      "Stochastic Gradient Descent(37574): loss=0.16542698244269136\n",
      "Stochastic Gradient Descent(37575): loss=1.2297836071452108\n",
      "Stochastic Gradient Descent(37576): loss=1.7245663348917935\n",
      "Stochastic Gradient Descent(37577): loss=0.3983861357114334\n",
      "Stochastic Gradient Descent(37578): loss=0.05748775399280769\n",
      "Stochastic Gradient Descent(37579): loss=14.580101987715032\n",
      "Stochastic Gradient Descent(37580): loss=0.0026404532960168365\n",
      "Stochastic Gradient Descent(37581): loss=0.08675908899662292\n",
      "Stochastic Gradient Descent(37582): loss=8.369145855523454\n",
      "Stochastic Gradient Descent(37583): loss=0.2746571673542335\n",
      "Stochastic Gradient Descent(37584): loss=0.23741334469889072\n",
      "Stochastic Gradient Descent(37585): loss=11.275534385368523\n",
      "Stochastic Gradient Descent(37586): loss=1.6871551806273573\n",
      "Stochastic Gradient Descent(37587): loss=0.013465600450741954\n",
      "Stochastic Gradient Descent(37588): loss=0.3845290364671813\n",
      "Stochastic Gradient Descent(37589): loss=0.36743737978807234\n",
      "Stochastic Gradient Descent(37590): loss=0.6980716761022338\n",
      "Stochastic Gradient Descent(37591): loss=0.21612790439384053\n",
      "Stochastic Gradient Descent(37592): loss=27.11478022408314\n",
      "Stochastic Gradient Descent(37593): loss=0.1489064474851388\n",
      "Stochastic Gradient Descent(37594): loss=3.079518536694924\n",
      "Stochastic Gradient Descent(37595): loss=1.0699680592774392\n",
      "Stochastic Gradient Descent(37596): loss=9.138057960439705\n",
      "Stochastic Gradient Descent(37597): loss=0.8695304512086709\n",
      "Stochastic Gradient Descent(37598): loss=0.35137485667799145\n",
      "Stochastic Gradient Descent(37599): loss=0.011865234489930347\n",
      "Stochastic Gradient Descent(37600): loss=12.685490550713952\n",
      "Stochastic Gradient Descent(37601): loss=0.0030180951537644267\n",
      "Stochastic Gradient Descent(37602): loss=1.5954163047142884\n",
      "Stochastic Gradient Descent(37603): loss=2.4078586843242866\n",
      "Stochastic Gradient Descent(37604): loss=0.044430634332294446\n",
      "Stochastic Gradient Descent(37605): loss=0.3609064719596049\n",
      "Stochastic Gradient Descent(37606): loss=0.04949462308662704\n",
      "Stochastic Gradient Descent(37607): loss=5.52609190935223\n",
      "Stochastic Gradient Descent(37608): loss=7.999380035121298\n",
      "Stochastic Gradient Descent(37609): loss=3.5494085023187147\n",
      "Stochastic Gradient Descent(37610): loss=0.32252323706829605\n",
      "Stochastic Gradient Descent(37611): loss=0.4315070089844405\n",
      "Stochastic Gradient Descent(37612): loss=1.7169620817346123\n",
      "Stochastic Gradient Descent(37613): loss=0.8534378718646702\n",
      "Stochastic Gradient Descent(37614): loss=0.0002661291055447931\n",
      "Stochastic Gradient Descent(37615): loss=4.54128376153063\n",
      "Stochastic Gradient Descent(37616): loss=3.720068857381802\n",
      "Stochastic Gradient Descent(37617): loss=0.29065300448526693\n",
      "Stochastic Gradient Descent(37618): loss=0.04959107929280994\n",
      "Stochastic Gradient Descent(37619): loss=0.07722802200776917\n",
      "Stochastic Gradient Descent(37620): loss=6.4175877907121865\n",
      "Stochastic Gradient Descent(37621): loss=0.13181776208772794\n",
      "Stochastic Gradient Descent(37622): loss=2.8383829503201463\n",
      "Stochastic Gradient Descent(37623): loss=1.1487889714742003\n",
      "Stochastic Gradient Descent(37624): loss=0.0006486529962162655\n",
      "Stochastic Gradient Descent(37625): loss=0.22754934044404782\n",
      "Stochastic Gradient Descent(37626): loss=2.2508840760633246\n",
      "Stochastic Gradient Descent(37627): loss=0.14096514387078932\n",
      "Stochastic Gradient Descent(37628): loss=4.172020812822605\n",
      "Stochastic Gradient Descent(37629): loss=19.03199627876562\n",
      "Stochastic Gradient Descent(37630): loss=3.860933175890304\n",
      "Stochastic Gradient Descent(37631): loss=12.052034521856278\n",
      "Stochastic Gradient Descent(37632): loss=6.991909145111135\n",
      "Stochastic Gradient Descent(37633): loss=1.7464658127622017\n",
      "Stochastic Gradient Descent(37634): loss=0.5731935689411527\n",
      "Stochastic Gradient Descent(37635): loss=6.060149706130939\n",
      "Stochastic Gradient Descent(37636): loss=5.199079998611056\n",
      "Stochastic Gradient Descent(37637): loss=3.355053267956011\n",
      "Stochastic Gradient Descent(37638): loss=0.06506615250294298\n",
      "Stochastic Gradient Descent(37639): loss=0.6517324352210407\n",
      "Stochastic Gradient Descent(37640): loss=2.423903347932789\n",
      "Stochastic Gradient Descent(37641): loss=8.831153176028229\n",
      "Stochastic Gradient Descent(37642): loss=2.09471932652504\n",
      "Stochastic Gradient Descent(37643): loss=0.44950127187880345\n",
      "Stochastic Gradient Descent(37644): loss=0.01701709255635047\n",
      "Stochastic Gradient Descent(37645): loss=0.2189575680978264\n",
      "Stochastic Gradient Descent(37646): loss=0.274515863665385\n",
      "Stochastic Gradient Descent(37647): loss=0.2478957005946493\n",
      "Stochastic Gradient Descent(37648): loss=0.5758813844153781\n",
      "Stochastic Gradient Descent(37649): loss=2.63077020414628\n",
      "Stochastic Gradient Descent(37650): loss=0.054245732722559246\n",
      "Stochastic Gradient Descent(37651): loss=0.4828959688022171\n",
      "Stochastic Gradient Descent(37652): loss=12.250544315872336\n",
      "Stochastic Gradient Descent(37653): loss=6.612849807755431\n",
      "Stochastic Gradient Descent(37654): loss=1.161457186041304\n",
      "Stochastic Gradient Descent(37655): loss=6.327816205981281\n",
      "Stochastic Gradient Descent(37656): loss=5.988640562141492\n",
      "Stochastic Gradient Descent(37657): loss=18.486074796002644\n",
      "Stochastic Gradient Descent(37658): loss=2.6530193032026856\n",
      "Stochastic Gradient Descent(37659): loss=6.994448864874281\n",
      "Stochastic Gradient Descent(37660): loss=3.51454626425468\n",
      "Stochastic Gradient Descent(37661): loss=3.4588482657965995\n",
      "Stochastic Gradient Descent(37662): loss=3.1376743482869576\n",
      "Stochastic Gradient Descent(37663): loss=1.7487069736439704\n",
      "Stochastic Gradient Descent(37664): loss=1.0453255078862256\n",
      "Stochastic Gradient Descent(37665): loss=15.613844115752963\n",
      "Stochastic Gradient Descent(37666): loss=1.1882994859546328\n",
      "Stochastic Gradient Descent(37667): loss=0.01561327721539763\n",
      "Stochastic Gradient Descent(37668): loss=26.549102827426573\n",
      "Stochastic Gradient Descent(37669): loss=1.03806411419317\n",
      "Stochastic Gradient Descent(37670): loss=0.754287598400668\n",
      "Stochastic Gradient Descent(37671): loss=5.752950062748778\n",
      "Stochastic Gradient Descent(37672): loss=3.609636403796665\n",
      "Stochastic Gradient Descent(37673): loss=2.4942420085505606\n",
      "Stochastic Gradient Descent(37674): loss=1.0087731269500853\n",
      "Stochastic Gradient Descent(37675): loss=8.589945070985154\n",
      "Stochastic Gradient Descent(37676): loss=0.8904551731286751\n",
      "Stochastic Gradient Descent(37677): loss=1.525245891259457\n",
      "Stochastic Gradient Descent(37678): loss=5.268100397805266\n",
      "Stochastic Gradient Descent(37679): loss=4.465922462645087\n",
      "Stochastic Gradient Descent(37680): loss=0.22211517681526463\n",
      "Stochastic Gradient Descent(37681): loss=2.308298398281637\n",
      "Stochastic Gradient Descent(37682): loss=1.1039263682764247\n",
      "Stochastic Gradient Descent(37683): loss=0.3529361341328432\n",
      "Stochastic Gradient Descent(37684): loss=0.01324457478805379\n",
      "Stochastic Gradient Descent(37685): loss=3.2766542195891124\n",
      "Stochastic Gradient Descent(37686): loss=1.2867486739377494\n",
      "Stochastic Gradient Descent(37687): loss=0.004954003064477395\n",
      "Stochastic Gradient Descent(37688): loss=0.20651141793476127\n",
      "Stochastic Gradient Descent(37689): loss=0.05698636276483057\n",
      "Stochastic Gradient Descent(37690): loss=0.9144430375293053\n",
      "Stochastic Gradient Descent(37691): loss=11.616237703464943\n",
      "Stochastic Gradient Descent(37692): loss=2.270725023140184\n",
      "Stochastic Gradient Descent(37693): loss=1.0653532275168618\n",
      "Stochastic Gradient Descent(37694): loss=0.035009876441253515\n",
      "Stochastic Gradient Descent(37695): loss=0.10132056603606492\n",
      "Stochastic Gradient Descent(37696): loss=15.868665276803373\n",
      "Stochastic Gradient Descent(37697): loss=0.5072204317062915\n",
      "Stochastic Gradient Descent(37698): loss=0.2371450633919497\n",
      "Stochastic Gradient Descent(37699): loss=0.2547689296290473\n",
      "Stochastic Gradient Descent(37700): loss=1.1719899853735511\n",
      "Stochastic Gradient Descent(37701): loss=0.5584962819770567\n",
      "Stochastic Gradient Descent(37702): loss=1.3247348529953633\n",
      "Stochastic Gradient Descent(37703): loss=0.08480747284907553\n",
      "Stochastic Gradient Descent(37704): loss=0.13610218659031667\n",
      "Stochastic Gradient Descent(37705): loss=13.95220209430695\n",
      "Stochastic Gradient Descent(37706): loss=5.29161051195795\n",
      "Stochastic Gradient Descent(37707): loss=1.3635618707293777\n",
      "Stochastic Gradient Descent(37708): loss=0.004224433630162529\n",
      "Stochastic Gradient Descent(37709): loss=5.294380380783764\n",
      "Stochastic Gradient Descent(37710): loss=19.486701488032555\n",
      "Stochastic Gradient Descent(37711): loss=37.4566921817717\n",
      "Stochastic Gradient Descent(37712): loss=12.331367686403386\n",
      "Stochastic Gradient Descent(37713): loss=4.686382908559061\n",
      "Stochastic Gradient Descent(37714): loss=0.1434384300500768\n",
      "Stochastic Gradient Descent(37715): loss=0.20409845317279138\n",
      "Stochastic Gradient Descent(37716): loss=0.714587703324682\n",
      "Stochastic Gradient Descent(37717): loss=0.002065818393649953\n",
      "Stochastic Gradient Descent(37718): loss=0.3297233740316722\n",
      "Stochastic Gradient Descent(37719): loss=0.27066040672034397\n",
      "Stochastic Gradient Descent(37720): loss=5.014761068889516\n",
      "Stochastic Gradient Descent(37721): loss=0.017613734699786165\n",
      "Stochastic Gradient Descent(37722): loss=1.8501442919809987\n",
      "Stochastic Gradient Descent(37723): loss=9.451652692755694\n",
      "Stochastic Gradient Descent(37724): loss=6.472270817552967\n",
      "Stochastic Gradient Descent(37725): loss=8.554869639157719\n",
      "Stochastic Gradient Descent(37726): loss=0.3686964062660278\n",
      "Stochastic Gradient Descent(37727): loss=0.04341005104438475\n",
      "Stochastic Gradient Descent(37728): loss=5.058688438283038\n",
      "Stochastic Gradient Descent(37729): loss=6.939062894120568\n",
      "Stochastic Gradient Descent(37730): loss=0.03235284862523083\n",
      "Stochastic Gradient Descent(37731): loss=0.07924089473661979\n",
      "Stochastic Gradient Descent(37732): loss=1.6375157048887483\n",
      "Stochastic Gradient Descent(37733): loss=18.13959223729976\n",
      "Stochastic Gradient Descent(37734): loss=20.73308340115738\n",
      "Stochastic Gradient Descent(37735): loss=2.1841643629659333\n",
      "Stochastic Gradient Descent(37736): loss=3.6975141263539983\n",
      "Stochastic Gradient Descent(37737): loss=5.074855742294872\n",
      "Stochastic Gradient Descent(37738): loss=0.1408208700551638\n",
      "Stochastic Gradient Descent(37739): loss=7.733678952294845\n",
      "Stochastic Gradient Descent(37740): loss=3.722572843554162\n",
      "Stochastic Gradient Descent(37741): loss=0.020199850187448878\n",
      "Stochastic Gradient Descent(37742): loss=3.9099904378413335\n",
      "Stochastic Gradient Descent(37743): loss=2.592003498973695\n",
      "Stochastic Gradient Descent(37744): loss=7.3372545323046845\n",
      "Stochastic Gradient Descent(37745): loss=14.718745443076896\n",
      "Stochastic Gradient Descent(37746): loss=0.745654157955566\n",
      "Stochastic Gradient Descent(37747): loss=0.5231811630373056\n",
      "Stochastic Gradient Descent(37748): loss=0.008683030305360337\n",
      "Stochastic Gradient Descent(37749): loss=0.0018966124843713301\n",
      "Stochastic Gradient Descent(37750): loss=1.7549924432735315\n",
      "Stochastic Gradient Descent(37751): loss=4.303622050768907\n",
      "Stochastic Gradient Descent(37752): loss=13.487704632433903\n",
      "Stochastic Gradient Descent(37753): loss=0.7214027872429067\n",
      "Stochastic Gradient Descent(37754): loss=1.0121337508977677\n",
      "Stochastic Gradient Descent(37755): loss=4.172713575981546\n",
      "Stochastic Gradient Descent(37756): loss=4.663689455354859\n",
      "Stochastic Gradient Descent(37757): loss=1.0302789849092553\n",
      "Stochastic Gradient Descent(37758): loss=9.736595335074218\n",
      "Stochastic Gradient Descent(37759): loss=0.018582069963348474\n",
      "Stochastic Gradient Descent(37760): loss=0.6333749699516832\n",
      "Stochastic Gradient Descent(37761): loss=1.0165974219306795\n",
      "Stochastic Gradient Descent(37762): loss=19.61653935817398\n",
      "Stochastic Gradient Descent(37763): loss=4.6417695675307264\n",
      "Stochastic Gradient Descent(37764): loss=1.956527321722114\n",
      "Stochastic Gradient Descent(37765): loss=1.9661165826233429\n",
      "Stochastic Gradient Descent(37766): loss=0.9431149354365976\n",
      "Stochastic Gradient Descent(37767): loss=3.8556332880101536\n",
      "Stochastic Gradient Descent(37768): loss=6.763671075794829\n",
      "Stochastic Gradient Descent(37769): loss=0.7655147043019798\n",
      "Stochastic Gradient Descent(37770): loss=0.21746205430904295\n",
      "Stochastic Gradient Descent(37771): loss=6.262019369210194\n",
      "Stochastic Gradient Descent(37772): loss=24.7130702159248\n",
      "Stochastic Gradient Descent(37773): loss=22.62222567923265\n",
      "Stochastic Gradient Descent(37774): loss=7.73400440610094\n",
      "Stochastic Gradient Descent(37775): loss=3.9538005148259665\n",
      "Stochastic Gradient Descent(37776): loss=0.08134647916634037\n",
      "Stochastic Gradient Descent(37777): loss=1.1100781904975356\n",
      "Stochastic Gradient Descent(37778): loss=0.22687686514043603\n",
      "Stochastic Gradient Descent(37779): loss=3.2537015766722033\n",
      "Stochastic Gradient Descent(37780): loss=9.854347339143144\n",
      "Stochastic Gradient Descent(37781): loss=0.18237656944188196\n",
      "Stochastic Gradient Descent(37782): loss=1.3929987544962816\n",
      "Stochastic Gradient Descent(37783): loss=0.05800633432232483\n",
      "Stochastic Gradient Descent(37784): loss=0.017798192917270025\n",
      "Stochastic Gradient Descent(37785): loss=5.112968053515986\n",
      "Stochastic Gradient Descent(37786): loss=0.7722248322204169\n",
      "Stochastic Gradient Descent(37787): loss=0.10002885950026222\n",
      "Stochastic Gradient Descent(37788): loss=5.225174582990525\n",
      "Stochastic Gradient Descent(37789): loss=0.5564160092594714\n",
      "Stochastic Gradient Descent(37790): loss=1.0377021658655208\n",
      "Stochastic Gradient Descent(37791): loss=8.603696643373018\n",
      "Stochastic Gradient Descent(37792): loss=0.4946366072165295\n",
      "Stochastic Gradient Descent(37793): loss=0.015148185913386005\n",
      "Stochastic Gradient Descent(37794): loss=1.97117917532114\n",
      "Stochastic Gradient Descent(37795): loss=10.666639174191438\n",
      "Stochastic Gradient Descent(37796): loss=4.078865520965719\n",
      "Stochastic Gradient Descent(37797): loss=5.239509971535134\n",
      "Stochastic Gradient Descent(37798): loss=1.4305594559657406\n",
      "Stochastic Gradient Descent(37799): loss=1.6750106517482584\n",
      "Stochastic Gradient Descent(37800): loss=0.272528117857989\n",
      "Stochastic Gradient Descent(37801): loss=3.181894074442104\n",
      "Stochastic Gradient Descent(37802): loss=0.9451397594449467\n",
      "Stochastic Gradient Descent(37803): loss=0.8007247968194262\n",
      "Stochastic Gradient Descent(37804): loss=4.326628109747181\n",
      "Stochastic Gradient Descent(37805): loss=1.0358305936977006\n",
      "Stochastic Gradient Descent(37806): loss=0.9498761115848137\n",
      "Stochastic Gradient Descent(37807): loss=0.02777433996832956\n",
      "Stochastic Gradient Descent(37808): loss=1.5242788662135365\n",
      "Stochastic Gradient Descent(37809): loss=0.09477453923167152\n",
      "Stochastic Gradient Descent(37810): loss=0.0040433083345881304\n",
      "Stochastic Gradient Descent(37811): loss=3.8485730159500964\n",
      "Stochastic Gradient Descent(37812): loss=2.5491988281058764\n",
      "Stochastic Gradient Descent(37813): loss=10.333814892715091\n",
      "Stochastic Gradient Descent(37814): loss=7.187248290900137\n",
      "Stochastic Gradient Descent(37815): loss=0.6068535750242722\n",
      "Stochastic Gradient Descent(37816): loss=0.6928456725557649\n",
      "Stochastic Gradient Descent(37817): loss=3.228048259702564\n",
      "Stochastic Gradient Descent(37818): loss=0.5900356787787446\n",
      "Stochastic Gradient Descent(37819): loss=0.6433844513878991\n",
      "Stochastic Gradient Descent(37820): loss=3.364688379889084\n",
      "Stochastic Gradient Descent(37821): loss=7.914240639636724\n",
      "Stochastic Gradient Descent(37822): loss=5.17466771724706\n",
      "Stochastic Gradient Descent(37823): loss=0.7556376645149908\n",
      "Stochastic Gradient Descent(37824): loss=0.17401922402577344\n",
      "Stochastic Gradient Descent(37825): loss=3.2738499661092972\n",
      "Stochastic Gradient Descent(37826): loss=0.1876244713611479\n",
      "Stochastic Gradient Descent(37827): loss=4.819751899189826\n",
      "Stochastic Gradient Descent(37828): loss=0.36774700484387024\n",
      "Stochastic Gradient Descent(37829): loss=8.766025136986551\n",
      "Stochastic Gradient Descent(37830): loss=0.7821440517207084\n",
      "Stochastic Gradient Descent(37831): loss=0.10914450973451424\n",
      "Stochastic Gradient Descent(37832): loss=0.07857104470296089\n",
      "Stochastic Gradient Descent(37833): loss=3.146145335888779\n",
      "Stochastic Gradient Descent(37834): loss=0.07921693025386921\n",
      "Stochastic Gradient Descent(37835): loss=1.1339699463072652\n",
      "Stochastic Gradient Descent(37836): loss=0.29631775681195066\n",
      "Stochastic Gradient Descent(37837): loss=14.600223092504587\n",
      "Stochastic Gradient Descent(37838): loss=0.42950926548672824\n",
      "Stochastic Gradient Descent(37839): loss=0.26741071355222384\n",
      "Stochastic Gradient Descent(37840): loss=0.6349322170970441\n",
      "Stochastic Gradient Descent(37841): loss=4.6503540150140905\n",
      "Stochastic Gradient Descent(37842): loss=2.93972091646633\n",
      "Stochastic Gradient Descent(37843): loss=2.2400578848140045\n",
      "Stochastic Gradient Descent(37844): loss=2.441765554065947\n",
      "Stochastic Gradient Descent(37845): loss=2.6264738635871585\n",
      "Stochastic Gradient Descent(37846): loss=0.17283748650365635\n",
      "Stochastic Gradient Descent(37847): loss=16.895285582736943\n",
      "Stochastic Gradient Descent(37848): loss=1.5827918005006145\n",
      "Stochastic Gradient Descent(37849): loss=0.181819829616998\n",
      "Stochastic Gradient Descent(37850): loss=9.010257761887459\n",
      "Stochastic Gradient Descent(37851): loss=1.9841691690592336\n",
      "Stochastic Gradient Descent(37852): loss=0.17748978571476562\n",
      "Stochastic Gradient Descent(37853): loss=0.009239113199895573\n",
      "Stochastic Gradient Descent(37854): loss=7.3675932036126826\n",
      "Stochastic Gradient Descent(37855): loss=6.554480834547733\n",
      "Stochastic Gradient Descent(37856): loss=0.09500392223600776\n",
      "Stochastic Gradient Descent(37857): loss=1.4371399011639752\n",
      "Stochastic Gradient Descent(37858): loss=0.9175133183741755\n",
      "Stochastic Gradient Descent(37859): loss=0.009606586152336822\n",
      "Stochastic Gradient Descent(37860): loss=0.030686891451940625\n",
      "Stochastic Gradient Descent(37861): loss=7.533534466807078\n",
      "Stochastic Gradient Descent(37862): loss=31.520791276161447\n",
      "Stochastic Gradient Descent(37863): loss=1.1080556936924884\n",
      "Stochastic Gradient Descent(37864): loss=10.29858448371686\n",
      "Stochastic Gradient Descent(37865): loss=2.668386988686056\n",
      "Stochastic Gradient Descent(37866): loss=0.6150783886116762\n",
      "Stochastic Gradient Descent(37867): loss=3.415554652668454\n",
      "Stochastic Gradient Descent(37868): loss=1.7241595879073295\n",
      "Stochastic Gradient Descent(37869): loss=0.32059820704758496\n",
      "Stochastic Gradient Descent(37870): loss=13.710238798807081\n",
      "Stochastic Gradient Descent(37871): loss=4.482806037380051\n",
      "Stochastic Gradient Descent(37872): loss=35.37170808342663\n",
      "Stochastic Gradient Descent(37873): loss=36.49511012972829\n",
      "Stochastic Gradient Descent(37874): loss=1.9502177115986954\n",
      "Stochastic Gradient Descent(37875): loss=5.833237627230202\n",
      "Stochastic Gradient Descent(37876): loss=0.27118159852135454\n",
      "Stochastic Gradient Descent(37877): loss=0.07079852066569732\n",
      "Stochastic Gradient Descent(37878): loss=4.731085247413177\n",
      "Stochastic Gradient Descent(37879): loss=4.668507712405992\n",
      "Stochastic Gradient Descent(37880): loss=1.8840463650639203\n",
      "Stochastic Gradient Descent(37881): loss=1.2239010857685024\n",
      "Stochastic Gradient Descent(37882): loss=7.634805065648486\n",
      "Stochastic Gradient Descent(37883): loss=12.197692204014364\n",
      "Stochastic Gradient Descent(37884): loss=1.374465388383425\n",
      "Stochastic Gradient Descent(37885): loss=11.221041697178833\n",
      "Stochastic Gradient Descent(37886): loss=13.497832825494445\n",
      "Stochastic Gradient Descent(37887): loss=6.9345978062555504\n",
      "Stochastic Gradient Descent(37888): loss=0.6878487598035626\n",
      "Stochastic Gradient Descent(37889): loss=9.11220769127397\n",
      "Stochastic Gradient Descent(37890): loss=1.8073740689573758\n",
      "Stochastic Gradient Descent(37891): loss=0.9668175325384177\n",
      "Stochastic Gradient Descent(37892): loss=0.08959660323479487\n",
      "Stochastic Gradient Descent(37893): loss=0.542252808930975\n",
      "Stochastic Gradient Descent(37894): loss=2.978632433150808\n",
      "Stochastic Gradient Descent(37895): loss=1.8532373424175286\n",
      "Stochastic Gradient Descent(37896): loss=3.7187129890013337\n",
      "Stochastic Gradient Descent(37897): loss=0.024493920385854365\n",
      "Stochastic Gradient Descent(37898): loss=0.5066359687973245\n",
      "Stochastic Gradient Descent(37899): loss=4.230366611621665\n",
      "Stochastic Gradient Descent(37900): loss=0.23528066980117165\n",
      "Stochastic Gradient Descent(37901): loss=1.8162934195675242\n",
      "Stochastic Gradient Descent(37902): loss=0.25475239286039153\n",
      "Stochastic Gradient Descent(37903): loss=6.642521725832817\n",
      "Stochastic Gradient Descent(37904): loss=0.7018358095356139\n",
      "Stochastic Gradient Descent(37905): loss=65.11734108534614\n",
      "Stochastic Gradient Descent(37906): loss=60.167253475278386\n",
      "Stochastic Gradient Descent(37907): loss=0.8518808979066128\n",
      "Stochastic Gradient Descent(37908): loss=9.55310196052652\n",
      "Stochastic Gradient Descent(37909): loss=9.876828396116522\n",
      "Stochastic Gradient Descent(37910): loss=14.594581268977045\n",
      "Stochastic Gradient Descent(37911): loss=4.980207785364795\n",
      "Stochastic Gradient Descent(37912): loss=0.08580118861611384\n",
      "Stochastic Gradient Descent(37913): loss=13.799212103506228\n",
      "Stochastic Gradient Descent(37914): loss=0.1979898207781299\n",
      "Stochastic Gradient Descent(37915): loss=7.996554508086827\n",
      "Stochastic Gradient Descent(37916): loss=1.329576803948698\n",
      "Stochastic Gradient Descent(37917): loss=9.777984696070101\n",
      "Stochastic Gradient Descent(37918): loss=0.4998481250502354\n",
      "Stochastic Gradient Descent(37919): loss=8.38134143905617\n",
      "Stochastic Gradient Descent(37920): loss=6.667756491122041\n",
      "Stochastic Gradient Descent(37921): loss=1.9456377456240932\n",
      "Stochastic Gradient Descent(37922): loss=0.16474906311662907\n",
      "Stochastic Gradient Descent(37923): loss=13.187885368579044\n",
      "Stochastic Gradient Descent(37924): loss=1.0092721625777556\n",
      "Stochastic Gradient Descent(37925): loss=0.008881026131436631\n",
      "Stochastic Gradient Descent(37926): loss=7.5883935114404375\n",
      "Stochastic Gradient Descent(37927): loss=0.1037142782086192\n",
      "Stochastic Gradient Descent(37928): loss=0.7375529057372744\n",
      "Stochastic Gradient Descent(37929): loss=0.8550783869520897\n",
      "Stochastic Gradient Descent(37930): loss=0.17694909360080988\n",
      "Stochastic Gradient Descent(37931): loss=0.5541297383790539\n",
      "Stochastic Gradient Descent(37932): loss=0.04860629603826281\n",
      "Stochastic Gradient Descent(37933): loss=0.6659723058320699\n",
      "Stochastic Gradient Descent(37934): loss=0.20297871614460608\n",
      "Stochastic Gradient Descent(37935): loss=3.6855672680062903\n",
      "Stochastic Gradient Descent(37936): loss=7.199525659073061\n",
      "Stochastic Gradient Descent(37937): loss=0.29130813174318526\n",
      "Stochastic Gradient Descent(37938): loss=3.828585453907206\n",
      "Stochastic Gradient Descent(37939): loss=4.091245102351286\n",
      "Stochastic Gradient Descent(37940): loss=1.949203832178411\n",
      "Stochastic Gradient Descent(37941): loss=5.742595722399999\n",
      "Stochastic Gradient Descent(37942): loss=0.0014852162173074356\n",
      "Stochastic Gradient Descent(37943): loss=0.23012827147006848\n",
      "Stochastic Gradient Descent(37944): loss=0.012983300812124834\n",
      "Stochastic Gradient Descent(37945): loss=9.774070572031155\n",
      "Stochastic Gradient Descent(37946): loss=0.20672083595108373\n",
      "Stochastic Gradient Descent(37947): loss=0.0481113327238501\n",
      "Stochastic Gradient Descent(37948): loss=13.393398981795382\n",
      "Stochastic Gradient Descent(37949): loss=10.60236357581707\n",
      "Stochastic Gradient Descent(37950): loss=12.39856027914937\n",
      "Stochastic Gradient Descent(37951): loss=0.4494306015815976\n",
      "Stochastic Gradient Descent(37952): loss=1.9711915296468692\n",
      "Stochastic Gradient Descent(37953): loss=8.549836217260507\n",
      "Stochastic Gradient Descent(37954): loss=0.037959438015447586\n",
      "Stochastic Gradient Descent(37955): loss=0.34989386433838837\n",
      "Stochastic Gradient Descent(37956): loss=1.3671300436249851\n",
      "Stochastic Gradient Descent(37957): loss=0.09467614744012129\n",
      "Stochastic Gradient Descent(37958): loss=4.136066649368174\n",
      "Stochastic Gradient Descent(37959): loss=1.3286707013682106\n",
      "Stochastic Gradient Descent(37960): loss=0.009537888577178778\n",
      "Stochastic Gradient Descent(37961): loss=0.1669331000280761\n",
      "Stochastic Gradient Descent(37962): loss=0.34715609382479895\n",
      "Stochastic Gradient Descent(37963): loss=15.242728841765222\n",
      "Stochastic Gradient Descent(37964): loss=0.2747349605833101\n",
      "Stochastic Gradient Descent(37965): loss=0.6997838761487585\n",
      "Stochastic Gradient Descent(37966): loss=3.9454364228635477\n",
      "Stochastic Gradient Descent(37967): loss=0.34704835051568866\n",
      "Stochastic Gradient Descent(37968): loss=0.47645666952565896\n",
      "Stochastic Gradient Descent(37969): loss=0.23543749307147177\n",
      "Stochastic Gradient Descent(37970): loss=7.509547736447034\n",
      "Stochastic Gradient Descent(37971): loss=9.898236583731945\n",
      "Stochastic Gradient Descent(37972): loss=0.5339968722754088\n",
      "Stochastic Gradient Descent(37973): loss=14.787618928963772\n",
      "Stochastic Gradient Descent(37974): loss=0.17419115620135586\n",
      "Stochastic Gradient Descent(37975): loss=3.9978513274842444\n",
      "Stochastic Gradient Descent(37976): loss=4.972969686589032\n",
      "Stochastic Gradient Descent(37977): loss=15.95159577932751\n",
      "Stochastic Gradient Descent(37978): loss=1.9026892370606794\n",
      "Stochastic Gradient Descent(37979): loss=11.954686963115476\n",
      "Stochastic Gradient Descent(37980): loss=0.2523430163028473\n",
      "Stochastic Gradient Descent(37981): loss=1.1066446442040285\n",
      "Stochastic Gradient Descent(37982): loss=0.007913322948112598\n",
      "Stochastic Gradient Descent(37983): loss=5.064841292034044\n",
      "Stochastic Gradient Descent(37984): loss=0.9884784697314136\n",
      "Stochastic Gradient Descent(37985): loss=2.0185369965697886\n",
      "Stochastic Gradient Descent(37986): loss=0.6489255201079457\n",
      "Stochastic Gradient Descent(37987): loss=2.227005350512108\n",
      "Stochastic Gradient Descent(37988): loss=0.8627564358263519\n",
      "Stochastic Gradient Descent(37989): loss=2.616016637808864\n",
      "Stochastic Gradient Descent(37990): loss=1.2601463546497438\n",
      "Stochastic Gradient Descent(37991): loss=0.2041812081884973\n",
      "Stochastic Gradient Descent(37992): loss=0.10034384959151232\n",
      "Stochastic Gradient Descent(37993): loss=0.0007944771033371122\n",
      "Stochastic Gradient Descent(37994): loss=8.022448640527719\n",
      "Stochastic Gradient Descent(37995): loss=1.8571546176326397\n",
      "Stochastic Gradient Descent(37996): loss=3.920460658071763\n",
      "Stochastic Gradient Descent(37997): loss=3.0674885459124686\n",
      "Stochastic Gradient Descent(37998): loss=5.6348024186387144\n",
      "Stochastic Gradient Descent(37999): loss=3.327894154403697\n",
      "Stochastic Gradient Descent(38000): loss=0.09448439408475921\n",
      "Stochastic Gradient Descent(38001): loss=4.16213984100632\n",
      "Stochastic Gradient Descent(38002): loss=0.4017864212922358\n",
      "Stochastic Gradient Descent(38003): loss=0.2501040388824248\n",
      "Stochastic Gradient Descent(38004): loss=25.34244716139404\n",
      "Stochastic Gradient Descent(38005): loss=9.220369761061864\n",
      "Stochastic Gradient Descent(38006): loss=0.22704424110709165\n",
      "Stochastic Gradient Descent(38007): loss=2.5111946109737193\n",
      "Stochastic Gradient Descent(38008): loss=0.07855455846465358\n",
      "Stochastic Gradient Descent(38009): loss=11.145623643636261\n",
      "Stochastic Gradient Descent(38010): loss=13.860860463246862\n",
      "Stochastic Gradient Descent(38011): loss=7.632071320555187\n",
      "Stochastic Gradient Descent(38012): loss=1.2776171950381683\n",
      "Stochastic Gradient Descent(38013): loss=3.2236517956472137\n",
      "Stochastic Gradient Descent(38014): loss=0.3352935716829545\n",
      "Stochastic Gradient Descent(38015): loss=3.5190121588800274\n",
      "Stochastic Gradient Descent(38016): loss=1.4726136705798485\n",
      "Stochastic Gradient Descent(38017): loss=13.792621522484682\n",
      "Stochastic Gradient Descent(38018): loss=0.10494289566213688\n",
      "Stochastic Gradient Descent(38019): loss=0.483469930525086\n",
      "Stochastic Gradient Descent(38020): loss=0.5323536673635357\n",
      "Stochastic Gradient Descent(38021): loss=5.391303187465875\n",
      "Stochastic Gradient Descent(38022): loss=10.869544847189053\n",
      "Stochastic Gradient Descent(38023): loss=3.923211585676031\n",
      "Stochastic Gradient Descent(38024): loss=2.995059209967175\n",
      "Stochastic Gradient Descent(38025): loss=2.3048885886541015\n",
      "Stochastic Gradient Descent(38026): loss=12.186529071406802\n",
      "Stochastic Gradient Descent(38027): loss=6.573075633672794\n",
      "Stochastic Gradient Descent(38028): loss=1.5116240132049512\n",
      "Stochastic Gradient Descent(38029): loss=2.3647626873125556\n",
      "Stochastic Gradient Descent(38030): loss=10.518640878827123\n",
      "Stochastic Gradient Descent(38031): loss=1.7784658158082767\n",
      "Stochastic Gradient Descent(38032): loss=2.432883762048101\n",
      "Stochastic Gradient Descent(38033): loss=1.451530279328473\n",
      "Stochastic Gradient Descent(38034): loss=3.6481962454216865\n",
      "Stochastic Gradient Descent(38035): loss=1.3446213275213121\n",
      "Stochastic Gradient Descent(38036): loss=7.73985923830246\n",
      "Stochastic Gradient Descent(38037): loss=3.958357367609646\n",
      "Stochastic Gradient Descent(38038): loss=7.3598287704431655\n",
      "Stochastic Gradient Descent(38039): loss=0.31891743402085715\n",
      "Stochastic Gradient Descent(38040): loss=2.328663702038671\n",
      "Stochastic Gradient Descent(38041): loss=2.501963530817863\n",
      "Stochastic Gradient Descent(38042): loss=1.9297423054161083\n",
      "Stochastic Gradient Descent(38043): loss=0.9598288384478234\n",
      "Stochastic Gradient Descent(38044): loss=1.9786319709330686\n",
      "Stochastic Gradient Descent(38045): loss=21.35264614113498\n",
      "Stochastic Gradient Descent(38046): loss=1.4303136498365319\n",
      "Stochastic Gradient Descent(38047): loss=5.655084415283708\n",
      "Stochastic Gradient Descent(38048): loss=0.0005961395089735546\n",
      "Stochastic Gradient Descent(38049): loss=0.7897476370629013\n",
      "Stochastic Gradient Descent(38050): loss=6.014733570997973\n",
      "Stochastic Gradient Descent(38051): loss=2.888292840188676\n",
      "Stochastic Gradient Descent(38052): loss=0.24973593674960073\n",
      "Stochastic Gradient Descent(38053): loss=0.06690869089961818\n",
      "Stochastic Gradient Descent(38054): loss=5.272219684987484\n",
      "Stochastic Gradient Descent(38055): loss=7.800170658276439\n",
      "Stochastic Gradient Descent(38056): loss=1.89010564412086\n",
      "Stochastic Gradient Descent(38057): loss=4.633997112502165\n",
      "Stochastic Gradient Descent(38058): loss=1.4191702550963314\n",
      "Stochastic Gradient Descent(38059): loss=4.3482533085248525\n",
      "Stochastic Gradient Descent(38060): loss=5.753477020553149\n",
      "Stochastic Gradient Descent(38061): loss=5.8784552061536044e-05\n",
      "Stochastic Gradient Descent(38062): loss=3.8121358041592917\n",
      "Stochastic Gradient Descent(38063): loss=10.273783058746695\n",
      "Stochastic Gradient Descent(38064): loss=2.0730232838969114\n",
      "Stochastic Gradient Descent(38065): loss=0.8228162031506518\n",
      "Stochastic Gradient Descent(38066): loss=5.219082401549154\n",
      "Stochastic Gradient Descent(38067): loss=0.03201645225353824\n",
      "Stochastic Gradient Descent(38068): loss=0.04138075134404945\n",
      "Stochastic Gradient Descent(38069): loss=0.08903472422775814\n",
      "Stochastic Gradient Descent(38070): loss=0.09984839819110702\n",
      "Stochastic Gradient Descent(38071): loss=0.46769627244114953\n",
      "Stochastic Gradient Descent(38072): loss=0.03595886419775759\n",
      "Stochastic Gradient Descent(38073): loss=6.69933188754279\n",
      "Stochastic Gradient Descent(38074): loss=0.1642725677804846\n",
      "Stochastic Gradient Descent(38075): loss=1.6819129863116025\n",
      "Stochastic Gradient Descent(38076): loss=1.8647516261783865\n",
      "Stochastic Gradient Descent(38077): loss=1.3408262660635997\n",
      "Stochastic Gradient Descent(38078): loss=0.9654292271751429\n",
      "Stochastic Gradient Descent(38079): loss=6.925247009852674\n",
      "Stochastic Gradient Descent(38080): loss=0.786434042565955\n",
      "Stochastic Gradient Descent(38081): loss=0.02590023826658501\n",
      "Stochastic Gradient Descent(38082): loss=37.85976457426809\n",
      "Stochastic Gradient Descent(38083): loss=0.03002522507802012\n",
      "Stochastic Gradient Descent(38084): loss=3.1795514683541852\n",
      "Stochastic Gradient Descent(38085): loss=0.032303345831046076\n",
      "Stochastic Gradient Descent(38086): loss=0.1825716329293572\n",
      "Stochastic Gradient Descent(38087): loss=8.812474823255151\n",
      "Stochastic Gradient Descent(38088): loss=0.2186993204513421\n",
      "Stochastic Gradient Descent(38089): loss=4.856821475622494\n",
      "Stochastic Gradient Descent(38090): loss=13.931912292022645\n",
      "Stochastic Gradient Descent(38091): loss=0.03363086346543783\n",
      "Stochastic Gradient Descent(38092): loss=4.192310677205309\n",
      "Stochastic Gradient Descent(38093): loss=0.8138729477106695\n",
      "Stochastic Gradient Descent(38094): loss=0.9012643365839684\n",
      "Stochastic Gradient Descent(38095): loss=1.0048657242470156\n",
      "Stochastic Gradient Descent(38096): loss=0.0008029471019917786\n",
      "Stochastic Gradient Descent(38097): loss=0.003172653150839042\n",
      "Stochastic Gradient Descent(38098): loss=1.5666271212113598\n",
      "Stochastic Gradient Descent(38099): loss=3.737824582512337\n",
      "Stochastic Gradient Descent(38100): loss=0.3476740082367265\n",
      "Stochastic Gradient Descent(38101): loss=0.7890564323187474\n",
      "Stochastic Gradient Descent(38102): loss=3.1897356533860215\n",
      "Stochastic Gradient Descent(38103): loss=2.793187873812402\n",
      "Stochastic Gradient Descent(38104): loss=0.26200697561664366\n",
      "Stochastic Gradient Descent(38105): loss=6.316345107831016\n",
      "Stochastic Gradient Descent(38106): loss=7.458482007063103\n",
      "Stochastic Gradient Descent(38107): loss=2.4935743077524433\n",
      "Stochastic Gradient Descent(38108): loss=6.693862852235867\n",
      "Stochastic Gradient Descent(38109): loss=8.816999687697301\n",
      "Stochastic Gradient Descent(38110): loss=2.6648033636374855\n",
      "Stochastic Gradient Descent(38111): loss=3.9079885110315424\n",
      "Stochastic Gradient Descent(38112): loss=2.9785144013561484\n",
      "Stochastic Gradient Descent(38113): loss=2.320656635457674\n",
      "Stochastic Gradient Descent(38114): loss=4.478328777567576\n",
      "Stochastic Gradient Descent(38115): loss=0.0338494177897207\n",
      "Stochastic Gradient Descent(38116): loss=3.181111593905286\n",
      "Stochastic Gradient Descent(38117): loss=13.767033582448795\n",
      "Stochastic Gradient Descent(38118): loss=4.920807227365466\n",
      "Stochastic Gradient Descent(38119): loss=0.026541923079370464\n",
      "Stochastic Gradient Descent(38120): loss=7.147818533718808\n",
      "Stochastic Gradient Descent(38121): loss=1.452813953975452\n",
      "Stochastic Gradient Descent(38122): loss=0.7466426592182008\n",
      "Stochastic Gradient Descent(38123): loss=4.673018055837872\n",
      "Stochastic Gradient Descent(38124): loss=2.5036852729294203\n",
      "Stochastic Gradient Descent(38125): loss=6.766810610499313\n",
      "Stochastic Gradient Descent(38126): loss=1.5489846654246378\n",
      "Stochastic Gradient Descent(38127): loss=3.6155596709789046\n",
      "Stochastic Gradient Descent(38128): loss=0.8721969424953179\n",
      "Stochastic Gradient Descent(38129): loss=1.8187942943927773\n",
      "Stochastic Gradient Descent(38130): loss=7.483270744234817\n",
      "Stochastic Gradient Descent(38131): loss=1.7438258865767755\n",
      "Stochastic Gradient Descent(38132): loss=0.06043355430963782\n",
      "Stochastic Gradient Descent(38133): loss=0.3554447688079432\n",
      "Stochastic Gradient Descent(38134): loss=0.029818090942598432\n",
      "Stochastic Gradient Descent(38135): loss=0.03609222669434794\n",
      "Stochastic Gradient Descent(38136): loss=11.65351388648732\n",
      "Stochastic Gradient Descent(38137): loss=12.394056459481389\n",
      "Stochastic Gradient Descent(38138): loss=11.664833111413666\n",
      "Stochastic Gradient Descent(38139): loss=10.708435477159984\n",
      "Stochastic Gradient Descent(38140): loss=1.9458818447985198\n",
      "Stochastic Gradient Descent(38141): loss=0.468197086558077\n",
      "Stochastic Gradient Descent(38142): loss=0.28216363452932625\n",
      "Stochastic Gradient Descent(38143): loss=0.00023218897427050296\n",
      "Stochastic Gradient Descent(38144): loss=3.649491652655746\n",
      "Stochastic Gradient Descent(38145): loss=0.6610517772034182\n",
      "Stochastic Gradient Descent(38146): loss=0.2713402636723119\n",
      "Stochastic Gradient Descent(38147): loss=1.7005036409903198\n",
      "Stochastic Gradient Descent(38148): loss=3.953894517135888\n",
      "Stochastic Gradient Descent(38149): loss=3.612662437274795\n",
      "Stochastic Gradient Descent(38150): loss=0.6419401936456981\n",
      "Stochastic Gradient Descent(38151): loss=4.80944730520662\n",
      "Stochastic Gradient Descent(38152): loss=3.2552815640999313\n",
      "Stochastic Gradient Descent(38153): loss=0.004056014923950707\n",
      "Stochastic Gradient Descent(38154): loss=9.641353543155123\n",
      "Stochastic Gradient Descent(38155): loss=0.6681759442769454\n",
      "Stochastic Gradient Descent(38156): loss=0.8165828159609538\n",
      "Stochastic Gradient Descent(38157): loss=1.4327100710064729\n",
      "Stochastic Gradient Descent(38158): loss=1.8160665527793043\n",
      "Stochastic Gradient Descent(38159): loss=0.08227690618497141\n",
      "Stochastic Gradient Descent(38160): loss=8.991934922052362\n",
      "Stochastic Gradient Descent(38161): loss=0.700194493866462\n",
      "Stochastic Gradient Descent(38162): loss=0.44969647942516217\n",
      "Stochastic Gradient Descent(38163): loss=0.15843365316149574\n",
      "Stochastic Gradient Descent(38164): loss=6.989346128051046\n",
      "Stochastic Gradient Descent(38165): loss=0.8749809177833299\n",
      "Stochastic Gradient Descent(38166): loss=0.8179876735158873\n",
      "Stochastic Gradient Descent(38167): loss=1.9566107594588098\n",
      "Stochastic Gradient Descent(38168): loss=5.358360562337294\n",
      "Stochastic Gradient Descent(38169): loss=4.93234145230966\n",
      "Stochastic Gradient Descent(38170): loss=0.1516407331021328\n",
      "Stochastic Gradient Descent(38171): loss=2.1927049686129774\n",
      "Stochastic Gradient Descent(38172): loss=13.987287124862466\n",
      "Stochastic Gradient Descent(38173): loss=16.9593337458189\n",
      "Stochastic Gradient Descent(38174): loss=0.25531096252707647\n",
      "Stochastic Gradient Descent(38175): loss=1.8220367393033523\n",
      "Stochastic Gradient Descent(38176): loss=0.005932059485772805\n",
      "Stochastic Gradient Descent(38177): loss=0.07062299772763826\n",
      "Stochastic Gradient Descent(38178): loss=0.03475187603603687\n",
      "Stochastic Gradient Descent(38179): loss=8.197902115780082\n",
      "Stochastic Gradient Descent(38180): loss=3.2098149489083645\n",
      "Stochastic Gradient Descent(38181): loss=0.38028905305359473\n",
      "Stochastic Gradient Descent(38182): loss=8.33407681579942\n",
      "Stochastic Gradient Descent(38183): loss=4.905656212435449\n",
      "Stochastic Gradient Descent(38184): loss=2.0453345284044553\n",
      "Stochastic Gradient Descent(38185): loss=0.7765067304504504\n",
      "Stochastic Gradient Descent(38186): loss=5.555204774660631\n",
      "Stochastic Gradient Descent(38187): loss=0.05606824988970791\n",
      "Stochastic Gradient Descent(38188): loss=0.033143603917498446\n",
      "Stochastic Gradient Descent(38189): loss=7.570794126463887\n",
      "Stochastic Gradient Descent(38190): loss=23.123814840366162\n",
      "Stochastic Gradient Descent(38191): loss=0.0017755380809000292\n",
      "Stochastic Gradient Descent(38192): loss=3.8435022250368336\n",
      "Stochastic Gradient Descent(38193): loss=7.123786299454862\n",
      "Stochastic Gradient Descent(38194): loss=6.065925362991423\n",
      "Stochastic Gradient Descent(38195): loss=5.7739256102776935\n",
      "Stochastic Gradient Descent(38196): loss=9.473547620431694\n",
      "Stochastic Gradient Descent(38197): loss=0.9755169643320195\n",
      "Stochastic Gradient Descent(38198): loss=1.1270054838301378\n",
      "Stochastic Gradient Descent(38199): loss=0.3082213822938944\n",
      "Stochastic Gradient Descent(38200): loss=0.5387969389805023\n",
      "Stochastic Gradient Descent(38201): loss=0.334363301278654\n",
      "Stochastic Gradient Descent(38202): loss=1.7238803392211373\n",
      "Stochastic Gradient Descent(38203): loss=1.8474738649885167\n",
      "Stochastic Gradient Descent(38204): loss=3.3845925640870704\n",
      "Stochastic Gradient Descent(38205): loss=6.738919118827088\n",
      "Stochastic Gradient Descent(38206): loss=2.9614136010428598\n",
      "Stochastic Gradient Descent(38207): loss=0.0017284147807393214\n",
      "Stochastic Gradient Descent(38208): loss=3.654687127943306\n",
      "Stochastic Gradient Descent(38209): loss=4.503455608845384\n",
      "Stochastic Gradient Descent(38210): loss=0.04996035137310528\n",
      "Stochastic Gradient Descent(38211): loss=2.5771420056744065\n",
      "Stochastic Gradient Descent(38212): loss=10.419795633394942\n",
      "Stochastic Gradient Descent(38213): loss=0.058007386838552784\n",
      "Stochastic Gradient Descent(38214): loss=0.2511423451605418\n",
      "Stochastic Gradient Descent(38215): loss=0.8449699398261364\n",
      "Stochastic Gradient Descent(38216): loss=0.7297707306100475\n",
      "Stochastic Gradient Descent(38217): loss=14.657759923029609\n",
      "Stochastic Gradient Descent(38218): loss=0.08273326394637454\n",
      "Stochastic Gradient Descent(38219): loss=0.07680709210103329\n",
      "Stochastic Gradient Descent(38220): loss=3.2763605523999577\n",
      "Stochastic Gradient Descent(38221): loss=4.777032376048985\n",
      "Stochastic Gradient Descent(38222): loss=0.012077892468247663\n",
      "Stochastic Gradient Descent(38223): loss=0.6716943494446854\n",
      "Stochastic Gradient Descent(38224): loss=0.5741400928622268\n",
      "Stochastic Gradient Descent(38225): loss=2.445165063336246\n",
      "Stochastic Gradient Descent(38226): loss=1.0496639535795262\n",
      "Stochastic Gradient Descent(38227): loss=0.7484915701641792\n",
      "Stochastic Gradient Descent(38228): loss=0.025314124105418015\n",
      "Stochastic Gradient Descent(38229): loss=0.028748929481495145\n",
      "Stochastic Gradient Descent(38230): loss=0.08976842229704712\n",
      "Stochastic Gradient Descent(38231): loss=0.5948370484779326\n",
      "Stochastic Gradient Descent(38232): loss=9.168950348652993e-05\n",
      "Stochastic Gradient Descent(38233): loss=17.561822295302147\n",
      "Stochastic Gradient Descent(38234): loss=4.875912702210573\n",
      "Stochastic Gradient Descent(38235): loss=6.399540465714241\n",
      "Stochastic Gradient Descent(38236): loss=3.1235240010334855\n",
      "Stochastic Gradient Descent(38237): loss=4.269737339907377\n",
      "Stochastic Gradient Descent(38238): loss=9.783953778210485\n",
      "Stochastic Gradient Descent(38239): loss=9.564649053407907\n",
      "Stochastic Gradient Descent(38240): loss=0.07201357590686254\n",
      "Stochastic Gradient Descent(38241): loss=5.418092614443375\n",
      "Stochastic Gradient Descent(38242): loss=2.8293553193297303\n",
      "Stochastic Gradient Descent(38243): loss=19.90767995073012\n",
      "Stochastic Gradient Descent(38244): loss=0.20299562352206452\n",
      "Stochastic Gradient Descent(38245): loss=0.9908683299985992\n",
      "Stochastic Gradient Descent(38246): loss=5.9014400314444995\n",
      "Stochastic Gradient Descent(38247): loss=0.4370779606991822\n",
      "Stochastic Gradient Descent(38248): loss=27.967450769443985\n",
      "Stochastic Gradient Descent(38249): loss=2.39652209061482\n",
      "Stochastic Gradient Descent(38250): loss=2.46667075051774\n",
      "Stochastic Gradient Descent(38251): loss=0.1248090297407494\n",
      "Stochastic Gradient Descent(38252): loss=0.1802280398654684\n",
      "Stochastic Gradient Descent(38253): loss=2.847067957742357\n",
      "Stochastic Gradient Descent(38254): loss=14.59578776037604\n",
      "Stochastic Gradient Descent(38255): loss=11.512540128719397\n",
      "Stochastic Gradient Descent(38256): loss=2.604740492842289\n",
      "Stochastic Gradient Descent(38257): loss=6.923243796302001\n",
      "Stochastic Gradient Descent(38258): loss=5.36438152993894\n",
      "Stochastic Gradient Descent(38259): loss=0.6459616865374416\n",
      "Stochastic Gradient Descent(38260): loss=4.814229572478007\n",
      "Stochastic Gradient Descent(38261): loss=0.7291634372473343\n",
      "Stochastic Gradient Descent(38262): loss=2.0255297763514206\n",
      "Stochastic Gradient Descent(38263): loss=7.739860190684901\n",
      "Stochastic Gradient Descent(38264): loss=0.5006485765084044\n",
      "Stochastic Gradient Descent(38265): loss=0.9953051073888329\n",
      "Stochastic Gradient Descent(38266): loss=1.0979715445136853\n",
      "Stochastic Gradient Descent(38267): loss=12.02682942711335\n",
      "Stochastic Gradient Descent(38268): loss=0.3504659950293776\n",
      "Stochastic Gradient Descent(38269): loss=3.7745572171672537\n",
      "Stochastic Gradient Descent(38270): loss=2.197531131764736\n",
      "Stochastic Gradient Descent(38271): loss=3.0971064601481175\n",
      "Stochastic Gradient Descent(38272): loss=2.202509593912718\n",
      "Stochastic Gradient Descent(38273): loss=5.248670567937025\n",
      "Stochastic Gradient Descent(38274): loss=0.3989770468890462\n",
      "Stochastic Gradient Descent(38275): loss=0.014678140138138918\n",
      "Stochastic Gradient Descent(38276): loss=13.120676934214458\n",
      "Stochastic Gradient Descent(38277): loss=0.4495513331589232\n",
      "Stochastic Gradient Descent(38278): loss=8.420490532559175\n",
      "Stochastic Gradient Descent(38279): loss=0.22590191709204768\n",
      "Stochastic Gradient Descent(38280): loss=7.41546939246619\n",
      "Stochastic Gradient Descent(38281): loss=0.00888131193688103\n",
      "Stochastic Gradient Descent(38282): loss=2.164243571567161\n",
      "Stochastic Gradient Descent(38283): loss=0.20950495760481208\n",
      "Stochastic Gradient Descent(38284): loss=21.048945117256984\n",
      "Stochastic Gradient Descent(38285): loss=6.712450877261864\n",
      "Stochastic Gradient Descent(38286): loss=35.807994843459895\n",
      "Stochastic Gradient Descent(38287): loss=5.49307670736805\n",
      "Stochastic Gradient Descent(38288): loss=6.386851876443392\n",
      "Stochastic Gradient Descent(38289): loss=1.0423711577049188\n",
      "Stochastic Gradient Descent(38290): loss=3.174044849918262\n",
      "Stochastic Gradient Descent(38291): loss=1.9776392606070878\n",
      "Stochastic Gradient Descent(38292): loss=0.736286052843977\n",
      "Stochastic Gradient Descent(38293): loss=0.5139571174128338\n",
      "Stochastic Gradient Descent(38294): loss=0.9930413636028593\n",
      "Stochastic Gradient Descent(38295): loss=1.4739395791777052\n",
      "Stochastic Gradient Descent(38296): loss=17.88275738715297\n",
      "Stochastic Gradient Descent(38297): loss=1.7564536771041186\n",
      "Stochastic Gradient Descent(38298): loss=7.750562100873684\n",
      "Stochastic Gradient Descent(38299): loss=0.005052907022051798\n",
      "Stochastic Gradient Descent(38300): loss=0.5217663542758567\n",
      "Stochastic Gradient Descent(38301): loss=0.686632979760928\n",
      "Stochastic Gradient Descent(38302): loss=12.895938878222665\n",
      "Stochastic Gradient Descent(38303): loss=2.632545831123256\n",
      "Stochastic Gradient Descent(38304): loss=6.780814363600966\n",
      "Stochastic Gradient Descent(38305): loss=14.516123467638824\n",
      "Stochastic Gradient Descent(38306): loss=27.801668113601902\n",
      "Stochastic Gradient Descent(38307): loss=0.3862136827677882\n",
      "Stochastic Gradient Descent(38308): loss=1.081176912875009\n",
      "Stochastic Gradient Descent(38309): loss=7.799008748606856\n",
      "Stochastic Gradient Descent(38310): loss=5.011641039501062\n",
      "Stochastic Gradient Descent(38311): loss=13.096436454496573\n",
      "Stochastic Gradient Descent(38312): loss=0.05128766872800802\n",
      "Stochastic Gradient Descent(38313): loss=0.15774399081145463\n",
      "Stochastic Gradient Descent(38314): loss=4.641942454584148\n",
      "Stochastic Gradient Descent(38315): loss=1.412314863870678\n",
      "Stochastic Gradient Descent(38316): loss=1.9165088910464907\n",
      "Stochastic Gradient Descent(38317): loss=3.823292778586991\n",
      "Stochastic Gradient Descent(38318): loss=0.0007158788141735335\n",
      "Stochastic Gradient Descent(38319): loss=1.7515828034358645\n",
      "Stochastic Gradient Descent(38320): loss=7.646310597960888\n",
      "Stochastic Gradient Descent(38321): loss=0.029909599537474667\n",
      "Stochastic Gradient Descent(38322): loss=4.944543338267177\n",
      "Stochastic Gradient Descent(38323): loss=0.03894898479201125\n",
      "Stochastic Gradient Descent(38324): loss=5.482873947689623\n",
      "Stochastic Gradient Descent(38325): loss=12.366093997912083\n",
      "Stochastic Gradient Descent(38326): loss=0.1326991279773613\n",
      "Stochastic Gradient Descent(38327): loss=5.332582345234596\n",
      "Stochastic Gradient Descent(38328): loss=4.8439556868178055\n",
      "Stochastic Gradient Descent(38329): loss=4.995446277694275\n",
      "Stochastic Gradient Descent(38330): loss=1.4311761204563298\n",
      "Stochastic Gradient Descent(38331): loss=1.9379262125461156\n",
      "Stochastic Gradient Descent(38332): loss=0.017859691031103374\n",
      "Stochastic Gradient Descent(38333): loss=2.974048404983836\n",
      "Stochastic Gradient Descent(38334): loss=1.6883045739301237\n",
      "Stochastic Gradient Descent(38335): loss=11.796905078249921\n",
      "Stochastic Gradient Descent(38336): loss=1.5062846260497318\n",
      "Stochastic Gradient Descent(38337): loss=0.8685432561927197\n",
      "Stochastic Gradient Descent(38338): loss=1.1690635557849096\n",
      "Stochastic Gradient Descent(38339): loss=0.025648218273551106\n",
      "Stochastic Gradient Descent(38340): loss=3.2422872050920954\n",
      "Stochastic Gradient Descent(38341): loss=1.7848285646259794\n",
      "Stochastic Gradient Descent(38342): loss=5.315954864587763\n",
      "Stochastic Gradient Descent(38343): loss=4.4003370562505815\n",
      "Stochastic Gradient Descent(38344): loss=15.449042914710805\n",
      "Stochastic Gradient Descent(38345): loss=0.14918927331695447\n",
      "Stochastic Gradient Descent(38346): loss=1.6826840897923439\n",
      "Stochastic Gradient Descent(38347): loss=1.6155044830124865\n",
      "Stochastic Gradient Descent(38348): loss=29.779987181497116\n",
      "Stochastic Gradient Descent(38349): loss=0.4658860202429226\n",
      "Stochastic Gradient Descent(38350): loss=2.0932559843519214\n",
      "Stochastic Gradient Descent(38351): loss=0.14440567480355995\n",
      "Stochastic Gradient Descent(38352): loss=9.849850521560597\n",
      "Stochastic Gradient Descent(38353): loss=0.9532085836774247\n",
      "Stochastic Gradient Descent(38354): loss=0.26526140569580386\n",
      "Stochastic Gradient Descent(38355): loss=1.3952102309440362\n",
      "Stochastic Gradient Descent(38356): loss=0.00034362930378409307\n",
      "Stochastic Gradient Descent(38357): loss=1.1688050396084806\n",
      "Stochastic Gradient Descent(38358): loss=6.196225940909279\n",
      "Stochastic Gradient Descent(38359): loss=0.0038007055288068934\n",
      "Stochastic Gradient Descent(38360): loss=13.669014948733407\n",
      "Stochastic Gradient Descent(38361): loss=8.329009990143406\n",
      "Stochastic Gradient Descent(38362): loss=5.460563709568056\n",
      "Stochastic Gradient Descent(38363): loss=6.387414686963777\n",
      "Stochastic Gradient Descent(38364): loss=0.2536248956096753\n",
      "Stochastic Gradient Descent(38365): loss=5.756358151184408\n",
      "Stochastic Gradient Descent(38366): loss=1.3017868102728698\n",
      "Stochastic Gradient Descent(38367): loss=11.795182572120806\n",
      "Stochastic Gradient Descent(38368): loss=20.83337076207112\n",
      "Stochastic Gradient Descent(38369): loss=0.270383731058988\n",
      "Stochastic Gradient Descent(38370): loss=0.07488797418993229\n",
      "Stochastic Gradient Descent(38371): loss=0.7111582229164023\n",
      "Stochastic Gradient Descent(38372): loss=1.3442546607558499\n",
      "Stochastic Gradient Descent(38373): loss=3.0347745294346318\n",
      "Stochastic Gradient Descent(38374): loss=0.2667667465975058\n",
      "Stochastic Gradient Descent(38375): loss=0.28414230788852035\n",
      "Stochastic Gradient Descent(38376): loss=0.061964053919101036\n",
      "Stochastic Gradient Descent(38377): loss=0.49385763214411343\n",
      "Stochastic Gradient Descent(38378): loss=0.9423918040149832\n",
      "Stochastic Gradient Descent(38379): loss=3.8859612528268492\n",
      "Stochastic Gradient Descent(38380): loss=1.9491019100076337\n",
      "Stochastic Gradient Descent(38381): loss=40.79841093127168\n",
      "Stochastic Gradient Descent(38382): loss=0.023455072501247527\n",
      "Stochastic Gradient Descent(38383): loss=19.893852195226923\n",
      "Stochastic Gradient Descent(38384): loss=0.8337299695821877\n",
      "Stochastic Gradient Descent(38385): loss=0.0002852301413929751\n",
      "Stochastic Gradient Descent(38386): loss=7.21047289479585\n",
      "Stochastic Gradient Descent(38387): loss=10.382803697890171\n",
      "Stochastic Gradient Descent(38388): loss=0.7492172269900834\n",
      "Stochastic Gradient Descent(38389): loss=17.100852790189624\n",
      "Stochastic Gradient Descent(38390): loss=4.348024442390622\n",
      "Stochastic Gradient Descent(38391): loss=2.679380121635183\n",
      "Stochastic Gradient Descent(38392): loss=3.6220800902309693\n",
      "Stochastic Gradient Descent(38393): loss=29.422951450969393\n",
      "Stochastic Gradient Descent(38394): loss=4.985788902507929\n",
      "Stochastic Gradient Descent(38395): loss=8.83034956505932\n",
      "Stochastic Gradient Descent(38396): loss=9.585044895064216\n",
      "Stochastic Gradient Descent(38397): loss=0.670392457944881\n",
      "Stochastic Gradient Descent(38398): loss=12.696412406106504\n",
      "Stochastic Gradient Descent(38399): loss=11.857820170127008\n",
      "Stochastic Gradient Descent(38400): loss=0.05609817733278638\n",
      "Stochastic Gradient Descent(38401): loss=1.849014409649408\n",
      "Stochastic Gradient Descent(38402): loss=12.839596539256632\n",
      "Stochastic Gradient Descent(38403): loss=3.492872317699581\n",
      "Stochastic Gradient Descent(38404): loss=7.220452943057637\n",
      "Stochastic Gradient Descent(38405): loss=21.746816192120214\n",
      "Stochastic Gradient Descent(38406): loss=4.450992267626048\n",
      "Stochastic Gradient Descent(38407): loss=0.944443869055346\n",
      "Stochastic Gradient Descent(38408): loss=5.34715130477919\n",
      "Stochastic Gradient Descent(38409): loss=0.0989347897420332\n",
      "Stochastic Gradient Descent(38410): loss=0.8698663184282976\n",
      "Stochastic Gradient Descent(38411): loss=12.48265457361914\n",
      "Stochastic Gradient Descent(38412): loss=0.617983706711122\n",
      "Stochastic Gradient Descent(38413): loss=0.02176512184234654\n",
      "Stochastic Gradient Descent(38414): loss=1.6721244068179901\n",
      "Stochastic Gradient Descent(38415): loss=0.04546337481271076\n",
      "Stochastic Gradient Descent(38416): loss=2.319359421175949\n",
      "Stochastic Gradient Descent(38417): loss=0.5313667848249967\n",
      "Stochastic Gradient Descent(38418): loss=0.9607561326932265\n",
      "Stochastic Gradient Descent(38419): loss=0.432026515333289\n",
      "Stochastic Gradient Descent(38420): loss=0.9082703593367277\n",
      "Stochastic Gradient Descent(38421): loss=0.06071481679535896\n",
      "Stochastic Gradient Descent(38422): loss=1.5538274187114165\n",
      "Stochastic Gradient Descent(38423): loss=1.5748737910964496\n",
      "Stochastic Gradient Descent(38424): loss=6.489785281238917\n",
      "Stochastic Gradient Descent(38425): loss=3.183796740418815\n",
      "Stochastic Gradient Descent(38426): loss=0.15866856624989295\n",
      "Stochastic Gradient Descent(38427): loss=0.45347055924605895\n",
      "Stochastic Gradient Descent(38428): loss=1.7598253686187615\n",
      "Stochastic Gradient Descent(38429): loss=0.23221530729039297\n",
      "Stochastic Gradient Descent(38430): loss=1.4963437127761727\n",
      "Stochastic Gradient Descent(38431): loss=0.5072571960914688\n",
      "Stochastic Gradient Descent(38432): loss=0.156146168906458\n",
      "Stochastic Gradient Descent(38433): loss=2.459145192071895\n",
      "Stochastic Gradient Descent(38434): loss=20.461100712597897\n",
      "Stochastic Gradient Descent(38435): loss=0.516874482962332\n",
      "Stochastic Gradient Descent(38436): loss=0.3581863431602264\n",
      "Stochastic Gradient Descent(38437): loss=10.472465577096779\n",
      "Stochastic Gradient Descent(38438): loss=2.3357336958589885\n",
      "Stochastic Gradient Descent(38439): loss=10.037032654659791\n",
      "Stochastic Gradient Descent(38440): loss=6.149199160395785\n",
      "Stochastic Gradient Descent(38441): loss=14.681906561076856\n",
      "Stochastic Gradient Descent(38442): loss=0.16490818226280504\n",
      "Stochastic Gradient Descent(38443): loss=0.10481378451540839\n",
      "Stochastic Gradient Descent(38444): loss=0.2938628626174781\n",
      "Stochastic Gradient Descent(38445): loss=13.302256393545592\n",
      "Stochastic Gradient Descent(38446): loss=9.326884384966071\n",
      "Stochastic Gradient Descent(38447): loss=1.0640757012493847\n",
      "Stochastic Gradient Descent(38448): loss=1.2567873984563596\n",
      "Stochastic Gradient Descent(38449): loss=16.276370785768993\n",
      "Stochastic Gradient Descent(38450): loss=4.743677348440212\n",
      "Stochastic Gradient Descent(38451): loss=1.0673285446481893\n",
      "Stochastic Gradient Descent(38452): loss=9.278166641879226\n",
      "Stochastic Gradient Descent(38453): loss=0.26801682855493814\n",
      "Stochastic Gradient Descent(38454): loss=3.182502571730086\n",
      "Stochastic Gradient Descent(38455): loss=11.52578966162821\n",
      "Stochastic Gradient Descent(38456): loss=6.365274494174573\n",
      "Stochastic Gradient Descent(38457): loss=0.11473493228066235\n",
      "Stochastic Gradient Descent(38458): loss=0.6849197858506905\n",
      "Stochastic Gradient Descent(38459): loss=3.3714467132866472\n",
      "Stochastic Gradient Descent(38460): loss=29.744210019817018\n",
      "Stochastic Gradient Descent(38461): loss=4.282934808001575\n",
      "Stochastic Gradient Descent(38462): loss=6.154580389883875\n",
      "Stochastic Gradient Descent(38463): loss=11.34787411680039\n",
      "Stochastic Gradient Descent(38464): loss=14.15057619864693\n",
      "Stochastic Gradient Descent(38465): loss=8.119225032075397\n",
      "Stochastic Gradient Descent(38466): loss=0.13828009133206348\n",
      "Stochastic Gradient Descent(38467): loss=8.893458371671704\n",
      "Stochastic Gradient Descent(38468): loss=0.09117669896227552\n",
      "Stochastic Gradient Descent(38469): loss=3.79318284934509\n",
      "Stochastic Gradient Descent(38470): loss=0.676620262714226\n",
      "Stochastic Gradient Descent(38471): loss=21.48615761804435\n",
      "Stochastic Gradient Descent(38472): loss=1.511157285431384\n",
      "Stochastic Gradient Descent(38473): loss=6.17028215786265\n",
      "Stochastic Gradient Descent(38474): loss=0.8430798333725971\n",
      "Stochastic Gradient Descent(38475): loss=1.6723586523921596\n",
      "Stochastic Gradient Descent(38476): loss=0.24366215354387658\n",
      "Stochastic Gradient Descent(38477): loss=2.9687797833704743\n",
      "Stochastic Gradient Descent(38478): loss=0.13435163826216864\n",
      "Stochastic Gradient Descent(38479): loss=0.3272183062240813\n",
      "Stochastic Gradient Descent(38480): loss=0.012972435639423487\n",
      "Stochastic Gradient Descent(38481): loss=9.04311492692873\n",
      "Stochastic Gradient Descent(38482): loss=0.3408069687524352\n",
      "Stochastic Gradient Descent(38483): loss=0.05649630228461685\n",
      "Stochastic Gradient Descent(38484): loss=5.352795445938255\n",
      "Stochastic Gradient Descent(38485): loss=2.704592730788637\n",
      "Stochastic Gradient Descent(38486): loss=3.232214487555027\n",
      "Stochastic Gradient Descent(38487): loss=12.604349004419122\n",
      "Stochastic Gradient Descent(38488): loss=3.724816375779356\n",
      "Stochastic Gradient Descent(38489): loss=0.24922445009130523\n",
      "Stochastic Gradient Descent(38490): loss=1.5238941204147596\n",
      "Stochastic Gradient Descent(38491): loss=4.232448851911548\n",
      "Stochastic Gradient Descent(38492): loss=1.8394729811150983\n",
      "Stochastic Gradient Descent(38493): loss=1.1755308325538214\n",
      "Stochastic Gradient Descent(38494): loss=0.6769492770079154\n",
      "Stochastic Gradient Descent(38495): loss=41.27132112686614\n",
      "Stochastic Gradient Descent(38496): loss=3.8215940299657016\n",
      "Stochastic Gradient Descent(38497): loss=0.12033999692334785\n",
      "Stochastic Gradient Descent(38498): loss=5.431076942933336\n",
      "Stochastic Gradient Descent(38499): loss=1.1970847229234185\n",
      "Stochastic Gradient Descent(38500): loss=4.690922122744549\n",
      "Stochastic Gradient Descent(38501): loss=0.23035707809585942\n",
      "Stochastic Gradient Descent(38502): loss=1.4733231289075128\n",
      "Stochastic Gradient Descent(38503): loss=1.0342474084046323\n",
      "Stochastic Gradient Descent(38504): loss=6.4499904203603275\n",
      "Stochastic Gradient Descent(38505): loss=12.468205425109211\n",
      "Stochastic Gradient Descent(38506): loss=2.094874846196102\n",
      "Stochastic Gradient Descent(38507): loss=4.410203442374255\n",
      "Stochastic Gradient Descent(38508): loss=12.15368837578728\n",
      "Stochastic Gradient Descent(38509): loss=8.4331140746719\n",
      "Stochastic Gradient Descent(38510): loss=2.581856301340891\n",
      "Stochastic Gradient Descent(38511): loss=0.7527618106853844\n",
      "Stochastic Gradient Descent(38512): loss=0.6315549303682513\n",
      "Stochastic Gradient Descent(38513): loss=5.411569198008046\n",
      "Stochastic Gradient Descent(38514): loss=37.866849228955836\n",
      "Stochastic Gradient Descent(38515): loss=2.604158542515131\n",
      "Stochastic Gradient Descent(38516): loss=3.910028362509364\n",
      "Stochastic Gradient Descent(38517): loss=10.089362190737631\n",
      "Stochastic Gradient Descent(38518): loss=0.6073087794615518\n",
      "Stochastic Gradient Descent(38519): loss=7.657393638142679\n",
      "Stochastic Gradient Descent(38520): loss=5.5750918315931655\n",
      "Stochastic Gradient Descent(38521): loss=3.7422796384733488\n",
      "Stochastic Gradient Descent(38522): loss=1.8984300920895523\n",
      "Stochastic Gradient Descent(38523): loss=2.8839664944424315\n",
      "Stochastic Gradient Descent(38524): loss=0.0006163235042225631\n",
      "Stochastic Gradient Descent(38525): loss=1.2698883715458829\n",
      "Stochastic Gradient Descent(38526): loss=21.517628505556413\n",
      "Stochastic Gradient Descent(38527): loss=0.2501665663715919\n",
      "Stochastic Gradient Descent(38528): loss=0.6641648252060555\n",
      "Stochastic Gradient Descent(38529): loss=7.299377070131118\n",
      "Stochastic Gradient Descent(38530): loss=15.417405979115612\n",
      "Stochastic Gradient Descent(38531): loss=6.542179197685806\n",
      "Stochastic Gradient Descent(38532): loss=21.67785504591184\n",
      "Stochastic Gradient Descent(38533): loss=0.005944276009267088\n",
      "Stochastic Gradient Descent(38534): loss=0.06233107936449698\n",
      "Stochastic Gradient Descent(38535): loss=0.3623634916859794\n",
      "Stochastic Gradient Descent(38536): loss=1.468546209994512\n",
      "Stochastic Gradient Descent(38537): loss=3.6104958866120787\n",
      "Stochastic Gradient Descent(38538): loss=1.4015139570459534\n",
      "Stochastic Gradient Descent(38539): loss=8.745580879738945\n",
      "Stochastic Gradient Descent(38540): loss=5.655802617927642\n",
      "Stochastic Gradient Descent(38541): loss=5.317713545940628\n",
      "Stochastic Gradient Descent(38542): loss=11.382644100761\n",
      "Stochastic Gradient Descent(38543): loss=0.002552048757818988\n",
      "Stochastic Gradient Descent(38544): loss=0.8371109844363841\n",
      "Stochastic Gradient Descent(38545): loss=1.257432002892056\n",
      "Stochastic Gradient Descent(38546): loss=0.3556895939086023\n",
      "Stochastic Gradient Descent(38547): loss=5.303008705260758\n",
      "Stochastic Gradient Descent(38548): loss=15.210588429052933\n",
      "Stochastic Gradient Descent(38549): loss=0.009352344736327341\n",
      "Stochastic Gradient Descent(38550): loss=0.4144047005561794\n",
      "Stochastic Gradient Descent(38551): loss=3.3561411697277292\n",
      "Stochastic Gradient Descent(38552): loss=4.004578844720353\n",
      "Stochastic Gradient Descent(38553): loss=0.44640800170988687\n",
      "Stochastic Gradient Descent(38554): loss=0.08660848862747073\n",
      "Stochastic Gradient Descent(38555): loss=0.0007372395086525064\n",
      "Stochastic Gradient Descent(38556): loss=5.374815605001021\n",
      "Stochastic Gradient Descent(38557): loss=0.26815173948176174\n",
      "Stochastic Gradient Descent(38558): loss=2.083233340515184\n",
      "Stochastic Gradient Descent(38559): loss=7.437761224102016\n",
      "Stochastic Gradient Descent(38560): loss=3.954397633853946\n",
      "Stochastic Gradient Descent(38561): loss=21.312837075883333\n",
      "Stochastic Gradient Descent(38562): loss=3.9227258491029455\n",
      "Stochastic Gradient Descent(38563): loss=0.6307375749144922\n",
      "Stochastic Gradient Descent(38564): loss=6.606999816899849\n",
      "Stochastic Gradient Descent(38565): loss=0.011260997564845875\n",
      "Stochastic Gradient Descent(38566): loss=2.180368289883527\n",
      "Stochastic Gradient Descent(38567): loss=8.224025580695244\n",
      "Stochastic Gradient Descent(38568): loss=0.5731202065952855\n",
      "Stochastic Gradient Descent(38569): loss=11.483586848057678\n",
      "Stochastic Gradient Descent(38570): loss=2.80598511660911\n",
      "Stochastic Gradient Descent(38571): loss=8.83616518515754\n",
      "Stochastic Gradient Descent(38572): loss=0.06132227541140089\n",
      "Stochastic Gradient Descent(38573): loss=0.9164112853585934\n",
      "Stochastic Gradient Descent(38574): loss=2.2530530108611364\n",
      "Stochastic Gradient Descent(38575): loss=0.08581357536967563\n",
      "Stochastic Gradient Descent(38576): loss=1.8312839257138136\n",
      "Stochastic Gradient Descent(38577): loss=0.7036978244602252\n",
      "Stochastic Gradient Descent(38578): loss=0.10624042264928737\n",
      "Stochastic Gradient Descent(38579): loss=24.012136743817845\n",
      "Stochastic Gradient Descent(38580): loss=11.220974552970135\n",
      "Stochastic Gradient Descent(38581): loss=21.86891149504705\n",
      "Stochastic Gradient Descent(38582): loss=0.19909071895516336\n",
      "Stochastic Gradient Descent(38583): loss=23.61049309032593\n",
      "Stochastic Gradient Descent(38584): loss=54.43277471646839\n",
      "Stochastic Gradient Descent(38585): loss=5.246461798567299\n",
      "Stochastic Gradient Descent(38586): loss=7.978689912761266\n",
      "Stochastic Gradient Descent(38587): loss=9.483938840437618\n",
      "Stochastic Gradient Descent(38588): loss=25.578829675491153\n",
      "Stochastic Gradient Descent(38589): loss=7.24448243505774\n",
      "Stochastic Gradient Descent(38590): loss=0.03008597736895621\n",
      "Stochastic Gradient Descent(38591): loss=6.955618079970935\n",
      "Stochastic Gradient Descent(38592): loss=10.302650354625408\n",
      "Stochastic Gradient Descent(38593): loss=7.981909578161101\n",
      "Stochastic Gradient Descent(38594): loss=2.035066605385758\n",
      "Stochastic Gradient Descent(38595): loss=0.11749756049029515\n",
      "Stochastic Gradient Descent(38596): loss=2.6841648163138268\n",
      "Stochastic Gradient Descent(38597): loss=4.158644333233454\n",
      "Stochastic Gradient Descent(38598): loss=2.6176282152222816\n",
      "Stochastic Gradient Descent(38599): loss=6.849591324482265\n",
      "Stochastic Gradient Descent(38600): loss=1.4417827719440683\n",
      "Stochastic Gradient Descent(38601): loss=0.9174302826761118\n",
      "Stochastic Gradient Descent(38602): loss=0.2746321707470561\n",
      "Stochastic Gradient Descent(38603): loss=1.2846818997490315\n",
      "Stochastic Gradient Descent(38604): loss=11.715005206805664\n",
      "Stochastic Gradient Descent(38605): loss=12.93637153595127\n",
      "Stochastic Gradient Descent(38606): loss=4.327037415933764\n",
      "Stochastic Gradient Descent(38607): loss=5.15463593396541\n",
      "Stochastic Gradient Descent(38608): loss=0.6895591778510228\n",
      "Stochastic Gradient Descent(38609): loss=1.9037286793339319\n",
      "Stochastic Gradient Descent(38610): loss=0.13580997306289477\n",
      "Stochastic Gradient Descent(38611): loss=4.267507153058674\n",
      "Stochastic Gradient Descent(38612): loss=4.260018949588121\n",
      "Stochastic Gradient Descent(38613): loss=1.3617760537480117\n",
      "Stochastic Gradient Descent(38614): loss=4.020532807298662\n",
      "Stochastic Gradient Descent(38615): loss=1.7301818541333749\n",
      "Stochastic Gradient Descent(38616): loss=3.7490375184104114\n",
      "Stochastic Gradient Descent(38617): loss=2.9675989131734823\n",
      "Stochastic Gradient Descent(38618): loss=0.11093057065455113\n",
      "Stochastic Gradient Descent(38619): loss=1.476151949621868\n",
      "Stochastic Gradient Descent(38620): loss=0.28584978144912254\n",
      "Stochastic Gradient Descent(38621): loss=0.7929404187141353\n",
      "Stochastic Gradient Descent(38622): loss=1.9302090014805018\n",
      "Stochastic Gradient Descent(38623): loss=0.9012777384289307\n",
      "Stochastic Gradient Descent(38624): loss=0.1791743519416234\n",
      "Stochastic Gradient Descent(38625): loss=1.0778014503265756\n",
      "Stochastic Gradient Descent(38626): loss=19.341017988819896\n",
      "Stochastic Gradient Descent(38627): loss=1.8497802565562642\n",
      "Stochastic Gradient Descent(38628): loss=0.5049893877724139\n",
      "Stochastic Gradient Descent(38629): loss=0.2317363790296569\n",
      "Stochastic Gradient Descent(38630): loss=6.685910050764068\n",
      "Stochastic Gradient Descent(38631): loss=0.17983614403981468\n",
      "Stochastic Gradient Descent(38632): loss=0.01877746853623298\n",
      "Stochastic Gradient Descent(38633): loss=22.056947513688094\n",
      "Stochastic Gradient Descent(38634): loss=0.022311547599525382\n",
      "Stochastic Gradient Descent(38635): loss=15.483342561563887\n",
      "Stochastic Gradient Descent(38636): loss=0.44999083040769783\n",
      "Stochastic Gradient Descent(38637): loss=0.007908994513542685\n",
      "Stochastic Gradient Descent(38638): loss=0.17992881050072965\n",
      "Stochastic Gradient Descent(38639): loss=0.2954732468984339\n",
      "Stochastic Gradient Descent(38640): loss=1.1732183561111758\n",
      "Stochastic Gradient Descent(38641): loss=0.17132717877568115\n",
      "Stochastic Gradient Descent(38642): loss=0.13868822799974123\n",
      "Stochastic Gradient Descent(38643): loss=2.649894311924179\n",
      "Stochastic Gradient Descent(38644): loss=0.24257599500965382\n",
      "Stochastic Gradient Descent(38645): loss=0.22078704976683464\n",
      "Stochastic Gradient Descent(38646): loss=0.08921797693777782\n",
      "Stochastic Gradient Descent(38647): loss=0.21040272716873654\n",
      "Stochastic Gradient Descent(38648): loss=6.576570417245071\n",
      "Stochastic Gradient Descent(38649): loss=0.06644191060417588\n",
      "Stochastic Gradient Descent(38650): loss=0.40676024174647313\n",
      "Stochastic Gradient Descent(38651): loss=1.5690272380008128\n",
      "Stochastic Gradient Descent(38652): loss=2.5597388850115483\n",
      "Stochastic Gradient Descent(38653): loss=0.5917412582523691\n",
      "Stochastic Gradient Descent(38654): loss=1.2197221142969894\n",
      "Stochastic Gradient Descent(38655): loss=1.659239920677368\n",
      "Stochastic Gradient Descent(38656): loss=3.0212128381636885\n",
      "Stochastic Gradient Descent(38657): loss=3.6251631180310335\n",
      "Stochastic Gradient Descent(38658): loss=0.8899765717892473\n",
      "Stochastic Gradient Descent(38659): loss=3.6567259200710853\n",
      "Stochastic Gradient Descent(38660): loss=7.582933837675573\n",
      "Stochastic Gradient Descent(38661): loss=0.06607123659391878\n",
      "Stochastic Gradient Descent(38662): loss=0.8974613884371977\n",
      "Stochastic Gradient Descent(38663): loss=5.0644518092547655\n",
      "Stochastic Gradient Descent(38664): loss=0.7384733784217081\n",
      "Stochastic Gradient Descent(38665): loss=0.9342041661611817\n",
      "Stochastic Gradient Descent(38666): loss=4.06254918302198\n",
      "Stochastic Gradient Descent(38667): loss=0.16062393949681816\n",
      "Stochastic Gradient Descent(38668): loss=3.126967734823265\n",
      "Stochastic Gradient Descent(38669): loss=3.505232527104386\n",
      "Stochastic Gradient Descent(38670): loss=18.90130679465348\n",
      "Stochastic Gradient Descent(38671): loss=2.099275326227209\n",
      "Stochastic Gradient Descent(38672): loss=0.03911832878365541\n",
      "Stochastic Gradient Descent(38673): loss=8.206714426728219\n",
      "Stochastic Gradient Descent(38674): loss=1.4476372007305167\n",
      "Stochastic Gradient Descent(38675): loss=3.4464656488819703\n",
      "Stochastic Gradient Descent(38676): loss=6.119892269934866\n",
      "Stochastic Gradient Descent(38677): loss=2.5957217577201113\n",
      "Stochastic Gradient Descent(38678): loss=0.23228563136806668\n",
      "Stochastic Gradient Descent(38679): loss=0.7079405167889763\n",
      "Stochastic Gradient Descent(38680): loss=4.978956944581714\n",
      "Stochastic Gradient Descent(38681): loss=20.3596989368625\n",
      "Stochastic Gradient Descent(38682): loss=0.35404927440545275\n",
      "Stochastic Gradient Descent(38683): loss=5.065060763082517\n",
      "Stochastic Gradient Descent(38684): loss=0.31645229380266215\n",
      "Stochastic Gradient Descent(38685): loss=0.0004050579002290038\n",
      "Stochastic Gradient Descent(38686): loss=4.092761543710616\n",
      "Stochastic Gradient Descent(38687): loss=6.21705782082639e-05\n",
      "Stochastic Gradient Descent(38688): loss=0.7143269342774851\n",
      "Stochastic Gradient Descent(38689): loss=10.280492324440758\n",
      "Stochastic Gradient Descent(38690): loss=0.0005575301377282359\n",
      "Stochastic Gradient Descent(38691): loss=2.017059250519488\n",
      "Stochastic Gradient Descent(38692): loss=15.29129238959836\n",
      "Stochastic Gradient Descent(38693): loss=1.088348569155575\n",
      "Stochastic Gradient Descent(38694): loss=11.066815769518128\n",
      "Stochastic Gradient Descent(38695): loss=0.3722496329587925\n",
      "Stochastic Gradient Descent(38696): loss=8.443667741237668\n",
      "Stochastic Gradient Descent(38697): loss=0.09796250887764603\n",
      "Stochastic Gradient Descent(38698): loss=0.25607471701895856\n",
      "Stochastic Gradient Descent(38699): loss=0.11814840114683109\n",
      "Stochastic Gradient Descent(38700): loss=12.705490737287361\n",
      "Stochastic Gradient Descent(38701): loss=0.4682675759266613\n",
      "Stochastic Gradient Descent(38702): loss=0.04717642018057157\n",
      "Stochastic Gradient Descent(38703): loss=2.483821497926903\n",
      "Stochastic Gradient Descent(38704): loss=6.2309532366098574\n",
      "Stochastic Gradient Descent(38705): loss=3.2572934774258946\n",
      "Stochastic Gradient Descent(38706): loss=0.020108877675012998\n",
      "Stochastic Gradient Descent(38707): loss=0.047147485820018785\n",
      "Stochastic Gradient Descent(38708): loss=1.2660156183117819\n",
      "Stochastic Gradient Descent(38709): loss=0.13650883617452156\n",
      "Stochastic Gradient Descent(38710): loss=7.131631667800989\n",
      "Stochastic Gradient Descent(38711): loss=0.24988359538149887\n",
      "Stochastic Gradient Descent(38712): loss=0.27712961463106994\n",
      "Stochastic Gradient Descent(38713): loss=0.49181520468829754\n",
      "Stochastic Gradient Descent(38714): loss=2.569311636779257\n",
      "Stochastic Gradient Descent(38715): loss=5.497229807154492\n",
      "Stochastic Gradient Descent(38716): loss=2.237755492384707\n",
      "Stochastic Gradient Descent(38717): loss=0.5530207366641984\n",
      "Stochastic Gradient Descent(38718): loss=0.814455673486648\n",
      "Stochastic Gradient Descent(38719): loss=0.16729345992604694\n",
      "Stochastic Gradient Descent(38720): loss=4.393098198146965\n",
      "Stochastic Gradient Descent(38721): loss=1.2115639088865449\n",
      "Stochastic Gradient Descent(38722): loss=0.3383848086709951\n",
      "Stochastic Gradient Descent(38723): loss=0.3035605328764047\n",
      "Stochastic Gradient Descent(38724): loss=0.00045491809713187303\n",
      "Stochastic Gradient Descent(38725): loss=4.01323820809634\n",
      "Stochastic Gradient Descent(38726): loss=0.7296208513011035\n",
      "Stochastic Gradient Descent(38727): loss=7.871088429757807\n",
      "Stochastic Gradient Descent(38728): loss=1.6694763421727414\n",
      "Stochastic Gradient Descent(38729): loss=0.40001800120472975\n",
      "Stochastic Gradient Descent(38730): loss=4.678218601417813\n",
      "Stochastic Gradient Descent(38731): loss=4.331432444924036\n",
      "Stochastic Gradient Descent(38732): loss=4.0687480687998425\n",
      "Stochastic Gradient Descent(38733): loss=3.3987248148544524\n",
      "Stochastic Gradient Descent(38734): loss=5.239312898640174\n",
      "Stochastic Gradient Descent(38735): loss=13.950654559736877\n",
      "Stochastic Gradient Descent(38736): loss=3.8970508266004047\n",
      "Stochastic Gradient Descent(38737): loss=13.179818458186183\n",
      "Stochastic Gradient Descent(38738): loss=7.2440110674050535\n",
      "Stochastic Gradient Descent(38739): loss=0.39848022252400367\n",
      "Stochastic Gradient Descent(38740): loss=3.8416847670261323\n",
      "Stochastic Gradient Descent(38741): loss=0.16784402787484679\n",
      "Stochastic Gradient Descent(38742): loss=11.307190818955904\n",
      "Stochastic Gradient Descent(38743): loss=31.851946751188017\n",
      "Stochastic Gradient Descent(38744): loss=3.858980185770928\n",
      "Stochastic Gradient Descent(38745): loss=3.040084053408598\n",
      "Stochastic Gradient Descent(38746): loss=4.611206007496585\n",
      "Stochastic Gradient Descent(38747): loss=7.408965920607534\n",
      "Stochastic Gradient Descent(38748): loss=0.16017412107645673\n",
      "Stochastic Gradient Descent(38749): loss=2.3532374273095233\n",
      "Stochastic Gradient Descent(38750): loss=0.06960934782043862\n",
      "Stochastic Gradient Descent(38751): loss=0.7794499395473649\n",
      "Stochastic Gradient Descent(38752): loss=2.9216707779240956\n",
      "Stochastic Gradient Descent(38753): loss=0.04022881351264673\n",
      "Stochastic Gradient Descent(38754): loss=5.840351072470403\n",
      "Stochastic Gradient Descent(38755): loss=2.91897350780862\n",
      "Stochastic Gradient Descent(38756): loss=5.466269061210627e-05\n",
      "Stochastic Gradient Descent(38757): loss=18.826656881278435\n",
      "Stochastic Gradient Descent(38758): loss=17.718459433342247\n",
      "Stochastic Gradient Descent(38759): loss=5.246134832866751\n",
      "Stochastic Gradient Descent(38760): loss=1.300846965411762\n",
      "Stochastic Gradient Descent(38761): loss=12.4721358957115\n",
      "Stochastic Gradient Descent(38762): loss=0.255829164810234\n",
      "Stochastic Gradient Descent(38763): loss=15.077600651931421\n",
      "Stochastic Gradient Descent(38764): loss=2.6609021610665673\n",
      "Stochastic Gradient Descent(38765): loss=7.53299526607634\n",
      "Stochastic Gradient Descent(38766): loss=7.17697715431046\n",
      "Stochastic Gradient Descent(38767): loss=5.646943743482122\n",
      "Stochastic Gradient Descent(38768): loss=1.9081863959732492\n",
      "Stochastic Gradient Descent(38769): loss=0.2028271464288996\n",
      "Stochastic Gradient Descent(38770): loss=0.46266812102118193\n",
      "Stochastic Gradient Descent(38771): loss=5.088603898922134\n",
      "Stochastic Gradient Descent(38772): loss=5.944781524041747\n",
      "Stochastic Gradient Descent(38773): loss=1.0147988775211236\n",
      "Stochastic Gradient Descent(38774): loss=0.47862270841036864\n",
      "Stochastic Gradient Descent(38775): loss=0.009918155515294803\n",
      "Stochastic Gradient Descent(38776): loss=0.017003130962215934\n",
      "Stochastic Gradient Descent(38777): loss=3.3174771245860004\n",
      "Stochastic Gradient Descent(38778): loss=11.601745040663092\n",
      "Stochastic Gradient Descent(38779): loss=0.16632132495660262\n",
      "Stochastic Gradient Descent(38780): loss=5.739954025732546\n",
      "Stochastic Gradient Descent(38781): loss=7.054929951332334\n",
      "Stochastic Gradient Descent(38782): loss=30.100710880796743\n",
      "Stochastic Gradient Descent(38783): loss=1.5162774829392707\n",
      "Stochastic Gradient Descent(38784): loss=0.5761581070369675\n",
      "Stochastic Gradient Descent(38785): loss=0.0015308809290680034\n",
      "Stochastic Gradient Descent(38786): loss=5.412712487790045\n",
      "Stochastic Gradient Descent(38787): loss=13.076470565472233\n",
      "Stochastic Gradient Descent(38788): loss=1.3539742246922086\n",
      "Stochastic Gradient Descent(38789): loss=16.408003592545494\n",
      "Stochastic Gradient Descent(38790): loss=2.8333564800807522\n",
      "Stochastic Gradient Descent(38791): loss=27.899549882668392\n",
      "Stochastic Gradient Descent(38792): loss=34.73803945654544\n",
      "Stochastic Gradient Descent(38793): loss=2.4322166812240993\n",
      "Stochastic Gradient Descent(38794): loss=17.05388240153796\n",
      "Stochastic Gradient Descent(38795): loss=1.5025289281535772\n",
      "Stochastic Gradient Descent(38796): loss=3.835484073833424\n",
      "Stochastic Gradient Descent(38797): loss=2.9013466327016983\n",
      "Stochastic Gradient Descent(38798): loss=2.5082739883323404\n",
      "Stochastic Gradient Descent(38799): loss=0.5138315515055641\n",
      "Stochastic Gradient Descent(38800): loss=0.05469263379715057\n",
      "Stochastic Gradient Descent(38801): loss=0.19215176072298049\n",
      "Stochastic Gradient Descent(38802): loss=0.6828018037151147\n",
      "Stochastic Gradient Descent(38803): loss=1.0112218091244016\n",
      "Stochastic Gradient Descent(38804): loss=2.5160601403713168\n",
      "Stochastic Gradient Descent(38805): loss=0.1907652641644669\n",
      "Stochastic Gradient Descent(38806): loss=2.349512996005652\n",
      "Stochastic Gradient Descent(38807): loss=0.5533002867591941\n",
      "Stochastic Gradient Descent(38808): loss=0.25202099903463093\n",
      "Stochastic Gradient Descent(38809): loss=5.5497934706738965\n",
      "Stochastic Gradient Descent(38810): loss=0.5577728207146577\n",
      "Stochastic Gradient Descent(38811): loss=2.83448204802365\n",
      "Stochastic Gradient Descent(38812): loss=0.1731391830133674\n",
      "Stochastic Gradient Descent(38813): loss=2.3981445242073622\n",
      "Stochastic Gradient Descent(38814): loss=2.999955932508135\n",
      "Stochastic Gradient Descent(38815): loss=0.002444596580046219\n",
      "Stochastic Gradient Descent(38816): loss=1.2510563911786234\n",
      "Stochastic Gradient Descent(38817): loss=1.0138517101709268\n",
      "Stochastic Gradient Descent(38818): loss=0.00014047023772638037\n",
      "Stochastic Gradient Descent(38819): loss=2.4550959391206053\n",
      "Stochastic Gradient Descent(38820): loss=1.2117979515778479\n",
      "Stochastic Gradient Descent(38821): loss=1.8332867845733836\n",
      "Stochastic Gradient Descent(38822): loss=8.20956065142436\n",
      "Stochastic Gradient Descent(38823): loss=2.1090905510600373\n",
      "Stochastic Gradient Descent(38824): loss=2.592758865600127\n",
      "Stochastic Gradient Descent(38825): loss=4.322905350579228\n",
      "Stochastic Gradient Descent(38826): loss=4.780097362850537\n",
      "Stochastic Gradient Descent(38827): loss=2.1560054099138\n",
      "Stochastic Gradient Descent(38828): loss=1.9962077560341653\n",
      "Stochastic Gradient Descent(38829): loss=11.839955636662125\n",
      "Stochastic Gradient Descent(38830): loss=7.025041354580108\n",
      "Stochastic Gradient Descent(38831): loss=1.604166521608495\n",
      "Stochastic Gradient Descent(38832): loss=3.0049066213766125\n",
      "Stochastic Gradient Descent(38833): loss=1.234203313773255\n",
      "Stochastic Gradient Descent(38834): loss=0.08846873520109512\n",
      "Stochastic Gradient Descent(38835): loss=0.004903485086260264\n",
      "Stochastic Gradient Descent(38836): loss=0.7268905626055535\n",
      "Stochastic Gradient Descent(38837): loss=0.011035495134089962\n",
      "Stochastic Gradient Descent(38838): loss=0.5288658042242794\n",
      "Stochastic Gradient Descent(38839): loss=0.022583306587222678\n",
      "Stochastic Gradient Descent(38840): loss=0.20813987277451892\n",
      "Stochastic Gradient Descent(38841): loss=4.907197939313458\n",
      "Stochastic Gradient Descent(38842): loss=0.8362801283973501\n",
      "Stochastic Gradient Descent(38843): loss=0.004188199452365152\n",
      "Stochastic Gradient Descent(38844): loss=0.8037644836931203\n",
      "Stochastic Gradient Descent(38845): loss=0.4354321227024814\n",
      "Stochastic Gradient Descent(38846): loss=21.273941758114354\n",
      "Stochastic Gradient Descent(38847): loss=4.366119885209308\n",
      "Stochastic Gradient Descent(38848): loss=6.3949605613383955\n",
      "Stochastic Gradient Descent(38849): loss=0.03304106493739702\n",
      "Stochastic Gradient Descent(38850): loss=5.211949018080878\n",
      "Stochastic Gradient Descent(38851): loss=0.06502564234516953\n",
      "Stochastic Gradient Descent(38852): loss=21.58874964225452\n",
      "Stochastic Gradient Descent(38853): loss=0.5872381424022202\n",
      "Stochastic Gradient Descent(38854): loss=3.2898848990213825\n",
      "Stochastic Gradient Descent(38855): loss=4.99995494248962\n",
      "Stochastic Gradient Descent(38856): loss=11.156854483191257\n",
      "Stochastic Gradient Descent(38857): loss=1.1218947325676476\n",
      "Stochastic Gradient Descent(38858): loss=1.7829188503516793\n",
      "Stochastic Gradient Descent(38859): loss=0.3090374393024499\n",
      "Stochastic Gradient Descent(38860): loss=12.947044003095066\n",
      "Stochastic Gradient Descent(38861): loss=0.21383306189216714\n",
      "Stochastic Gradient Descent(38862): loss=7.9363773229135335\n",
      "Stochastic Gradient Descent(38863): loss=0.6806836730699773\n",
      "Stochastic Gradient Descent(38864): loss=0.05098724198340769\n",
      "Stochastic Gradient Descent(38865): loss=1.3454605237030092e-05\n",
      "Stochastic Gradient Descent(38866): loss=0.19091831431788725\n",
      "Stochastic Gradient Descent(38867): loss=3.5016116650839773\n",
      "Stochastic Gradient Descent(38868): loss=0.7455889167867107\n",
      "Stochastic Gradient Descent(38869): loss=0.007084795129637354\n",
      "Stochastic Gradient Descent(38870): loss=2.6317933361546944\n",
      "Stochastic Gradient Descent(38871): loss=3.076316596886943\n",
      "Stochastic Gradient Descent(38872): loss=1.6416039454115363\n",
      "Stochastic Gradient Descent(38873): loss=1.24755126427524\n",
      "Stochastic Gradient Descent(38874): loss=0.21195990051167976\n",
      "Stochastic Gradient Descent(38875): loss=0.5289967483197479\n",
      "Stochastic Gradient Descent(38876): loss=6.408445369503186\n",
      "Stochastic Gradient Descent(38877): loss=7.6465951570669715\n",
      "Stochastic Gradient Descent(38878): loss=0.09269275512258666\n",
      "Stochastic Gradient Descent(38879): loss=0.8347560246792675\n",
      "Stochastic Gradient Descent(38880): loss=0.05394839836374425\n",
      "Stochastic Gradient Descent(38881): loss=0.2909036107545613\n",
      "Stochastic Gradient Descent(38882): loss=5.822112680482398\n",
      "Stochastic Gradient Descent(38883): loss=0.2357506025460866\n",
      "Stochastic Gradient Descent(38884): loss=2.583389784549841\n",
      "Stochastic Gradient Descent(38885): loss=0.03550918676582908\n",
      "Stochastic Gradient Descent(38886): loss=3.4453277030629668\n",
      "Stochastic Gradient Descent(38887): loss=0.07025791689254558\n",
      "Stochastic Gradient Descent(38888): loss=0.483992357732539\n",
      "Stochastic Gradient Descent(38889): loss=0.9416241460225473\n",
      "Stochastic Gradient Descent(38890): loss=1.785325109449994\n",
      "Stochastic Gradient Descent(38891): loss=27.340600761241046\n",
      "Stochastic Gradient Descent(38892): loss=11.65204896108455\n",
      "Stochastic Gradient Descent(38893): loss=3.158318914735402\n",
      "Stochastic Gradient Descent(38894): loss=1.1809226484413804\n",
      "Stochastic Gradient Descent(38895): loss=0.9101794351878106\n",
      "Stochastic Gradient Descent(38896): loss=1.3329829000973308\n",
      "Stochastic Gradient Descent(38897): loss=5.64531808338789\n",
      "Stochastic Gradient Descent(38898): loss=0.8560123294909427\n",
      "Stochastic Gradient Descent(38899): loss=2.72559022041279\n",
      "Stochastic Gradient Descent(38900): loss=5.35424264872713\n",
      "Stochastic Gradient Descent(38901): loss=0.22521160155533176\n",
      "Stochastic Gradient Descent(38902): loss=1.118091073271595\n",
      "Stochastic Gradient Descent(38903): loss=0.6281276570546245\n",
      "Stochastic Gradient Descent(38904): loss=1.122585101306788\n",
      "Stochastic Gradient Descent(38905): loss=9.908086572913415\n",
      "Stochastic Gradient Descent(38906): loss=1.163971526403897\n",
      "Stochastic Gradient Descent(38907): loss=0.9368053180138012\n",
      "Stochastic Gradient Descent(38908): loss=0.28190582639372397\n",
      "Stochastic Gradient Descent(38909): loss=17.553457577345092\n",
      "Stochastic Gradient Descent(38910): loss=110.05990777098766\n",
      "Stochastic Gradient Descent(38911): loss=47.617854181607555\n",
      "Stochastic Gradient Descent(38912): loss=8.015386328291981\n",
      "Stochastic Gradient Descent(38913): loss=0.6956699193750883\n",
      "Stochastic Gradient Descent(38914): loss=0.14708402526132236\n",
      "Stochastic Gradient Descent(38915): loss=2.396352908175972\n",
      "Stochastic Gradient Descent(38916): loss=0.535767006951921\n",
      "Stochastic Gradient Descent(38917): loss=0.0012195537793828891\n",
      "Stochastic Gradient Descent(38918): loss=2.252192277284579\n",
      "Stochastic Gradient Descent(38919): loss=19.18390486312492\n",
      "Stochastic Gradient Descent(38920): loss=1.123523418123613\n",
      "Stochastic Gradient Descent(38921): loss=5.5967172587899725\n",
      "Stochastic Gradient Descent(38922): loss=25.87130535096021\n",
      "Stochastic Gradient Descent(38923): loss=0.08745820335693166\n",
      "Stochastic Gradient Descent(38924): loss=0.6279815240418156\n",
      "Stochastic Gradient Descent(38925): loss=0.15793853487793857\n",
      "Stochastic Gradient Descent(38926): loss=0.5455938250292837\n",
      "Stochastic Gradient Descent(38927): loss=20.29905455973301\n",
      "Stochastic Gradient Descent(38928): loss=4.566409627732406\n",
      "Stochastic Gradient Descent(38929): loss=0.3133161524220361\n",
      "Stochastic Gradient Descent(38930): loss=0.864389931476713\n",
      "Stochastic Gradient Descent(38931): loss=1.2214522610646066\n",
      "Stochastic Gradient Descent(38932): loss=2.2740766180652368\n",
      "Stochastic Gradient Descent(38933): loss=0.405199789619586\n",
      "Stochastic Gradient Descent(38934): loss=1.0375842863605542\n",
      "Stochastic Gradient Descent(38935): loss=0.5474662683909555\n",
      "Stochastic Gradient Descent(38936): loss=0.6546413376496559\n",
      "Stochastic Gradient Descent(38937): loss=5.249961811406338\n",
      "Stochastic Gradient Descent(38938): loss=1.1026212675144564\n",
      "Stochastic Gradient Descent(38939): loss=3.6884225874277092\n",
      "Stochastic Gradient Descent(38940): loss=0.18555350902605794\n",
      "Stochastic Gradient Descent(38941): loss=0.010952142079932508\n",
      "Stochastic Gradient Descent(38942): loss=0.1227995890009012\n",
      "Stochastic Gradient Descent(38943): loss=1.6066867092239936\n",
      "Stochastic Gradient Descent(38944): loss=3.1958822346090066\n",
      "Stochastic Gradient Descent(38945): loss=6.360857248801207\n",
      "Stochastic Gradient Descent(38946): loss=3.446974190353598\n",
      "Stochastic Gradient Descent(38947): loss=0.6188758129172988\n",
      "Stochastic Gradient Descent(38948): loss=0.984996030861984\n",
      "Stochastic Gradient Descent(38949): loss=0.3647813694420572\n",
      "Stochastic Gradient Descent(38950): loss=13.47286847066883\n",
      "Stochastic Gradient Descent(38951): loss=1.9863260899387478\n",
      "Stochastic Gradient Descent(38952): loss=0.5745256675811663\n",
      "Stochastic Gradient Descent(38953): loss=0.12213672750779858\n",
      "Stochastic Gradient Descent(38954): loss=0.06047647368631847\n",
      "Stochastic Gradient Descent(38955): loss=0.13447431411541388\n",
      "Stochastic Gradient Descent(38956): loss=2.303004414156303\n",
      "Stochastic Gradient Descent(38957): loss=7.981110381118017\n",
      "Stochastic Gradient Descent(38958): loss=14.820818105366616\n",
      "Stochastic Gradient Descent(38959): loss=0.09713500562738739\n",
      "Stochastic Gradient Descent(38960): loss=12.121478553427956\n",
      "Stochastic Gradient Descent(38961): loss=1.2478152453522042\n",
      "Stochastic Gradient Descent(38962): loss=3.6522619787940864\n",
      "Stochastic Gradient Descent(38963): loss=1.3675770652389707\n",
      "Stochastic Gradient Descent(38964): loss=2.4876458250484235\n",
      "Stochastic Gradient Descent(38965): loss=4.718796326157722\n",
      "Stochastic Gradient Descent(38966): loss=4.053278684695342\n",
      "Stochastic Gradient Descent(38967): loss=2.110688015817946\n",
      "Stochastic Gradient Descent(38968): loss=18.9343228439788\n",
      "Stochastic Gradient Descent(38969): loss=0.5586347524207783\n",
      "Stochastic Gradient Descent(38970): loss=3.1190651271589944\n",
      "Stochastic Gradient Descent(38971): loss=3.0106517909482404\n",
      "Stochastic Gradient Descent(38972): loss=0.05243303982135733\n",
      "Stochastic Gradient Descent(38973): loss=0.0001042132981294002\n",
      "Stochastic Gradient Descent(38974): loss=2.470304499465217\n",
      "Stochastic Gradient Descent(38975): loss=3.01639942510123\n",
      "Stochastic Gradient Descent(38976): loss=0.01949666753559029\n",
      "Stochastic Gradient Descent(38977): loss=0.4206718934267007\n",
      "Stochastic Gradient Descent(38978): loss=7.7661301712997535\n",
      "Stochastic Gradient Descent(38979): loss=0.3007410357008731\n",
      "Stochastic Gradient Descent(38980): loss=6.326410881979064\n",
      "Stochastic Gradient Descent(38981): loss=2.3287549450730682\n",
      "Stochastic Gradient Descent(38982): loss=0.5482811973479811\n",
      "Stochastic Gradient Descent(38983): loss=0.1955314939523255\n",
      "Stochastic Gradient Descent(38984): loss=0.10615726051630012\n",
      "Stochastic Gradient Descent(38985): loss=0.31079596760033146\n",
      "Stochastic Gradient Descent(38986): loss=0.08767382066332313\n",
      "Stochastic Gradient Descent(38987): loss=0.009129642755588547\n",
      "Stochastic Gradient Descent(38988): loss=1.0678288310706985\n",
      "Stochastic Gradient Descent(38989): loss=0.022693205571471666\n",
      "Stochastic Gradient Descent(38990): loss=2.835299358905373\n",
      "Stochastic Gradient Descent(38991): loss=0.24164580735306507\n",
      "Stochastic Gradient Descent(38992): loss=6.912317805427197\n",
      "Stochastic Gradient Descent(38993): loss=0.0925013844693201\n",
      "Stochastic Gradient Descent(38994): loss=0.9936485555821282\n",
      "Stochastic Gradient Descent(38995): loss=2.4350337771237958\n",
      "Stochastic Gradient Descent(38996): loss=2.7653905002230244\n",
      "Stochastic Gradient Descent(38997): loss=6.8103719487339305\n",
      "Stochastic Gradient Descent(38998): loss=3.2855932918155974\n",
      "Stochastic Gradient Descent(38999): loss=0.03248428832605622\n",
      "Stochastic Gradient Descent(39000): loss=2.179914060924543\n",
      "Stochastic Gradient Descent(39001): loss=0.4187492329369692\n",
      "Stochastic Gradient Descent(39002): loss=7.901761240621849\n",
      "Stochastic Gradient Descent(39003): loss=0.1379046421325309\n",
      "Stochastic Gradient Descent(39004): loss=0.07931296444706626\n",
      "Stochastic Gradient Descent(39005): loss=0.0420835417764979\n",
      "Stochastic Gradient Descent(39006): loss=0.022706242511037436\n",
      "Stochastic Gradient Descent(39007): loss=9.29334114924387\n",
      "Stochastic Gradient Descent(39008): loss=1.2798814260341438\n",
      "Stochastic Gradient Descent(39009): loss=3.041074529877634\n",
      "Stochastic Gradient Descent(39010): loss=0.42469589740614283\n",
      "Stochastic Gradient Descent(39011): loss=18.344783246246866\n",
      "Stochastic Gradient Descent(39012): loss=5.87876174491303\n",
      "Stochastic Gradient Descent(39013): loss=0.05572923762216938\n",
      "Stochastic Gradient Descent(39014): loss=0.9411620509149952\n",
      "Stochastic Gradient Descent(39015): loss=0.06318137988781991\n",
      "Stochastic Gradient Descent(39016): loss=0.0063880454567290905\n",
      "Stochastic Gradient Descent(39017): loss=0.6645710865659447\n",
      "Stochastic Gradient Descent(39018): loss=2.702561863147887\n",
      "Stochastic Gradient Descent(39019): loss=2.511253491309355\n",
      "Stochastic Gradient Descent(39020): loss=13.31327462074031\n",
      "Stochastic Gradient Descent(39021): loss=0.19232931588298938\n",
      "Stochastic Gradient Descent(39022): loss=5.865117779808675\n",
      "Stochastic Gradient Descent(39023): loss=0.8695381254672534\n",
      "Stochastic Gradient Descent(39024): loss=14.65571813092286\n",
      "Stochastic Gradient Descent(39025): loss=2.1382092743382217\n",
      "Stochastic Gradient Descent(39026): loss=3.679983019459753\n",
      "Stochastic Gradient Descent(39027): loss=1.1892486561027396\n",
      "Stochastic Gradient Descent(39028): loss=9.585391280416532\n",
      "Stochastic Gradient Descent(39029): loss=0.07236516243541674\n",
      "Stochastic Gradient Descent(39030): loss=2.333845858789437\n",
      "Stochastic Gradient Descent(39031): loss=0.39137274472943284\n",
      "Stochastic Gradient Descent(39032): loss=3.3166154259422744\n",
      "Stochastic Gradient Descent(39033): loss=6.003378861605178\n",
      "Stochastic Gradient Descent(39034): loss=0.002126118086652768\n",
      "Stochastic Gradient Descent(39035): loss=5.123793946562298\n",
      "Stochastic Gradient Descent(39036): loss=1.253199768154357\n",
      "Stochastic Gradient Descent(39037): loss=20.1744406963163\n",
      "Stochastic Gradient Descent(39038): loss=2.9361435647011795\n",
      "Stochastic Gradient Descent(39039): loss=14.46760178134162\n",
      "Stochastic Gradient Descent(39040): loss=6.0841137448527265\n",
      "Stochastic Gradient Descent(39041): loss=8.69166352618496\n",
      "Stochastic Gradient Descent(39042): loss=2.9063292602359208\n",
      "Stochastic Gradient Descent(39043): loss=1.7320257608939986\n",
      "Stochastic Gradient Descent(39044): loss=0.14832817688216401\n",
      "Stochastic Gradient Descent(39045): loss=5.0200889831337765\n",
      "Stochastic Gradient Descent(39046): loss=2.734313126134588\n",
      "Stochastic Gradient Descent(39047): loss=3.8695573073296323\n",
      "Stochastic Gradient Descent(39048): loss=0.883606661281743\n",
      "Stochastic Gradient Descent(39049): loss=2.841482346832765\n",
      "Stochastic Gradient Descent(39050): loss=4.385290419203222\n",
      "Stochastic Gradient Descent(39051): loss=4.64128912986155\n",
      "Stochastic Gradient Descent(39052): loss=3.7892945862573497\n",
      "Stochastic Gradient Descent(39053): loss=5.8643264544298335\n",
      "Stochastic Gradient Descent(39054): loss=2.4047415329768236\n",
      "Stochastic Gradient Descent(39055): loss=3.5517704394043648\n",
      "Stochastic Gradient Descent(39056): loss=0.7283509041413663\n",
      "Stochastic Gradient Descent(39057): loss=1.056345888155442\n",
      "Stochastic Gradient Descent(39058): loss=0.06728792539511708\n",
      "Stochastic Gradient Descent(39059): loss=0.46952016130146956\n",
      "Stochastic Gradient Descent(39060): loss=6.895831193132496\n",
      "Stochastic Gradient Descent(39061): loss=2.5997344750006817\n",
      "Stochastic Gradient Descent(39062): loss=0.6631211426579695\n",
      "Stochastic Gradient Descent(39063): loss=0.25357024075452167\n",
      "Stochastic Gradient Descent(39064): loss=2.0915107045966224\n",
      "Stochastic Gradient Descent(39065): loss=2.219695758053794\n",
      "Stochastic Gradient Descent(39066): loss=2.558914080517634\n",
      "Stochastic Gradient Descent(39067): loss=0.00023788643087601196\n",
      "Stochastic Gradient Descent(39068): loss=0.07904015781738931\n",
      "Stochastic Gradient Descent(39069): loss=6.012217306908916\n",
      "Stochastic Gradient Descent(39070): loss=0.14801300945035928\n",
      "Stochastic Gradient Descent(39071): loss=6.453951844637369\n",
      "Stochastic Gradient Descent(39072): loss=2.151423609676934\n",
      "Stochastic Gradient Descent(39073): loss=9.964112546322324\n",
      "Stochastic Gradient Descent(39074): loss=7.3722186830434975\n",
      "Stochastic Gradient Descent(39075): loss=8.925568900134728\n",
      "Stochastic Gradient Descent(39076): loss=0.015866731213631968\n",
      "Stochastic Gradient Descent(39077): loss=1.8985582374551166\n",
      "Stochastic Gradient Descent(39078): loss=6.073609821578593\n",
      "Stochastic Gradient Descent(39079): loss=4.505466227611197\n",
      "Stochastic Gradient Descent(39080): loss=1.2047507718303776\n",
      "Stochastic Gradient Descent(39081): loss=4.176038483454631\n",
      "Stochastic Gradient Descent(39082): loss=1.2749934287444848\n",
      "Stochastic Gradient Descent(39083): loss=0.026694753836024095\n",
      "Stochastic Gradient Descent(39084): loss=0.8510298838712187\n",
      "Stochastic Gradient Descent(39085): loss=14.059903063219414\n",
      "Stochastic Gradient Descent(39086): loss=0.021084403137853015\n",
      "Stochastic Gradient Descent(39087): loss=2.6329782833508073\n",
      "Stochastic Gradient Descent(39088): loss=0.1194198487188318\n",
      "Stochastic Gradient Descent(39089): loss=2.5351143548958883\n",
      "Stochastic Gradient Descent(39090): loss=4.569013176255441\n",
      "Stochastic Gradient Descent(39091): loss=0.7676540920580889\n",
      "Stochastic Gradient Descent(39092): loss=6.188766712421011e-06\n",
      "Stochastic Gradient Descent(39093): loss=14.121211270414431\n",
      "Stochastic Gradient Descent(39094): loss=0.874342969158148\n",
      "Stochastic Gradient Descent(39095): loss=8.022259398758122\n",
      "Stochastic Gradient Descent(39096): loss=2.774409434557344\n",
      "Stochastic Gradient Descent(39097): loss=1.1169643379759127\n",
      "Stochastic Gradient Descent(39098): loss=1.8289377218888079\n",
      "Stochastic Gradient Descent(39099): loss=0.8155701110723045\n",
      "Stochastic Gradient Descent(39100): loss=0.5280119108932687\n",
      "Stochastic Gradient Descent(39101): loss=13.216828469430528\n",
      "Stochastic Gradient Descent(39102): loss=3.052982694587918\n",
      "Stochastic Gradient Descent(39103): loss=14.037117466451136\n",
      "Stochastic Gradient Descent(39104): loss=0.04875831809616927\n",
      "Stochastic Gradient Descent(39105): loss=3.48656887000855\n",
      "Stochastic Gradient Descent(39106): loss=0.08441113350741825\n",
      "Stochastic Gradient Descent(39107): loss=0.7411613691211248\n",
      "Stochastic Gradient Descent(39108): loss=0.04413577096628529\n",
      "Stochastic Gradient Descent(39109): loss=3.794222765491562\n",
      "Stochastic Gradient Descent(39110): loss=3.347820837959926\n",
      "Stochastic Gradient Descent(39111): loss=7.76410482977113\n",
      "Stochastic Gradient Descent(39112): loss=1.5836055332288976\n",
      "Stochastic Gradient Descent(39113): loss=0.11529725772181158\n",
      "Stochastic Gradient Descent(39114): loss=8.862960680500748\n",
      "Stochastic Gradient Descent(39115): loss=8.280610086199204\n",
      "Stochastic Gradient Descent(39116): loss=5.443774795268063\n",
      "Stochastic Gradient Descent(39117): loss=0.9130077962442539\n",
      "Stochastic Gradient Descent(39118): loss=0.07881866215900038\n",
      "Stochastic Gradient Descent(39119): loss=3.5173194411129156\n",
      "Stochastic Gradient Descent(39120): loss=0.03428903540732309\n",
      "Stochastic Gradient Descent(39121): loss=1.7081470873247664\n",
      "Stochastic Gradient Descent(39122): loss=0.001343452506616214\n",
      "Stochastic Gradient Descent(39123): loss=0.40498068844377616\n",
      "Stochastic Gradient Descent(39124): loss=10.875673034518295\n",
      "Stochastic Gradient Descent(39125): loss=6.0418825744229805\n",
      "Stochastic Gradient Descent(39126): loss=0.06675611038566019\n",
      "Stochastic Gradient Descent(39127): loss=0.9736696228716897\n",
      "Stochastic Gradient Descent(39128): loss=2.354485958612557\n",
      "Stochastic Gradient Descent(39129): loss=14.488339946388807\n",
      "Stochastic Gradient Descent(39130): loss=2.385919631632219\n",
      "Stochastic Gradient Descent(39131): loss=5.280218059240647\n",
      "Stochastic Gradient Descent(39132): loss=2.231167284003935\n",
      "Stochastic Gradient Descent(39133): loss=0.006720903017656898\n",
      "Stochastic Gradient Descent(39134): loss=28.850454772596102\n",
      "Stochastic Gradient Descent(39135): loss=5.997123619602346\n",
      "Stochastic Gradient Descent(39136): loss=0.1282073354167071\n",
      "Stochastic Gradient Descent(39137): loss=0.06179893331008426\n",
      "Stochastic Gradient Descent(39138): loss=17.11942508710595\n",
      "Stochastic Gradient Descent(39139): loss=0.00014543160962118598\n",
      "Stochastic Gradient Descent(39140): loss=2.3998104080871223\n",
      "Stochastic Gradient Descent(39141): loss=2.5472316053170125\n",
      "Stochastic Gradient Descent(39142): loss=0.8413819558351311\n",
      "Stochastic Gradient Descent(39143): loss=6.711649444848193\n",
      "Stochastic Gradient Descent(39144): loss=6.451703957244646\n",
      "Stochastic Gradient Descent(39145): loss=6.116404025919453\n",
      "Stochastic Gradient Descent(39146): loss=2.0974953391687254\n",
      "Stochastic Gradient Descent(39147): loss=1.204900683502554\n",
      "Stochastic Gradient Descent(39148): loss=0.07067521153421895\n",
      "Stochastic Gradient Descent(39149): loss=0.8220074256939628\n",
      "Stochastic Gradient Descent(39150): loss=1.1059033468079633\n",
      "Stochastic Gradient Descent(39151): loss=0.4100261845319606\n",
      "Stochastic Gradient Descent(39152): loss=0.004115048793417357\n",
      "Stochastic Gradient Descent(39153): loss=0.431482642124744\n",
      "Stochastic Gradient Descent(39154): loss=1.1479325561486124\n",
      "Stochastic Gradient Descent(39155): loss=0.6016347236679138\n",
      "Stochastic Gradient Descent(39156): loss=3.374127193971241\n",
      "Stochastic Gradient Descent(39157): loss=0.2871182766653874\n",
      "Stochastic Gradient Descent(39158): loss=2.7116122714638746\n",
      "Stochastic Gradient Descent(39159): loss=0.2521221213199198\n",
      "Stochastic Gradient Descent(39160): loss=9.008696961451198\n",
      "Stochastic Gradient Descent(39161): loss=6.510183999692673\n",
      "Stochastic Gradient Descent(39162): loss=5.003528206719458\n",
      "Stochastic Gradient Descent(39163): loss=5.210816127566936\n",
      "Stochastic Gradient Descent(39164): loss=38.402552663745986\n",
      "Stochastic Gradient Descent(39165): loss=0.7619073599586818\n",
      "Stochastic Gradient Descent(39166): loss=18.672077204053014\n",
      "Stochastic Gradient Descent(39167): loss=0.06070419659631335\n",
      "Stochastic Gradient Descent(39168): loss=0.04093232061761342\n",
      "Stochastic Gradient Descent(39169): loss=11.642815568100543\n",
      "Stochastic Gradient Descent(39170): loss=7.024423866640813\n",
      "Stochastic Gradient Descent(39171): loss=0.04532282321461683\n",
      "Stochastic Gradient Descent(39172): loss=7.72434360540439\n",
      "Stochastic Gradient Descent(39173): loss=1.3358083411243826\n",
      "Stochastic Gradient Descent(39174): loss=0.7238477687323104\n",
      "Stochastic Gradient Descent(39175): loss=6.806299704658835\n",
      "Stochastic Gradient Descent(39176): loss=0.00029757402722911045\n",
      "Stochastic Gradient Descent(39177): loss=0.0004707817274428147\n",
      "Stochastic Gradient Descent(39178): loss=5.65487468757955\n",
      "Stochastic Gradient Descent(39179): loss=1.451427409160898\n",
      "Stochastic Gradient Descent(39180): loss=5.068398506451726\n",
      "Stochastic Gradient Descent(39181): loss=0.39095706020005544\n",
      "Stochastic Gradient Descent(39182): loss=0.15242650835156774\n",
      "Stochastic Gradient Descent(39183): loss=15.613651503081751\n",
      "Stochastic Gradient Descent(39184): loss=4.626641806260721e-08\n",
      "Stochastic Gradient Descent(39185): loss=1.1454127151423856\n",
      "Stochastic Gradient Descent(39186): loss=4.3607869021771295\n",
      "Stochastic Gradient Descent(39187): loss=3.652196603174039\n",
      "Stochastic Gradient Descent(39188): loss=17.10970209159599\n",
      "Stochastic Gradient Descent(39189): loss=12.899002108991956\n",
      "Stochastic Gradient Descent(39190): loss=3.42553982854373\n",
      "Stochastic Gradient Descent(39191): loss=1.8642780800764547\n",
      "Stochastic Gradient Descent(39192): loss=13.390508134101825\n",
      "Stochastic Gradient Descent(39193): loss=0.1382476625880514\n",
      "Stochastic Gradient Descent(39194): loss=4.5425921925152\n",
      "Stochastic Gradient Descent(39195): loss=3.850163091346422\n",
      "Stochastic Gradient Descent(39196): loss=4.251149044412054\n",
      "Stochastic Gradient Descent(39197): loss=0.04051381045822619\n",
      "Stochastic Gradient Descent(39198): loss=0.1268579660917488\n",
      "Stochastic Gradient Descent(39199): loss=0.6311916226268199\n",
      "Stochastic Gradient Descent(39200): loss=2.0612712287240855\n",
      "Stochastic Gradient Descent(39201): loss=0.13111810379344047\n",
      "Stochastic Gradient Descent(39202): loss=4.710031537652772\n",
      "Stochastic Gradient Descent(39203): loss=3.346061624200995\n",
      "Stochastic Gradient Descent(39204): loss=0.9018140471461799\n",
      "Stochastic Gradient Descent(39205): loss=6.803698324725812\n",
      "Stochastic Gradient Descent(39206): loss=5.252280850012189\n",
      "Stochastic Gradient Descent(39207): loss=0.45697623427330614\n",
      "Stochastic Gradient Descent(39208): loss=0.014133217380980964\n",
      "Stochastic Gradient Descent(39209): loss=5.616178078076295\n",
      "Stochastic Gradient Descent(39210): loss=8.302742225924028\n",
      "Stochastic Gradient Descent(39211): loss=1.991637958407354\n",
      "Stochastic Gradient Descent(39212): loss=11.866744621907163\n",
      "Stochastic Gradient Descent(39213): loss=7.337748538317913\n",
      "Stochastic Gradient Descent(39214): loss=0.5848650934977484\n",
      "Stochastic Gradient Descent(39215): loss=4.658957875507339\n",
      "Stochastic Gradient Descent(39216): loss=2.162783345310436\n",
      "Stochastic Gradient Descent(39217): loss=5.40374654969955\n",
      "Stochastic Gradient Descent(39218): loss=8.068168400770267\n",
      "Stochastic Gradient Descent(39219): loss=242.09715658727725\n",
      "Stochastic Gradient Descent(39220): loss=359.7134562853687\n",
      "Stochastic Gradient Descent(39221): loss=20.984834416142604\n",
      "Stochastic Gradient Descent(39222): loss=2.4270839785606864\n",
      "Stochastic Gradient Descent(39223): loss=0.023862321598034447\n",
      "Stochastic Gradient Descent(39224): loss=0.42554449108012726\n",
      "Stochastic Gradient Descent(39225): loss=0.009535758301609772\n",
      "Stochastic Gradient Descent(39226): loss=0.6938212784833268\n",
      "Stochastic Gradient Descent(39227): loss=5.54889418324855\n",
      "Stochastic Gradient Descent(39228): loss=1.4976278763993534\n",
      "Stochastic Gradient Descent(39229): loss=4.238884533432932\n",
      "Stochastic Gradient Descent(39230): loss=0.9414226491802408\n",
      "Stochastic Gradient Descent(39231): loss=0.0030958355925555956\n",
      "Stochastic Gradient Descent(39232): loss=0.3189285589318302\n",
      "Stochastic Gradient Descent(39233): loss=0.3012305716422595\n",
      "Stochastic Gradient Descent(39234): loss=3.137999464215686\n",
      "Stochastic Gradient Descent(39235): loss=2.104034043530637\n",
      "Stochastic Gradient Descent(39236): loss=13.11232537642511\n",
      "Stochastic Gradient Descent(39237): loss=0.45663090659907196\n",
      "Stochastic Gradient Descent(39238): loss=7.704472772498872\n",
      "Stochastic Gradient Descent(39239): loss=0.7790822956458667\n",
      "Stochastic Gradient Descent(39240): loss=0.564105347931482\n",
      "Stochastic Gradient Descent(39241): loss=0.8255596809415163\n",
      "Stochastic Gradient Descent(39242): loss=0.503980686581718\n",
      "Stochastic Gradient Descent(39243): loss=0.014582993688052378\n",
      "Stochastic Gradient Descent(39244): loss=6.227238307451233\n",
      "Stochastic Gradient Descent(39245): loss=0.022899070652192394\n",
      "Stochastic Gradient Descent(39246): loss=0.07958004080007137\n",
      "Stochastic Gradient Descent(39247): loss=0.0016230841041872653\n",
      "Stochastic Gradient Descent(39248): loss=1.1270253877323722\n",
      "Stochastic Gradient Descent(39249): loss=2.0187889561426595\n",
      "Stochastic Gradient Descent(39250): loss=0.4291354077442979\n",
      "Stochastic Gradient Descent(39251): loss=0.1238324118316887\n",
      "Stochastic Gradient Descent(39252): loss=7.326846551926255\n",
      "Stochastic Gradient Descent(39253): loss=0.15187951089730092\n",
      "Stochastic Gradient Descent(39254): loss=4.284586178215127\n",
      "Stochastic Gradient Descent(39255): loss=3.509286521434675\n",
      "Stochastic Gradient Descent(39256): loss=4.529498653797413\n",
      "Stochastic Gradient Descent(39257): loss=0.22826383417305926\n",
      "Stochastic Gradient Descent(39258): loss=1.7397996531963564\n",
      "Stochastic Gradient Descent(39259): loss=1.175561081611739\n",
      "Stochastic Gradient Descent(39260): loss=9.577651853019825\n",
      "Stochastic Gradient Descent(39261): loss=0.24001918922509394\n",
      "Stochastic Gradient Descent(39262): loss=2.628273099192085\n",
      "Stochastic Gradient Descent(39263): loss=3.156163504771341\n",
      "Stochastic Gradient Descent(39264): loss=1.0868896183531869\n",
      "Stochastic Gradient Descent(39265): loss=0.2869287240882317\n",
      "Stochastic Gradient Descent(39266): loss=3.0722630174032184\n",
      "Stochastic Gradient Descent(39267): loss=0.5573333104106198\n",
      "Stochastic Gradient Descent(39268): loss=0.4568999023479003\n",
      "Stochastic Gradient Descent(39269): loss=0.3116495807898847\n",
      "Stochastic Gradient Descent(39270): loss=12.171015075522178\n",
      "Stochastic Gradient Descent(39271): loss=0.1620284817559206\n",
      "Stochastic Gradient Descent(39272): loss=2.4030821034675034\n",
      "Stochastic Gradient Descent(39273): loss=0.31293550465648934\n",
      "Stochastic Gradient Descent(39274): loss=7.345727773370247\n",
      "Stochastic Gradient Descent(39275): loss=2.6540776351492963\n",
      "Stochastic Gradient Descent(39276): loss=3.6383669297449748\n",
      "Stochastic Gradient Descent(39277): loss=1.4505885379322365\n",
      "Stochastic Gradient Descent(39278): loss=0.271028729421861\n",
      "Stochastic Gradient Descent(39279): loss=0.5813088444480197\n",
      "Stochastic Gradient Descent(39280): loss=0.11210472473877851\n",
      "Stochastic Gradient Descent(39281): loss=0.5872503427910098\n",
      "Stochastic Gradient Descent(39282): loss=0.8001205484027201\n",
      "Stochastic Gradient Descent(39283): loss=0.34909765827511685\n",
      "Stochastic Gradient Descent(39284): loss=8.730194894278902\n",
      "Stochastic Gradient Descent(39285): loss=5.4094791503736\n",
      "Stochastic Gradient Descent(39286): loss=0.22561341918236189\n",
      "Stochastic Gradient Descent(39287): loss=3.3319749501248714\n",
      "Stochastic Gradient Descent(39288): loss=0.5355021594073163\n",
      "Stochastic Gradient Descent(39289): loss=14.578566817999375\n",
      "Stochastic Gradient Descent(39290): loss=1.7524907189640988\n",
      "Stochastic Gradient Descent(39291): loss=1.0328695248253887\n",
      "Stochastic Gradient Descent(39292): loss=0.9285823475248126\n",
      "Stochastic Gradient Descent(39293): loss=0.25431884153238127\n",
      "Stochastic Gradient Descent(39294): loss=2.2869982135313607\n",
      "Stochastic Gradient Descent(39295): loss=92.63447375147524\n",
      "Stochastic Gradient Descent(39296): loss=2.472827254905791\n",
      "Stochastic Gradient Descent(39297): loss=30.42195248281637\n",
      "Stochastic Gradient Descent(39298): loss=26.533098980455655\n",
      "Stochastic Gradient Descent(39299): loss=5.266569981104547\n",
      "Stochastic Gradient Descent(39300): loss=4.789335565416186\n",
      "Stochastic Gradient Descent(39301): loss=5.072557676185883e-05\n",
      "Stochastic Gradient Descent(39302): loss=0.3016171912289098\n",
      "Stochastic Gradient Descent(39303): loss=6.870717094859167\n",
      "Stochastic Gradient Descent(39304): loss=40.00007714682228\n",
      "Stochastic Gradient Descent(39305): loss=19.04216687687884\n",
      "Stochastic Gradient Descent(39306): loss=18.029560828512793\n",
      "Stochastic Gradient Descent(39307): loss=1.211858002483026\n",
      "Stochastic Gradient Descent(39308): loss=1.0391302361599308\n",
      "Stochastic Gradient Descent(39309): loss=0.34415547456568474\n",
      "Stochastic Gradient Descent(39310): loss=5.464245127242549\n",
      "Stochastic Gradient Descent(39311): loss=3.318355102456726\n",
      "Stochastic Gradient Descent(39312): loss=2.2169018219045085\n",
      "Stochastic Gradient Descent(39313): loss=1.3964956238270398\n",
      "Stochastic Gradient Descent(39314): loss=11.964983330122683\n",
      "Stochastic Gradient Descent(39315): loss=0.3970320606900212\n",
      "Stochastic Gradient Descent(39316): loss=16.581083076764216\n",
      "Stochastic Gradient Descent(39317): loss=0.5774944067563541\n",
      "Stochastic Gradient Descent(39318): loss=1.8737978651321285\n",
      "Stochastic Gradient Descent(39319): loss=0.536787001492016\n",
      "Stochastic Gradient Descent(39320): loss=0.0027415726603728673\n",
      "Stochastic Gradient Descent(39321): loss=0.0529839290623108\n",
      "Stochastic Gradient Descent(39322): loss=0.4943517401205351\n",
      "Stochastic Gradient Descent(39323): loss=0.46863518238361335\n",
      "Stochastic Gradient Descent(39324): loss=0.09684099152365815\n",
      "Stochastic Gradient Descent(39325): loss=2.2970271774364135\n",
      "Stochastic Gradient Descent(39326): loss=0.026430438756328025\n",
      "Stochastic Gradient Descent(39327): loss=6.545646260983517\n",
      "Stochastic Gradient Descent(39328): loss=12.682906177756731\n",
      "Stochastic Gradient Descent(39329): loss=2.162635863537598\n",
      "Stochastic Gradient Descent(39330): loss=0.32533939301202686\n",
      "Stochastic Gradient Descent(39331): loss=3.9388352206959643\n",
      "Stochastic Gradient Descent(39332): loss=2.1195563702363662\n",
      "Stochastic Gradient Descent(39333): loss=4.853423899904733\n",
      "Stochastic Gradient Descent(39334): loss=10.513728707599157\n",
      "Stochastic Gradient Descent(39335): loss=7.651564494422445\n",
      "Stochastic Gradient Descent(39336): loss=10.542215356373998\n",
      "Stochastic Gradient Descent(39337): loss=8.945821806445464\n",
      "Stochastic Gradient Descent(39338): loss=6.5688000962058295\n",
      "Stochastic Gradient Descent(39339): loss=7.793266514163395\n",
      "Stochastic Gradient Descent(39340): loss=4.349864734669483\n",
      "Stochastic Gradient Descent(39341): loss=1.2227403054707588\n",
      "Stochastic Gradient Descent(39342): loss=0.3447811966413474\n",
      "Stochastic Gradient Descent(39343): loss=0.4849393902841858\n",
      "Stochastic Gradient Descent(39344): loss=1.8198917712652165\n",
      "Stochastic Gradient Descent(39345): loss=1.161095573682768\n",
      "Stochastic Gradient Descent(39346): loss=2.8414424458623424\n",
      "Stochastic Gradient Descent(39347): loss=0.2080847706191869\n",
      "Stochastic Gradient Descent(39348): loss=11.739646545496685\n",
      "Stochastic Gradient Descent(39349): loss=4.534263238386359\n",
      "Stochastic Gradient Descent(39350): loss=0.6066639466446191\n",
      "Stochastic Gradient Descent(39351): loss=0.3788381959112896\n",
      "Stochastic Gradient Descent(39352): loss=0.5571485386156209\n",
      "Stochastic Gradient Descent(39353): loss=6.167583505745166\n",
      "Stochastic Gradient Descent(39354): loss=6.769955647359213\n",
      "Stochastic Gradient Descent(39355): loss=2.942299942417605\n",
      "Stochastic Gradient Descent(39356): loss=2.2445420521434265\n",
      "Stochastic Gradient Descent(39357): loss=2.5494730336011733\n",
      "Stochastic Gradient Descent(39358): loss=1.4155800942973542\n",
      "Stochastic Gradient Descent(39359): loss=9.10344487313037\n",
      "Stochastic Gradient Descent(39360): loss=1.5958449447983525\n",
      "Stochastic Gradient Descent(39361): loss=2.070589261250786\n",
      "Stochastic Gradient Descent(39362): loss=11.01183740064435\n",
      "Stochastic Gradient Descent(39363): loss=1.4893234249988312\n",
      "Stochastic Gradient Descent(39364): loss=9.507075801159365\n",
      "Stochastic Gradient Descent(39365): loss=1.0571232607031202\n",
      "Stochastic Gradient Descent(39366): loss=0.0008347246739658314\n",
      "Stochastic Gradient Descent(39367): loss=0.24413964506473462\n",
      "Stochastic Gradient Descent(39368): loss=6.340304943363502\n",
      "Stochastic Gradient Descent(39369): loss=0.9838554203088248\n",
      "Stochastic Gradient Descent(39370): loss=3.4295071005490296\n",
      "Stochastic Gradient Descent(39371): loss=0.8999755467098162\n",
      "Stochastic Gradient Descent(39372): loss=2.366090459074971\n",
      "Stochastic Gradient Descent(39373): loss=0.43169347114886214\n",
      "Stochastic Gradient Descent(39374): loss=26.50229012900074\n",
      "Stochastic Gradient Descent(39375): loss=0.983052442068826\n",
      "Stochastic Gradient Descent(39376): loss=0.5184680507071243\n",
      "Stochastic Gradient Descent(39377): loss=8.511162803874239\n",
      "Stochastic Gradient Descent(39378): loss=1.781985619180991\n",
      "Stochastic Gradient Descent(39379): loss=1.7340432960862395\n",
      "Stochastic Gradient Descent(39380): loss=0.45007908418822473\n",
      "Stochastic Gradient Descent(39381): loss=1.6351954164956788\n",
      "Stochastic Gradient Descent(39382): loss=0.1073292036923284\n",
      "Stochastic Gradient Descent(39383): loss=0.4018979266880001\n",
      "Stochastic Gradient Descent(39384): loss=0.20659463396616876\n",
      "Stochastic Gradient Descent(39385): loss=1.4242466551197592\n",
      "Stochastic Gradient Descent(39386): loss=2.752371711469528\n",
      "Stochastic Gradient Descent(39387): loss=0.14366490395328516\n",
      "Stochastic Gradient Descent(39388): loss=1.2958346348405532\n",
      "Stochastic Gradient Descent(39389): loss=4.176426845189755\n",
      "Stochastic Gradient Descent(39390): loss=1.1238176060901661\n",
      "Stochastic Gradient Descent(39391): loss=0.0712489206862587\n",
      "Stochastic Gradient Descent(39392): loss=0.021812471592022355\n",
      "Stochastic Gradient Descent(39393): loss=2.457458903922261\n",
      "Stochastic Gradient Descent(39394): loss=0.8651614804636696\n",
      "Stochastic Gradient Descent(39395): loss=1.4611232034746128e-05\n",
      "Stochastic Gradient Descent(39396): loss=4.727013773204275\n",
      "Stochastic Gradient Descent(39397): loss=2.4052741460971823\n",
      "Stochastic Gradient Descent(39398): loss=15.353075009743863\n",
      "Stochastic Gradient Descent(39399): loss=0.12488231133577644\n",
      "Stochastic Gradient Descent(39400): loss=8.56234032980895\n",
      "Stochastic Gradient Descent(39401): loss=18.081718131002848\n",
      "Stochastic Gradient Descent(39402): loss=0.13146579866202968\n",
      "Stochastic Gradient Descent(39403): loss=2.1607429733774937\n",
      "Stochastic Gradient Descent(39404): loss=0.26233090019887606\n",
      "Stochastic Gradient Descent(39405): loss=4.652529875024886\n",
      "Stochastic Gradient Descent(39406): loss=0.5903019305835451\n",
      "Stochastic Gradient Descent(39407): loss=2.003997020482664\n",
      "Stochastic Gradient Descent(39408): loss=4.4038766496181845\n",
      "Stochastic Gradient Descent(39409): loss=1.6457087196570086\n",
      "Stochastic Gradient Descent(39410): loss=8.259397798041514\n",
      "Stochastic Gradient Descent(39411): loss=0.543155393068645\n",
      "Stochastic Gradient Descent(39412): loss=4.6804971760956455\n",
      "Stochastic Gradient Descent(39413): loss=41.61159055128796\n",
      "Stochastic Gradient Descent(39414): loss=16.96440578624173\n",
      "Stochastic Gradient Descent(39415): loss=65.03241257058082\n",
      "Stochastic Gradient Descent(39416): loss=8.849498441237944\n",
      "Stochastic Gradient Descent(39417): loss=37.51434321666971\n",
      "Stochastic Gradient Descent(39418): loss=1.8813052377206616\n",
      "Stochastic Gradient Descent(39419): loss=25.34339207023948\n",
      "Stochastic Gradient Descent(39420): loss=0.2941923151679549\n",
      "Stochastic Gradient Descent(39421): loss=0.7285834453181433\n",
      "Stochastic Gradient Descent(39422): loss=1.2254272570966975\n",
      "Stochastic Gradient Descent(39423): loss=16.46306377901789\n",
      "Stochastic Gradient Descent(39424): loss=1.971157339234165\n",
      "Stochastic Gradient Descent(39425): loss=3.287321370389626\n",
      "Stochastic Gradient Descent(39426): loss=0.227288819907299\n",
      "Stochastic Gradient Descent(39427): loss=0.150352117650768\n",
      "Stochastic Gradient Descent(39428): loss=0.6632709810335712\n",
      "Stochastic Gradient Descent(39429): loss=2.950209420614039\n",
      "Stochastic Gradient Descent(39430): loss=4.601086965885727\n",
      "Stochastic Gradient Descent(39431): loss=0.5169876024242429\n",
      "Stochastic Gradient Descent(39432): loss=12.093892518203457\n",
      "Stochastic Gradient Descent(39433): loss=0.06779315050782798\n",
      "Stochastic Gradient Descent(39434): loss=0.04328524110756756\n",
      "Stochastic Gradient Descent(39435): loss=0.47263847939348685\n",
      "Stochastic Gradient Descent(39436): loss=1.0637359536781548\n",
      "Stochastic Gradient Descent(39437): loss=2.5436607861735947\n",
      "Stochastic Gradient Descent(39438): loss=1.0836448407711983\n",
      "Stochastic Gradient Descent(39439): loss=0.005782835098089293\n",
      "Stochastic Gradient Descent(39440): loss=5.022835369838728\n",
      "Stochastic Gradient Descent(39441): loss=9.674976886109988\n",
      "Stochastic Gradient Descent(39442): loss=0.06981140797712301\n",
      "Stochastic Gradient Descent(39443): loss=0.615666929099831\n",
      "Stochastic Gradient Descent(39444): loss=6.067159960824694\n",
      "Stochastic Gradient Descent(39445): loss=2.819530041840443\n",
      "Stochastic Gradient Descent(39446): loss=6.451535224954958\n",
      "Stochastic Gradient Descent(39447): loss=15.529650826732304\n",
      "Stochastic Gradient Descent(39448): loss=1.036159028384753\n",
      "Stochastic Gradient Descent(39449): loss=1.5836177217862744\n",
      "Stochastic Gradient Descent(39450): loss=4.172921115066725\n",
      "Stochastic Gradient Descent(39451): loss=0.17621667178423808\n",
      "Stochastic Gradient Descent(39452): loss=1.4113041797095105\n",
      "Stochastic Gradient Descent(39453): loss=0.7261349989114021\n",
      "Stochastic Gradient Descent(39454): loss=1.331496497910271\n",
      "Stochastic Gradient Descent(39455): loss=0.3921905961789019\n",
      "Stochastic Gradient Descent(39456): loss=0.326785617227687\n",
      "Stochastic Gradient Descent(39457): loss=0.0361396733576566\n",
      "Stochastic Gradient Descent(39458): loss=2.948967466395142\n",
      "Stochastic Gradient Descent(39459): loss=1.760603502496332\n",
      "Stochastic Gradient Descent(39460): loss=0.019503242296381163\n",
      "Stochastic Gradient Descent(39461): loss=10.092605695251212\n",
      "Stochastic Gradient Descent(39462): loss=0.47549963859341643\n",
      "Stochastic Gradient Descent(39463): loss=0.6706598057736086\n",
      "Stochastic Gradient Descent(39464): loss=2.0553088422011347\n",
      "Stochastic Gradient Descent(39465): loss=22.42742872472601\n",
      "Stochastic Gradient Descent(39466): loss=0.4299641619371249\n",
      "Stochastic Gradient Descent(39467): loss=5.867743244508774\n",
      "Stochastic Gradient Descent(39468): loss=17.119979648245867\n",
      "Stochastic Gradient Descent(39469): loss=0.6296410112261858\n",
      "Stochastic Gradient Descent(39470): loss=0.11188581992407119\n",
      "Stochastic Gradient Descent(39471): loss=4.975396624812274\n",
      "Stochastic Gradient Descent(39472): loss=12.113703837508922\n",
      "Stochastic Gradient Descent(39473): loss=3.174157065998075\n",
      "Stochastic Gradient Descent(39474): loss=8.05902077155721\n",
      "Stochastic Gradient Descent(39475): loss=0.8744998821118111\n",
      "Stochastic Gradient Descent(39476): loss=1.1174355020419495\n",
      "Stochastic Gradient Descent(39477): loss=3.3229434422893203\n",
      "Stochastic Gradient Descent(39478): loss=2.7647068510902355\n",
      "Stochastic Gradient Descent(39479): loss=12.953117762530479\n",
      "Stochastic Gradient Descent(39480): loss=1.4323519997260865\n",
      "Stochastic Gradient Descent(39481): loss=3.2990185516584307\n",
      "Stochastic Gradient Descent(39482): loss=4.096713673286309\n",
      "Stochastic Gradient Descent(39483): loss=1.6642256935990938\n",
      "Stochastic Gradient Descent(39484): loss=5.540867329375335\n",
      "Stochastic Gradient Descent(39485): loss=0.0304964161468125\n",
      "Stochastic Gradient Descent(39486): loss=5.468700105501732\n",
      "Stochastic Gradient Descent(39487): loss=8.498753081412106\n",
      "Stochastic Gradient Descent(39488): loss=3.0252426292971775\n",
      "Stochastic Gradient Descent(39489): loss=7.29225238984157\n",
      "Stochastic Gradient Descent(39490): loss=7.0879841577705065\n",
      "Stochastic Gradient Descent(39491): loss=0.5753595217464225\n",
      "Stochastic Gradient Descent(39492): loss=5.694984815317603\n",
      "Stochastic Gradient Descent(39493): loss=0.09906394812015008\n",
      "Stochastic Gradient Descent(39494): loss=2.3996675915027614\n",
      "Stochastic Gradient Descent(39495): loss=0.021499503928977343\n",
      "Stochastic Gradient Descent(39496): loss=14.089725014162639\n",
      "Stochastic Gradient Descent(39497): loss=3.9884800544908394\n",
      "Stochastic Gradient Descent(39498): loss=12.395046712097729\n",
      "Stochastic Gradient Descent(39499): loss=0.3111737764691377\n",
      "Stochastic Gradient Descent(39500): loss=17.150954681949028\n",
      "Stochastic Gradient Descent(39501): loss=3.7308553953461745\n",
      "Stochastic Gradient Descent(39502): loss=0.1082249980989773\n",
      "Stochastic Gradient Descent(39503): loss=0.4137645102716917\n",
      "Stochastic Gradient Descent(39504): loss=1.4896110767235333\n",
      "Stochastic Gradient Descent(39505): loss=0.29159924255448\n",
      "Stochastic Gradient Descent(39506): loss=18.231501678249348\n",
      "Stochastic Gradient Descent(39507): loss=22.071199647778915\n",
      "Stochastic Gradient Descent(39508): loss=1.5896492308050545\n",
      "Stochastic Gradient Descent(39509): loss=5.575078181512305\n",
      "Stochastic Gradient Descent(39510): loss=3.592791166864769\n",
      "Stochastic Gradient Descent(39511): loss=1.8497099255467426\n",
      "Stochastic Gradient Descent(39512): loss=0.4143832027579621\n",
      "Stochastic Gradient Descent(39513): loss=1.6316985135221733\n",
      "Stochastic Gradient Descent(39514): loss=7.399685451568709\n",
      "Stochastic Gradient Descent(39515): loss=0.931278117497941\n",
      "Stochastic Gradient Descent(39516): loss=6.965111936943668\n",
      "Stochastic Gradient Descent(39517): loss=0.0048461954528112305\n",
      "Stochastic Gradient Descent(39518): loss=0.7559602091756829\n",
      "Stochastic Gradient Descent(39519): loss=0.06345240344263162\n",
      "Stochastic Gradient Descent(39520): loss=0.38779345929895515\n",
      "Stochastic Gradient Descent(39521): loss=7.452086558237297\n",
      "Stochastic Gradient Descent(39522): loss=0.07635834758375953\n",
      "Stochastic Gradient Descent(39523): loss=1.6452080790464836\n",
      "Stochastic Gradient Descent(39524): loss=4.420448573114077\n",
      "Stochastic Gradient Descent(39525): loss=0.41244730327159923\n",
      "Stochastic Gradient Descent(39526): loss=3.480084195659009\n",
      "Stochastic Gradient Descent(39527): loss=10.589307154382062\n",
      "Stochastic Gradient Descent(39528): loss=0.45201730155452113\n",
      "Stochastic Gradient Descent(39529): loss=2.260901186268933\n",
      "Stochastic Gradient Descent(39530): loss=0.011724404447307721\n",
      "Stochastic Gradient Descent(39531): loss=0.8560961499762764\n",
      "Stochastic Gradient Descent(39532): loss=1.8855444332697204\n",
      "Stochastic Gradient Descent(39533): loss=0.5241942492049009\n",
      "Stochastic Gradient Descent(39534): loss=2.2200250492275306\n",
      "Stochastic Gradient Descent(39535): loss=0.1799981971377697\n",
      "Stochastic Gradient Descent(39536): loss=0.24030103407468684\n",
      "Stochastic Gradient Descent(39537): loss=0.030952758156761125\n",
      "Stochastic Gradient Descent(39538): loss=1.919968550998812\n",
      "Stochastic Gradient Descent(39539): loss=2.4922688172403498\n",
      "Stochastic Gradient Descent(39540): loss=1.0378977798084865\n",
      "Stochastic Gradient Descent(39541): loss=0.29985944869534087\n",
      "Stochastic Gradient Descent(39542): loss=0.12010848816609183\n",
      "Stochastic Gradient Descent(39543): loss=6.1975015890785174\n",
      "Stochastic Gradient Descent(39544): loss=33.103807852258115\n",
      "Stochastic Gradient Descent(39545): loss=3.2999069368753586\n",
      "Stochastic Gradient Descent(39546): loss=2.6104118426049316\n",
      "Stochastic Gradient Descent(39547): loss=66.68354886432161\n",
      "Stochastic Gradient Descent(39548): loss=11.80761385296775\n",
      "Stochastic Gradient Descent(39549): loss=1.1640917528632424\n",
      "Stochastic Gradient Descent(39550): loss=4.787300828905958\n",
      "Stochastic Gradient Descent(39551): loss=13.603689823385906\n",
      "Stochastic Gradient Descent(39552): loss=0.053308938318769136\n",
      "Stochastic Gradient Descent(39553): loss=3.570418869780341\n",
      "Stochastic Gradient Descent(39554): loss=0.7049607532134409\n",
      "Stochastic Gradient Descent(39555): loss=0.6754955642527309\n",
      "Stochastic Gradient Descent(39556): loss=0.1856615928826978\n",
      "Stochastic Gradient Descent(39557): loss=31.85616182862123\n",
      "Stochastic Gradient Descent(39558): loss=3.5334532254010016\n",
      "Stochastic Gradient Descent(39559): loss=1.9190988733137415\n",
      "Stochastic Gradient Descent(39560): loss=0.5828862624911649\n",
      "Stochastic Gradient Descent(39561): loss=0.9822938029659063\n",
      "Stochastic Gradient Descent(39562): loss=4.889664497044727\n",
      "Stochastic Gradient Descent(39563): loss=1.2226164617854516\n",
      "Stochastic Gradient Descent(39564): loss=3.1040216357051515\n",
      "Stochastic Gradient Descent(39565): loss=5.391830754175427\n",
      "Stochastic Gradient Descent(39566): loss=7.758728870577727\n",
      "Stochastic Gradient Descent(39567): loss=0.9633121746365723\n",
      "Stochastic Gradient Descent(39568): loss=1.3219503077778048\n",
      "Stochastic Gradient Descent(39569): loss=1.4213359335007396\n",
      "Stochastic Gradient Descent(39570): loss=3.2844941232096487\n",
      "Stochastic Gradient Descent(39571): loss=4.5874282418002394\n",
      "Stochastic Gradient Descent(39572): loss=0.10306756688131748\n",
      "Stochastic Gradient Descent(39573): loss=3.207599944575644\n",
      "Stochastic Gradient Descent(39574): loss=0.0193474046324148\n",
      "Stochastic Gradient Descent(39575): loss=1.8067571667210285\n",
      "Stochastic Gradient Descent(39576): loss=1.0743876718581897\n",
      "Stochastic Gradient Descent(39577): loss=10.407076281128024\n",
      "Stochastic Gradient Descent(39578): loss=3.8895245667370046\n",
      "Stochastic Gradient Descent(39579): loss=0.023624695221817475\n",
      "Stochastic Gradient Descent(39580): loss=0.06907137371200656\n",
      "Stochastic Gradient Descent(39581): loss=0.14243823265139854\n",
      "Stochastic Gradient Descent(39582): loss=0.0004709685565532401\n",
      "Stochastic Gradient Descent(39583): loss=6.555676820534216\n",
      "Stochastic Gradient Descent(39584): loss=1.1708956382160278\n",
      "Stochastic Gradient Descent(39585): loss=2.4939116222161672\n",
      "Stochastic Gradient Descent(39586): loss=16.841449501612097\n",
      "Stochastic Gradient Descent(39587): loss=7.20152064747798\n",
      "Stochastic Gradient Descent(39588): loss=4.222852557328076\n",
      "Stochastic Gradient Descent(39589): loss=24.189882754223788\n",
      "Stochastic Gradient Descent(39590): loss=0.7123773390530198\n",
      "Stochastic Gradient Descent(39591): loss=0.02985695798722542\n",
      "Stochastic Gradient Descent(39592): loss=3.2463501101495322\n",
      "Stochastic Gradient Descent(39593): loss=0.8771526010936411\n",
      "Stochastic Gradient Descent(39594): loss=5.767358309427954\n",
      "Stochastic Gradient Descent(39595): loss=0.9924263841902686\n",
      "Stochastic Gradient Descent(39596): loss=10.99444559690324\n",
      "Stochastic Gradient Descent(39597): loss=0.39296671031238495\n",
      "Stochastic Gradient Descent(39598): loss=0.13767649933452983\n",
      "Stochastic Gradient Descent(39599): loss=0.0858090390308624\n",
      "Stochastic Gradient Descent(39600): loss=20.533809085910722\n",
      "Stochastic Gradient Descent(39601): loss=4.618482914581758\n",
      "Stochastic Gradient Descent(39602): loss=1.1559797242145204\n",
      "Stochastic Gradient Descent(39603): loss=6.9668157252039835\n",
      "Stochastic Gradient Descent(39604): loss=2.086242174386399\n",
      "Stochastic Gradient Descent(39605): loss=2.3897625072108797\n",
      "Stochastic Gradient Descent(39606): loss=0.18703173751598043\n",
      "Stochastic Gradient Descent(39607): loss=3.3661871900394202\n",
      "Stochastic Gradient Descent(39608): loss=3.826085388710316\n",
      "Stochastic Gradient Descent(39609): loss=0.10567458237590711\n",
      "Stochastic Gradient Descent(39610): loss=1.0891752228882094\n",
      "Stochastic Gradient Descent(39611): loss=0.35696542271757276\n",
      "Stochastic Gradient Descent(39612): loss=1.3509174512281967\n",
      "Stochastic Gradient Descent(39613): loss=3.9662650701682556\n",
      "Stochastic Gradient Descent(39614): loss=2.211228283893886\n",
      "Stochastic Gradient Descent(39615): loss=3.5825016467255453\n",
      "Stochastic Gradient Descent(39616): loss=3.0940358677168307\n",
      "Stochastic Gradient Descent(39617): loss=0.8159659581924884\n",
      "Stochastic Gradient Descent(39618): loss=3.1784718396368334\n",
      "Stochastic Gradient Descent(39619): loss=0.11107390295443688\n",
      "Stochastic Gradient Descent(39620): loss=7.322991680622759\n",
      "Stochastic Gradient Descent(39621): loss=10.661552985710717\n",
      "Stochastic Gradient Descent(39622): loss=2.3014022687223377\n",
      "Stochastic Gradient Descent(39623): loss=1.992999002339125e-05\n",
      "Stochastic Gradient Descent(39624): loss=15.204699827358555\n",
      "Stochastic Gradient Descent(39625): loss=0.0001879536375338321\n",
      "Stochastic Gradient Descent(39626): loss=0.4862976121386647\n",
      "Stochastic Gradient Descent(39627): loss=14.345596348539855\n",
      "Stochastic Gradient Descent(39628): loss=1.291488636264371\n",
      "Stochastic Gradient Descent(39629): loss=0.911492839624101\n",
      "Stochastic Gradient Descent(39630): loss=7.363486449291185\n",
      "Stochastic Gradient Descent(39631): loss=2.7585822976245256\n",
      "Stochastic Gradient Descent(39632): loss=14.981034752049446\n",
      "Stochastic Gradient Descent(39633): loss=1.034676660462629\n",
      "Stochastic Gradient Descent(39634): loss=0.16450743659282216\n",
      "Stochastic Gradient Descent(39635): loss=0.7382479830141502\n",
      "Stochastic Gradient Descent(39636): loss=11.53810322958169\n",
      "Stochastic Gradient Descent(39637): loss=2.0406637960648033\n",
      "Stochastic Gradient Descent(39638): loss=30.86626760386244\n",
      "Stochastic Gradient Descent(39639): loss=3.713774525645696\n",
      "Stochastic Gradient Descent(39640): loss=6.967136218156785\n",
      "Stochastic Gradient Descent(39641): loss=6.818700390309767\n",
      "Stochastic Gradient Descent(39642): loss=0.24660903069161355\n",
      "Stochastic Gradient Descent(39643): loss=60.0577718997384\n",
      "Stochastic Gradient Descent(39644): loss=11.279536676729075\n",
      "Stochastic Gradient Descent(39645): loss=116.65850247662243\n",
      "Stochastic Gradient Descent(39646): loss=3.4687983570535046\n",
      "Stochastic Gradient Descent(39647): loss=10.867406507517819\n",
      "Stochastic Gradient Descent(39648): loss=5.687782395522604\n",
      "Stochastic Gradient Descent(39649): loss=14.467594513538476\n",
      "Stochastic Gradient Descent(39650): loss=3.8716591237968037\n",
      "Stochastic Gradient Descent(39651): loss=0.001644830319984963\n",
      "Stochastic Gradient Descent(39652): loss=0.07672511304673249\n",
      "Stochastic Gradient Descent(39653): loss=5.795470902983092\n",
      "Stochastic Gradient Descent(39654): loss=7.65221767257201e-05\n",
      "Stochastic Gradient Descent(39655): loss=7.017175257760148\n",
      "Stochastic Gradient Descent(39656): loss=10.707130457919494\n",
      "Stochastic Gradient Descent(39657): loss=1.6963538886690441\n",
      "Stochastic Gradient Descent(39658): loss=3.917229059746863\n",
      "Stochastic Gradient Descent(39659): loss=0.34858635993316106\n",
      "Stochastic Gradient Descent(39660): loss=0.4567287408404451\n",
      "Stochastic Gradient Descent(39661): loss=0.2842686155026998\n",
      "Stochastic Gradient Descent(39662): loss=1.48908821314607\n",
      "Stochastic Gradient Descent(39663): loss=0.2866822906394159\n",
      "Stochastic Gradient Descent(39664): loss=4.139103336541237\n",
      "Stochastic Gradient Descent(39665): loss=0.5626500678873269\n",
      "Stochastic Gradient Descent(39666): loss=8.069623915510267\n",
      "Stochastic Gradient Descent(39667): loss=0.5720378558521464\n",
      "Stochastic Gradient Descent(39668): loss=2.566038548211498\n",
      "Stochastic Gradient Descent(39669): loss=3.0712550926541486\n",
      "Stochastic Gradient Descent(39670): loss=0.6770687423689955\n",
      "Stochastic Gradient Descent(39671): loss=0.02012231096511839\n",
      "Stochastic Gradient Descent(39672): loss=0.22688419571446464\n",
      "Stochastic Gradient Descent(39673): loss=10.238315328525804\n",
      "Stochastic Gradient Descent(39674): loss=0.2831671034645099\n",
      "Stochastic Gradient Descent(39675): loss=2.109693853567532\n",
      "Stochastic Gradient Descent(39676): loss=0.029323131809087563\n",
      "Stochastic Gradient Descent(39677): loss=2.2658149164579364\n",
      "Stochastic Gradient Descent(39678): loss=0.03449872730293249\n",
      "Stochastic Gradient Descent(39679): loss=21.896898551733383\n",
      "Stochastic Gradient Descent(39680): loss=7.091491506303631\n",
      "Stochastic Gradient Descent(39681): loss=0.2053784894187782\n",
      "Stochastic Gradient Descent(39682): loss=0.30648708494478727\n",
      "Stochastic Gradient Descent(39683): loss=0.13016991628483582\n",
      "Stochastic Gradient Descent(39684): loss=0.24733208583203922\n",
      "Stochastic Gradient Descent(39685): loss=2.9961973191039926\n",
      "Stochastic Gradient Descent(39686): loss=0.9007923141675245\n",
      "Stochastic Gradient Descent(39687): loss=3.1536416160113694\n",
      "Stochastic Gradient Descent(39688): loss=1.268290200164125\n",
      "Stochastic Gradient Descent(39689): loss=7.105229616071382\n",
      "Stochastic Gradient Descent(39690): loss=7.840989600549233\n",
      "Stochastic Gradient Descent(39691): loss=0.8762008065363783\n",
      "Stochastic Gradient Descent(39692): loss=4.314654885128054\n",
      "Stochastic Gradient Descent(39693): loss=1.269602616540408\n",
      "Stochastic Gradient Descent(39694): loss=4.131631263239111\n",
      "Stochastic Gradient Descent(39695): loss=26.19739558613168\n",
      "Stochastic Gradient Descent(39696): loss=2.5799298698135313\n",
      "Stochastic Gradient Descent(39697): loss=1.4061189526504663\n",
      "Stochastic Gradient Descent(39698): loss=0.8039586054084885\n",
      "Stochastic Gradient Descent(39699): loss=6.506152097276075\n",
      "Stochastic Gradient Descent(39700): loss=0.25666039783337097\n",
      "Stochastic Gradient Descent(39701): loss=12.334551092990907\n",
      "Stochastic Gradient Descent(39702): loss=10.08591764886459\n",
      "Stochastic Gradient Descent(39703): loss=0.16208152020925232\n",
      "Stochastic Gradient Descent(39704): loss=1.0663582405907284\n",
      "Stochastic Gradient Descent(39705): loss=1.4820735133712553\n",
      "Stochastic Gradient Descent(39706): loss=15.48751543794479\n",
      "Stochastic Gradient Descent(39707): loss=0.3818341581488039\n",
      "Stochastic Gradient Descent(39708): loss=0.6144701713067954\n",
      "Stochastic Gradient Descent(39709): loss=0.9765238442516563\n",
      "Stochastic Gradient Descent(39710): loss=0.2866530798178497\n",
      "Stochastic Gradient Descent(39711): loss=4.887330837318677\n",
      "Stochastic Gradient Descent(39712): loss=8.604433474180858\n",
      "Stochastic Gradient Descent(39713): loss=1.011724027393678\n",
      "Stochastic Gradient Descent(39714): loss=1.527515123090694\n",
      "Stochastic Gradient Descent(39715): loss=0.7004399336068446\n",
      "Stochastic Gradient Descent(39716): loss=6.024806085579327\n",
      "Stochastic Gradient Descent(39717): loss=2.8278303409239363\n",
      "Stochastic Gradient Descent(39718): loss=1.3218399063985293\n",
      "Stochastic Gradient Descent(39719): loss=0.003865157327949242\n",
      "Stochastic Gradient Descent(39720): loss=0.002244265189132354\n",
      "Stochastic Gradient Descent(39721): loss=0.6934642442034448\n",
      "Stochastic Gradient Descent(39722): loss=1.5612833245520823\n",
      "Stochastic Gradient Descent(39723): loss=2.222236154072624\n",
      "Stochastic Gradient Descent(39724): loss=0.31288755809162117\n",
      "Stochastic Gradient Descent(39725): loss=3.045904983747722\n",
      "Stochastic Gradient Descent(39726): loss=3.9829450790713876\n",
      "Stochastic Gradient Descent(39727): loss=0.14030902660004094\n",
      "Stochastic Gradient Descent(39728): loss=0.41136949738583095\n",
      "Stochastic Gradient Descent(39729): loss=0.0005105242442277453\n",
      "Stochastic Gradient Descent(39730): loss=0.04875643885562064\n",
      "Stochastic Gradient Descent(39731): loss=6.596792128922496\n",
      "Stochastic Gradient Descent(39732): loss=0.5052447726936802\n",
      "Stochastic Gradient Descent(39733): loss=0.638698086785409\n",
      "Stochastic Gradient Descent(39734): loss=0.0847321170519534\n",
      "Stochastic Gradient Descent(39735): loss=6.567059876665566\n",
      "Stochastic Gradient Descent(39736): loss=3.873775563634273\n",
      "Stochastic Gradient Descent(39737): loss=0.18288867117106863\n",
      "Stochastic Gradient Descent(39738): loss=0.7379934623321615\n",
      "Stochastic Gradient Descent(39739): loss=2.3974923682141136\n",
      "Stochastic Gradient Descent(39740): loss=4.911173354575493\n",
      "Stochastic Gradient Descent(39741): loss=0.7085837060701329\n",
      "Stochastic Gradient Descent(39742): loss=0.4087841622169965\n",
      "Stochastic Gradient Descent(39743): loss=1.2615029980412111\n",
      "Stochastic Gradient Descent(39744): loss=0.015206388278399556\n",
      "Stochastic Gradient Descent(39745): loss=0.11998548889662349\n",
      "Stochastic Gradient Descent(39746): loss=7.847565326332857\n",
      "Stochastic Gradient Descent(39747): loss=0.043534191776597495\n",
      "Stochastic Gradient Descent(39748): loss=7.700693405076432\n",
      "Stochastic Gradient Descent(39749): loss=0.5771406483650176\n",
      "Stochastic Gradient Descent(39750): loss=1.0914182273331727\n",
      "Stochastic Gradient Descent(39751): loss=27.615706400972297\n",
      "Stochastic Gradient Descent(39752): loss=0.8796325738321761\n",
      "Stochastic Gradient Descent(39753): loss=0.13856585063700014\n",
      "Stochastic Gradient Descent(39754): loss=0.04979700004187592\n",
      "Stochastic Gradient Descent(39755): loss=4.140178337387324\n",
      "Stochastic Gradient Descent(39756): loss=0.19887738374130182\n",
      "Stochastic Gradient Descent(39757): loss=0.629339455861869\n",
      "Stochastic Gradient Descent(39758): loss=2.0649349531205\n",
      "Stochastic Gradient Descent(39759): loss=2.6416141304178096\n",
      "Stochastic Gradient Descent(39760): loss=14.028952515772211\n",
      "Stochastic Gradient Descent(39761): loss=1.847339340292656\n",
      "Stochastic Gradient Descent(39762): loss=45.30750740775148\n",
      "Stochastic Gradient Descent(39763): loss=24.72380268407322\n",
      "Stochastic Gradient Descent(39764): loss=4.976493741523321\n",
      "Stochastic Gradient Descent(39765): loss=0.010574834297683588\n",
      "Stochastic Gradient Descent(39766): loss=26.661800329922375\n",
      "Stochastic Gradient Descent(39767): loss=0.8635822373046569\n",
      "Stochastic Gradient Descent(39768): loss=0.14489767205553153\n",
      "Stochastic Gradient Descent(39769): loss=0.24801530101896818\n",
      "Stochastic Gradient Descent(39770): loss=2.986078000966711\n",
      "Stochastic Gradient Descent(39771): loss=1.1157625938158975\n",
      "Stochastic Gradient Descent(39772): loss=0.15573100907182838\n",
      "Stochastic Gradient Descent(39773): loss=1.8585455562944087\n",
      "Stochastic Gradient Descent(39774): loss=2.670190297480559\n",
      "Stochastic Gradient Descent(39775): loss=2.1058289109358688\n",
      "Stochastic Gradient Descent(39776): loss=2.963826069728456\n",
      "Stochastic Gradient Descent(39777): loss=1.2617897363623316\n",
      "Stochastic Gradient Descent(39778): loss=0.6867365991619475\n",
      "Stochastic Gradient Descent(39779): loss=4.405556261795846\n",
      "Stochastic Gradient Descent(39780): loss=4.641071530322148\n",
      "Stochastic Gradient Descent(39781): loss=3.020649118332855\n",
      "Stochastic Gradient Descent(39782): loss=19.005519824828536\n",
      "Stochastic Gradient Descent(39783): loss=0.7097573227135014\n",
      "Stochastic Gradient Descent(39784): loss=0.4036050620392595\n",
      "Stochastic Gradient Descent(39785): loss=0.5999335381699155\n",
      "Stochastic Gradient Descent(39786): loss=0.08992576254351956\n",
      "Stochastic Gradient Descent(39787): loss=1.44567390359813\n",
      "Stochastic Gradient Descent(39788): loss=0.2137713003034707\n",
      "Stochastic Gradient Descent(39789): loss=2.2357367046066012\n",
      "Stochastic Gradient Descent(39790): loss=5.05394550484922\n",
      "Stochastic Gradient Descent(39791): loss=1.558083099426146\n",
      "Stochastic Gradient Descent(39792): loss=2.1186884143847164\n",
      "Stochastic Gradient Descent(39793): loss=4.814366218537398\n",
      "Stochastic Gradient Descent(39794): loss=5.672475100592231\n",
      "Stochastic Gradient Descent(39795): loss=0.0344567453617473\n",
      "Stochastic Gradient Descent(39796): loss=11.889840725132107\n",
      "Stochastic Gradient Descent(39797): loss=11.9952839344065\n",
      "Stochastic Gradient Descent(39798): loss=1.1996869021679777\n",
      "Stochastic Gradient Descent(39799): loss=7.745127385550392\n",
      "Stochastic Gradient Descent(39800): loss=6.662491662624836\n",
      "Stochastic Gradient Descent(39801): loss=4.293638913811237\n",
      "Stochastic Gradient Descent(39802): loss=2.5527704625015453\n",
      "Stochastic Gradient Descent(39803): loss=2.0220767992784423\n",
      "Stochastic Gradient Descent(39804): loss=0.45902002426295296\n",
      "Stochastic Gradient Descent(39805): loss=0.024384754502216176\n",
      "Stochastic Gradient Descent(39806): loss=3.534985389235562\n",
      "Stochastic Gradient Descent(39807): loss=3.649406075512755\n",
      "Stochastic Gradient Descent(39808): loss=9.967804828194545\n",
      "Stochastic Gradient Descent(39809): loss=1.5708908796805006\n",
      "Stochastic Gradient Descent(39810): loss=12.09314519852949\n",
      "Stochastic Gradient Descent(39811): loss=4.818557979644907\n",
      "Stochastic Gradient Descent(39812): loss=0.0014558247482036625\n",
      "Stochastic Gradient Descent(39813): loss=28.066895595305528\n",
      "Stochastic Gradient Descent(39814): loss=0.26112896402401836\n",
      "Stochastic Gradient Descent(39815): loss=0.04576291659984057\n",
      "Stochastic Gradient Descent(39816): loss=0.5878871377516179\n",
      "Stochastic Gradient Descent(39817): loss=1.701417837623408\n",
      "Stochastic Gradient Descent(39818): loss=3.1352625467154254\n",
      "Stochastic Gradient Descent(39819): loss=0.08279364171452712\n",
      "Stochastic Gradient Descent(39820): loss=1.1971819922576825\n",
      "Stochastic Gradient Descent(39821): loss=18.939382566059646\n",
      "Stochastic Gradient Descent(39822): loss=5.909301104578492\n",
      "Stochastic Gradient Descent(39823): loss=10.256997812539556\n",
      "Stochastic Gradient Descent(39824): loss=31.39400785919776\n",
      "Stochastic Gradient Descent(39825): loss=0.7700677298756728\n",
      "Stochastic Gradient Descent(39826): loss=2.0732708288266664\n",
      "Stochastic Gradient Descent(39827): loss=0.7543857675647581\n",
      "Stochastic Gradient Descent(39828): loss=4.3476142236502415\n",
      "Stochastic Gradient Descent(39829): loss=0.0009096335023734787\n",
      "Stochastic Gradient Descent(39830): loss=0.36134430797289757\n",
      "Stochastic Gradient Descent(39831): loss=0.08147946111440926\n",
      "Stochastic Gradient Descent(39832): loss=1.5699272623830032\n",
      "Stochastic Gradient Descent(39833): loss=0.0019352867270034369\n",
      "Stochastic Gradient Descent(39834): loss=0.5114498786986174\n",
      "Stochastic Gradient Descent(39835): loss=3.437084737364039\n",
      "Stochastic Gradient Descent(39836): loss=0.0007992439836478757\n",
      "Stochastic Gradient Descent(39837): loss=5.267319013747961\n",
      "Stochastic Gradient Descent(39838): loss=3.846263283572748\n",
      "Stochastic Gradient Descent(39839): loss=5.742986827900721\n",
      "Stochastic Gradient Descent(39840): loss=0.7558939051143401\n",
      "Stochastic Gradient Descent(39841): loss=5.204001128590277\n",
      "Stochastic Gradient Descent(39842): loss=5.565707079013672\n",
      "Stochastic Gradient Descent(39843): loss=0.2858401531973205\n",
      "Stochastic Gradient Descent(39844): loss=5.163679958808714\n",
      "Stochastic Gradient Descent(39845): loss=0.09908164562637883\n",
      "Stochastic Gradient Descent(39846): loss=0.015997735513201723\n",
      "Stochastic Gradient Descent(39847): loss=1.2663491338994137\n",
      "Stochastic Gradient Descent(39848): loss=0.002763193758123057\n",
      "Stochastic Gradient Descent(39849): loss=0.5516788915135472\n",
      "Stochastic Gradient Descent(39850): loss=4.691141112186861\n",
      "Stochastic Gradient Descent(39851): loss=0.21577126158728177\n",
      "Stochastic Gradient Descent(39852): loss=0.19497726185546677\n",
      "Stochastic Gradient Descent(39853): loss=4.5343828156951895\n",
      "Stochastic Gradient Descent(39854): loss=0.18477072967810487\n",
      "Stochastic Gradient Descent(39855): loss=0.07825922325956161\n",
      "Stochastic Gradient Descent(39856): loss=8.933461241640089\n",
      "Stochastic Gradient Descent(39857): loss=13.103086547432705\n",
      "Stochastic Gradient Descent(39858): loss=2.7291479814438584\n",
      "Stochastic Gradient Descent(39859): loss=9.485175809244067\n",
      "Stochastic Gradient Descent(39860): loss=3.1499587042225263\n",
      "Stochastic Gradient Descent(39861): loss=4.08648241473997\n",
      "Stochastic Gradient Descent(39862): loss=4.002966746674669\n",
      "Stochastic Gradient Descent(39863): loss=8.4623684212093\n",
      "Stochastic Gradient Descent(39864): loss=1.9478190590140223\n",
      "Stochastic Gradient Descent(39865): loss=4.488734618183264\n",
      "Stochastic Gradient Descent(39866): loss=0.5649759398075783\n",
      "Stochastic Gradient Descent(39867): loss=8.245855278583475e-05\n",
      "Stochastic Gradient Descent(39868): loss=0.07379730098538538\n",
      "Stochastic Gradient Descent(39869): loss=6.2618053612553535\n",
      "Stochastic Gradient Descent(39870): loss=0.22230515046850854\n",
      "Stochastic Gradient Descent(39871): loss=7.208064453986509\n",
      "Stochastic Gradient Descent(39872): loss=18.395308026539197\n",
      "Stochastic Gradient Descent(39873): loss=1.6068874787027654\n",
      "Stochastic Gradient Descent(39874): loss=23.62056206618833\n",
      "Stochastic Gradient Descent(39875): loss=0.006346471747426631\n",
      "Stochastic Gradient Descent(39876): loss=0.0008585174359169626\n",
      "Stochastic Gradient Descent(39877): loss=4.34522797008248\n",
      "Stochastic Gradient Descent(39878): loss=1.970904681218092\n",
      "Stochastic Gradient Descent(39879): loss=0.05925698860282749\n",
      "Stochastic Gradient Descent(39880): loss=0.15861110330737668\n",
      "Stochastic Gradient Descent(39881): loss=1.6261988266171343\n",
      "Stochastic Gradient Descent(39882): loss=0.014264236212844074\n",
      "Stochastic Gradient Descent(39883): loss=20.05893573235312\n",
      "Stochastic Gradient Descent(39884): loss=4.486693992934165\n",
      "Stochastic Gradient Descent(39885): loss=0.21287202808553363\n",
      "Stochastic Gradient Descent(39886): loss=0.15393154144564036\n",
      "Stochastic Gradient Descent(39887): loss=0.6721575544152213\n",
      "Stochastic Gradient Descent(39888): loss=6.845791729775707\n",
      "Stochastic Gradient Descent(39889): loss=8.972727455051476\n",
      "Stochastic Gradient Descent(39890): loss=2.4200631308831326\n",
      "Stochastic Gradient Descent(39891): loss=4.379235736100154\n",
      "Stochastic Gradient Descent(39892): loss=6.321289895514841\n",
      "Stochastic Gradient Descent(39893): loss=2.5206530205049913\n",
      "Stochastic Gradient Descent(39894): loss=0.005896827550997988\n",
      "Stochastic Gradient Descent(39895): loss=0.9936875867811397\n",
      "Stochastic Gradient Descent(39896): loss=0.22844172568716567\n",
      "Stochastic Gradient Descent(39897): loss=1.7889158018113496\n",
      "Stochastic Gradient Descent(39898): loss=1.6314392296064322\n",
      "Stochastic Gradient Descent(39899): loss=5.75731625548786\n",
      "Stochastic Gradient Descent(39900): loss=0.00043857598987774327\n",
      "Stochastic Gradient Descent(39901): loss=4.9920617718563\n",
      "Stochastic Gradient Descent(39902): loss=0.06294576441640067\n",
      "Stochastic Gradient Descent(39903): loss=0.030573052351281334\n",
      "Stochastic Gradient Descent(39904): loss=0.2710030090833036\n",
      "Stochastic Gradient Descent(39905): loss=0.6028723889152162\n",
      "Stochastic Gradient Descent(39906): loss=0.40961248215727125\n",
      "Stochastic Gradient Descent(39907): loss=1.0884338516201495\n",
      "Stochastic Gradient Descent(39908): loss=0.14215487511673425\n",
      "Stochastic Gradient Descent(39909): loss=3.11898965418111\n",
      "Stochastic Gradient Descent(39910): loss=0.0007596545606500552\n",
      "Stochastic Gradient Descent(39911): loss=9.753204370785047\n",
      "Stochastic Gradient Descent(39912): loss=0.7664796697706919\n",
      "Stochastic Gradient Descent(39913): loss=5.260052510038609\n",
      "Stochastic Gradient Descent(39914): loss=0.9655909757334675\n",
      "Stochastic Gradient Descent(39915): loss=5.649390424480169\n",
      "Stochastic Gradient Descent(39916): loss=0.7042180103959108\n",
      "Stochastic Gradient Descent(39917): loss=2.155647853145699\n",
      "Stochastic Gradient Descent(39918): loss=0.9714568401107971\n",
      "Stochastic Gradient Descent(39919): loss=7.80510181767421\n",
      "Stochastic Gradient Descent(39920): loss=0.2514888292810718\n",
      "Stochastic Gradient Descent(39921): loss=0.5695036351961065\n",
      "Stochastic Gradient Descent(39922): loss=3.143524500314792\n",
      "Stochastic Gradient Descent(39923): loss=2.7857984296886236\n",
      "Stochastic Gradient Descent(39924): loss=0.9394291730762289\n",
      "Stochastic Gradient Descent(39925): loss=2.4280105884845162\n",
      "Stochastic Gradient Descent(39926): loss=5.766472527851624\n",
      "Stochastic Gradient Descent(39927): loss=6.910352490491893\n",
      "Stochastic Gradient Descent(39928): loss=1.0876502766200764\n",
      "Stochastic Gradient Descent(39929): loss=0.1611153091321821\n",
      "Stochastic Gradient Descent(39930): loss=0.15038471115222765\n",
      "Stochastic Gradient Descent(39931): loss=0.4944054482173023\n",
      "Stochastic Gradient Descent(39932): loss=5.230867082536258\n",
      "Stochastic Gradient Descent(39933): loss=0.0005832567791540846\n",
      "Stochastic Gradient Descent(39934): loss=5.536376826323391\n",
      "Stochastic Gradient Descent(39935): loss=3.513476968628127\n",
      "Stochastic Gradient Descent(39936): loss=6.24135370052296\n",
      "Stochastic Gradient Descent(39937): loss=1.524523473691594\n",
      "Stochastic Gradient Descent(39938): loss=3.30061760353697\n",
      "Stochastic Gradient Descent(39939): loss=1.529687858457679\n",
      "Stochastic Gradient Descent(39940): loss=0.010363879769147322\n",
      "Stochastic Gradient Descent(39941): loss=0.09849812465208181\n",
      "Stochastic Gradient Descent(39942): loss=0.015209965114395181\n",
      "Stochastic Gradient Descent(39943): loss=1.4603776596684466\n",
      "Stochastic Gradient Descent(39944): loss=0.009046439879095224\n",
      "Stochastic Gradient Descent(39945): loss=0.008248879795861903\n",
      "Stochastic Gradient Descent(39946): loss=18.14881742937857\n",
      "Stochastic Gradient Descent(39947): loss=0.4737113965000696\n",
      "Stochastic Gradient Descent(39948): loss=5.131713904736118\n",
      "Stochastic Gradient Descent(39949): loss=5.055300000484156\n",
      "Stochastic Gradient Descent(39950): loss=0.6864859857872783\n",
      "Stochastic Gradient Descent(39951): loss=0.039940884014845045\n",
      "Stochastic Gradient Descent(39952): loss=0.23978713016387768\n",
      "Stochastic Gradient Descent(39953): loss=0.2787555855431057\n",
      "Stochastic Gradient Descent(39954): loss=1.736151855472535\n",
      "Stochastic Gradient Descent(39955): loss=2.149658496727716\n",
      "Stochastic Gradient Descent(39956): loss=2.1390945747781283\n",
      "Stochastic Gradient Descent(39957): loss=0.1228185375340942\n",
      "Stochastic Gradient Descent(39958): loss=4.018337953326216\n",
      "Stochastic Gradient Descent(39959): loss=0.12424733595515321\n",
      "Stochastic Gradient Descent(39960): loss=7.144750762706046\n",
      "Stochastic Gradient Descent(39961): loss=1.6658911639261773\n",
      "Stochastic Gradient Descent(39962): loss=4.013703423904545\n",
      "Stochastic Gradient Descent(39963): loss=4.152754545598024\n",
      "Stochastic Gradient Descent(39964): loss=17.20792440976484\n",
      "Stochastic Gradient Descent(39965): loss=0.08345229656658498\n",
      "Stochastic Gradient Descent(39966): loss=1.8199888764020344\n",
      "Stochastic Gradient Descent(39967): loss=0.8740576857715026\n",
      "Stochastic Gradient Descent(39968): loss=1.7204413488022463\n",
      "Stochastic Gradient Descent(39969): loss=0.03572745316357273\n",
      "Stochastic Gradient Descent(39970): loss=4.2354919411265755\n",
      "Stochastic Gradient Descent(39971): loss=0.8665063647965381\n",
      "Stochastic Gradient Descent(39972): loss=7.734351406360879\n",
      "Stochastic Gradient Descent(39973): loss=11.626294858164092\n",
      "Stochastic Gradient Descent(39974): loss=1.126611323357834\n",
      "Stochastic Gradient Descent(39975): loss=14.982036722748564\n",
      "Stochastic Gradient Descent(39976): loss=0.0593181308338712\n",
      "Stochastic Gradient Descent(39977): loss=2.472926840778786\n",
      "Stochastic Gradient Descent(39978): loss=1.248699038068912\n",
      "Stochastic Gradient Descent(39979): loss=0.25504193788196916\n",
      "Stochastic Gradient Descent(39980): loss=9.487067085526826\n",
      "Stochastic Gradient Descent(39981): loss=7.5682078990232675\n",
      "Stochastic Gradient Descent(39982): loss=0.43530354570232754\n",
      "Stochastic Gradient Descent(39983): loss=3.4792623161751197\n",
      "Stochastic Gradient Descent(39984): loss=3.7732705597358978\n",
      "Stochastic Gradient Descent(39985): loss=0.8237247308848702\n",
      "Stochastic Gradient Descent(39986): loss=3.1898069981642645\n",
      "Stochastic Gradient Descent(39987): loss=8.767144611184444\n",
      "Stochastic Gradient Descent(39988): loss=3.973467834533129\n",
      "Stochastic Gradient Descent(39989): loss=0.004585920653478822\n",
      "Stochastic Gradient Descent(39990): loss=0.32716105432413467\n",
      "Stochastic Gradient Descent(39991): loss=0.6348158155916362\n",
      "Stochastic Gradient Descent(39992): loss=0.08036849418903946\n",
      "Stochastic Gradient Descent(39993): loss=3.1129199116083353\n",
      "Stochastic Gradient Descent(39994): loss=1.7963904976676324\n",
      "Stochastic Gradient Descent(39995): loss=6.118866703401878\n",
      "Stochastic Gradient Descent(39996): loss=1.4402026755394781\n",
      "Stochastic Gradient Descent(39997): loss=1.3729441264289401\n",
      "Stochastic Gradient Descent(39998): loss=3.478184387654335\n",
      "Stochastic Gradient Descent(39999): loss=0.008700070172250723\n",
      "Stochastic Gradient Descent(40000): loss=0.033138568639382124\n",
      "Stochastic Gradient Descent(40001): loss=13.967345753609148\n",
      "Stochastic Gradient Descent(40002): loss=4.754250800782178\n",
      "Stochastic Gradient Descent(40003): loss=2.992733891680339\n",
      "Stochastic Gradient Descent(40004): loss=4.046942936127664\n",
      "Stochastic Gradient Descent(40005): loss=7.95277049508856\n",
      "Stochastic Gradient Descent(40006): loss=0.08610446534470106\n",
      "Stochastic Gradient Descent(40007): loss=2.045876466271058\n",
      "Stochastic Gradient Descent(40008): loss=8.70324043099895\n",
      "Stochastic Gradient Descent(40009): loss=22.101288566004197\n",
      "Stochastic Gradient Descent(40010): loss=2.6478002187165464\n",
      "Stochastic Gradient Descent(40011): loss=0.22563635397692944\n",
      "Stochastic Gradient Descent(40012): loss=0.329898864332987\n",
      "Stochastic Gradient Descent(40013): loss=3.274157322871429\n",
      "Stochastic Gradient Descent(40014): loss=10.508155711496498\n",
      "Stochastic Gradient Descent(40015): loss=12.925460767729536\n",
      "Stochastic Gradient Descent(40016): loss=2.3579421939242073\n",
      "Stochastic Gradient Descent(40017): loss=0.162964064112314\n",
      "Stochastic Gradient Descent(40018): loss=0.9985027243973317\n",
      "Stochastic Gradient Descent(40019): loss=1.589223166506704\n",
      "Stochastic Gradient Descent(40020): loss=7.378098954514865\n",
      "Stochastic Gradient Descent(40021): loss=0.12220026831094682\n",
      "Stochastic Gradient Descent(40022): loss=0.19530484911302806\n",
      "Stochastic Gradient Descent(40023): loss=0.3158037205852512\n",
      "Stochastic Gradient Descent(40024): loss=0.7017574384561567\n",
      "Stochastic Gradient Descent(40025): loss=0.6857483403619791\n",
      "Stochastic Gradient Descent(40026): loss=3.330181989059363\n",
      "Stochastic Gradient Descent(40027): loss=0.044995400288104906\n",
      "Stochastic Gradient Descent(40028): loss=12.789785410152602\n",
      "Stochastic Gradient Descent(40029): loss=0.0771156909023455\n",
      "Stochastic Gradient Descent(40030): loss=0.7491086044410242\n",
      "Stochastic Gradient Descent(40031): loss=3.9717636492449913\n",
      "Stochastic Gradient Descent(40032): loss=5.835832909270185e-05\n",
      "Stochastic Gradient Descent(40033): loss=0.06582978624219261\n",
      "Stochastic Gradient Descent(40034): loss=2.110886172145737\n",
      "Stochastic Gradient Descent(40035): loss=12.179338696264507\n",
      "Stochastic Gradient Descent(40036): loss=0.9522793674828988\n",
      "Stochastic Gradient Descent(40037): loss=5.631567005105289\n",
      "Stochastic Gradient Descent(40038): loss=1.1680285489364226\n",
      "Stochastic Gradient Descent(40039): loss=2.959919017065506\n",
      "Stochastic Gradient Descent(40040): loss=3.026770079725458\n",
      "Stochastic Gradient Descent(40041): loss=0.6202697116348871\n",
      "Stochastic Gradient Descent(40042): loss=2.9365443987439774\n",
      "Stochastic Gradient Descent(40043): loss=6.05202723595764\n",
      "Stochastic Gradient Descent(40044): loss=0.0028625846904065137\n",
      "Stochastic Gradient Descent(40045): loss=26.214935882129566\n",
      "Stochastic Gradient Descent(40046): loss=5.797299367830732\n",
      "Stochastic Gradient Descent(40047): loss=7.066201371728815\n",
      "Stochastic Gradient Descent(40048): loss=120.79341445929899\n",
      "Stochastic Gradient Descent(40049): loss=0.099765832201028\n",
      "Stochastic Gradient Descent(40050): loss=0.8394058419309219\n",
      "Stochastic Gradient Descent(40051): loss=4.927645113946221\n",
      "Stochastic Gradient Descent(40052): loss=1.8154868606886945\n",
      "Stochastic Gradient Descent(40053): loss=3.2422243790921343\n",
      "Stochastic Gradient Descent(40054): loss=4.332735368418426\n",
      "Stochastic Gradient Descent(40055): loss=0.13854291996777565\n",
      "Stochastic Gradient Descent(40056): loss=0.6894879700230314\n",
      "Stochastic Gradient Descent(40057): loss=0.4241259318912842\n",
      "Stochastic Gradient Descent(40058): loss=3.7431541353516056\n",
      "Stochastic Gradient Descent(40059): loss=0.002156068736411192\n",
      "Stochastic Gradient Descent(40060): loss=2.090847912015134\n",
      "Stochastic Gradient Descent(40061): loss=0.6195876390322269\n",
      "Stochastic Gradient Descent(40062): loss=0.3634750066852527\n",
      "Stochastic Gradient Descent(40063): loss=3.9204740404117446\n",
      "Stochastic Gradient Descent(40064): loss=1.2524162320474235\n",
      "Stochastic Gradient Descent(40065): loss=1.8616220872752272\n",
      "Stochastic Gradient Descent(40066): loss=16.36858623192167\n",
      "Stochastic Gradient Descent(40067): loss=6.54373883596135\n",
      "Stochastic Gradient Descent(40068): loss=0.06905987662982471\n",
      "Stochastic Gradient Descent(40069): loss=0.031791814603770875\n",
      "Stochastic Gradient Descent(40070): loss=3.7361300778957673\n",
      "Stochastic Gradient Descent(40071): loss=3.9270822644979777\n",
      "Stochastic Gradient Descent(40072): loss=6.4733871712627975\n",
      "Stochastic Gradient Descent(40073): loss=2.337000768840067\n",
      "Stochastic Gradient Descent(40074): loss=0.646686313714094\n",
      "Stochastic Gradient Descent(40075): loss=1.0549619133853867\n",
      "Stochastic Gradient Descent(40076): loss=0.5486326136852337\n",
      "Stochastic Gradient Descent(40077): loss=0.33551328746793246\n",
      "Stochastic Gradient Descent(40078): loss=2.4801179098792328\n",
      "Stochastic Gradient Descent(40079): loss=3.099669424409594\n",
      "Stochastic Gradient Descent(40080): loss=0.5341073205826296\n",
      "Stochastic Gradient Descent(40081): loss=2.7925655217914245\n",
      "Stochastic Gradient Descent(40082): loss=3.870463778497264\n",
      "Stochastic Gradient Descent(40083): loss=10.787373814964297\n",
      "Stochastic Gradient Descent(40084): loss=0.5968194173238662\n",
      "Stochastic Gradient Descent(40085): loss=1.489838443027475\n",
      "Stochastic Gradient Descent(40086): loss=3.470140613116304\n",
      "Stochastic Gradient Descent(40087): loss=2.1015742833997417\n",
      "Stochastic Gradient Descent(40088): loss=1.8416298110083027\n",
      "Stochastic Gradient Descent(40089): loss=0.5769632674283541\n",
      "Stochastic Gradient Descent(40090): loss=0.1347276222868401\n",
      "Stochastic Gradient Descent(40091): loss=0.30979405018877904\n",
      "Stochastic Gradient Descent(40092): loss=0.18255703531975498\n",
      "Stochastic Gradient Descent(40093): loss=11.889084987161839\n",
      "Stochastic Gradient Descent(40094): loss=4.201999510460673\n",
      "Stochastic Gradient Descent(40095): loss=4.582446332652632\n",
      "Stochastic Gradient Descent(40096): loss=0.4190909034503713\n",
      "Stochastic Gradient Descent(40097): loss=0.41367358698182644\n",
      "Stochastic Gradient Descent(40098): loss=0.0067991462392932725\n",
      "Stochastic Gradient Descent(40099): loss=1.0897501791705295\n",
      "Stochastic Gradient Descent(40100): loss=2.5580947253853403\n",
      "Stochastic Gradient Descent(40101): loss=0.005510002796374097\n",
      "Stochastic Gradient Descent(40102): loss=6.118819373647497\n",
      "Stochastic Gradient Descent(40103): loss=0.05108756492575155\n",
      "Stochastic Gradient Descent(40104): loss=7.33153628583957\n",
      "Stochastic Gradient Descent(40105): loss=3.020689459393527\n",
      "Stochastic Gradient Descent(40106): loss=4.508808251896577\n",
      "Stochastic Gradient Descent(40107): loss=2.752160279750772\n",
      "Stochastic Gradient Descent(40108): loss=1.6874988397200366\n",
      "Stochastic Gradient Descent(40109): loss=0.3628868802563905\n",
      "Stochastic Gradient Descent(40110): loss=0.7420438094116495\n",
      "Stochastic Gradient Descent(40111): loss=0.014146684994917846\n",
      "Stochastic Gradient Descent(40112): loss=0.018742191569104817\n",
      "Stochastic Gradient Descent(40113): loss=2.5922722455724676\n",
      "Stochastic Gradient Descent(40114): loss=2.7683809759700853\n",
      "Stochastic Gradient Descent(40115): loss=1.8038650380792\n",
      "Stochastic Gradient Descent(40116): loss=24.383407401925822\n",
      "Stochastic Gradient Descent(40117): loss=0.6634054222767312\n",
      "Stochastic Gradient Descent(40118): loss=1.6260905841574753\n",
      "Stochastic Gradient Descent(40119): loss=10.442812516557002\n",
      "Stochastic Gradient Descent(40120): loss=6.222682704818046\n",
      "Stochastic Gradient Descent(40121): loss=0.16820464765001575\n",
      "Stochastic Gradient Descent(40122): loss=12.723390514281158\n",
      "Stochastic Gradient Descent(40123): loss=2.936137496480174\n",
      "Stochastic Gradient Descent(40124): loss=31.08677407662713\n",
      "Stochastic Gradient Descent(40125): loss=2.1887877315373965\n",
      "Stochastic Gradient Descent(40126): loss=1.6071416260565214\n",
      "Stochastic Gradient Descent(40127): loss=22.509126278232642\n",
      "Stochastic Gradient Descent(40128): loss=0.06878417813422096\n",
      "Stochastic Gradient Descent(40129): loss=0.20866632818842723\n",
      "Stochastic Gradient Descent(40130): loss=2.279376896787998\n",
      "Stochastic Gradient Descent(40131): loss=5.8709556737181305\n",
      "Stochastic Gradient Descent(40132): loss=0.02473655114024014\n",
      "Stochastic Gradient Descent(40133): loss=0.9569914593110973\n",
      "Stochastic Gradient Descent(40134): loss=0.8083232330636825\n",
      "Stochastic Gradient Descent(40135): loss=4.993704970441233\n",
      "Stochastic Gradient Descent(40136): loss=4.372017835126226\n",
      "Stochastic Gradient Descent(40137): loss=1.548991953342809\n",
      "Stochastic Gradient Descent(40138): loss=2.1708653925149215\n",
      "Stochastic Gradient Descent(40139): loss=9.852334686926836\n",
      "Stochastic Gradient Descent(40140): loss=0.24625782717735725\n",
      "Stochastic Gradient Descent(40141): loss=16.560803582982683\n",
      "Stochastic Gradient Descent(40142): loss=0.1450204103251643\n",
      "Stochastic Gradient Descent(40143): loss=0.9708640774662571\n",
      "Stochastic Gradient Descent(40144): loss=0.1775410550649728\n",
      "Stochastic Gradient Descent(40145): loss=1.1348005797901073\n",
      "Stochastic Gradient Descent(40146): loss=4.172559474369375\n",
      "Stochastic Gradient Descent(40147): loss=3.1152117211937296\n",
      "Stochastic Gradient Descent(40148): loss=3.2915708626106612\n",
      "Stochastic Gradient Descent(40149): loss=2.7760937381892523\n",
      "Stochastic Gradient Descent(40150): loss=3.241656869505492\n",
      "Stochastic Gradient Descent(40151): loss=7.285809873868748\n",
      "Stochastic Gradient Descent(40152): loss=3.694411064474049\n",
      "Stochastic Gradient Descent(40153): loss=1.3521855036161685\n",
      "Stochastic Gradient Descent(40154): loss=0.8733904371171187\n",
      "Stochastic Gradient Descent(40155): loss=1.3773722377392374\n",
      "Stochastic Gradient Descent(40156): loss=11.481795962346588\n",
      "Stochastic Gradient Descent(40157): loss=1.3439107312612129\n",
      "Stochastic Gradient Descent(40158): loss=0.09081625128257997\n",
      "Stochastic Gradient Descent(40159): loss=1.9877783184781348\n",
      "Stochastic Gradient Descent(40160): loss=0.001354677805560451\n",
      "Stochastic Gradient Descent(40161): loss=0.9487629812025365\n",
      "Stochastic Gradient Descent(40162): loss=0.8811417067783228\n",
      "Stochastic Gradient Descent(40163): loss=1.3335452467204547\n",
      "Stochastic Gradient Descent(40164): loss=4.416588013274444\n",
      "Stochastic Gradient Descent(40165): loss=0.006812463030399779\n",
      "Stochastic Gradient Descent(40166): loss=0.15071250108552703\n",
      "Stochastic Gradient Descent(40167): loss=0.28542308538836886\n",
      "Stochastic Gradient Descent(40168): loss=7.053407000581139\n",
      "Stochastic Gradient Descent(40169): loss=14.28602420431162\n",
      "Stochastic Gradient Descent(40170): loss=0.3272274378499895\n",
      "Stochastic Gradient Descent(40171): loss=8.212607943933921\n",
      "Stochastic Gradient Descent(40172): loss=0.9462718304326507\n",
      "Stochastic Gradient Descent(40173): loss=1.163457547923856\n",
      "Stochastic Gradient Descent(40174): loss=3.3294824790816375\n",
      "Stochastic Gradient Descent(40175): loss=0.6332630811029304\n",
      "Stochastic Gradient Descent(40176): loss=1.6465468885914962\n",
      "Stochastic Gradient Descent(40177): loss=0.5559637373054347\n",
      "Stochastic Gradient Descent(40178): loss=15.368569476857873\n",
      "Stochastic Gradient Descent(40179): loss=0.68475998955403\n",
      "Stochastic Gradient Descent(40180): loss=3.8093234426785294\n",
      "Stochastic Gradient Descent(40181): loss=0.03956799311610068\n",
      "Stochastic Gradient Descent(40182): loss=1.0292972914896354\n",
      "Stochastic Gradient Descent(40183): loss=0.1674090137845834\n",
      "Stochastic Gradient Descent(40184): loss=0.08971334178668756\n",
      "Stochastic Gradient Descent(40185): loss=2.0872539107240202\n",
      "Stochastic Gradient Descent(40186): loss=0.11177309072404434\n",
      "Stochastic Gradient Descent(40187): loss=1.576587708552676\n",
      "Stochastic Gradient Descent(40188): loss=0.23041947923401918\n",
      "Stochastic Gradient Descent(40189): loss=0.021660806138586706\n",
      "Stochastic Gradient Descent(40190): loss=5.957398564967476\n",
      "Stochastic Gradient Descent(40191): loss=1.910982083046885\n",
      "Stochastic Gradient Descent(40192): loss=19.222541503390836\n",
      "Stochastic Gradient Descent(40193): loss=1.0118543109541718\n",
      "Stochastic Gradient Descent(40194): loss=9.215856512763958\n",
      "Stochastic Gradient Descent(40195): loss=0.9608949371744111\n",
      "Stochastic Gradient Descent(40196): loss=0.5344925005303863\n",
      "Stochastic Gradient Descent(40197): loss=1.3053815226965015\n",
      "Stochastic Gradient Descent(40198): loss=1.4506965630247612\n",
      "Stochastic Gradient Descent(40199): loss=0.24690265069291306\n",
      "Stochastic Gradient Descent(40200): loss=4.527821187005559\n",
      "Stochastic Gradient Descent(40201): loss=7.924548285753435\n",
      "Stochastic Gradient Descent(40202): loss=0.11227143480701536\n",
      "Stochastic Gradient Descent(40203): loss=16.336509058903083\n",
      "Stochastic Gradient Descent(40204): loss=0.3498723040162208\n",
      "Stochastic Gradient Descent(40205): loss=5.150110576612814\n",
      "Stochastic Gradient Descent(40206): loss=0.3331952273800001\n",
      "Stochastic Gradient Descent(40207): loss=0.8363698680275027\n",
      "Stochastic Gradient Descent(40208): loss=2.17627169173435\n",
      "Stochastic Gradient Descent(40209): loss=0.3118775106720524\n",
      "Stochastic Gradient Descent(40210): loss=25.307559775560875\n",
      "Stochastic Gradient Descent(40211): loss=1.960893786079136\n",
      "Stochastic Gradient Descent(40212): loss=1.9441045803769064\n",
      "Stochastic Gradient Descent(40213): loss=5.842277320742548\n",
      "Stochastic Gradient Descent(40214): loss=1.790591562993423\n",
      "Stochastic Gradient Descent(40215): loss=1.1027386598108615\n",
      "Stochastic Gradient Descent(40216): loss=1.1203444360510406\n",
      "Stochastic Gradient Descent(40217): loss=0.0668935702155024\n",
      "Stochastic Gradient Descent(40218): loss=3.2135479199820223\n",
      "Stochastic Gradient Descent(40219): loss=14.608989563937273\n",
      "Stochastic Gradient Descent(40220): loss=1.2011822740717955\n",
      "Stochastic Gradient Descent(40221): loss=2.777784058244171\n",
      "Stochastic Gradient Descent(40222): loss=5.725030336498165\n",
      "Stochastic Gradient Descent(40223): loss=5.959432649844721\n",
      "Stochastic Gradient Descent(40224): loss=4.521713309581064\n",
      "Stochastic Gradient Descent(40225): loss=1.7749211462084966\n",
      "Stochastic Gradient Descent(40226): loss=1.0886109495283625\n",
      "Stochastic Gradient Descent(40227): loss=2.483303900216459\n",
      "Stochastic Gradient Descent(40228): loss=16.686279757679888\n",
      "Stochastic Gradient Descent(40229): loss=0.05862475877091944\n",
      "Stochastic Gradient Descent(40230): loss=13.186618010023873\n",
      "Stochastic Gradient Descent(40231): loss=1.532968451569064\n",
      "Stochastic Gradient Descent(40232): loss=15.134713573454802\n",
      "Stochastic Gradient Descent(40233): loss=0.05461658921504802\n",
      "Stochastic Gradient Descent(40234): loss=5.25910983864585\n",
      "Stochastic Gradient Descent(40235): loss=0.0923045488097232\n",
      "Stochastic Gradient Descent(40236): loss=2.3061049631592203\n",
      "Stochastic Gradient Descent(40237): loss=0.005922051334491073\n",
      "Stochastic Gradient Descent(40238): loss=0.17347291644158816\n",
      "Stochastic Gradient Descent(40239): loss=3.330177593353816\n",
      "Stochastic Gradient Descent(40240): loss=0.7091288497715145\n",
      "Stochastic Gradient Descent(40241): loss=5.884955961621477\n",
      "Stochastic Gradient Descent(40242): loss=0.5193560465209074\n",
      "Stochastic Gradient Descent(40243): loss=0.019765678742666364\n",
      "Stochastic Gradient Descent(40244): loss=0.24224742137732572\n",
      "Stochastic Gradient Descent(40245): loss=2.311301284280589\n",
      "Stochastic Gradient Descent(40246): loss=2.335458269262327\n",
      "Stochastic Gradient Descent(40247): loss=0.04926109297291379\n",
      "Stochastic Gradient Descent(40248): loss=0.911736813464845\n",
      "Stochastic Gradient Descent(40249): loss=2.8091564577307295\n",
      "Stochastic Gradient Descent(40250): loss=0.5831921418869868\n",
      "Stochastic Gradient Descent(40251): loss=0.3228820670327381\n",
      "Stochastic Gradient Descent(40252): loss=3.364011970004439\n",
      "Stochastic Gradient Descent(40253): loss=6.064247940461006\n",
      "Stochastic Gradient Descent(40254): loss=4.8731563537759435\n",
      "Stochastic Gradient Descent(40255): loss=16.80275981302031\n",
      "Stochastic Gradient Descent(40256): loss=0.01195891411100597\n",
      "Stochastic Gradient Descent(40257): loss=0.986878124985956\n",
      "Stochastic Gradient Descent(40258): loss=11.347654311600142\n",
      "Stochastic Gradient Descent(40259): loss=6.327298890929613e-06\n",
      "Stochastic Gradient Descent(40260): loss=0.31565616052121626\n",
      "Stochastic Gradient Descent(40261): loss=0.17972304623434\n",
      "Stochastic Gradient Descent(40262): loss=2.761213962755859\n",
      "Stochastic Gradient Descent(40263): loss=5.499197729282775\n",
      "Stochastic Gradient Descent(40264): loss=0.6321898167324872\n",
      "Stochastic Gradient Descent(40265): loss=1.6220523200663859\n",
      "Stochastic Gradient Descent(40266): loss=9.342001001609738\n",
      "Stochastic Gradient Descent(40267): loss=3.13666078509085\n",
      "Stochastic Gradient Descent(40268): loss=0.9347586188069066\n",
      "Stochastic Gradient Descent(40269): loss=2.949241448914972\n",
      "Stochastic Gradient Descent(40270): loss=0.8633922205085276\n",
      "Stochastic Gradient Descent(40271): loss=7.889074418697087\n",
      "Stochastic Gradient Descent(40272): loss=0.000337173318565093\n",
      "Stochastic Gradient Descent(40273): loss=0.7030792838060896\n",
      "Stochastic Gradient Descent(40274): loss=0.7181654488024369\n",
      "Stochastic Gradient Descent(40275): loss=2.4132041006079827\n",
      "Stochastic Gradient Descent(40276): loss=8.33337151751494\n",
      "Stochastic Gradient Descent(40277): loss=1.1110801391047251\n",
      "Stochastic Gradient Descent(40278): loss=0.3232916802542185\n",
      "Stochastic Gradient Descent(40279): loss=1.4483774210256595\n",
      "Stochastic Gradient Descent(40280): loss=1.3246153008799555\n",
      "Stochastic Gradient Descent(40281): loss=1.076842935346918\n",
      "Stochastic Gradient Descent(40282): loss=0.045899615869124014\n",
      "Stochastic Gradient Descent(40283): loss=3.720136796968643\n",
      "Stochastic Gradient Descent(40284): loss=1.1639451431404233\n",
      "Stochastic Gradient Descent(40285): loss=4.4704628806583315\n",
      "Stochastic Gradient Descent(40286): loss=3.733807598432731\n",
      "Stochastic Gradient Descent(40287): loss=6.505598418670039\n",
      "Stochastic Gradient Descent(40288): loss=0.01775405513198166\n",
      "Stochastic Gradient Descent(40289): loss=0.07964380730834153\n",
      "Stochastic Gradient Descent(40290): loss=0.9683059291704558\n",
      "Stochastic Gradient Descent(40291): loss=3.060178269392852\n",
      "Stochastic Gradient Descent(40292): loss=0.1546715399622403\n",
      "Stochastic Gradient Descent(40293): loss=5.955262414552263\n",
      "Stochastic Gradient Descent(40294): loss=1.9920974868936407\n",
      "Stochastic Gradient Descent(40295): loss=0.6021991485993062\n",
      "Stochastic Gradient Descent(40296): loss=17.775568066202172\n",
      "Stochastic Gradient Descent(40297): loss=0.9275961986227775\n",
      "Stochastic Gradient Descent(40298): loss=0.03719936390437746\n",
      "Stochastic Gradient Descent(40299): loss=2.3782069600820543\n",
      "Stochastic Gradient Descent(40300): loss=0.008683150560189134\n",
      "Stochastic Gradient Descent(40301): loss=7.402391504314374\n",
      "Stochastic Gradient Descent(40302): loss=13.983969797617089\n",
      "Stochastic Gradient Descent(40303): loss=2.8483914110110806\n",
      "Stochastic Gradient Descent(40304): loss=0.7064872133638398\n",
      "Stochastic Gradient Descent(40305): loss=10.846773505459941\n",
      "Stochastic Gradient Descent(40306): loss=0.15341122323898942\n",
      "Stochastic Gradient Descent(40307): loss=0.02805593076067099\n",
      "Stochastic Gradient Descent(40308): loss=1.8664648916309394\n",
      "Stochastic Gradient Descent(40309): loss=4.179400413354871\n",
      "Stochastic Gradient Descent(40310): loss=0.9061865279605422\n",
      "Stochastic Gradient Descent(40311): loss=3.346634813606698\n",
      "Stochastic Gradient Descent(40312): loss=6.012647591741515\n",
      "Stochastic Gradient Descent(40313): loss=4.299057142812123\n",
      "Stochastic Gradient Descent(40314): loss=4.106085870887603\n",
      "Stochastic Gradient Descent(40315): loss=0.7722227689257549\n",
      "Stochastic Gradient Descent(40316): loss=0.4141778940302153\n",
      "Stochastic Gradient Descent(40317): loss=0.8979924532461492\n",
      "Stochastic Gradient Descent(40318): loss=0.15757212496181155\n",
      "Stochastic Gradient Descent(40319): loss=3.1989677513985586\n",
      "Stochastic Gradient Descent(40320): loss=2.631252327541682\n",
      "Stochastic Gradient Descent(40321): loss=6.569964673956089\n",
      "Stochastic Gradient Descent(40322): loss=0.05160330559843858\n",
      "Stochastic Gradient Descent(40323): loss=0.4456303451482011\n",
      "Stochastic Gradient Descent(40324): loss=28.850339795149498\n",
      "Stochastic Gradient Descent(40325): loss=132.56292550132028\n",
      "Stochastic Gradient Descent(40326): loss=19.424233313561174\n",
      "Stochastic Gradient Descent(40327): loss=38.36050694982582\n",
      "Stochastic Gradient Descent(40328): loss=0.8475989257728002\n",
      "Stochastic Gradient Descent(40329): loss=38.74661605030628\n",
      "Stochastic Gradient Descent(40330): loss=10.343763001857187\n",
      "Stochastic Gradient Descent(40331): loss=0.0030032331473414204\n",
      "Stochastic Gradient Descent(40332): loss=114.40915329489803\n",
      "Stochastic Gradient Descent(40333): loss=0.0055429182549632475\n",
      "Stochastic Gradient Descent(40334): loss=5.677140204193699\n",
      "Stochastic Gradient Descent(40335): loss=7.420616609136794\n",
      "Stochastic Gradient Descent(40336): loss=5.399729044037201\n",
      "Stochastic Gradient Descent(40337): loss=5.212405797902931\n",
      "Stochastic Gradient Descent(40338): loss=3.423153272788058\n",
      "Stochastic Gradient Descent(40339): loss=9.603635117542959\n",
      "Stochastic Gradient Descent(40340): loss=1.23667605672163\n",
      "Stochastic Gradient Descent(40341): loss=0.8329360234479397\n",
      "Stochastic Gradient Descent(40342): loss=4.303060471372989\n",
      "Stochastic Gradient Descent(40343): loss=4.47126181807743\n",
      "Stochastic Gradient Descent(40344): loss=0.004829283103541312\n",
      "Stochastic Gradient Descent(40345): loss=4.658933756141861\n",
      "Stochastic Gradient Descent(40346): loss=4.415142640457167\n",
      "Stochastic Gradient Descent(40347): loss=0.4523019709180633\n",
      "Stochastic Gradient Descent(40348): loss=3.5414490142572994\n",
      "Stochastic Gradient Descent(40349): loss=1.221386794546789\n",
      "Stochastic Gradient Descent(40350): loss=0.019577408218274552\n",
      "Stochastic Gradient Descent(40351): loss=7.1761715634054575\n",
      "Stochastic Gradient Descent(40352): loss=0.0008500103416286109\n",
      "Stochastic Gradient Descent(40353): loss=2.716364685524761\n",
      "Stochastic Gradient Descent(40354): loss=0.0038151846319381256\n",
      "Stochastic Gradient Descent(40355): loss=1.852735605182038\n",
      "Stochastic Gradient Descent(40356): loss=4.021379815098166\n",
      "Stochastic Gradient Descent(40357): loss=0.46629644614421284\n",
      "Stochastic Gradient Descent(40358): loss=2.916873650202224\n",
      "Stochastic Gradient Descent(40359): loss=10.607793263302721\n",
      "Stochastic Gradient Descent(40360): loss=0.3273961253428972\n",
      "Stochastic Gradient Descent(40361): loss=0.01669241435822523\n",
      "Stochastic Gradient Descent(40362): loss=15.660578793665152\n",
      "Stochastic Gradient Descent(40363): loss=0.1434834899519863\n",
      "Stochastic Gradient Descent(40364): loss=1.1560125979599822\n",
      "Stochastic Gradient Descent(40365): loss=15.206264073055156\n",
      "Stochastic Gradient Descent(40366): loss=0.01343049511360265\n",
      "Stochastic Gradient Descent(40367): loss=0.3387927894952635\n",
      "Stochastic Gradient Descent(40368): loss=3.0963245216891147\n",
      "Stochastic Gradient Descent(40369): loss=2.1691766117654137\n",
      "Stochastic Gradient Descent(40370): loss=1.1320323066486147\n",
      "Stochastic Gradient Descent(40371): loss=4.092259582967397\n",
      "Stochastic Gradient Descent(40372): loss=3.0710432516644977\n",
      "Stochastic Gradient Descent(40373): loss=12.800188817923834\n",
      "Stochastic Gradient Descent(40374): loss=7.212692352844998\n",
      "Stochastic Gradient Descent(40375): loss=0.07841724504258299\n",
      "Stochastic Gradient Descent(40376): loss=4.228116858798186\n",
      "Stochastic Gradient Descent(40377): loss=2.2469820554248043\n",
      "Stochastic Gradient Descent(40378): loss=14.383325178239922\n",
      "Stochastic Gradient Descent(40379): loss=8.305668946529801\n",
      "Stochastic Gradient Descent(40380): loss=0.8735258048427984\n",
      "Stochastic Gradient Descent(40381): loss=1.1285275688325909\n",
      "Stochastic Gradient Descent(40382): loss=4.044168494397377\n",
      "Stochastic Gradient Descent(40383): loss=5.48409652144423\n",
      "Stochastic Gradient Descent(40384): loss=0.5856938614446039\n",
      "Stochastic Gradient Descent(40385): loss=0.4026397238763335\n",
      "Stochastic Gradient Descent(40386): loss=3.467212182316793\n",
      "Stochastic Gradient Descent(40387): loss=6.985486094195904\n",
      "Stochastic Gradient Descent(40388): loss=19.10958775769602\n",
      "Stochastic Gradient Descent(40389): loss=3.077858961918644\n",
      "Stochastic Gradient Descent(40390): loss=1.5189018461946204\n",
      "Stochastic Gradient Descent(40391): loss=7.535466524991635\n",
      "Stochastic Gradient Descent(40392): loss=2.787023532921688\n",
      "Stochastic Gradient Descent(40393): loss=18.345839116829374\n",
      "Stochastic Gradient Descent(40394): loss=3.947701761574756\n",
      "Stochastic Gradient Descent(40395): loss=0.23255402299791256\n",
      "Stochastic Gradient Descent(40396): loss=0.0015481445953689375\n",
      "Stochastic Gradient Descent(40397): loss=2.4532951685089297\n",
      "Stochastic Gradient Descent(40398): loss=0.5210679262323501\n",
      "Stochastic Gradient Descent(40399): loss=2.680441303598198\n",
      "Stochastic Gradient Descent(40400): loss=0.12489371656144473\n",
      "Stochastic Gradient Descent(40401): loss=0.9536600843629605\n",
      "Stochastic Gradient Descent(40402): loss=0.6776477790076023\n",
      "Stochastic Gradient Descent(40403): loss=2.7682426798640147\n",
      "Stochastic Gradient Descent(40404): loss=0.000424143373880706\n",
      "Stochastic Gradient Descent(40405): loss=2.040455860524905\n",
      "Stochastic Gradient Descent(40406): loss=13.476776786574451\n",
      "Stochastic Gradient Descent(40407): loss=3.258339840690137\n",
      "Stochastic Gradient Descent(40408): loss=2.909223934040979\n",
      "Stochastic Gradient Descent(40409): loss=12.45408206461023\n",
      "Stochastic Gradient Descent(40410): loss=1.8184961984222734\n",
      "Stochastic Gradient Descent(40411): loss=6.085566592852095\n",
      "Stochastic Gradient Descent(40412): loss=2.2650383211246976\n",
      "Stochastic Gradient Descent(40413): loss=2.587418873671648\n",
      "Stochastic Gradient Descent(40414): loss=0.022599771305278852\n",
      "Stochastic Gradient Descent(40415): loss=36.978633358243506\n",
      "Stochastic Gradient Descent(40416): loss=16.527676043436273\n",
      "Stochastic Gradient Descent(40417): loss=11.588250608313837\n",
      "Stochastic Gradient Descent(40418): loss=0.12019589965487897\n",
      "Stochastic Gradient Descent(40419): loss=5.466213516954802\n",
      "Stochastic Gradient Descent(40420): loss=8.613192240134499\n",
      "Stochastic Gradient Descent(40421): loss=1.024172054479399\n",
      "Stochastic Gradient Descent(40422): loss=6.210987149752057\n",
      "Stochastic Gradient Descent(40423): loss=1.3073927548748878\n",
      "Stochastic Gradient Descent(40424): loss=1.653631779766628\n",
      "Stochastic Gradient Descent(40425): loss=0.11424556931783542\n",
      "Stochastic Gradient Descent(40426): loss=1.6477615897209477\n",
      "Stochastic Gradient Descent(40427): loss=18.023957389386673\n",
      "Stochastic Gradient Descent(40428): loss=0.09822610671896864\n",
      "Stochastic Gradient Descent(40429): loss=0.007402503535485095\n",
      "Stochastic Gradient Descent(40430): loss=4.806528607096825\n",
      "Stochastic Gradient Descent(40431): loss=0.055007403704881316\n",
      "Stochastic Gradient Descent(40432): loss=0.6069146581856857\n",
      "Stochastic Gradient Descent(40433): loss=17.587926112307454\n",
      "Stochastic Gradient Descent(40434): loss=3.17392506824852\n",
      "Stochastic Gradient Descent(40435): loss=0.08467162578448957\n",
      "Stochastic Gradient Descent(40436): loss=4.015918818516215\n",
      "Stochastic Gradient Descent(40437): loss=19.620639042130843\n",
      "Stochastic Gradient Descent(40438): loss=0.14499429410004397\n",
      "Stochastic Gradient Descent(40439): loss=16.277318888966967\n",
      "Stochastic Gradient Descent(40440): loss=2.228595470389573\n",
      "Stochastic Gradient Descent(40441): loss=6.030268393550555\n",
      "Stochastic Gradient Descent(40442): loss=0.821411180939349\n",
      "Stochastic Gradient Descent(40443): loss=4.482919996855929\n",
      "Stochastic Gradient Descent(40444): loss=0.5289367276265923\n",
      "Stochastic Gradient Descent(40445): loss=4.388108617549544\n",
      "Stochastic Gradient Descent(40446): loss=0.6649473786710035\n",
      "Stochastic Gradient Descent(40447): loss=0.6332893951811132\n",
      "Stochastic Gradient Descent(40448): loss=4.483543316935225\n",
      "Stochastic Gradient Descent(40449): loss=8.864260084652601\n",
      "Stochastic Gradient Descent(40450): loss=0.1053596362710462\n",
      "Stochastic Gradient Descent(40451): loss=10.395973704315779\n",
      "Stochastic Gradient Descent(40452): loss=4.292149210586546\n",
      "Stochastic Gradient Descent(40453): loss=3.0927233130009375\n",
      "Stochastic Gradient Descent(40454): loss=0.12066014332898269\n",
      "Stochastic Gradient Descent(40455): loss=6.850910166160713\n",
      "Stochastic Gradient Descent(40456): loss=0.21490917171681853\n",
      "Stochastic Gradient Descent(40457): loss=1.958395774821325\n",
      "Stochastic Gradient Descent(40458): loss=0.5270014117941649\n",
      "Stochastic Gradient Descent(40459): loss=22.353135259242915\n",
      "Stochastic Gradient Descent(40460): loss=22.072281863017302\n",
      "Stochastic Gradient Descent(40461): loss=0.028081590560036673\n",
      "Stochastic Gradient Descent(40462): loss=9.861450363084234\n",
      "Stochastic Gradient Descent(40463): loss=0.08573466258767874\n",
      "Stochastic Gradient Descent(40464): loss=0.8780264933915846\n",
      "Stochastic Gradient Descent(40465): loss=9.07328565319987\n",
      "Stochastic Gradient Descent(40466): loss=4.396421938920037\n",
      "Stochastic Gradient Descent(40467): loss=0.39833761885358543\n",
      "Stochastic Gradient Descent(40468): loss=4.175474695884367\n",
      "Stochastic Gradient Descent(40469): loss=3.4489072059271946\n",
      "Stochastic Gradient Descent(40470): loss=0.3789944342864345\n",
      "Stochastic Gradient Descent(40471): loss=0.25743845037582674\n",
      "Stochastic Gradient Descent(40472): loss=2.374081865907599\n",
      "Stochastic Gradient Descent(40473): loss=6.284524354349267\n",
      "Stochastic Gradient Descent(40474): loss=0.03434947728168455\n",
      "Stochastic Gradient Descent(40475): loss=12.950921758012425\n",
      "Stochastic Gradient Descent(40476): loss=0.10567060142241758\n",
      "Stochastic Gradient Descent(40477): loss=0.1878145232701823\n",
      "Stochastic Gradient Descent(40478): loss=6.546023181839017\n",
      "Stochastic Gradient Descent(40479): loss=0.1337826347244634\n",
      "Stochastic Gradient Descent(40480): loss=0.6241689987636283\n",
      "Stochastic Gradient Descent(40481): loss=1.2498111728440349\n",
      "Stochastic Gradient Descent(40482): loss=0.04223579249019004\n",
      "Stochastic Gradient Descent(40483): loss=2.15433462968314\n",
      "Stochastic Gradient Descent(40484): loss=0.24182748481050967\n",
      "Stochastic Gradient Descent(40485): loss=1.278128868291211\n",
      "Stochastic Gradient Descent(40486): loss=8.35072896318199\n",
      "Stochastic Gradient Descent(40487): loss=0.6458553667181582\n",
      "Stochastic Gradient Descent(40488): loss=2.3810126604043202\n",
      "Stochastic Gradient Descent(40489): loss=9.827796728192089\n",
      "Stochastic Gradient Descent(40490): loss=4.067047135051574\n",
      "Stochastic Gradient Descent(40491): loss=8.218599817840543\n",
      "Stochastic Gradient Descent(40492): loss=11.323288139542779\n",
      "Stochastic Gradient Descent(40493): loss=0.4401766592537862\n",
      "Stochastic Gradient Descent(40494): loss=2.819185373929038\n",
      "Stochastic Gradient Descent(40495): loss=1.976286309224027\n",
      "Stochastic Gradient Descent(40496): loss=0.293827768629938\n",
      "Stochastic Gradient Descent(40497): loss=8.171391176504448e-05\n",
      "Stochastic Gradient Descent(40498): loss=13.250989109006\n",
      "Stochastic Gradient Descent(40499): loss=3.2072597160042595\n",
      "Stochastic Gradient Descent(40500): loss=0.6257560756433481\n",
      "Stochastic Gradient Descent(40501): loss=1.0313873807742595\n",
      "Stochastic Gradient Descent(40502): loss=4.992584161822722\n",
      "Stochastic Gradient Descent(40503): loss=2.6881389606315196\n",
      "Stochastic Gradient Descent(40504): loss=4.995283442021765\n",
      "Stochastic Gradient Descent(40505): loss=6.986938627958081\n",
      "Stochastic Gradient Descent(40506): loss=5.039336216788298\n",
      "Stochastic Gradient Descent(40507): loss=0.02255042321186504\n",
      "Stochastic Gradient Descent(40508): loss=1.567057268326234\n",
      "Stochastic Gradient Descent(40509): loss=1.85659163984482\n",
      "Stochastic Gradient Descent(40510): loss=22.49095463251125\n",
      "Stochastic Gradient Descent(40511): loss=1.0818697209221388\n",
      "Stochastic Gradient Descent(40512): loss=3.043430768181322\n",
      "Stochastic Gradient Descent(40513): loss=4.8806493000972075\n",
      "Stochastic Gradient Descent(40514): loss=1.4490892993370066\n",
      "Stochastic Gradient Descent(40515): loss=10.33410605657826\n",
      "Stochastic Gradient Descent(40516): loss=1.2156848696837126\n",
      "Stochastic Gradient Descent(40517): loss=5.705730612501711\n",
      "Stochastic Gradient Descent(40518): loss=1.5436538939406028\n",
      "Stochastic Gradient Descent(40519): loss=15.398370200914396\n",
      "Stochastic Gradient Descent(40520): loss=10.97672785817968\n",
      "Stochastic Gradient Descent(40521): loss=4.789346078961454\n",
      "Stochastic Gradient Descent(40522): loss=6.3872942633614755\n",
      "Stochastic Gradient Descent(40523): loss=3.564943053690523\n",
      "Stochastic Gradient Descent(40524): loss=0.0747319174973733\n",
      "Stochastic Gradient Descent(40525): loss=5.411281828680187\n",
      "Stochastic Gradient Descent(40526): loss=1.0257441318206333\n",
      "Stochastic Gradient Descent(40527): loss=9.623654890680575\n",
      "Stochastic Gradient Descent(40528): loss=9.588223961578809\n",
      "Stochastic Gradient Descent(40529): loss=1.8220082873203454\n",
      "Stochastic Gradient Descent(40530): loss=7.753025203811425\n",
      "Stochastic Gradient Descent(40531): loss=8.892230240163583\n",
      "Stochastic Gradient Descent(40532): loss=0.6160182963816522\n",
      "Stochastic Gradient Descent(40533): loss=1.0220028254665612\n",
      "Stochastic Gradient Descent(40534): loss=0.6398700637185332\n",
      "Stochastic Gradient Descent(40535): loss=0.007566039463226742\n",
      "Stochastic Gradient Descent(40536): loss=0.20682258607337076\n",
      "Stochastic Gradient Descent(40537): loss=23.698875130089313\n",
      "Stochastic Gradient Descent(40538): loss=0.032657049148187435\n",
      "Stochastic Gradient Descent(40539): loss=3.348185237093394\n",
      "Stochastic Gradient Descent(40540): loss=3.3562339798984246\n",
      "Stochastic Gradient Descent(40541): loss=3.155411278644974\n",
      "Stochastic Gradient Descent(40542): loss=4.869251798510053\n",
      "Stochastic Gradient Descent(40543): loss=4.27279875060521\n",
      "Stochastic Gradient Descent(40544): loss=5.329752529831006\n",
      "Stochastic Gradient Descent(40545): loss=0.02189075743028315\n",
      "Stochastic Gradient Descent(40546): loss=2.952349804988535\n",
      "Stochastic Gradient Descent(40547): loss=0.8967271203994424\n",
      "Stochastic Gradient Descent(40548): loss=0.608183200038202\n",
      "Stochastic Gradient Descent(40549): loss=0.18729388767556232\n",
      "Stochastic Gradient Descent(40550): loss=18.591266626950084\n",
      "Stochastic Gradient Descent(40551): loss=0.5760800394221893\n",
      "Stochastic Gradient Descent(40552): loss=6.886642645099621\n",
      "Stochastic Gradient Descent(40553): loss=8.459353692058391\n",
      "Stochastic Gradient Descent(40554): loss=0.3330280317208045\n",
      "Stochastic Gradient Descent(40555): loss=0.15179835283534512\n",
      "Stochastic Gradient Descent(40556): loss=3.3046557921135813\n",
      "Stochastic Gradient Descent(40557): loss=0.02033923660886606\n",
      "Stochastic Gradient Descent(40558): loss=1.1121799916642414\n",
      "Stochastic Gradient Descent(40559): loss=2.2641240922801718\n",
      "Stochastic Gradient Descent(40560): loss=11.481942947630962\n",
      "Stochastic Gradient Descent(40561): loss=0.0013436098979710215\n",
      "Stochastic Gradient Descent(40562): loss=1.0919587007135425\n",
      "Stochastic Gradient Descent(40563): loss=10.705569701081536\n",
      "Stochastic Gradient Descent(40564): loss=0.4036922442277773\n",
      "Stochastic Gradient Descent(40565): loss=2.894947575199241\n",
      "Stochastic Gradient Descent(40566): loss=2.3632336243923846\n",
      "Stochastic Gradient Descent(40567): loss=0.9439764367890136\n",
      "Stochastic Gradient Descent(40568): loss=0.4778137232572304\n",
      "Stochastic Gradient Descent(40569): loss=0.36078431917278875\n",
      "Stochastic Gradient Descent(40570): loss=2.0409285764535974\n",
      "Stochastic Gradient Descent(40571): loss=1.1384836113139238\n",
      "Stochastic Gradient Descent(40572): loss=0.011189793000380331\n",
      "Stochastic Gradient Descent(40573): loss=22.305980152541913\n",
      "Stochastic Gradient Descent(40574): loss=0.7463126531773117\n",
      "Stochastic Gradient Descent(40575): loss=10.260038434600654\n",
      "Stochastic Gradient Descent(40576): loss=5.16555541182857\n",
      "Stochastic Gradient Descent(40577): loss=1.9941818197000867\n",
      "Stochastic Gradient Descent(40578): loss=0.004967904473247231\n",
      "Stochastic Gradient Descent(40579): loss=0.08984226785438523\n",
      "Stochastic Gradient Descent(40580): loss=22.964432591630228\n",
      "Stochastic Gradient Descent(40581): loss=0.01554130935827565\n",
      "Stochastic Gradient Descent(40582): loss=0.12734364875720858\n",
      "Stochastic Gradient Descent(40583): loss=0.00437088997328578\n",
      "Stochastic Gradient Descent(40584): loss=13.381730413926297\n",
      "Stochastic Gradient Descent(40585): loss=0.022481242771566604\n",
      "Stochastic Gradient Descent(40586): loss=7.6919894421571575\n",
      "Stochastic Gradient Descent(40587): loss=2.592111451595318\n",
      "Stochastic Gradient Descent(40588): loss=21.34243583838896\n",
      "Stochastic Gradient Descent(40589): loss=5.520626051365818\n",
      "Stochastic Gradient Descent(40590): loss=5.066430205922641\n",
      "Stochastic Gradient Descent(40591): loss=1.820804092685226\n",
      "Stochastic Gradient Descent(40592): loss=0.5490641945397489\n",
      "Stochastic Gradient Descent(40593): loss=10.133517599962682\n",
      "Stochastic Gradient Descent(40594): loss=4.051285035013603\n",
      "Stochastic Gradient Descent(40595): loss=0.0236834340403386\n",
      "Stochastic Gradient Descent(40596): loss=5.104377963433837\n",
      "Stochastic Gradient Descent(40597): loss=0.23592572849900478\n",
      "Stochastic Gradient Descent(40598): loss=1.3222221823050169\n",
      "Stochastic Gradient Descent(40599): loss=14.33675490147332\n",
      "Stochastic Gradient Descent(40600): loss=3.4824139119175124\n",
      "Stochastic Gradient Descent(40601): loss=0.17052924378295134\n",
      "Stochastic Gradient Descent(40602): loss=8.112364635702583\n",
      "Stochastic Gradient Descent(40603): loss=0.28571832320674406\n",
      "Stochastic Gradient Descent(40604): loss=1.2306664155415272\n",
      "Stochastic Gradient Descent(40605): loss=2.4232457882014202\n",
      "Stochastic Gradient Descent(40606): loss=0.005032421558993586\n",
      "Stochastic Gradient Descent(40607): loss=0.5829285516304852\n",
      "Stochastic Gradient Descent(40608): loss=1.7809289978119267\n",
      "Stochastic Gradient Descent(40609): loss=0.9639709896999115\n",
      "Stochastic Gradient Descent(40610): loss=0.30545066309117913\n",
      "Stochastic Gradient Descent(40611): loss=5.9385597203576035\n",
      "Stochastic Gradient Descent(40612): loss=0.04580973033341263\n",
      "Stochastic Gradient Descent(40613): loss=0.03796163200149136\n",
      "Stochastic Gradient Descent(40614): loss=7.121354333261489\n",
      "Stochastic Gradient Descent(40615): loss=2.374542221859941\n",
      "Stochastic Gradient Descent(40616): loss=0.8114124451518175\n",
      "Stochastic Gradient Descent(40617): loss=0.12583970546609696\n",
      "Stochastic Gradient Descent(40618): loss=8.572890824678721\n",
      "Stochastic Gradient Descent(40619): loss=0.19473970154548126\n",
      "Stochastic Gradient Descent(40620): loss=4.185277680586695\n",
      "Stochastic Gradient Descent(40621): loss=1.1724077264143926\n",
      "Stochastic Gradient Descent(40622): loss=1.5940641142810201\n",
      "Stochastic Gradient Descent(40623): loss=6.109035077730242\n",
      "Stochastic Gradient Descent(40624): loss=2.225165709497858\n",
      "Stochastic Gradient Descent(40625): loss=3.7151926164766436\n",
      "Stochastic Gradient Descent(40626): loss=11.954757015612385\n",
      "Stochastic Gradient Descent(40627): loss=3.64990072567817\n",
      "Stochastic Gradient Descent(40628): loss=0.7251819414481323\n",
      "Stochastic Gradient Descent(40629): loss=1.106896561348287\n",
      "Stochastic Gradient Descent(40630): loss=1.3288148758682363\n",
      "Stochastic Gradient Descent(40631): loss=2.5663136024410056\n",
      "Stochastic Gradient Descent(40632): loss=0.5158012889902134\n",
      "Stochastic Gradient Descent(40633): loss=3.826494791487231\n",
      "Stochastic Gradient Descent(40634): loss=8.684879956884453\n",
      "Stochastic Gradient Descent(40635): loss=7.913497224700973\n",
      "Stochastic Gradient Descent(40636): loss=2.1577560048714366\n",
      "Stochastic Gradient Descent(40637): loss=7.758291505553466\n",
      "Stochastic Gradient Descent(40638): loss=5.7109073730293565\n",
      "Stochastic Gradient Descent(40639): loss=0.1635799356134991\n",
      "Stochastic Gradient Descent(40640): loss=6.090330991679983\n",
      "Stochastic Gradient Descent(40641): loss=2.1223652305570435\n",
      "Stochastic Gradient Descent(40642): loss=1.599774610060835\n",
      "Stochastic Gradient Descent(40643): loss=0.12946817038057756\n",
      "Stochastic Gradient Descent(40644): loss=5.28647635776462\n",
      "Stochastic Gradient Descent(40645): loss=3.0374353641245997\n",
      "Stochastic Gradient Descent(40646): loss=2.0699949917359444\n",
      "Stochastic Gradient Descent(40647): loss=5.948351301802752\n",
      "Stochastic Gradient Descent(40648): loss=0.17841126375983227\n",
      "Stochastic Gradient Descent(40649): loss=0.3369872307055228\n",
      "Stochastic Gradient Descent(40650): loss=6.194654381573914\n",
      "Stochastic Gradient Descent(40651): loss=2.1043012965837162\n",
      "Stochastic Gradient Descent(40652): loss=10.713687582689916\n",
      "Stochastic Gradient Descent(40653): loss=3.7870753399720893\n",
      "Stochastic Gradient Descent(40654): loss=0.11793307588705013\n",
      "Stochastic Gradient Descent(40655): loss=23.005929992129484\n",
      "Stochastic Gradient Descent(40656): loss=5.327290266508723\n",
      "Stochastic Gradient Descent(40657): loss=7.664420875977255\n",
      "Stochastic Gradient Descent(40658): loss=6.011867389933382\n",
      "Stochastic Gradient Descent(40659): loss=10.857577853162638\n",
      "Stochastic Gradient Descent(40660): loss=0.21761531340370677\n",
      "Stochastic Gradient Descent(40661): loss=0.3581635383434511\n",
      "Stochastic Gradient Descent(40662): loss=5.113818668209419\n",
      "Stochastic Gradient Descent(40663): loss=1.0037242492231297\n",
      "Stochastic Gradient Descent(40664): loss=0.24887240868462404\n",
      "Stochastic Gradient Descent(40665): loss=5.667187525596591\n",
      "Stochastic Gradient Descent(40666): loss=1.241025686660186\n",
      "Stochastic Gradient Descent(40667): loss=0.26507067325963274\n",
      "Stochastic Gradient Descent(40668): loss=2.6193065790792716\n",
      "Stochastic Gradient Descent(40669): loss=1.1012362120478913\n",
      "Stochastic Gradient Descent(40670): loss=4.925271170603166\n",
      "Stochastic Gradient Descent(40671): loss=0.873545853268475\n",
      "Stochastic Gradient Descent(40672): loss=0.15081877404177346\n",
      "Stochastic Gradient Descent(40673): loss=0.12238256492983574\n",
      "Stochastic Gradient Descent(40674): loss=7.691077688338708\n",
      "Stochastic Gradient Descent(40675): loss=0.12071400910172599\n",
      "Stochastic Gradient Descent(40676): loss=2.8641710478685023\n",
      "Stochastic Gradient Descent(40677): loss=0.7824027731089652\n",
      "Stochastic Gradient Descent(40678): loss=18.35358481645147\n",
      "Stochastic Gradient Descent(40679): loss=12.332520457590318\n",
      "Stochastic Gradient Descent(40680): loss=63.71333814977085\n",
      "Stochastic Gradient Descent(40681): loss=1.7122270006742952\n",
      "Stochastic Gradient Descent(40682): loss=8.619096344798841\n",
      "Stochastic Gradient Descent(40683): loss=2.792982461708168\n",
      "Stochastic Gradient Descent(40684): loss=1.8477293997039994\n",
      "Stochastic Gradient Descent(40685): loss=3.7632433815998003\n",
      "Stochastic Gradient Descent(40686): loss=2.843520982280825\n",
      "Stochastic Gradient Descent(40687): loss=0.12411236898239887\n",
      "Stochastic Gradient Descent(40688): loss=3.4779098479984794\n",
      "Stochastic Gradient Descent(40689): loss=0.4152357745215569\n",
      "Stochastic Gradient Descent(40690): loss=1.7105330959872904\n",
      "Stochastic Gradient Descent(40691): loss=0.3874101959302179\n",
      "Stochastic Gradient Descent(40692): loss=0.3757640570196265\n",
      "Stochastic Gradient Descent(40693): loss=24.409667853670925\n",
      "Stochastic Gradient Descent(40694): loss=38.11588899479229\n",
      "Stochastic Gradient Descent(40695): loss=7.6326972770500845\n",
      "Stochastic Gradient Descent(40696): loss=3.4383385477802273\n",
      "Stochastic Gradient Descent(40697): loss=0.0006951857876559843\n",
      "Stochastic Gradient Descent(40698): loss=0.012320275708768758\n",
      "Stochastic Gradient Descent(40699): loss=0.6881858204953066\n",
      "Stochastic Gradient Descent(40700): loss=2.155665575246494\n",
      "Stochastic Gradient Descent(40701): loss=5.972673145898024\n",
      "Stochastic Gradient Descent(40702): loss=2.189326543480369\n",
      "Stochastic Gradient Descent(40703): loss=10.235411872113666\n",
      "Stochastic Gradient Descent(40704): loss=0.15496434065519415\n",
      "Stochastic Gradient Descent(40705): loss=2.3032758852805606\n",
      "Stochastic Gradient Descent(40706): loss=10.40469205598312\n",
      "Stochastic Gradient Descent(40707): loss=0.05372366156578473\n",
      "Stochastic Gradient Descent(40708): loss=0.0002505759257396602\n",
      "Stochastic Gradient Descent(40709): loss=0.5625133314901006\n",
      "Stochastic Gradient Descent(40710): loss=13.02272342266851\n",
      "Stochastic Gradient Descent(40711): loss=0.6208489723204679\n",
      "Stochastic Gradient Descent(40712): loss=8.401687470035453\n",
      "Stochastic Gradient Descent(40713): loss=0.463468922335389\n",
      "Stochastic Gradient Descent(40714): loss=6.764809130875801\n",
      "Stochastic Gradient Descent(40715): loss=0.3415626103586009\n",
      "Stochastic Gradient Descent(40716): loss=1.2289330731819579\n",
      "Stochastic Gradient Descent(40717): loss=28.006769855486805\n",
      "Stochastic Gradient Descent(40718): loss=9.334112882308142\n",
      "Stochastic Gradient Descent(40719): loss=0.5907030287038426\n",
      "Stochastic Gradient Descent(40720): loss=0.31280357590076036\n",
      "Stochastic Gradient Descent(40721): loss=0.1931359129791554\n",
      "Stochastic Gradient Descent(40722): loss=18.030243765523625\n",
      "Stochastic Gradient Descent(40723): loss=4.156706320076229\n",
      "Stochastic Gradient Descent(40724): loss=5.1284482611532045\n",
      "Stochastic Gradient Descent(40725): loss=12.209798243405519\n",
      "Stochastic Gradient Descent(40726): loss=3.1076661047544025\n",
      "Stochastic Gradient Descent(40727): loss=1.1507833025611536\n",
      "Stochastic Gradient Descent(40728): loss=0.7671629423762188\n",
      "Stochastic Gradient Descent(40729): loss=14.95936891581711\n",
      "Stochastic Gradient Descent(40730): loss=0.06484920355167395\n",
      "Stochastic Gradient Descent(40731): loss=36.09088357811177\n",
      "Stochastic Gradient Descent(40732): loss=35.9061657768771\n",
      "Stochastic Gradient Descent(40733): loss=57.010309873313794\n",
      "Stochastic Gradient Descent(40734): loss=6.671552047020588\n",
      "Stochastic Gradient Descent(40735): loss=7.158039738617996\n",
      "Stochastic Gradient Descent(40736): loss=0.031464271131363984\n",
      "Stochastic Gradient Descent(40737): loss=6.638165766307731\n",
      "Stochastic Gradient Descent(40738): loss=0.9357781500892621\n",
      "Stochastic Gradient Descent(40739): loss=2.2759420987589594\n",
      "Stochastic Gradient Descent(40740): loss=0.2879517712255558\n",
      "Stochastic Gradient Descent(40741): loss=0.06558809915551131\n",
      "Stochastic Gradient Descent(40742): loss=0.37501057777778385\n",
      "Stochastic Gradient Descent(40743): loss=10.286797027200924\n",
      "Stochastic Gradient Descent(40744): loss=6.503547945762306\n",
      "Stochastic Gradient Descent(40745): loss=0.19724662832546155\n",
      "Stochastic Gradient Descent(40746): loss=3.098440116092348\n",
      "Stochastic Gradient Descent(40747): loss=2.8860600909268554\n",
      "Stochastic Gradient Descent(40748): loss=0.2841776679945333\n",
      "Stochastic Gradient Descent(40749): loss=7.382478581163265\n",
      "Stochastic Gradient Descent(40750): loss=0.1234120521097613\n",
      "Stochastic Gradient Descent(40751): loss=0.6045679807720921\n",
      "Stochastic Gradient Descent(40752): loss=0.055187795057315386\n",
      "Stochastic Gradient Descent(40753): loss=11.532191896073357\n",
      "Stochastic Gradient Descent(40754): loss=1.2204024639693916\n",
      "Stochastic Gradient Descent(40755): loss=0.1437953334763723\n",
      "Stochastic Gradient Descent(40756): loss=26.517427890500983\n",
      "Stochastic Gradient Descent(40757): loss=2.3865119563191666\n",
      "Stochastic Gradient Descent(40758): loss=2.5854301418229677\n",
      "Stochastic Gradient Descent(40759): loss=1.2347550375069891\n",
      "Stochastic Gradient Descent(40760): loss=2.072131196914847\n",
      "Stochastic Gradient Descent(40761): loss=5.179296510206052\n",
      "Stochastic Gradient Descent(40762): loss=0.2389984089722747\n",
      "Stochastic Gradient Descent(40763): loss=4.024222776064105\n",
      "Stochastic Gradient Descent(40764): loss=12.669570619403869\n",
      "Stochastic Gradient Descent(40765): loss=0.016889670962003986\n",
      "Stochastic Gradient Descent(40766): loss=1.991389613068372\n",
      "Stochastic Gradient Descent(40767): loss=9.922457366943789\n",
      "Stochastic Gradient Descent(40768): loss=3.550064819835633\n",
      "Stochastic Gradient Descent(40769): loss=0.38073556593946306\n",
      "Stochastic Gradient Descent(40770): loss=11.830335243993707\n",
      "Stochastic Gradient Descent(40771): loss=0.2948247029038979\n",
      "Stochastic Gradient Descent(40772): loss=7.317827194973315\n",
      "Stochastic Gradient Descent(40773): loss=0.4998912250784741\n",
      "Stochastic Gradient Descent(40774): loss=0.6396153142287128\n",
      "Stochastic Gradient Descent(40775): loss=1.2459468152131885\n",
      "Stochastic Gradient Descent(40776): loss=1.0493495739725822\n",
      "Stochastic Gradient Descent(40777): loss=3.572189062180232\n",
      "Stochastic Gradient Descent(40778): loss=9.771056622358175\n",
      "Stochastic Gradient Descent(40779): loss=0.7213429224818819\n",
      "Stochastic Gradient Descent(40780): loss=0.16329929416265346\n",
      "Stochastic Gradient Descent(40781): loss=0.5668484119950131\n",
      "Stochastic Gradient Descent(40782): loss=7.260908287733657\n",
      "Stochastic Gradient Descent(40783): loss=0.008990923228677075\n",
      "Stochastic Gradient Descent(40784): loss=14.53481590318631\n",
      "Stochastic Gradient Descent(40785): loss=0.8825772567440741\n",
      "Stochastic Gradient Descent(40786): loss=1.4989428891889445\n",
      "Stochastic Gradient Descent(40787): loss=10.910150928734167\n",
      "Stochastic Gradient Descent(40788): loss=1.1793437571727707\n",
      "Stochastic Gradient Descent(40789): loss=0.10248993242531337\n",
      "Stochastic Gradient Descent(40790): loss=8.594760738542794\n",
      "Stochastic Gradient Descent(40791): loss=11.482216756381169\n",
      "Stochastic Gradient Descent(40792): loss=15.0469220708811\n",
      "Stochastic Gradient Descent(40793): loss=0.1082729859172556\n",
      "Stochastic Gradient Descent(40794): loss=0.004551728571799915\n",
      "Stochastic Gradient Descent(40795): loss=0.41768383760457195\n",
      "Stochastic Gradient Descent(40796): loss=0.8246959347349577\n",
      "Stochastic Gradient Descent(40797): loss=1.9555387872976722\n",
      "Stochastic Gradient Descent(40798): loss=1.6554431665526355\n",
      "Stochastic Gradient Descent(40799): loss=0.0008042571279892919\n",
      "Stochastic Gradient Descent(40800): loss=1.1963490524776212\n",
      "Stochastic Gradient Descent(40801): loss=0.4822743878256045\n",
      "Stochastic Gradient Descent(40802): loss=2.534359788415621\n",
      "Stochastic Gradient Descent(40803): loss=0.1548782892224985\n",
      "Stochastic Gradient Descent(40804): loss=3.395102833692115\n",
      "Stochastic Gradient Descent(40805): loss=0.1705145628525079\n",
      "Stochastic Gradient Descent(40806): loss=23.093110728089542\n",
      "Stochastic Gradient Descent(40807): loss=4.458595876175655\n",
      "Stochastic Gradient Descent(40808): loss=0.02502725938140859\n",
      "Stochastic Gradient Descent(40809): loss=0.35227968943449817\n",
      "Stochastic Gradient Descent(40810): loss=3.241497927503699\n",
      "Stochastic Gradient Descent(40811): loss=22.39995829797864\n",
      "Stochastic Gradient Descent(40812): loss=11.531584143370544\n",
      "Stochastic Gradient Descent(40813): loss=0.40495304042529656\n",
      "Stochastic Gradient Descent(40814): loss=0.9590049848255635\n",
      "Stochastic Gradient Descent(40815): loss=0.13482930389374123\n",
      "Stochastic Gradient Descent(40816): loss=1.013306311489706\n",
      "Stochastic Gradient Descent(40817): loss=0.06128061205532252\n",
      "Stochastic Gradient Descent(40818): loss=1.740722553516817\n",
      "Stochastic Gradient Descent(40819): loss=0.1224094231206102\n",
      "Stochastic Gradient Descent(40820): loss=0.8631362959422523\n",
      "Stochastic Gradient Descent(40821): loss=0.5279962741737382\n",
      "Stochastic Gradient Descent(40822): loss=0.4909196885206757\n",
      "Stochastic Gradient Descent(40823): loss=1.3858825292057089\n",
      "Stochastic Gradient Descent(40824): loss=1.5679525558278071\n",
      "Stochastic Gradient Descent(40825): loss=0.8517231161419496\n",
      "Stochastic Gradient Descent(40826): loss=13.909085424396475\n",
      "Stochastic Gradient Descent(40827): loss=1.7290101571831893\n",
      "Stochastic Gradient Descent(40828): loss=8.946218260318453\n",
      "Stochastic Gradient Descent(40829): loss=0.006923860151030177\n",
      "Stochastic Gradient Descent(40830): loss=0.18293225323673076\n",
      "Stochastic Gradient Descent(40831): loss=0.25670323388639865\n",
      "Stochastic Gradient Descent(40832): loss=3.064471006427526\n",
      "Stochastic Gradient Descent(40833): loss=1.0348269366948302\n",
      "Stochastic Gradient Descent(40834): loss=0.8577765106382963\n",
      "Stochastic Gradient Descent(40835): loss=1.3633820214722983\n",
      "Stochastic Gradient Descent(40836): loss=0.0034489569911094068\n",
      "Stochastic Gradient Descent(40837): loss=0.6031530387603655\n",
      "Stochastic Gradient Descent(40838): loss=1.1141556964928532\n",
      "Stochastic Gradient Descent(40839): loss=1.2296965488478788\n",
      "Stochastic Gradient Descent(40840): loss=16.247112820769114\n",
      "Stochastic Gradient Descent(40841): loss=7.393460512283612\n",
      "Stochastic Gradient Descent(40842): loss=1.6745902767697107\n",
      "Stochastic Gradient Descent(40843): loss=0.29265556774842016\n",
      "Stochastic Gradient Descent(40844): loss=0.013845312057340107\n",
      "Stochastic Gradient Descent(40845): loss=1.993628646717857\n",
      "Stochastic Gradient Descent(40846): loss=0.16894676493086486\n",
      "Stochastic Gradient Descent(40847): loss=0.012871500806323247\n",
      "Stochastic Gradient Descent(40848): loss=1.0206562117907294\n",
      "Stochastic Gradient Descent(40849): loss=2.9288246238590134\n",
      "Stochastic Gradient Descent(40850): loss=3.5952306585563716\n",
      "Stochastic Gradient Descent(40851): loss=0.0058441718861056416\n",
      "Stochastic Gradient Descent(40852): loss=0.0004898061522774945\n",
      "Stochastic Gradient Descent(40853): loss=1.9387559905956837\n",
      "Stochastic Gradient Descent(40854): loss=0.24878207638404062\n",
      "Stochastic Gradient Descent(40855): loss=15.436115061940766\n",
      "Stochastic Gradient Descent(40856): loss=0.02281271446556302\n",
      "Stochastic Gradient Descent(40857): loss=16.59246602875932\n",
      "Stochastic Gradient Descent(40858): loss=3.1113045427676758\n",
      "Stochastic Gradient Descent(40859): loss=3.7476087781292575\n",
      "Stochastic Gradient Descent(40860): loss=0.0013386475658093444\n",
      "Stochastic Gradient Descent(40861): loss=3.125387169987856\n",
      "Stochastic Gradient Descent(40862): loss=0.7361428722719471\n",
      "Stochastic Gradient Descent(40863): loss=1.0706717692624323\n",
      "Stochastic Gradient Descent(40864): loss=6.943401763138338\n",
      "Stochastic Gradient Descent(40865): loss=2.657179446383204\n",
      "Stochastic Gradient Descent(40866): loss=2.31416516939728\n",
      "Stochastic Gradient Descent(40867): loss=3.462577203053475\n",
      "Stochastic Gradient Descent(40868): loss=10.938548003375539\n",
      "Stochastic Gradient Descent(40869): loss=1.802282863461549\n",
      "Stochastic Gradient Descent(40870): loss=3.1576966462335814\n",
      "Stochastic Gradient Descent(40871): loss=0.2917461841845164\n",
      "Stochastic Gradient Descent(40872): loss=10.985070159742307\n",
      "Stochastic Gradient Descent(40873): loss=7.338976521763117\n",
      "Stochastic Gradient Descent(40874): loss=0.123296754687103\n",
      "Stochastic Gradient Descent(40875): loss=0.0009364703541854361\n",
      "Stochastic Gradient Descent(40876): loss=2.6071601258118853\n",
      "Stochastic Gradient Descent(40877): loss=1.2353690442096583\n",
      "Stochastic Gradient Descent(40878): loss=1.0133431097409082\n",
      "Stochastic Gradient Descent(40879): loss=27.31266331044798\n",
      "Stochastic Gradient Descent(40880): loss=0.18628790098690903\n",
      "Stochastic Gradient Descent(40881): loss=4.511653853823423\n",
      "Stochastic Gradient Descent(40882): loss=0.10961982972453196\n",
      "Stochastic Gradient Descent(40883): loss=0.2677167750082538\n",
      "Stochastic Gradient Descent(40884): loss=3.068044378830245\n",
      "Stochastic Gradient Descent(40885): loss=13.32610525947585\n",
      "Stochastic Gradient Descent(40886): loss=10.929359366256001\n",
      "Stochastic Gradient Descent(40887): loss=1.198621426533535\n",
      "Stochastic Gradient Descent(40888): loss=2.1811396161619654\n",
      "Stochastic Gradient Descent(40889): loss=1.229298991632104\n",
      "Stochastic Gradient Descent(40890): loss=9.409065069961446\n",
      "Stochastic Gradient Descent(40891): loss=0.03801722931012108\n",
      "Stochastic Gradient Descent(40892): loss=3.6553802687495436\n",
      "Stochastic Gradient Descent(40893): loss=1.6737070186124168\n",
      "Stochastic Gradient Descent(40894): loss=2.3212798456781636\n",
      "Stochastic Gradient Descent(40895): loss=0.11112635666770096\n",
      "Stochastic Gradient Descent(40896): loss=1.9908914924605383\n",
      "Stochastic Gradient Descent(40897): loss=4.084447535780994\n",
      "Stochastic Gradient Descent(40898): loss=4.616697487085842\n",
      "Stochastic Gradient Descent(40899): loss=9.32040908784903\n",
      "Stochastic Gradient Descent(40900): loss=3.5892396830690565\n",
      "Stochastic Gradient Descent(40901): loss=2.207201075901\n",
      "Stochastic Gradient Descent(40902): loss=2.287090180526048\n",
      "Stochastic Gradient Descent(40903): loss=0.2924317886607958\n",
      "Stochastic Gradient Descent(40904): loss=0.11222997227995843\n",
      "Stochastic Gradient Descent(40905): loss=6.076036416025275\n",
      "Stochastic Gradient Descent(40906): loss=0.9475886756777889\n",
      "Stochastic Gradient Descent(40907): loss=1.127517554644213\n",
      "Stochastic Gradient Descent(40908): loss=0.2746461947179779\n",
      "Stochastic Gradient Descent(40909): loss=0.9651809032579474\n",
      "Stochastic Gradient Descent(40910): loss=0.10139778882301725\n",
      "Stochastic Gradient Descent(40911): loss=1.8137968769550632\n",
      "Stochastic Gradient Descent(40912): loss=0.4514560488000058\n",
      "Stochastic Gradient Descent(40913): loss=31.277230894946946\n",
      "Stochastic Gradient Descent(40914): loss=1.7554195688841978\n",
      "Stochastic Gradient Descent(40915): loss=1.4932828962783447\n",
      "Stochastic Gradient Descent(40916): loss=0.00451995097826022\n",
      "Stochastic Gradient Descent(40917): loss=0.07108551961283865\n",
      "Stochastic Gradient Descent(40918): loss=0.0038069069219820143\n",
      "Stochastic Gradient Descent(40919): loss=0.056493885170134574\n",
      "Stochastic Gradient Descent(40920): loss=0.8668687111439989\n",
      "Stochastic Gradient Descent(40921): loss=1.3644611540301999\n",
      "Stochastic Gradient Descent(40922): loss=0.33175935781773774\n",
      "Stochastic Gradient Descent(40923): loss=7.673953601204352e-05\n",
      "Stochastic Gradient Descent(40924): loss=4.165690673187772\n",
      "Stochastic Gradient Descent(40925): loss=0.6934526315958538\n",
      "Stochastic Gradient Descent(40926): loss=3.9897185734208933\n",
      "Stochastic Gradient Descent(40927): loss=1.6023134625013602\n",
      "Stochastic Gradient Descent(40928): loss=0.8372398116447796\n",
      "Stochastic Gradient Descent(40929): loss=5.134400393561827\n",
      "Stochastic Gradient Descent(40930): loss=0.709659186210737\n",
      "Stochastic Gradient Descent(40931): loss=1.1914861524885423\n",
      "Stochastic Gradient Descent(40932): loss=13.697831159581183\n",
      "Stochastic Gradient Descent(40933): loss=0.0047198680934166445\n",
      "Stochastic Gradient Descent(40934): loss=1.1315629817965895\n",
      "Stochastic Gradient Descent(40935): loss=1.7923420851499614\n",
      "Stochastic Gradient Descent(40936): loss=0.3723214878848704\n",
      "Stochastic Gradient Descent(40937): loss=2.2763286867392694\n",
      "Stochastic Gradient Descent(40938): loss=0.9658174634164105\n",
      "Stochastic Gradient Descent(40939): loss=0.14708638255902332\n",
      "Stochastic Gradient Descent(40940): loss=7.803658489223448\n",
      "Stochastic Gradient Descent(40941): loss=0.011133987081877204\n",
      "Stochastic Gradient Descent(40942): loss=12.39804306836987\n",
      "Stochastic Gradient Descent(40943): loss=0.0952219402505454\n",
      "Stochastic Gradient Descent(40944): loss=1.1965819908169921\n",
      "Stochastic Gradient Descent(40945): loss=2.1787122083675916\n",
      "Stochastic Gradient Descent(40946): loss=1.3111373162433488\n",
      "Stochastic Gradient Descent(40947): loss=0.027244129181870847\n",
      "Stochastic Gradient Descent(40948): loss=2.604639715134605e-05\n",
      "Stochastic Gradient Descent(40949): loss=3.1436508006989077\n",
      "Stochastic Gradient Descent(40950): loss=1.468968469046696\n",
      "Stochastic Gradient Descent(40951): loss=8.007216366376012\n",
      "Stochastic Gradient Descent(40952): loss=4.453401096930467\n",
      "Stochastic Gradient Descent(40953): loss=1.7544516962011638\n",
      "Stochastic Gradient Descent(40954): loss=11.353566181221993\n",
      "Stochastic Gradient Descent(40955): loss=2.301716632294608\n",
      "Stochastic Gradient Descent(40956): loss=3.0836043465935314\n",
      "Stochastic Gradient Descent(40957): loss=7.877424229425669\n",
      "Stochastic Gradient Descent(40958): loss=0.7073814547276119\n",
      "Stochastic Gradient Descent(40959): loss=3.0402698229612617\n",
      "Stochastic Gradient Descent(40960): loss=1.0698064762869874\n",
      "Stochastic Gradient Descent(40961): loss=0.7866309394738256\n",
      "Stochastic Gradient Descent(40962): loss=0.01601273169089216\n",
      "Stochastic Gradient Descent(40963): loss=9.421799794332538\n",
      "Stochastic Gradient Descent(40964): loss=0.6599116987470578\n",
      "Stochastic Gradient Descent(40965): loss=3.0391884718669817\n",
      "Stochastic Gradient Descent(40966): loss=0.8560523039713688\n",
      "Stochastic Gradient Descent(40967): loss=5.79668611213152\n",
      "Stochastic Gradient Descent(40968): loss=1.1389197884438476\n",
      "Stochastic Gradient Descent(40969): loss=3.729637886819191\n",
      "Stochastic Gradient Descent(40970): loss=2.0029620517598463\n",
      "Stochastic Gradient Descent(40971): loss=2.011600549926235\n",
      "Stochastic Gradient Descent(40972): loss=0.1964897974813895\n",
      "Stochastic Gradient Descent(40973): loss=1.8927394476530237\n",
      "Stochastic Gradient Descent(40974): loss=2.2162186802826747\n",
      "Stochastic Gradient Descent(40975): loss=1.5296399737394772\n",
      "Stochastic Gradient Descent(40976): loss=0.005958049747839029\n",
      "Stochastic Gradient Descent(40977): loss=1.5738805949941024\n",
      "Stochastic Gradient Descent(40978): loss=10.590339594048544\n",
      "Stochastic Gradient Descent(40979): loss=0.10174276419393906\n",
      "Stochastic Gradient Descent(40980): loss=3.5977452307554523\n",
      "Stochastic Gradient Descent(40981): loss=13.073802973186016\n",
      "Stochastic Gradient Descent(40982): loss=1.5451522241782014\n",
      "Stochastic Gradient Descent(40983): loss=0.37361068221772364\n",
      "Stochastic Gradient Descent(40984): loss=3.570791530588623\n",
      "Stochastic Gradient Descent(40985): loss=0.2754643398073959\n",
      "Stochastic Gradient Descent(40986): loss=0.0020989183875848036\n",
      "Stochastic Gradient Descent(40987): loss=1.0280059165894275\n",
      "Stochastic Gradient Descent(40988): loss=2.0158188273563646\n",
      "Stochastic Gradient Descent(40989): loss=0.9930141228363519\n",
      "Stochastic Gradient Descent(40990): loss=0.3170275544187061\n",
      "Stochastic Gradient Descent(40991): loss=0.1504203009601348\n",
      "Stochastic Gradient Descent(40992): loss=4.27775099661566\n",
      "Stochastic Gradient Descent(40993): loss=2.8361688889969043e-06\n",
      "Stochastic Gradient Descent(40994): loss=3.3196738451960712\n",
      "Stochastic Gradient Descent(40995): loss=15.683777854082486\n",
      "Stochastic Gradient Descent(40996): loss=14.011974871466021\n",
      "Stochastic Gradient Descent(40997): loss=0.03123498628146969\n",
      "Stochastic Gradient Descent(40998): loss=0.09333556247906344\n",
      "Stochastic Gradient Descent(40999): loss=14.292751439463812\n",
      "Stochastic Gradient Descent(41000): loss=0.11830329991797703\n",
      "Stochastic Gradient Descent(41001): loss=2.5134709994161284\n",
      "Stochastic Gradient Descent(41002): loss=0.7272300017541776\n",
      "Stochastic Gradient Descent(41003): loss=0.2831796648882556\n",
      "Stochastic Gradient Descent(41004): loss=4.387971041860684\n",
      "Stochastic Gradient Descent(41005): loss=3.3199973750229654\n",
      "Stochastic Gradient Descent(41006): loss=12.922957966967855\n",
      "Stochastic Gradient Descent(41007): loss=0.851887604192566\n",
      "Stochastic Gradient Descent(41008): loss=0.11261727706511435\n",
      "Stochastic Gradient Descent(41009): loss=14.24295502215595\n",
      "Stochastic Gradient Descent(41010): loss=0.008092778579710656\n",
      "Stochastic Gradient Descent(41011): loss=0.747701790770511\n",
      "Stochastic Gradient Descent(41012): loss=0.5764508984539576\n",
      "Stochastic Gradient Descent(41013): loss=2.586614724289409\n",
      "Stochastic Gradient Descent(41014): loss=0.28324562902520073\n",
      "Stochastic Gradient Descent(41015): loss=0.8671807937303067\n",
      "Stochastic Gradient Descent(41016): loss=2.6637294498635646\n",
      "Stochastic Gradient Descent(41017): loss=14.66526832614192\n",
      "Stochastic Gradient Descent(41018): loss=0.03274052118917436\n",
      "Stochastic Gradient Descent(41019): loss=0.7158383612388073\n",
      "Stochastic Gradient Descent(41020): loss=1.5101218926250037\n",
      "Stochastic Gradient Descent(41021): loss=6.382336900408091\n",
      "Stochastic Gradient Descent(41022): loss=1.012600148753264\n",
      "Stochastic Gradient Descent(41023): loss=9.201662491816712\n",
      "Stochastic Gradient Descent(41024): loss=0.0006539225827377954\n",
      "Stochastic Gradient Descent(41025): loss=0.9186524351535368\n",
      "Stochastic Gradient Descent(41026): loss=0.7855489736410629\n",
      "Stochastic Gradient Descent(41027): loss=0.29479556317413286\n",
      "Stochastic Gradient Descent(41028): loss=0.005443698669627286\n",
      "Stochastic Gradient Descent(41029): loss=4.790170177891477\n",
      "Stochastic Gradient Descent(41030): loss=0.5860906862453561\n",
      "Stochastic Gradient Descent(41031): loss=2.6756202150753894\n",
      "Stochastic Gradient Descent(41032): loss=5.659417920880795\n",
      "Stochastic Gradient Descent(41033): loss=0.5104250070709606\n",
      "Stochastic Gradient Descent(41034): loss=4.988885283903031\n",
      "Stochastic Gradient Descent(41035): loss=22.431671582646594\n",
      "Stochastic Gradient Descent(41036): loss=8.042724202086825\n",
      "Stochastic Gradient Descent(41037): loss=19.71718713635095\n",
      "Stochastic Gradient Descent(41038): loss=0.1089992082737972\n",
      "Stochastic Gradient Descent(41039): loss=27.26863777995403\n",
      "Stochastic Gradient Descent(41040): loss=36.679363413296024\n",
      "Stochastic Gradient Descent(41041): loss=6.459129352887757\n",
      "Stochastic Gradient Descent(41042): loss=2.9722846792740465\n",
      "Stochastic Gradient Descent(41043): loss=14.619924023868938\n",
      "Stochastic Gradient Descent(41044): loss=0.019190902963964616\n",
      "Stochastic Gradient Descent(41045): loss=2.205709745553188\n",
      "Stochastic Gradient Descent(41046): loss=0.8264043408453527\n",
      "Stochastic Gradient Descent(41047): loss=0.6161109739785006\n",
      "Stochastic Gradient Descent(41048): loss=0.20826423134491667\n",
      "Stochastic Gradient Descent(41049): loss=1.4245629892647247\n",
      "Stochastic Gradient Descent(41050): loss=6.526821126806972\n",
      "Stochastic Gradient Descent(41051): loss=0.011824413493351773\n",
      "Stochastic Gradient Descent(41052): loss=5.153011763602722\n",
      "Stochastic Gradient Descent(41053): loss=0.3360875234334783\n",
      "Stochastic Gradient Descent(41054): loss=3.9828540354744595\n",
      "Stochastic Gradient Descent(41055): loss=3.7588751129568085\n",
      "Stochastic Gradient Descent(41056): loss=2.090113767791734\n",
      "Stochastic Gradient Descent(41057): loss=0.3031540931003952\n",
      "Stochastic Gradient Descent(41058): loss=2.0609269938593355\n",
      "Stochastic Gradient Descent(41059): loss=2.612548757393591\n",
      "Stochastic Gradient Descent(41060): loss=1.3159831891408216\n",
      "Stochastic Gradient Descent(41061): loss=2.7554405620590967\n",
      "Stochastic Gradient Descent(41062): loss=2.7114963385261563\n",
      "Stochastic Gradient Descent(41063): loss=0.06349747529487565\n",
      "Stochastic Gradient Descent(41064): loss=4.900873029337123\n",
      "Stochastic Gradient Descent(41065): loss=2.183194007541907\n",
      "Stochastic Gradient Descent(41066): loss=0.02792677199583297\n",
      "Stochastic Gradient Descent(41067): loss=18.898930899447105\n",
      "Stochastic Gradient Descent(41068): loss=6.838167457538777\n",
      "Stochastic Gradient Descent(41069): loss=0.09932439924876886\n",
      "Stochastic Gradient Descent(41070): loss=0.04040013614240581\n",
      "Stochastic Gradient Descent(41071): loss=0.06215449083936491\n",
      "Stochastic Gradient Descent(41072): loss=6.74945960297283\n",
      "Stochastic Gradient Descent(41073): loss=0.24325266319949357\n",
      "Stochastic Gradient Descent(41074): loss=0.9155072977408802\n",
      "Stochastic Gradient Descent(41075): loss=13.355885236178615\n",
      "Stochastic Gradient Descent(41076): loss=0.0011347849985753162\n",
      "Stochastic Gradient Descent(41077): loss=0.005257027997423756\n",
      "Stochastic Gradient Descent(41078): loss=0.44991093771733826\n",
      "Stochastic Gradient Descent(41079): loss=0.7882562227345994\n",
      "Stochastic Gradient Descent(41080): loss=0.3066733698248663\n",
      "Stochastic Gradient Descent(41081): loss=5.25214754998917\n",
      "Stochastic Gradient Descent(41082): loss=0.029809878318613453\n",
      "Stochastic Gradient Descent(41083): loss=48.179257110187635\n",
      "Stochastic Gradient Descent(41084): loss=0.05184341245567373\n",
      "Stochastic Gradient Descent(41085): loss=2.486455506801767\n",
      "Stochastic Gradient Descent(41086): loss=5.177926640978306\n",
      "Stochastic Gradient Descent(41087): loss=6.353134928584606\n",
      "Stochastic Gradient Descent(41088): loss=0.25496680241664316\n",
      "Stochastic Gradient Descent(41089): loss=2.781909343608347\n",
      "Stochastic Gradient Descent(41090): loss=2.103004743926189\n",
      "Stochastic Gradient Descent(41091): loss=34.90382795510852\n",
      "Stochastic Gradient Descent(41092): loss=18.614376146074\n",
      "Stochastic Gradient Descent(41093): loss=0.04304726100201502\n",
      "Stochastic Gradient Descent(41094): loss=0.3605679624937\n",
      "Stochastic Gradient Descent(41095): loss=0.5227827742563117\n",
      "Stochastic Gradient Descent(41096): loss=0.27649815725657323\n",
      "Stochastic Gradient Descent(41097): loss=0.40377584398024324\n",
      "Stochastic Gradient Descent(41098): loss=6.164257962094894\n",
      "Stochastic Gradient Descent(41099): loss=4.661392072899162\n",
      "Stochastic Gradient Descent(41100): loss=3.603298073718162\n",
      "Stochastic Gradient Descent(41101): loss=11.795973613650863\n",
      "Stochastic Gradient Descent(41102): loss=1.2010917235962166\n",
      "Stochastic Gradient Descent(41103): loss=0.8473515819768801\n",
      "Stochastic Gradient Descent(41104): loss=1.1239791787851616\n",
      "Stochastic Gradient Descent(41105): loss=0.5135608393616966\n",
      "Stochastic Gradient Descent(41106): loss=1.4367587451075052\n",
      "Stochastic Gradient Descent(41107): loss=4.3339345725231375\n",
      "Stochastic Gradient Descent(41108): loss=1.2142125035566695\n",
      "Stochastic Gradient Descent(41109): loss=0.25401197698901107\n",
      "Stochastic Gradient Descent(41110): loss=0.05262818925569636\n",
      "Stochastic Gradient Descent(41111): loss=2.7652360129387112\n",
      "Stochastic Gradient Descent(41112): loss=0.004446330284688472\n",
      "Stochastic Gradient Descent(41113): loss=0.7381039927283602\n",
      "Stochastic Gradient Descent(41114): loss=0.520129165477328\n",
      "Stochastic Gradient Descent(41115): loss=2.5637960507423294\n",
      "Stochastic Gradient Descent(41116): loss=1.8750733707567702\n",
      "Stochastic Gradient Descent(41117): loss=14.924767570367173\n",
      "Stochastic Gradient Descent(41118): loss=1.8379247135334151\n",
      "Stochastic Gradient Descent(41119): loss=4.981707018611621\n",
      "Stochastic Gradient Descent(41120): loss=2.037303271298554\n",
      "Stochastic Gradient Descent(41121): loss=3.510026536829024\n",
      "Stochastic Gradient Descent(41122): loss=0.0002735889992087327\n",
      "Stochastic Gradient Descent(41123): loss=2.157619550067283\n",
      "Stochastic Gradient Descent(41124): loss=0.22667742326035545\n",
      "Stochastic Gradient Descent(41125): loss=1.23536012042795\n",
      "Stochastic Gradient Descent(41126): loss=0.6723154666924938\n",
      "Stochastic Gradient Descent(41127): loss=1.027673483448194\n",
      "Stochastic Gradient Descent(41128): loss=8.560520918467544\n",
      "Stochastic Gradient Descent(41129): loss=0.24519967890111388\n",
      "Stochastic Gradient Descent(41130): loss=5.138226367436417\n",
      "Stochastic Gradient Descent(41131): loss=6.203177579233615\n",
      "Stochastic Gradient Descent(41132): loss=0.9892792654397333\n",
      "Stochastic Gradient Descent(41133): loss=8.776411332074142\n",
      "Stochastic Gradient Descent(41134): loss=0.7346414260379026\n",
      "Stochastic Gradient Descent(41135): loss=0.008440995889229275\n",
      "Stochastic Gradient Descent(41136): loss=0.5939108744039454\n",
      "Stochastic Gradient Descent(41137): loss=0.4232970708179742\n",
      "Stochastic Gradient Descent(41138): loss=3.910091787020964\n",
      "Stochastic Gradient Descent(41139): loss=11.509239196709077\n",
      "Stochastic Gradient Descent(41140): loss=10.831952130748528\n",
      "Stochastic Gradient Descent(41141): loss=6.480776804184033\n",
      "Stochastic Gradient Descent(41142): loss=3.629169561908126\n",
      "Stochastic Gradient Descent(41143): loss=22.183663373454127\n",
      "Stochastic Gradient Descent(41144): loss=0.2521106731827974\n",
      "Stochastic Gradient Descent(41145): loss=0.2175975098603742\n",
      "Stochastic Gradient Descent(41146): loss=9.97292721366516\n",
      "Stochastic Gradient Descent(41147): loss=0.07202139628287689\n",
      "Stochastic Gradient Descent(41148): loss=0.09909537027369424\n",
      "Stochastic Gradient Descent(41149): loss=0.8435960963476505\n",
      "Stochastic Gradient Descent(41150): loss=19.04821048488877\n",
      "Stochastic Gradient Descent(41151): loss=0.13598805501192812\n",
      "Stochastic Gradient Descent(41152): loss=0.7693904014667762\n",
      "Stochastic Gradient Descent(41153): loss=1.0743977804595632\n",
      "Stochastic Gradient Descent(41154): loss=0.010721136214725034\n",
      "Stochastic Gradient Descent(41155): loss=0.05134032964206751\n",
      "Stochastic Gradient Descent(41156): loss=0.0010264297000274875\n",
      "Stochastic Gradient Descent(41157): loss=1.463440142413057\n",
      "Stochastic Gradient Descent(41158): loss=0.15370332825547528\n",
      "Stochastic Gradient Descent(41159): loss=0.1147324912542803\n",
      "Stochastic Gradient Descent(41160): loss=0.5248894431907815\n",
      "Stochastic Gradient Descent(41161): loss=0.8734650186256357\n",
      "Stochastic Gradient Descent(41162): loss=3.44221156479414\n",
      "Stochastic Gradient Descent(41163): loss=2.565574061360208\n",
      "Stochastic Gradient Descent(41164): loss=15.301436107312039\n",
      "Stochastic Gradient Descent(41165): loss=14.778506175370314\n",
      "Stochastic Gradient Descent(41166): loss=0.4787594991973506\n",
      "Stochastic Gradient Descent(41167): loss=0.09199682451310194\n",
      "Stochastic Gradient Descent(41168): loss=15.7837256552292\n",
      "Stochastic Gradient Descent(41169): loss=10.517088901748105\n",
      "Stochastic Gradient Descent(41170): loss=1.3371392183309312\n",
      "Stochastic Gradient Descent(41171): loss=0.0808449555755277\n",
      "Stochastic Gradient Descent(41172): loss=21.88222036596418\n",
      "Stochastic Gradient Descent(41173): loss=0.16745535103438955\n",
      "Stochastic Gradient Descent(41174): loss=3.0706774730307287\n",
      "Stochastic Gradient Descent(41175): loss=0.0010752149776185038\n",
      "Stochastic Gradient Descent(41176): loss=5.27157546011297\n",
      "Stochastic Gradient Descent(41177): loss=0.6772042738554744\n",
      "Stochastic Gradient Descent(41178): loss=0.7227411506795391\n",
      "Stochastic Gradient Descent(41179): loss=8.607050523113191\n",
      "Stochastic Gradient Descent(41180): loss=2.016340922701209\n",
      "Stochastic Gradient Descent(41181): loss=17.021568771363377\n",
      "Stochastic Gradient Descent(41182): loss=0.05757280742219806\n",
      "Stochastic Gradient Descent(41183): loss=0.7889024493336053\n",
      "Stochastic Gradient Descent(41184): loss=9.263055731047526\n",
      "Stochastic Gradient Descent(41185): loss=0.16362948842419783\n",
      "Stochastic Gradient Descent(41186): loss=0.2911158862106899\n",
      "Stochastic Gradient Descent(41187): loss=0.24275752760124492\n",
      "Stochastic Gradient Descent(41188): loss=12.609464388869592\n",
      "Stochastic Gradient Descent(41189): loss=0.013185266092554906\n",
      "Stochastic Gradient Descent(41190): loss=3.303915079039395\n",
      "Stochastic Gradient Descent(41191): loss=0.05515796009898361\n",
      "Stochastic Gradient Descent(41192): loss=0.23527423715390358\n",
      "Stochastic Gradient Descent(41193): loss=4.875520804979721\n",
      "Stochastic Gradient Descent(41194): loss=0.021077767262962833\n",
      "Stochastic Gradient Descent(41195): loss=0.0004493403511617333\n",
      "Stochastic Gradient Descent(41196): loss=3.7018619505474115\n",
      "Stochastic Gradient Descent(41197): loss=0.427396786752291\n",
      "Stochastic Gradient Descent(41198): loss=0.011353532272731467\n",
      "Stochastic Gradient Descent(41199): loss=6.97916308239254\n",
      "Stochastic Gradient Descent(41200): loss=15.89242117401998\n",
      "Stochastic Gradient Descent(41201): loss=0.03730742135617783\n",
      "Stochastic Gradient Descent(41202): loss=8.493847565843957\n",
      "Stochastic Gradient Descent(41203): loss=11.860250816447753\n",
      "Stochastic Gradient Descent(41204): loss=3.7918819586587174\n",
      "Stochastic Gradient Descent(41205): loss=9.215086894136359\n",
      "Stochastic Gradient Descent(41206): loss=2.249610983886573\n",
      "Stochastic Gradient Descent(41207): loss=2.6245928064575743\n",
      "Stochastic Gradient Descent(41208): loss=2.7018584877711595e-05\n",
      "Stochastic Gradient Descent(41209): loss=0.044300170559743605\n",
      "Stochastic Gradient Descent(41210): loss=13.676715994070545\n",
      "Stochastic Gradient Descent(41211): loss=8.435608553361575\n",
      "Stochastic Gradient Descent(41212): loss=5.749490761883188\n",
      "Stochastic Gradient Descent(41213): loss=7.72236144263831\n",
      "Stochastic Gradient Descent(41214): loss=0.010232794434712556\n",
      "Stochastic Gradient Descent(41215): loss=15.881204795665315\n",
      "Stochastic Gradient Descent(41216): loss=2.1152782990311323\n",
      "Stochastic Gradient Descent(41217): loss=11.039842636127572\n",
      "Stochastic Gradient Descent(41218): loss=2.9458944281470676\n",
      "Stochastic Gradient Descent(41219): loss=3.0272663786308107\n",
      "Stochastic Gradient Descent(41220): loss=1.0561283078440264\n",
      "Stochastic Gradient Descent(41221): loss=0.3422644853303191\n",
      "Stochastic Gradient Descent(41222): loss=0.23807020479943844\n",
      "Stochastic Gradient Descent(41223): loss=6.618003295246108\n",
      "Stochastic Gradient Descent(41224): loss=0.0014072435715000457\n",
      "Stochastic Gradient Descent(41225): loss=8.152031587983595\n",
      "Stochastic Gradient Descent(41226): loss=0.23897257467081598\n",
      "Stochastic Gradient Descent(41227): loss=2.060459120261571\n",
      "Stochastic Gradient Descent(41228): loss=37.12512303038318\n",
      "Stochastic Gradient Descent(41229): loss=7.812141226381193\n",
      "Stochastic Gradient Descent(41230): loss=0.9701997379722638\n",
      "Stochastic Gradient Descent(41231): loss=2.6906464412575573\n",
      "Stochastic Gradient Descent(41232): loss=0.3370048975489182\n",
      "Stochastic Gradient Descent(41233): loss=0.8566551918511233\n",
      "Stochastic Gradient Descent(41234): loss=6.452474867209811\n",
      "Stochastic Gradient Descent(41235): loss=1.5705721105176496\n",
      "Stochastic Gradient Descent(41236): loss=0.7176278645714248\n",
      "Stochastic Gradient Descent(41237): loss=10.476226706753826\n",
      "Stochastic Gradient Descent(41238): loss=4.087474577740072\n",
      "Stochastic Gradient Descent(41239): loss=1.1722471573882876\n",
      "Stochastic Gradient Descent(41240): loss=0.9824127882061486\n",
      "Stochastic Gradient Descent(41241): loss=4.787553902869275\n",
      "Stochastic Gradient Descent(41242): loss=0.6967400963057951\n",
      "Stochastic Gradient Descent(41243): loss=0.020411066612987958\n",
      "Stochastic Gradient Descent(41244): loss=0.0006193594976771624\n",
      "Stochastic Gradient Descent(41245): loss=1.9989256843229557\n",
      "Stochastic Gradient Descent(41246): loss=1.1559326844423345\n",
      "Stochastic Gradient Descent(41247): loss=9.845737083967222\n",
      "Stochastic Gradient Descent(41248): loss=0.48862180023619883\n",
      "Stochastic Gradient Descent(41249): loss=0.06620923801066245\n",
      "Stochastic Gradient Descent(41250): loss=23.118443201113582\n",
      "Stochastic Gradient Descent(41251): loss=0.8649298139723357\n",
      "Stochastic Gradient Descent(41252): loss=0.1831688357581797\n",
      "Stochastic Gradient Descent(41253): loss=0.2235280617365351\n",
      "Stochastic Gradient Descent(41254): loss=27.279589250085845\n",
      "Stochastic Gradient Descent(41255): loss=1.1666548233492693\n",
      "Stochastic Gradient Descent(41256): loss=18.126094332228195\n",
      "Stochastic Gradient Descent(41257): loss=1.0895606781469809\n",
      "Stochastic Gradient Descent(41258): loss=2.615311345116719\n",
      "Stochastic Gradient Descent(41259): loss=1.5577757932303746\n",
      "Stochastic Gradient Descent(41260): loss=4.269479436049303\n",
      "Stochastic Gradient Descent(41261): loss=0.41224965900719845\n",
      "Stochastic Gradient Descent(41262): loss=5.870539998385621\n",
      "Stochastic Gradient Descent(41263): loss=5.839472028196576\n",
      "Stochastic Gradient Descent(41264): loss=1.4484288996596608\n",
      "Stochastic Gradient Descent(41265): loss=4.302976882300208\n",
      "Stochastic Gradient Descent(41266): loss=11.184789210264759\n",
      "Stochastic Gradient Descent(41267): loss=25.611916580026012\n",
      "Stochastic Gradient Descent(41268): loss=1.7128656398570665\n",
      "Stochastic Gradient Descent(41269): loss=0.18725563269024148\n",
      "Stochastic Gradient Descent(41270): loss=3.6068937604362024\n",
      "Stochastic Gradient Descent(41271): loss=0.15914712157334163\n",
      "Stochastic Gradient Descent(41272): loss=0.07717609570908708\n",
      "Stochastic Gradient Descent(41273): loss=0.0011026322820811083\n",
      "Stochastic Gradient Descent(41274): loss=0.4572045046482922\n",
      "Stochastic Gradient Descent(41275): loss=1.4177815828573457\n",
      "Stochastic Gradient Descent(41276): loss=0.5784144804864652\n",
      "Stochastic Gradient Descent(41277): loss=14.228169127344415\n",
      "Stochastic Gradient Descent(41278): loss=0.8350743003481891\n",
      "Stochastic Gradient Descent(41279): loss=5.130033221325584\n",
      "Stochastic Gradient Descent(41280): loss=1.7475261181018227\n",
      "Stochastic Gradient Descent(41281): loss=0.33815888731405225\n",
      "Stochastic Gradient Descent(41282): loss=0.5173740812067885\n",
      "Stochastic Gradient Descent(41283): loss=0.5330506237499664\n",
      "Stochastic Gradient Descent(41284): loss=1.4009624198912958\n",
      "Stochastic Gradient Descent(41285): loss=1.0242752662271035\n",
      "Stochastic Gradient Descent(41286): loss=15.788991889629617\n",
      "Stochastic Gradient Descent(41287): loss=27.378791271797244\n",
      "Stochastic Gradient Descent(41288): loss=0.6108436521670135\n",
      "Stochastic Gradient Descent(41289): loss=25.462662275805904\n",
      "Stochastic Gradient Descent(41290): loss=0.3521199075264495\n",
      "Stochastic Gradient Descent(41291): loss=6.475595543722278\n",
      "Stochastic Gradient Descent(41292): loss=0.533249727992843\n",
      "Stochastic Gradient Descent(41293): loss=4.084081479421776\n",
      "Stochastic Gradient Descent(41294): loss=0.004280222119513211\n",
      "Stochastic Gradient Descent(41295): loss=0.40994959707371253\n",
      "Stochastic Gradient Descent(41296): loss=14.185552629388784\n",
      "Stochastic Gradient Descent(41297): loss=6.689897228013342\n",
      "Stochastic Gradient Descent(41298): loss=4.847820553086199\n",
      "Stochastic Gradient Descent(41299): loss=12.939138087308827\n",
      "Stochastic Gradient Descent(41300): loss=0.13777204692713946\n",
      "Stochastic Gradient Descent(41301): loss=3.0591538296735252\n",
      "Stochastic Gradient Descent(41302): loss=0.425659704446426\n",
      "Stochastic Gradient Descent(41303): loss=1.2638398129380386\n",
      "Stochastic Gradient Descent(41304): loss=0.6142238070159539\n",
      "Stochastic Gradient Descent(41305): loss=8.16625688251556\n",
      "Stochastic Gradient Descent(41306): loss=13.269139069649928\n",
      "Stochastic Gradient Descent(41307): loss=1.2043713345752751\n",
      "Stochastic Gradient Descent(41308): loss=5.220735186396791\n",
      "Stochastic Gradient Descent(41309): loss=1.7704124925609963\n",
      "Stochastic Gradient Descent(41310): loss=1.341811446840582\n",
      "Stochastic Gradient Descent(41311): loss=1.4065667583633346\n",
      "Stochastic Gradient Descent(41312): loss=5.3544590800402965\n",
      "Stochastic Gradient Descent(41313): loss=1.1979184592674061\n",
      "Stochastic Gradient Descent(41314): loss=2.065584560597836\n",
      "Stochastic Gradient Descent(41315): loss=0.21870887215674686\n",
      "Stochastic Gradient Descent(41316): loss=0.06553369669260832\n",
      "Stochastic Gradient Descent(41317): loss=5.719028751027261\n",
      "Stochastic Gradient Descent(41318): loss=0.011434916756712812\n",
      "Stochastic Gradient Descent(41319): loss=14.772407772440399\n",
      "Stochastic Gradient Descent(41320): loss=1.8238979164751634\n",
      "Stochastic Gradient Descent(41321): loss=2.7893162069146387\n",
      "Stochastic Gradient Descent(41322): loss=0.013479012236495039\n",
      "Stochastic Gradient Descent(41323): loss=0.08396664403342054\n",
      "Stochastic Gradient Descent(41324): loss=3.4450296822120845\n",
      "Stochastic Gradient Descent(41325): loss=0.9012302554907219\n",
      "Stochastic Gradient Descent(41326): loss=2.2849006574217827\n",
      "Stochastic Gradient Descent(41327): loss=0.33466693360432936\n",
      "Stochastic Gradient Descent(41328): loss=0.02530999010189878\n",
      "Stochastic Gradient Descent(41329): loss=7.76104793566212\n",
      "Stochastic Gradient Descent(41330): loss=0.9487825232875968\n",
      "Stochastic Gradient Descent(41331): loss=2.3530390889326833\n",
      "Stochastic Gradient Descent(41332): loss=4.667888936179511\n",
      "Stochastic Gradient Descent(41333): loss=0.018061750081275696\n",
      "Stochastic Gradient Descent(41334): loss=6.256123079216077\n",
      "Stochastic Gradient Descent(41335): loss=3.7030898840462996\n",
      "Stochastic Gradient Descent(41336): loss=7.1210232741154815\n",
      "Stochastic Gradient Descent(41337): loss=1.1196761349476552\n",
      "Stochastic Gradient Descent(41338): loss=8.644701182312232\n",
      "Stochastic Gradient Descent(41339): loss=0.8797310165474114\n",
      "Stochastic Gradient Descent(41340): loss=6.322223475178869\n",
      "Stochastic Gradient Descent(41341): loss=0.08019356505456289\n",
      "Stochastic Gradient Descent(41342): loss=3.9695720892376536\n",
      "Stochastic Gradient Descent(41343): loss=8.441899576006112\n",
      "Stochastic Gradient Descent(41344): loss=0.2170600936088438\n",
      "Stochastic Gradient Descent(41345): loss=2.6634818577043355\n",
      "Stochastic Gradient Descent(41346): loss=0.13233235296941137\n",
      "Stochastic Gradient Descent(41347): loss=0.9613465711759622\n",
      "Stochastic Gradient Descent(41348): loss=8.446009726072557\n",
      "Stochastic Gradient Descent(41349): loss=6.628461539404503\n",
      "Stochastic Gradient Descent(41350): loss=0.13696832451411842\n",
      "Stochastic Gradient Descent(41351): loss=1.9639017893148936\n",
      "Stochastic Gradient Descent(41352): loss=13.674276244922336\n",
      "Stochastic Gradient Descent(41353): loss=2.8781850946211507\n",
      "Stochastic Gradient Descent(41354): loss=9.241879163221558\n",
      "Stochastic Gradient Descent(41355): loss=0.4550467333643105\n",
      "Stochastic Gradient Descent(41356): loss=2.134995001025328\n",
      "Stochastic Gradient Descent(41357): loss=2.4724445367274717\n",
      "Stochastic Gradient Descent(41358): loss=7.935439892046689\n",
      "Stochastic Gradient Descent(41359): loss=3.2736383884404185\n",
      "Stochastic Gradient Descent(41360): loss=2.2178927550141316\n",
      "Stochastic Gradient Descent(41361): loss=3.5776720088993943\n",
      "Stochastic Gradient Descent(41362): loss=2.3848800456740187\n",
      "Stochastic Gradient Descent(41363): loss=3.260597705339057\n",
      "Stochastic Gradient Descent(41364): loss=6.062098956674784\n",
      "Stochastic Gradient Descent(41365): loss=16.782767988845496\n",
      "Stochastic Gradient Descent(41366): loss=11.40113694141332\n",
      "Stochastic Gradient Descent(41367): loss=3.3292241588695006\n",
      "Stochastic Gradient Descent(41368): loss=7.600381000197736\n",
      "Stochastic Gradient Descent(41369): loss=13.331624232144163\n",
      "Stochastic Gradient Descent(41370): loss=4.334079967961842\n",
      "Stochastic Gradient Descent(41371): loss=5.062210104234985\n",
      "Stochastic Gradient Descent(41372): loss=0.05204093394357532\n",
      "Stochastic Gradient Descent(41373): loss=0.30269024830030056\n",
      "Stochastic Gradient Descent(41374): loss=2.2585726255523286\n",
      "Stochastic Gradient Descent(41375): loss=1.403386230771904\n",
      "Stochastic Gradient Descent(41376): loss=5.590972529208331\n",
      "Stochastic Gradient Descent(41377): loss=1.324216408896077\n",
      "Stochastic Gradient Descent(41378): loss=0.48773968316085775\n",
      "Stochastic Gradient Descent(41379): loss=15.077409878226451\n",
      "Stochastic Gradient Descent(41380): loss=39.60926984079237\n",
      "Stochastic Gradient Descent(41381): loss=63.461307479255595\n",
      "Stochastic Gradient Descent(41382): loss=0.1117230033524445\n",
      "Stochastic Gradient Descent(41383): loss=1.0603661659992005\n",
      "Stochastic Gradient Descent(41384): loss=2.1673700084654905\n",
      "Stochastic Gradient Descent(41385): loss=4.067158622684878\n",
      "Stochastic Gradient Descent(41386): loss=2.3744618966103177\n",
      "Stochastic Gradient Descent(41387): loss=1.935568414596204\n",
      "Stochastic Gradient Descent(41388): loss=3.891438919331471\n",
      "Stochastic Gradient Descent(41389): loss=24.51014739381368\n",
      "Stochastic Gradient Descent(41390): loss=0.02771501479147942\n",
      "Stochastic Gradient Descent(41391): loss=0.05679471629004281\n",
      "Stochastic Gradient Descent(41392): loss=0.11007879489798264\n",
      "Stochastic Gradient Descent(41393): loss=0.21299245986387436\n",
      "Stochastic Gradient Descent(41394): loss=0.03258353088923847\n",
      "Stochastic Gradient Descent(41395): loss=0.00615260918220586\n",
      "Stochastic Gradient Descent(41396): loss=17.541266757594556\n",
      "Stochastic Gradient Descent(41397): loss=1.2341378075832\n",
      "Stochastic Gradient Descent(41398): loss=11.94608749592136\n",
      "Stochastic Gradient Descent(41399): loss=0.10741613682322548\n",
      "Stochastic Gradient Descent(41400): loss=2.207804039277074\n",
      "Stochastic Gradient Descent(41401): loss=0.6030436293674039\n",
      "Stochastic Gradient Descent(41402): loss=0.31573889722403625\n",
      "Stochastic Gradient Descent(41403): loss=8.087949015382716\n",
      "Stochastic Gradient Descent(41404): loss=3.566052539834677\n",
      "Stochastic Gradient Descent(41405): loss=0.052575251316697026\n",
      "Stochastic Gradient Descent(41406): loss=0.6058416835473573\n",
      "Stochastic Gradient Descent(41407): loss=7.443098047896687\n",
      "Stochastic Gradient Descent(41408): loss=3.6187209288905575\n",
      "Stochastic Gradient Descent(41409): loss=1.224926966942511\n",
      "Stochastic Gradient Descent(41410): loss=10.832068388271576\n",
      "Stochastic Gradient Descent(41411): loss=8.626089481640939\n",
      "Stochastic Gradient Descent(41412): loss=0.09557989926034136\n",
      "Stochastic Gradient Descent(41413): loss=1.2792131514267133\n",
      "Stochastic Gradient Descent(41414): loss=3.0376995382964056\n",
      "Stochastic Gradient Descent(41415): loss=0.5885532147763402\n",
      "Stochastic Gradient Descent(41416): loss=0.3442301072523342\n",
      "Stochastic Gradient Descent(41417): loss=0.010266993977625764\n",
      "Stochastic Gradient Descent(41418): loss=0.9613179292154505\n",
      "Stochastic Gradient Descent(41419): loss=0.8771764391049622\n",
      "Stochastic Gradient Descent(41420): loss=4.192662132529449\n",
      "Stochastic Gradient Descent(41421): loss=4.621859067085214\n",
      "Stochastic Gradient Descent(41422): loss=16.717520415617702\n",
      "Stochastic Gradient Descent(41423): loss=0.09307536876294488\n",
      "Stochastic Gradient Descent(41424): loss=1.0616397648557734\n",
      "Stochastic Gradient Descent(41425): loss=1.0478611528527997\n",
      "Stochastic Gradient Descent(41426): loss=0.09497376465043586\n",
      "Stochastic Gradient Descent(41427): loss=0.03885761562401641\n",
      "Stochastic Gradient Descent(41428): loss=10.910496320102537\n",
      "Stochastic Gradient Descent(41429): loss=12.671411555218162\n",
      "Stochastic Gradient Descent(41430): loss=2.5050332925229277\n",
      "Stochastic Gradient Descent(41431): loss=1.3887995092081231\n",
      "Stochastic Gradient Descent(41432): loss=12.17860622973126\n",
      "Stochastic Gradient Descent(41433): loss=2.7466113563249395\n",
      "Stochastic Gradient Descent(41434): loss=1.0086623621542143\n",
      "Stochastic Gradient Descent(41435): loss=1.367322456571119\n",
      "Stochastic Gradient Descent(41436): loss=3.611341033005603\n",
      "Stochastic Gradient Descent(41437): loss=10.904791916585385\n",
      "Stochastic Gradient Descent(41438): loss=5.296208491163366\n",
      "Stochastic Gradient Descent(41439): loss=0.0633878085317562\n",
      "Stochastic Gradient Descent(41440): loss=5.277108200831381\n",
      "Stochastic Gradient Descent(41441): loss=0.6501123644077804\n",
      "Stochastic Gradient Descent(41442): loss=0.7875751328953985\n",
      "Stochastic Gradient Descent(41443): loss=15.614962271845247\n",
      "Stochastic Gradient Descent(41444): loss=6.483744626136246e-07\n",
      "Stochastic Gradient Descent(41445): loss=0.15990065655657862\n",
      "Stochastic Gradient Descent(41446): loss=1.8687401161824797\n",
      "Stochastic Gradient Descent(41447): loss=7.441738463563834\n",
      "Stochastic Gradient Descent(41448): loss=14.598392414542232\n",
      "Stochastic Gradient Descent(41449): loss=10.5369577988593\n",
      "Stochastic Gradient Descent(41450): loss=3.5646120014747327\n",
      "Stochastic Gradient Descent(41451): loss=0.002158889232991012\n",
      "Stochastic Gradient Descent(41452): loss=0.3914771151422432\n",
      "Stochastic Gradient Descent(41453): loss=4.717249864647591\n",
      "Stochastic Gradient Descent(41454): loss=0.28617507659539365\n",
      "Stochastic Gradient Descent(41455): loss=1.0482302713958993\n",
      "Stochastic Gradient Descent(41456): loss=1.2279299039975753\n",
      "Stochastic Gradient Descent(41457): loss=0.05152934544649761\n",
      "Stochastic Gradient Descent(41458): loss=0.25411345767702626\n",
      "Stochastic Gradient Descent(41459): loss=10.324197972238396\n",
      "Stochastic Gradient Descent(41460): loss=1.7318961807890774\n",
      "Stochastic Gradient Descent(41461): loss=1.3189692710281335\n",
      "Stochastic Gradient Descent(41462): loss=2.2648120471498148\n",
      "Stochastic Gradient Descent(41463): loss=3.998716451304286\n",
      "Stochastic Gradient Descent(41464): loss=3.678854107879606\n",
      "Stochastic Gradient Descent(41465): loss=0.005882103764969405\n",
      "Stochastic Gradient Descent(41466): loss=0.33863292261858446\n",
      "Stochastic Gradient Descent(41467): loss=0.6364408196891451\n",
      "Stochastic Gradient Descent(41468): loss=6.978573291915223\n",
      "Stochastic Gradient Descent(41469): loss=1.3486336894746427\n",
      "Stochastic Gradient Descent(41470): loss=6.043608648695485\n",
      "Stochastic Gradient Descent(41471): loss=0.6177104927165311\n",
      "Stochastic Gradient Descent(41472): loss=8.11865341411489\n",
      "Stochastic Gradient Descent(41473): loss=0.7134700806545481\n",
      "Stochastic Gradient Descent(41474): loss=0.009279121994179648\n",
      "Stochastic Gradient Descent(41475): loss=2.835253509718356\n",
      "Stochastic Gradient Descent(41476): loss=0.8393301298865236\n",
      "Stochastic Gradient Descent(41477): loss=6.035500212113216\n",
      "Stochastic Gradient Descent(41478): loss=8.290637679932667\n",
      "Stochastic Gradient Descent(41479): loss=0.15576889544637385\n",
      "Stochastic Gradient Descent(41480): loss=0.06130055960721475\n",
      "Stochastic Gradient Descent(41481): loss=6.410456813740453\n",
      "Stochastic Gradient Descent(41482): loss=0.9420388396670116\n",
      "Stochastic Gradient Descent(41483): loss=2.318589486049513\n",
      "Stochastic Gradient Descent(41484): loss=15.480143166945416\n",
      "Stochastic Gradient Descent(41485): loss=8.632613691348777\n",
      "Stochastic Gradient Descent(41486): loss=1.241928133681768\n",
      "Stochastic Gradient Descent(41487): loss=8.104674669385181\n",
      "Stochastic Gradient Descent(41488): loss=0.04358259397022282\n",
      "Stochastic Gradient Descent(41489): loss=2.5760594400704844\n",
      "Stochastic Gradient Descent(41490): loss=3.1567697106147223\n",
      "Stochastic Gradient Descent(41491): loss=0.8511617267650972\n",
      "Stochastic Gradient Descent(41492): loss=2.371924173836503\n",
      "Stochastic Gradient Descent(41493): loss=1.0123695304971014\n",
      "Stochastic Gradient Descent(41494): loss=2.734210514485734\n",
      "Stochastic Gradient Descent(41495): loss=1.1911143506343058\n",
      "Stochastic Gradient Descent(41496): loss=0.11759436184224015\n",
      "Stochastic Gradient Descent(41497): loss=5.08948825547015\n",
      "Stochastic Gradient Descent(41498): loss=0.31776615834974625\n",
      "Stochastic Gradient Descent(41499): loss=0.7093059989547159\n",
      "Stochastic Gradient Descent(41500): loss=5.301652703870203\n",
      "Stochastic Gradient Descent(41501): loss=2.9688521008330704\n",
      "Stochastic Gradient Descent(41502): loss=5.7306795368041\n",
      "Stochastic Gradient Descent(41503): loss=1.9174943145422374\n",
      "Stochastic Gradient Descent(41504): loss=0.04439493697362357\n",
      "Stochastic Gradient Descent(41505): loss=0.45285332857740584\n",
      "Stochastic Gradient Descent(41506): loss=0.6313214663874275\n",
      "Stochastic Gradient Descent(41507): loss=15.022810522133756\n",
      "Stochastic Gradient Descent(41508): loss=7.6210457896615065\n",
      "Stochastic Gradient Descent(41509): loss=0.9986971979911814\n",
      "Stochastic Gradient Descent(41510): loss=0.37589000641615905\n",
      "Stochastic Gradient Descent(41511): loss=0.010106133912490428\n",
      "Stochastic Gradient Descent(41512): loss=3.9005492311170546\n",
      "Stochastic Gradient Descent(41513): loss=9.252849950062682\n",
      "Stochastic Gradient Descent(41514): loss=0.175677610171182\n",
      "Stochastic Gradient Descent(41515): loss=1.2646322281983757\n",
      "Stochastic Gradient Descent(41516): loss=0.5544693129390831\n",
      "Stochastic Gradient Descent(41517): loss=6.032827574626768\n",
      "Stochastic Gradient Descent(41518): loss=1.7884830277788355\n",
      "Stochastic Gradient Descent(41519): loss=2.4361852102668275\n",
      "Stochastic Gradient Descent(41520): loss=1.3432656393883173\n",
      "Stochastic Gradient Descent(41521): loss=0.7501913029449512\n",
      "Stochastic Gradient Descent(41522): loss=0.08698522176288827\n",
      "Stochastic Gradient Descent(41523): loss=4.034075965359353\n",
      "Stochastic Gradient Descent(41524): loss=4.767302483781565\n",
      "Stochastic Gradient Descent(41525): loss=2.741228203763405\n",
      "Stochastic Gradient Descent(41526): loss=3.050779944420062\n",
      "Stochastic Gradient Descent(41527): loss=0.01533066212932095\n",
      "Stochastic Gradient Descent(41528): loss=8.971861220654038\n",
      "Stochastic Gradient Descent(41529): loss=4.255704547769997\n",
      "Stochastic Gradient Descent(41530): loss=7.662073096148948\n",
      "Stochastic Gradient Descent(41531): loss=1.899205776281111\n",
      "Stochastic Gradient Descent(41532): loss=1.2791645656900563\n",
      "Stochastic Gradient Descent(41533): loss=1.460775991985894\n",
      "Stochastic Gradient Descent(41534): loss=4.2621185777287325\n",
      "Stochastic Gradient Descent(41535): loss=0.952036573084899\n",
      "Stochastic Gradient Descent(41536): loss=6.683314902773243\n",
      "Stochastic Gradient Descent(41537): loss=0.022624063013476484\n",
      "Stochastic Gradient Descent(41538): loss=3.976015175789661\n",
      "Stochastic Gradient Descent(41539): loss=17.89444660436086\n",
      "Stochastic Gradient Descent(41540): loss=4.604359397485968\n",
      "Stochastic Gradient Descent(41541): loss=1.2065816204512458\n",
      "Stochastic Gradient Descent(41542): loss=0.004759175456614578\n",
      "Stochastic Gradient Descent(41543): loss=3.377645717297581\n",
      "Stochastic Gradient Descent(41544): loss=1.1833557467723717\n",
      "Stochastic Gradient Descent(41545): loss=0.756636113055959\n",
      "Stochastic Gradient Descent(41546): loss=3.2312456257996005\n",
      "Stochastic Gradient Descent(41547): loss=0.6711406300532927\n",
      "Stochastic Gradient Descent(41548): loss=0.06631402297018664\n",
      "Stochastic Gradient Descent(41549): loss=1.2578955642680132\n",
      "Stochastic Gradient Descent(41550): loss=0.30116102508694476\n",
      "Stochastic Gradient Descent(41551): loss=4.0171922704729015\n",
      "Stochastic Gradient Descent(41552): loss=2.398992591623725\n",
      "Stochastic Gradient Descent(41553): loss=0.5388092038020728\n",
      "Stochastic Gradient Descent(41554): loss=0.6127874244561974\n",
      "Stochastic Gradient Descent(41555): loss=2.295423093852402\n",
      "Stochastic Gradient Descent(41556): loss=0.03306064279338743\n",
      "Stochastic Gradient Descent(41557): loss=6.2307470058819705\n",
      "Stochastic Gradient Descent(41558): loss=0.684478926779234\n",
      "Stochastic Gradient Descent(41559): loss=2.4909211231735533\n",
      "Stochastic Gradient Descent(41560): loss=0.7851166811228936\n",
      "Stochastic Gradient Descent(41561): loss=2.9718196922713327\n",
      "Stochastic Gradient Descent(41562): loss=1.565429591912668\n",
      "Stochastic Gradient Descent(41563): loss=10.794388613714924\n",
      "Stochastic Gradient Descent(41564): loss=1.6217495884133686\n",
      "Stochastic Gradient Descent(41565): loss=2.2535198140860726\n",
      "Stochastic Gradient Descent(41566): loss=3.0343462101333407\n",
      "Stochastic Gradient Descent(41567): loss=0.34992169004811163\n",
      "Stochastic Gradient Descent(41568): loss=0.5964135214972522\n",
      "Stochastic Gradient Descent(41569): loss=2.1390323382639664\n",
      "Stochastic Gradient Descent(41570): loss=0.3867134108278441\n",
      "Stochastic Gradient Descent(41571): loss=0.34352260473706914\n",
      "Stochastic Gradient Descent(41572): loss=2.55682171357174\n",
      "Stochastic Gradient Descent(41573): loss=0.1922940540308568\n",
      "Stochastic Gradient Descent(41574): loss=0.21888239601928383\n",
      "Stochastic Gradient Descent(41575): loss=6.2589892823573985\n",
      "Stochastic Gradient Descent(41576): loss=11.0910010210538\n",
      "Stochastic Gradient Descent(41577): loss=4.725909018530967\n",
      "Stochastic Gradient Descent(41578): loss=1.8119695675971526\n",
      "Stochastic Gradient Descent(41579): loss=15.089719986718631\n",
      "Stochastic Gradient Descent(41580): loss=0.3175703276511238\n",
      "Stochastic Gradient Descent(41581): loss=0.03498378823077894\n",
      "Stochastic Gradient Descent(41582): loss=2.024899083852449\n",
      "Stochastic Gradient Descent(41583): loss=14.939371385456969\n",
      "Stochastic Gradient Descent(41584): loss=18.125979902900234\n",
      "Stochastic Gradient Descent(41585): loss=6.805411377370574\n",
      "Stochastic Gradient Descent(41586): loss=1.6949996297149763\n",
      "Stochastic Gradient Descent(41587): loss=0.00010549819196421441\n",
      "Stochastic Gradient Descent(41588): loss=16.224014277541063\n",
      "Stochastic Gradient Descent(41589): loss=0.2979619003825766\n",
      "Stochastic Gradient Descent(41590): loss=5.885784646388465\n",
      "Stochastic Gradient Descent(41591): loss=0.9833890093629027\n",
      "Stochastic Gradient Descent(41592): loss=0.6140117643835178\n",
      "Stochastic Gradient Descent(41593): loss=1.1977969068537182\n",
      "Stochastic Gradient Descent(41594): loss=2.975488788065441\n",
      "Stochastic Gradient Descent(41595): loss=2.207338540378565\n",
      "Stochastic Gradient Descent(41596): loss=0.7783734700990337\n",
      "Stochastic Gradient Descent(41597): loss=2.325074807813886\n",
      "Stochastic Gradient Descent(41598): loss=0.06408420120424518\n",
      "Stochastic Gradient Descent(41599): loss=1.6178008324240136\n",
      "Stochastic Gradient Descent(41600): loss=0.4120080851892766\n",
      "Stochastic Gradient Descent(41601): loss=3.7523523120531515\n",
      "Stochastic Gradient Descent(41602): loss=1.0387020498066655\n",
      "Stochastic Gradient Descent(41603): loss=2.2985798919458187\n",
      "Stochastic Gradient Descent(41604): loss=1.6063114657748574\n",
      "Stochastic Gradient Descent(41605): loss=2.069696984319369\n",
      "Stochastic Gradient Descent(41606): loss=0.0841640845022049\n",
      "Stochastic Gradient Descent(41607): loss=5.420822023917378\n",
      "Stochastic Gradient Descent(41608): loss=4.765729531129522\n",
      "Stochastic Gradient Descent(41609): loss=0.0012535410092720828\n",
      "Stochastic Gradient Descent(41610): loss=18.07833590242971\n",
      "Stochastic Gradient Descent(41611): loss=4.022611413792014\n",
      "Stochastic Gradient Descent(41612): loss=0.9031432048174121\n",
      "Stochastic Gradient Descent(41613): loss=7.003602996967031\n",
      "Stochastic Gradient Descent(41614): loss=1.9389859938539487\n",
      "Stochastic Gradient Descent(41615): loss=3.3259685077448546\n",
      "Stochastic Gradient Descent(41616): loss=0.02338892033581835\n",
      "Stochastic Gradient Descent(41617): loss=7.581361676704886\n",
      "Stochastic Gradient Descent(41618): loss=1.4962522826842943\n",
      "Stochastic Gradient Descent(41619): loss=5.10233598583741\n",
      "Stochastic Gradient Descent(41620): loss=8.136786071590473\n",
      "Stochastic Gradient Descent(41621): loss=0.0010204554393794263\n",
      "Stochastic Gradient Descent(41622): loss=2.0533075020334777\n",
      "Stochastic Gradient Descent(41623): loss=0.44446992205202107\n",
      "Stochastic Gradient Descent(41624): loss=0.007909542998094174\n",
      "Stochastic Gradient Descent(41625): loss=0.6337564197282453\n",
      "Stochastic Gradient Descent(41626): loss=0.003036426345955525\n",
      "Stochastic Gradient Descent(41627): loss=1.0002996118293446\n",
      "Stochastic Gradient Descent(41628): loss=14.579389160369308\n",
      "Stochastic Gradient Descent(41629): loss=0.2995020955710486\n",
      "Stochastic Gradient Descent(41630): loss=2.872287140822148\n",
      "Stochastic Gradient Descent(41631): loss=0.00010483399346961905\n",
      "Stochastic Gradient Descent(41632): loss=0.0053420819315589665\n",
      "Stochastic Gradient Descent(41633): loss=2.270510654286459\n",
      "Stochastic Gradient Descent(41634): loss=3.2288571075294246\n",
      "Stochastic Gradient Descent(41635): loss=0.010796730167694875\n",
      "Stochastic Gradient Descent(41636): loss=0.1499215473384676\n",
      "Stochastic Gradient Descent(41637): loss=4.355759901093444\n",
      "Stochastic Gradient Descent(41638): loss=1.5912656256825182\n",
      "Stochastic Gradient Descent(41639): loss=1.6654863071414852\n",
      "Stochastic Gradient Descent(41640): loss=25.106219863693255\n",
      "Stochastic Gradient Descent(41641): loss=102.82046138539663\n",
      "Stochastic Gradient Descent(41642): loss=1.7254685499908007\n",
      "Stochastic Gradient Descent(41643): loss=11.841073595118278\n",
      "Stochastic Gradient Descent(41644): loss=44.219504434173615\n",
      "Stochastic Gradient Descent(41645): loss=0.20317434997086403\n",
      "Stochastic Gradient Descent(41646): loss=3.8084072276061036\n",
      "Stochastic Gradient Descent(41647): loss=0.1578118704756959\n",
      "Stochastic Gradient Descent(41648): loss=1.187210364949455\n",
      "Stochastic Gradient Descent(41649): loss=0.2785381245247012\n",
      "Stochastic Gradient Descent(41650): loss=3.8164033430885618\n",
      "Stochastic Gradient Descent(41651): loss=3.551711912576799\n",
      "Stochastic Gradient Descent(41652): loss=0.6341584781201606\n",
      "Stochastic Gradient Descent(41653): loss=3.3925527681576293\n",
      "Stochastic Gradient Descent(41654): loss=3.3402367325886937\n",
      "Stochastic Gradient Descent(41655): loss=0.750097229179562\n",
      "Stochastic Gradient Descent(41656): loss=1.6565938683060493\n",
      "Stochastic Gradient Descent(41657): loss=1.9905901990951134\n",
      "Stochastic Gradient Descent(41658): loss=0.43430170846794086\n",
      "Stochastic Gradient Descent(41659): loss=0.0750207601718952\n",
      "Stochastic Gradient Descent(41660): loss=0.06609264500733109\n",
      "Stochastic Gradient Descent(41661): loss=0.7220464129048545\n",
      "Stochastic Gradient Descent(41662): loss=0.6789563403249951\n",
      "Stochastic Gradient Descent(41663): loss=0.009221216556953208\n",
      "Stochastic Gradient Descent(41664): loss=0.04188829427894628\n",
      "Stochastic Gradient Descent(41665): loss=0.02527602608383192\n",
      "Stochastic Gradient Descent(41666): loss=2.0844491520735993\n",
      "Stochastic Gradient Descent(41667): loss=4.466087195529911\n",
      "Stochastic Gradient Descent(41668): loss=7.860220892922673\n",
      "Stochastic Gradient Descent(41669): loss=10.655074519879715\n",
      "Stochastic Gradient Descent(41670): loss=3.285326309858924\n",
      "Stochastic Gradient Descent(41671): loss=4.130284358330405\n",
      "Stochastic Gradient Descent(41672): loss=13.95516056133859\n",
      "Stochastic Gradient Descent(41673): loss=1.2128389601775502\n",
      "Stochastic Gradient Descent(41674): loss=0.02325302673784386\n",
      "Stochastic Gradient Descent(41675): loss=0.9644296970533349\n",
      "Stochastic Gradient Descent(41676): loss=1.6992930736343406\n",
      "Stochastic Gradient Descent(41677): loss=3.7898696773707257\n",
      "Stochastic Gradient Descent(41678): loss=6.59033811194096\n",
      "Stochastic Gradient Descent(41679): loss=2.7336427135376407\n",
      "Stochastic Gradient Descent(41680): loss=11.454605423418991\n",
      "Stochastic Gradient Descent(41681): loss=1.2600994655071132\n",
      "Stochastic Gradient Descent(41682): loss=2.201005840962683\n",
      "Stochastic Gradient Descent(41683): loss=23.36527804467208\n",
      "Stochastic Gradient Descent(41684): loss=0.10873188824723784\n",
      "Stochastic Gradient Descent(41685): loss=0.3115933092176269\n",
      "Stochastic Gradient Descent(41686): loss=7.979000231303089\n",
      "Stochastic Gradient Descent(41687): loss=2.852235434251765\n",
      "Stochastic Gradient Descent(41688): loss=1.1243734237142815\n",
      "Stochastic Gradient Descent(41689): loss=15.6606017618692\n",
      "Stochastic Gradient Descent(41690): loss=0.4692943619707374\n",
      "Stochastic Gradient Descent(41691): loss=0.1272957725956479\n",
      "Stochastic Gradient Descent(41692): loss=0.8255067841631485\n",
      "Stochastic Gradient Descent(41693): loss=0.4387286562879133\n",
      "Stochastic Gradient Descent(41694): loss=0.9562969457600277\n",
      "Stochastic Gradient Descent(41695): loss=2.205006833473837\n",
      "Stochastic Gradient Descent(41696): loss=8.142800589460949\n",
      "Stochastic Gradient Descent(41697): loss=0.03791902773451\n",
      "Stochastic Gradient Descent(41698): loss=15.033951312750927\n",
      "Stochastic Gradient Descent(41699): loss=0.0135531507524808\n",
      "Stochastic Gradient Descent(41700): loss=0.16010054635016982\n",
      "Stochastic Gradient Descent(41701): loss=1.2141666873520676\n",
      "Stochastic Gradient Descent(41702): loss=7.738409389026074\n",
      "Stochastic Gradient Descent(41703): loss=0.15776653198568943\n",
      "Stochastic Gradient Descent(41704): loss=8.704955791172715\n",
      "Stochastic Gradient Descent(41705): loss=0.047886276747441246\n",
      "Stochastic Gradient Descent(41706): loss=7.475752501625545\n",
      "Stochastic Gradient Descent(41707): loss=8.086289132872363\n",
      "Stochastic Gradient Descent(41708): loss=9.598846144464263\n",
      "Stochastic Gradient Descent(41709): loss=0.025026955479924624\n",
      "Stochastic Gradient Descent(41710): loss=2.6399558541667947\n",
      "Stochastic Gradient Descent(41711): loss=6.957293164777317\n",
      "Stochastic Gradient Descent(41712): loss=1.1469973834093725\n",
      "Stochastic Gradient Descent(41713): loss=1.0425973130766637\n",
      "Stochastic Gradient Descent(41714): loss=0.2777950993392791\n",
      "Stochastic Gradient Descent(41715): loss=0.40824211301794827\n",
      "Stochastic Gradient Descent(41716): loss=2.1564686476400983\n",
      "Stochastic Gradient Descent(41717): loss=1.0488381505116076\n",
      "Stochastic Gradient Descent(41718): loss=1.2359319896620635\n",
      "Stochastic Gradient Descent(41719): loss=0.19773296156140877\n",
      "Stochastic Gradient Descent(41720): loss=0.5013556009583368\n",
      "Stochastic Gradient Descent(41721): loss=0.09217250524307778\n",
      "Stochastic Gradient Descent(41722): loss=2.20798965815146\n",
      "Stochastic Gradient Descent(41723): loss=2.5319547320988383\n",
      "Stochastic Gradient Descent(41724): loss=2.6466583264784482\n",
      "Stochastic Gradient Descent(41725): loss=10.2336494792362\n",
      "Stochastic Gradient Descent(41726): loss=0.03949941588730215\n",
      "Stochastic Gradient Descent(41727): loss=2.305035758929737\n",
      "Stochastic Gradient Descent(41728): loss=2.531344885505502\n",
      "Stochastic Gradient Descent(41729): loss=1.7473470307900185\n",
      "Stochastic Gradient Descent(41730): loss=0.5457729269158618\n",
      "Stochastic Gradient Descent(41731): loss=1.0378336470132372\n",
      "Stochastic Gradient Descent(41732): loss=1.0750347104973774\n",
      "Stochastic Gradient Descent(41733): loss=0.0011123188414152866\n",
      "Stochastic Gradient Descent(41734): loss=5.7894136230106135\n",
      "Stochastic Gradient Descent(41735): loss=21.73785122466657\n",
      "Stochastic Gradient Descent(41736): loss=0.05547498847688622\n",
      "Stochastic Gradient Descent(41737): loss=0.37699321364335986\n",
      "Stochastic Gradient Descent(41738): loss=4.477828889752592\n",
      "Stochastic Gradient Descent(41739): loss=1.1114808783223469\n",
      "Stochastic Gradient Descent(41740): loss=0.00018829272685249183\n",
      "Stochastic Gradient Descent(41741): loss=15.534762530437572\n",
      "Stochastic Gradient Descent(41742): loss=2.5424018713201653\n",
      "Stochastic Gradient Descent(41743): loss=0.0548604373641011\n",
      "Stochastic Gradient Descent(41744): loss=14.218601267102779\n",
      "Stochastic Gradient Descent(41745): loss=11.636965531373283\n",
      "Stochastic Gradient Descent(41746): loss=2.9460062334456922\n",
      "Stochastic Gradient Descent(41747): loss=3.3242286063110154\n",
      "Stochastic Gradient Descent(41748): loss=1.356300543968215\n",
      "Stochastic Gradient Descent(41749): loss=1.3809698841060103\n",
      "Stochastic Gradient Descent(41750): loss=3.293105200580073\n",
      "Stochastic Gradient Descent(41751): loss=0.8605074458544852\n",
      "Stochastic Gradient Descent(41752): loss=3.6822679388794293\n",
      "Stochastic Gradient Descent(41753): loss=0.5212538090914269\n",
      "Stochastic Gradient Descent(41754): loss=6.508685193947529\n",
      "Stochastic Gradient Descent(41755): loss=4.854871988287315\n",
      "Stochastic Gradient Descent(41756): loss=4.9973041754352385\n",
      "Stochastic Gradient Descent(41757): loss=1.5429573340858513\n",
      "Stochastic Gradient Descent(41758): loss=0.09381866771590823\n",
      "Stochastic Gradient Descent(41759): loss=3.0651761734739\n",
      "Stochastic Gradient Descent(41760): loss=3.416916036396338\n",
      "Stochastic Gradient Descent(41761): loss=0.9599787011498162\n",
      "Stochastic Gradient Descent(41762): loss=22.192885254138123\n",
      "Stochastic Gradient Descent(41763): loss=14.954367308863972\n",
      "Stochastic Gradient Descent(41764): loss=3.58818501439445\n",
      "Stochastic Gradient Descent(41765): loss=1.3679637887843539\n",
      "Stochastic Gradient Descent(41766): loss=0.11071851016194124\n",
      "Stochastic Gradient Descent(41767): loss=0.3562534740934485\n",
      "Stochastic Gradient Descent(41768): loss=6.41250050825189\n",
      "Stochastic Gradient Descent(41769): loss=0.07439601575620346\n",
      "Stochastic Gradient Descent(41770): loss=0.4613066091637053\n",
      "Stochastic Gradient Descent(41771): loss=2.6426623362947126\n",
      "Stochastic Gradient Descent(41772): loss=1.8660363402527986\n",
      "Stochastic Gradient Descent(41773): loss=0.8462467650862668\n",
      "Stochastic Gradient Descent(41774): loss=2.7764345419712937\n",
      "Stochastic Gradient Descent(41775): loss=6.29568442539359\n",
      "Stochastic Gradient Descent(41776): loss=0.3624964702683887\n",
      "Stochastic Gradient Descent(41777): loss=1.8635372503583063\n",
      "Stochastic Gradient Descent(41778): loss=2.259656571592424\n",
      "Stochastic Gradient Descent(41779): loss=0.03534947552473644\n",
      "Stochastic Gradient Descent(41780): loss=1.3747536311689257\n",
      "Stochastic Gradient Descent(41781): loss=0.27427057293925\n",
      "Stochastic Gradient Descent(41782): loss=5.3006788222673515\n",
      "Stochastic Gradient Descent(41783): loss=0.5561193631394755\n",
      "Stochastic Gradient Descent(41784): loss=0.006754793645950976\n",
      "Stochastic Gradient Descent(41785): loss=5.743145677853704\n",
      "Stochastic Gradient Descent(41786): loss=6.436484523155278\n",
      "Stochastic Gradient Descent(41787): loss=1.4087294909934547\n",
      "Stochastic Gradient Descent(41788): loss=5.758364321085981\n",
      "Stochastic Gradient Descent(41789): loss=0.6666971965438812\n",
      "Stochastic Gradient Descent(41790): loss=1.4958265653319915\n",
      "Stochastic Gradient Descent(41791): loss=1.2913220324768915\n",
      "Stochastic Gradient Descent(41792): loss=0.44093379948254924\n",
      "Stochastic Gradient Descent(41793): loss=15.146454424257747\n",
      "Stochastic Gradient Descent(41794): loss=0.0003362332084731123\n",
      "Stochastic Gradient Descent(41795): loss=3.4633374625524675\n",
      "Stochastic Gradient Descent(41796): loss=0.149159848492388\n",
      "Stochastic Gradient Descent(41797): loss=0.13666315802628456\n",
      "Stochastic Gradient Descent(41798): loss=4.160625579675905\n",
      "Stochastic Gradient Descent(41799): loss=0.5547268161976332\n",
      "Stochastic Gradient Descent(41800): loss=0.06790109513418666\n",
      "Stochastic Gradient Descent(41801): loss=7.4349899567172555\n",
      "Stochastic Gradient Descent(41802): loss=2.331237419839582\n",
      "Stochastic Gradient Descent(41803): loss=0.02733762023058223\n",
      "Stochastic Gradient Descent(41804): loss=3.7605667539879186\n",
      "Stochastic Gradient Descent(41805): loss=3.775675290105777\n",
      "Stochastic Gradient Descent(41806): loss=7.209731636742405\n",
      "Stochastic Gradient Descent(41807): loss=0.6335263880382511\n",
      "Stochastic Gradient Descent(41808): loss=4.915925074747183\n",
      "Stochastic Gradient Descent(41809): loss=0.0001900528434729538\n",
      "Stochastic Gradient Descent(41810): loss=2.611945445349788\n",
      "Stochastic Gradient Descent(41811): loss=1.4396823471905091\n",
      "Stochastic Gradient Descent(41812): loss=5.737570735059717\n",
      "Stochastic Gradient Descent(41813): loss=1.3506047232620253\n",
      "Stochastic Gradient Descent(41814): loss=0.008200706168930871\n",
      "Stochastic Gradient Descent(41815): loss=0.019785085975402027\n",
      "Stochastic Gradient Descent(41816): loss=0.7658307337158611\n",
      "Stochastic Gradient Descent(41817): loss=0.9633053605198891\n",
      "Stochastic Gradient Descent(41818): loss=4.006558232939797\n",
      "Stochastic Gradient Descent(41819): loss=0.17327743834020368\n",
      "Stochastic Gradient Descent(41820): loss=23.912231177886575\n",
      "Stochastic Gradient Descent(41821): loss=7.945182677688135\n",
      "Stochastic Gradient Descent(41822): loss=19.077720943163285\n",
      "Stochastic Gradient Descent(41823): loss=3.7250875860554458\n",
      "Stochastic Gradient Descent(41824): loss=0.7395547145230408\n",
      "Stochastic Gradient Descent(41825): loss=0.4464150274553128\n",
      "Stochastic Gradient Descent(41826): loss=2.0143798201466145\n",
      "Stochastic Gradient Descent(41827): loss=0.19895067617299136\n",
      "Stochastic Gradient Descent(41828): loss=0.22785129181092767\n",
      "Stochastic Gradient Descent(41829): loss=2.2245372825496603\n",
      "Stochastic Gradient Descent(41830): loss=1.4814678744272174\n",
      "Stochastic Gradient Descent(41831): loss=3.316425139145292\n",
      "Stochastic Gradient Descent(41832): loss=0.2927733482233231\n",
      "Stochastic Gradient Descent(41833): loss=20.40214443297884\n",
      "Stochastic Gradient Descent(41834): loss=14.16394391581176\n",
      "Stochastic Gradient Descent(41835): loss=0.10329558468630672\n",
      "Stochastic Gradient Descent(41836): loss=0.040415377003649604\n",
      "Stochastic Gradient Descent(41837): loss=1.1133808909270435\n",
      "Stochastic Gradient Descent(41838): loss=2.963930499254946\n",
      "Stochastic Gradient Descent(41839): loss=0.4824758520469667\n",
      "Stochastic Gradient Descent(41840): loss=1.2179881527005039\n",
      "Stochastic Gradient Descent(41841): loss=3.3581532268521053\n",
      "Stochastic Gradient Descent(41842): loss=26.614375623283053\n",
      "Stochastic Gradient Descent(41843): loss=4.7813580877326\n",
      "Stochastic Gradient Descent(41844): loss=0.32075599836228763\n",
      "Stochastic Gradient Descent(41845): loss=0.7642216676572923\n",
      "Stochastic Gradient Descent(41846): loss=0.5971278657762576\n",
      "Stochastic Gradient Descent(41847): loss=2.35566280334228\n",
      "Stochastic Gradient Descent(41848): loss=2.7397816445339664\n",
      "Stochastic Gradient Descent(41849): loss=2.443541448606825\n",
      "Stochastic Gradient Descent(41850): loss=8.838495564288301\n",
      "Stochastic Gradient Descent(41851): loss=0.19616994062750598\n",
      "Stochastic Gradient Descent(41852): loss=1.3819100934425943\n",
      "Stochastic Gradient Descent(41853): loss=0.046694531021400136\n",
      "Stochastic Gradient Descent(41854): loss=6.302381931384153\n",
      "Stochastic Gradient Descent(41855): loss=7.009751984371471\n",
      "Stochastic Gradient Descent(41856): loss=1.6512471765198165\n",
      "Stochastic Gradient Descent(41857): loss=1.3839558251750113\n",
      "Stochastic Gradient Descent(41858): loss=0.1279905827809974\n",
      "Stochastic Gradient Descent(41859): loss=2.1005816665638117\n",
      "Stochastic Gradient Descent(41860): loss=1.6632481276263724\n",
      "Stochastic Gradient Descent(41861): loss=1.3649635903933004\n",
      "Stochastic Gradient Descent(41862): loss=5.885048505909913\n",
      "Stochastic Gradient Descent(41863): loss=0.8867322451956624\n",
      "Stochastic Gradient Descent(41864): loss=0.18996726902477795\n",
      "Stochastic Gradient Descent(41865): loss=0.8162631671623823\n",
      "Stochastic Gradient Descent(41866): loss=0.5106613257410723\n",
      "Stochastic Gradient Descent(41867): loss=2.1794726022579907\n",
      "Stochastic Gradient Descent(41868): loss=1.961771706557232\n",
      "Stochastic Gradient Descent(41869): loss=0.951410374528451\n",
      "Stochastic Gradient Descent(41870): loss=0.0014783006364628693\n",
      "Stochastic Gradient Descent(41871): loss=0.6008349666963041\n",
      "Stochastic Gradient Descent(41872): loss=0.0014703954461629516\n",
      "Stochastic Gradient Descent(41873): loss=0.056839479238431065\n",
      "Stochastic Gradient Descent(41874): loss=0.1261701294092148\n",
      "Stochastic Gradient Descent(41875): loss=0.11285138523688157\n",
      "Stochastic Gradient Descent(41876): loss=0.49610800253394044\n",
      "Stochastic Gradient Descent(41877): loss=3.588665111502751\n",
      "Stochastic Gradient Descent(41878): loss=0.3142157768245254\n",
      "Stochastic Gradient Descent(41879): loss=2.266873714469211\n",
      "Stochastic Gradient Descent(41880): loss=0.15750264618523507\n",
      "Stochastic Gradient Descent(41881): loss=6.244196601880969\n",
      "Stochastic Gradient Descent(41882): loss=5.599459032669842\n",
      "Stochastic Gradient Descent(41883): loss=3.676962182388102\n",
      "Stochastic Gradient Descent(41884): loss=0.8732823963432294\n",
      "Stochastic Gradient Descent(41885): loss=0.0010537768145810165\n",
      "Stochastic Gradient Descent(41886): loss=0.48625207020467237\n",
      "Stochastic Gradient Descent(41887): loss=0.17059122582872385\n",
      "Stochastic Gradient Descent(41888): loss=3.5890616263057162\n",
      "Stochastic Gradient Descent(41889): loss=0.10703180599521137\n",
      "Stochastic Gradient Descent(41890): loss=13.853850962083046\n",
      "Stochastic Gradient Descent(41891): loss=2.7365654238575847\n",
      "Stochastic Gradient Descent(41892): loss=1.491151205948753\n",
      "Stochastic Gradient Descent(41893): loss=0.11425648927231531\n",
      "Stochastic Gradient Descent(41894): loss=0.446879098112157\n",
      "Stochastic Gradient Descent(41895): loss=0.42435787071516323\n",
      "Stochastic Gradient Descent(41896): loss=0.04281771926528606\n",
      "Stochastic Gradient Descent(41897): loss=0.4978021893284262\n",
      "Stochastic Gradient Descent(41898): loss=13.289225270566117\n",
      "Stochastic Gradient Descent(41899): loss=2.1589008775656144\n",
      "Stochastic Gradient Descent(41900): loss=1.4337935007556033\n",
      "Stochastic Gradient Descent(41901): loss=0.02853823275327035\n",
      "Stochastic Gradient Descent(41902): loss=22.252613130996174\n",
      "Stochastic Gradient Descent(41903): loss=6.979895996259404\n",
      "Stochastic Gradient Descent(41904): loss=30.651865383479063\n",
      "Stochastic Gradient Descent(41905): loss=0.593356739740226\n",
      "Stochastic Gradient Descent(41906): loss=4.618424775493582\n",
      "Stochastic Gradient Descent(41907): loss=0.5705818888457936\n",
      "Stochastic Gradient Descent(41908): loss=7.500049213046376\n",
      "Stochastic Gradient Descent(41909): loss=0.6398660349903974\n",
      "Stochastic Gradient Descent(41910): loss=7.802964207335157\n",
      "Stochastic Gradient Descent(41911): loss=1.1478258803728298\n",
      "Stochastic Gradient Descent(41912): loss=0.1846873665770712\n",
      "Stochastic Gradient Descent(41913): loss=7.933644804012588\n",
      "Stochastic Gradient Descent(41914): loss=1.4482627208946626\n",
      "Stochastic Gradient Descent(41915): loss=1.8867078259412333\n",
      "Stochastic Gradient Descent(41916): loss=0.12571271405017284\n",
      "Stochastic Gradient Descent(41917): loss=4.810944150909395\n",
      "Stochastic Gradient Descent(41918): loss=0.41787363514411885\n",
      "Stochastic Gradient Descent(41919): loss=1.1330481297908948\n",
      "Stochastic Gradient Descent(41920): loss=4.996440234053326\n",
      "Stochastic Gradient Descent(41921): loss=54.13643725637221\n",
      "Stochastic Gradient Descent(41922): loss=0.6279310878355765\n",
      "Stochastic Gradient Descent(41923): loss=4.989711300232581\n",
      "Stochastic Gradient Descent(41924): loss=14.419065635085527\n",
      "Stochastic Gradient Descent(41925): loss=0.15668110150862774\n",
      "Stochastic Gradient Descent(41926): loss=1.2467540574688032\n",
      "Stochastic Gradient Descent(41927): loss=30.470254389150302\n",
      "Stochastic Gradient Descent(41928): loss=0.14240333261984175\n",
      "Stochastic Gradient Descent(41929): loss=1.0121701928954054\n",
      "Stochastic Gradient Descent(41930): loss=7.747986947017213\n",
      "Stochastic Gradient Descent(41931): loss=0.5125490250029384\n",
      "Stochastic Gradient Descent(41932): loss=7.905326874606876\n",
      "Stochastic Gradient Descent(41933): loss=7.518571535964498\n",
      "Stochastic Gradient Descent(41934): loss=4.290438956384371\n",
      "Stochastic Gradient Descent(41935): loss=14.222460364068503\n",
      "Stochastic Gradient Descent(41936): loss=4.023762160061257\n",
      "Stochastic Gradient Descent(41937): loss=2.7198492025756207\n",
      "Stochastic Gradient Descent(41938): loss=32.45873821835675\n",
      "Stochastic Gradient Descent(41939): loss=1.9780437098359178\n",
      "Stochastic Gradient Descent(41940): loss=0.7445244387941851\n",
      "Stochastic Gradient Descent(41941): loss=4.82635713520464\n",
      "Stochastic Gradient Descent(41942): loss=0.8607829925570009\n",
      "Stochastic Gradient Descent(41943): loss=4.67799538056541\n",
      "Stochastic Gradient Descent(41944): loss=2.4812267249216244\n",
      "Stochastic Gradient Descent(41945): loss=3.438654195064183\n",
      "Stochastic Gradient Descent(41946): loss=0.0627669536850851\n",
      "Stochastic Gradient Descent(41947): loss=25.120328301058155\n",
      "Stochastic Gradient Descent(41948): loss=6.271582977543251\n",
      "Stochastic Gradient Descent(41949): loss=1.6538704093038226\n",
      "Stochastic Gradient Descent(41950): loss=17.680656993501376\n",
      "Stochastic Gradient Descent(41951): loss=12.141502941334856\n",
      "Stochastic Gradient Descent(41952): loss=4.773141951234661\n",
      "Stochastic Gradient Descent(41953): loss=0.4851978570640374\n",
      "Stochastic Gradient Descent(41954): loss=3.7221323347481134\n",
      "Stochastic Gradient Descent(41955): loss=1.2453148921298909\n",
      "Stochastic Gradient Descent(41956): loss=2.68539379925856\n",
      "Stochastic Gradient Descent(41957): loss=14.20510553246714\n",
      "Stochastic Gradient Descent(41958): loss=5.147724631918081\n",
      "Stochastic Gradient Descent(41959): loss=0.7470951980662636\n",
      "Stochastic Gradient Descent(41960): loss=0.10095390883522867\n",
      "Stochastic Gradient Descent(41961): loss=2.6995659043073204\n",
      "Stochastic Gradient Descent(41962): loss=1.8279180114705782\n",
      "Stochastic Gradient Descent(41963): loss=2.0809710616406503\n",
      "Stochastic Gradient Descent(41964): loss=10.387679725361883\n",
      "Stochastic Gradient Descent(41965): loss=14.447202624941655\n",
      "Stochastic Gradient Descent(41966): loss=3.5337967728258968\n",
      "Stochastic Gradient Descent(41967): loss=0.0014996168931519017\n",
      "Stochastic Gradient Descent(41968): loss=0.2861600514122587\n",
      "Stochastic Gradient Descent(41969): loss=2.394137077428296\n",
      "Stochastic Gradient Descent(41970): loss=1.808120616088822\n",
      "Stochastic Gradient Descent(41971): loss=0.7601038689579788\n",
      "Stochastic Gradient Descent(41972): loss=0.018049799629661346\n",
      "Stochastic Gradient Descent(41973): loss=0.17398092251724703\n",
      "Stochastic Gradient Descent(41974): loss=0.6727590726207967\n",
      "Stochastic Gradient Descent(41975): loss=0.12917266191946625\n",
      "Stochastic Gradient Descent(41976): loss=0.04071750789456497\n",
      "Stochastic Gradient Descent(41977): loss=0.006714884106585822\n",
      "Stochastic Gradient Descent(41978): loss=1.208349767176786\n",
      "Stochastic Gradient Descent(41979): loss=9.642171699741862\n",
      "Stochastic Gradient Descent(41980): loss=0.5277194317980884\n",
      "Stochastic Gradient Descent(41981): loss=0.1538772889294756\n",
      "Stochastic Gradient Descent(41982): loss=2.4888168060389626\n",
      "Stochastic Gradient Descent(41983): loss=0.024172237613794608\n",
      "Stochastic Gradient Descent(41984): loss=0.22721110223644075\n",
      "Stochastic Gradient Descent(41985): loss=3.2822796085364665\n",
      "Stochastic Gradient Descent(41986): loss=5.378276390187317\n",
      "Stochastic Gradient Descent(41987): loss=3.253401732313788\n",
      "Stochastic Gradient Descent(41988): loss=10.049857101758638\n",
      "Stochastic Gradient Descent(41989): loss=0.2731373046975896\n",
      "Stochastic Gradient Descent(41990): loss=33.01968826895567\n",
      "Stochastic Gradient Descent(41991): loss=8.43249820860427\n",
      "Stochastic Gradient Descent(41992): loss=3.8015625173796996\n",
      "Stochastic Gradient Descent(41993): loss=0.012512847945806633\n",
      "Stochastic Gradient Descent(41994): loss=5.677033865503567\n",
      "Stochastic Gradient Descent(41995): loss=0.8252988400919438\n",
      "Stochastic Gradient Descent(41996): loss=6.942078867274709\n",
      "Stochastic Gradient Descent(41997): loss=0.7833097773230263\n",
      "Stochastic Gradient Descent(41998): loss=1.9910205015709057\n",
      "Stochastic Gradient Descent(41999): loss=1.065580712156887\n",
      "Stochastic Gradient Descent(42000): loss=1.384525564212396\n",
      "Stochastic Gradient Descent(42001): loss=0.925783767407997\n",
      "Stochastic Gradient Descent(42002): loss=2.4606937543794314\n",
      "Stochastic Gradient Descent(42003): loss=3.330239142880796\n",
      "Stochastic Gradient Descent(42004): loss=7.725205884212377\n",
      "Stochastic Gradient Descent(42005): loss=2.8939806493949116\n",
      "Stochastic Gradient Descent(42006): loss=1.5678464567639736\n",
      "Stochastic Gradient Descent(42007): loss=1.3220893498144062\n",
      "Stochastic Gradient Descent(42008): loss=43.059219479217276\n",
      "Stochastic Gradient Descent(42009): loss=0.05912459415142429\n",
      "Stochastic Gradient Descent(42010): loss=45.62019995517304\n",
      "Stochastic Gradient Descent(42011): loss=0.22390556755499577\n",
      "Stochastic Gradient Descent(42012): loss=0.9978601247933104\n",
      "Stochastic Gradient Descent(42013): loss=1.9576179937712044\n",
      "Stochastic Gradient Descent(42014): loss=1.8927682551054978\n",
      "Stochastic Gradient Descent(42015): loss=6.190113663436559\n",
      "Stochastic Gradient Descent(42016): loss=0.21990092598450534\n",
      "Stochastic Gradient Descent(42017): loss=7.858636301592925\n",
      "Stochastic Gradient Descent(42018): loss=9.979084910671462\n",
      "Stochastic Gradient Descent(42019): loss=5.979059818109538\n",
      "Stochastic Gradient Descent(42020): loss=0.15026266396904142\n",
      "Stochastic Gradient Descent(42021): loss=1.5281670878897722\n",
      "Stochastic Gradient Descent(42022): loss=1.6557189864796902\n",
      "Stochastic Gradient Descent(42023): loss=23.939823862841887\n",
      "Stochastic Gradient Descent(42024): loss=1.7417798695108582\n",
      "Stochastic Gradient Descent(42025): loss=5.038305981594491\n",
      "Stochastic Gradient Descent(42026): loss=10.159559933763457\n",
      "Stochastic Gradient Descent(42027): loss=4.53602992385994\n",
      "Stochastic Gradient Descent(42028): loss=0.921914178282387\n",
      "Stochastic Gradient Descent(42029): loss=3.034973854106105\n",
      "Stochastic Gradient Descent(42030): loss=0.002620561479966693\n",
      "Stochastic Gradient Descent(42031): loss=0.000992973500637278\n",
      "Stochastic Gradient Descent(42032): loss=1.8560549544191487\n",
      "Stochastic Gradient Descent(42033): loss=6.195760264779108\n",
      "Stochastic Gradient Descent(42034): loss=3.9314870137848223\n",
      "Stochastic Gradient Descent(42035): loss=0.5118261258695821\n",
      "Stochastic Gradient Descent(42036): loss=0.4881696912124883\n",
      "Stochastic Gradient Descent(42037): loss=6.362406550460382\n",
      "Stochastic Gradient Descent(42038): loss=2.4204866883998957\n",
      "Stochastic Gradient Descent(42039): loss=3.4336904024524513\n",
      "Stochastic Gradient Descent(42040): loss=2.105539569337162\n",
      "Stochastic Gradient Descent(42041): loss=0.5451395666814075\n",
      "Stochastic Gradient Descent(42042): loss=0.6428057197188058\n",
      "Stochastic Gradient Descent(42043): loss=0.07364163077202732\n",
      "Stochastic Gradient Descent(42044): loss=0.33392694818438473\n",
      "Stochastic Gradient Descent(42045): loss=3.1018939299951023\n",
      "Stochastic Gradient Descent(42046): loss=0.10240621745715835\n",
      "Stochastic Gradient Descent(42047): loss=1.4512413281292207\n",
      "Stochastic Gradient Descent(42048): loss=0.7529166536537182\n",
      "Stochastic Gradient Descent(42049): loss=0.5332941003280979\n",
      "Stochastic Gradient Descent(42050): loss=1.621216814431577\n",
      "Stochastic Gradient Descent(42051): loss=6.066677121820814\n",
      "Stochastic Gradient Descent(42052): loss=1.9162924670302708\n",
      "Stochastic Gradient Descent(42053): loss=1.5326331724525972\n",
      "Stochastic Gradient Descent(42054): loss=7.79117564778561\n",
      "Stochastic Gradient Descent(42055): loss=0.45448787831800586\n",
      "Stochastic Gradient Descent(42056): loss=2.333265561904798\n",
      "Stochastic Gradient Descent(42057): loss=0.8739847859288419\n",
      "Stochastic Gradient Descent(42058): loss=1.8667397977603488\n",
      "Stochastic Gradient Descent(42059): loss=14.912412357746058\n",
      "Stochastic Gradient Descent(42060): loss=0.018058178036934106\n",
      "Stochastic Gradient Descent(42061): loss=2.0554226204178683\n",
      "Stochastic Gradient Descent(42062): loss=0.023120751143798017\n",
      "Stochastic Gradient Descent(42063): loss=1.4552413113783353\n",
      "Stochastic Gradient Descent(42064): loss=0.59819353673687\n",
      "Stochastic Gradient Descent(42065): loss=0.2515687453451213\n",
      "Stochastic Gradient Descent(42066): loss=1.958148513363328\n",
      "Stochastic Gradient Descent(42067): loss=7.499950878954913\n",
      "Stochastic Gradient Descent(42068): loss=0.16459860995605677\n",
      "Stochastic Gradient Descent(42069): loss=2.4383690046373876\n",
      "Stochastic Gradient Descent(42070): loss=5.6301026912449785\n",
      "Stochastic Gradient Descent(42071): loss=15.982179014729745\n",
      "Stochastic Gradient Descent(42072): loss=0.3095666505595778\n",
      "Stochastic Gradient Descent(42073): loss=1.0950605969491287\n",
      "Stochastic Gradient Descent(42074): loss=3.1164746294697214\n",
      "Stochastic Gradient Descent(42075): loss=0.002574951440560769\n",
      "Stochastic Gradient Descent(42076): loss=1.4039757782013798\n",
      "Stochastic Gradient Descent(42077): loss=4.487885672879235\n",
      "Stochastic Gradient Descent(42078): loss=1.2242625439070087\n",
      "Stochastic Gradient Descent(42079): loss=9.820270565670407\n",
      "Stochastic Gradient Descent(42080): loss=0.5582662593762572\n",
      "Stochastic Gradient Descent(42081): loss=25.273025286259625\n",
      "Stochastic Gradient Descent(42082): loss=1.571895953508976\n",
      "Stochastic Gradient Descent(42083): loss=16.2754303591224\n",
      "Stochastic Gradient Descent(42084): loss=0.16050740484422976\n",
      "Stochastic Gradient Descent(42085): loss=4.493355914791829\n",
      "Stochastic Gradient Descent(42086): loss=4.687958081889633\n",
      "Stochastic Gradient Descent(42087): loss=0.075901184260728\n",
      "Stochastic Gradient Descent(42088): loss=0.8249487679605033\n",
      "Stochastic Gradient Descent(42089): loss=3.866769640398498\n",
      "Stochastic Gradient Descent(42090): loss=7.409950329397429\n",
      "Stochastic Gradient Descent(42091): loss=0.3108242791031133\n",
      "Stochastic Gradient Descent(42092): loss=2.6681149451797173\n",
      "Stochastic Gradient Descent(42093): loss=4.9665963503582615\n",
      "Stochastic Gradient Descent(42094): loss=0.9069962478648083\n",
      "Stochastic Gradient Descent(42095): loss=6.923673582572463\n",
      "Stochastic Gradient Descent(42096): loss=1.8768146061425277\n",
      "Stochastic Gradient Descent(42097): loss=1.198170966926981\n",
      "Stochastic Gradient Descent(42098): loss=8.518807783861208\n",
      "Stochastic Gradient Descent(42099): loss=0.47352823645920955\n",
      "Stochastic Gradient Descent(42100): loss=0.08717697108319163\n",
      "Stochastic Gradient Descent(42101): loss=11.959683572933383\n",
      "Stochastic Gradient Descent(42102): loss=8.20299799583525\n",
      "Stochastic Gradient Descent(42103): loss=24.994959840660282\n",
      "Stochastic Gradient Descent(42104): loss=2.159841797371459\n",
      "Stochastic Gradient Descent(42105): loss=3.9243330683305477\n",
      "Stochastic Gradient Descent(42106): loss=0.2045800911148622\n",
      "Stochastic Gradient Descent(42107): loss=9.400224355742953\n",
      "Stochastic Gradient Descent(42108): loss=1.8647069497759294\n",
      "Stochastic Gradient Descent(42109): loss=7.460859857230973\n",
      "Stochastic Gradient Descent(42110): loss=1.3518109981053426\n",
      "Stochastic Gradient Descent(42111): loss=0.0013447132295360095\n",
      "Stochastic Gradient Descent(42112): loss=0.6619646037824705\n",
      "Stochastic Gradient Descent(42113): loss=0.004716670104876245\n",
      "Stochastic Gradient Descent(42114): loss=0.1254958962791232\n",
      "Stochastic Gradient Descent(42115): loss=3.2957195532090973\n",
      "Stochastic Gradient Descent(42116): loss=8.679813571168063\n",
      "Stochastic Gradient Descent(42117): loss=0.005088152441084181\n",
      "Stochastic Gradient Descent(42118): loss=2.7203470054860266\n",
      "Stochastic Gradient Descent(42119): loss=2.914148222042587\n",
      "Stochastic Gradient Descent(42120): loss=0.0006870081102314403\n",
      "Stochastic Gradient Descent(42121): loss=3.2745356737778772\n",
      "Stochastic Gradient Descent(42122): loss=1.3064716899305127\n",
      "Stochastic Gradient Descent(42123): loss=26.725668393470475\n",
      "Stochastic Gradient Descent(42124): loss=1.561123919118488\n",
      "Stochastic Gradient Descent(42125): loss=4.06209879345379\n",
      "Stochastic Gradient Descent(42126): loss=0.5620309837953992\n",
      "Stochastic Gradient Descent(42127): loss=0.00025102112640416663\n",
      "Stochastic Gradient Descent(42128): loss=5.7158631467926355\n",
      "Stochastic Gradient Descent(42129): loss=4.637180098378049\n",
      "Stochastic Gradient Descent(42130): loss=7.09468720496531\n",
      "Stochastic Gradient Descent(42131): loss=1.721852184484944\n",
      "Stochastic Gradient Descent(42132): loss=0.42346070175380573\n",
      "Stochastic Gradient Descent(42133): loss=5.250493462167259\n",
      "Stochastic Gradient Descent(42134): loss=1.8196300587629957\n",
      "Stochastic Gradient Descent(42135): loss=0.08392492590062536\n",
      "Stochastic Gradient Descent(42136): loss=0.0732742293327222\n",
      "Stochastic Gradient Descent(42137): loss=16.27299555023979\n",
      "Stochastic Gradient Descent(42138): loss=0.06838541596555545\n",
      "Stochastic Gradient Descent(42139): loss=0.04504527347032129\n",
      "Stochastic Gradient Descent(42140): loss=9.758574806016249\n",
      "Stochastic Gradient Descent(42141): loss=0.2570192621322128\n",
      "Stochastic Gradient Descent(42142): loss=3.531569001990568\n",
      "Stochastic Gradient Descent(42143): loss=7.029634028329947\n",
      "Stochastic Gradient Descent(42144): loss=2.643473021787395\n",
      "Stochastic Gradient Descent(42145): loss=2.2548140876983274\n",
      "Stochastic Gradient Descent(42146): loss=0.1695624597766997\n",
      "Stochastic Gradient Descent(42147): loss=0.004972910526331273\n",
      "Stochastic Gradient Descent(42148): loss=0.00035304410395995995\n",
      "Stochastic Gradient Descent(42149): loss=68.84181191750109\n",
      "Stochastic Gradient Descent(42150): loss=27.973707940403834\n",
      "Stochastic Gradient Descent(42151): loss=7.568878140272292\n",
      "Stochastic Gradient Descent(42152): loss=0.2063353607111584\n",
      "Stochastic Gradient Descent(42153): loss=43.12319695846165\n",
      "Stochastic Gradient Descent(42154): loss=0.3554790739239398\n",
      "Stochastic Gradient Descent(42155): loss=0.7539916996384229\n",
      "Stochastic Gradient Descent(42156): loss=3.0716587690200874\n",
      "Stochastic Gradient Descent(42157): loss=0.22690325511377613\n",
      "Stochastic Gradient Descent(42158): loss=9.048667093543083\n",
      "Stochastic Gradient Descent(42159): loss=1.1127311637376913\n",
      "Stochastic Gradient Descent(42160): loss=0.4699986210196413\n",
      "Stochastic Gradient Descent(42161): loss=4.7431423449307175\n",
      "Stochastic Gradient Descent(42162): loss=0.06959395418063112\n",
      "Stochastic Gradient Descent(42163): loss=3.3865308374544494\n",
      "Stochastic Gradient Descent(42164): loss=5.38281158920381\n",
      "Stochastic Gradient Descent(42165): loss=5.390826477886013\n",
      "Stochastic Gradient Descent(42166): loss=10.560489658359547\n",
      "Stochastic Gradient Descent(42167): loss=9.858017652149066\n",
      "Stochastic Gradient Descent(42168): loss=0.021176754857027736\n",
      "Stochastic Gradient Descent(42169): loss=1.7238560675881973\n",
      "Stochastic Gradient Descent(42170): loss=0.7792392119586424\n",
      "Stochastic Gradient Descent(42171): loss=1.2435185455970397\n",
      "Stochastic Gradient Descent(42172): loss=0.5138457940839122\n",
      "Stochastic Gradient Descent(42173): loss=0.5806717250182677\n",
      "Stochastic Gradient Descent(42174): loss=7.427533274219518\n",
      "Stochastic Gradient Descent(42175): loss=4.402935273417463\n",
      "Stochastic Gradient Descent(42176): loss=0.4195443388904423\n",
      "Stochastic Gradient Descent(42177): loss=3.83517485546819\n",
      "Stochastic Gradient Descent(42178): loss=5.6792158474657635\n",
      "Stochastic Gradient Descent(42179): loss=0.010349496448612638\n",
      "Stochastic Gradient Descent(42180): loss=5.833594395452715\n",
      "Stochastic Gradient Descent(42181): loss=1.5474108612281834\n",
      "Stochastic Gradient Descent(42182): loss=1.369745231454704\n",
      "Stochastic Gradient Descent(42183): loss=0.30496762937670124\n",
      "Stochastic Gradient Descent(42184): loss=2.7621615603500906\n",
      "Stochastic Gradient Descent(42185): loss=0.0960945104687106\n",
      "Stochastic Gradient Descent(42186): loss=0.08001279946282369\n",
      "Stochastic Gradient Descent(42187): loss=31.67417653397369\n",
      "Stochastic Gradient Descent(42188): loss=0.013250077718308596\n",
      "Stochastic Gradient Descent(42189): loss=8.027501872883706\n",
      "Stochastic Gradient Descent(42190): loss=13.270886080221471\n",
      "Stochastic Gradient Descent(42191): loss=13.989810332476601\n",
      "Stochastic Gradient Descent(42192): loss=0.14906430311761815\n",
      "Stochastic Gradient Descent(42193): loss=0.015881764258748085\n",
      "Stochastic Gradient Descent(42194): loss=16.381313007313537\n",
      "Stochastic Gradient Descent(42195): loss=1.274700228790777\n",
      "Stochastic Gradient Descent(42196): loss=0.3236397290644451\n",
      "Stochastic Gradient Descent(42197): loss=1.356537683178586\n",
      "Stochastic Gradient Descent(42198): loss=1.4810707672868477\n",
      "Stochastic Gradient Descent(42199): loss=0.03154879176582261\n",
      "Stochastic Gradient Descent(42200): loss=4.064681457567543\n",
      "Stochastic Gradient Descent(42201): loss=0.00047100972954039217\n",
      "Stochastic Gradient Descent(42202): loss=6.84397688978912\n",
      "Stochastic Gradient Descent(42203): loss=0.5521827492236671\n",
      "Stochastic Gradient Descent(42204): loss=0.428151441772514\n",
      "Stochastic Gradient Descent(42205): loss=1.9195414087828024\n",
      "Stochastic Gradient Descent(42206): loss=1.5175044067963364\n",
      "Stochastic Gradient Descent(42207): loss=2.060012118682781\n",
      "Stochastic Gradient Descent(42208): loss=5.563806167572475\n",
      "Stochastic Gradient Descent(42209): loss=0.1591090050026013\n",
      "Stochastic Gradient Descent(42210): loss=0.4910049259363883\n",
      "Stochastic Gradient Descent(42211): loss=1.0986088057296086\n",
      "Stochastic Gradient Descent(42212): loss=4.792609393929211\n",
      "Stochastic Gradient Descent(42213): loss=5.685105234376239\n",
      "Stochastic Gradient Descent(42214): loss=5.5776767233036555\n",
      "Stochastic Gradient Descent(42215): loss=2.0520319732099517\n",
      "Stochastic Gradient Descent(42216): loss=2.2315546607611387\n",
      "Stochastic Gradient Descent(42217): loss=1.5242099820467159\n",
      "Stochastic Gradient Descent(42218): loss=0.659019952107786\n",
      "Stochastic Gradient Descent(42219): loss=0.6362920642253636\n",
      "Stochastic Gradient Descent(42220): loss=0.13375551689315338\n",
      "Stochastic Gradient Descent(42221): loss=5.453819293531674\n",
      "Stochastic Gradient Descent(42222): loss=5.176531350582847\n",
      "Stochastic Gradient Descent(42223): loss=0.044026516115095456\n",
      "Stochastic Gradient Descent(42224): loss=3.971035727115786\n",
      "Stochastic Gradient Descent(42225): loss=1.9465759828198685\n",
      "Stochastic Gradient Descent(42226): loss=1.3769495518706887\n",
      "Stochastic Gradient Descent(42227): loss=7.31288250265684\n",
      "Stochastic Gradient Descent(42228): loss=0.6088123961833025\n",
      "Stochastic Gradient Descent(42229): loss=0.2659845728042806\n",
      "Stochastic Gradient Descent(42230): loss=0.0008884282907821225\n",
      "Stochastic Gradient Descent(42231): loss=8.152067721540092\n",
      "Stochastic Gradient Descent(42232): loss=0.02295164253039978\n",
      "Stochastic Gradient Descent(42233): loss=1.1870620510175884\n",
      "Stochastic Gradient Descent(42234): loss=0.00023148552446708017\n",
      "Stochastic Gradient Descent(42235): loss=3.132266657434285\n",
      "Stochastic Gradient Descent(42236): loss=0.01386766251985126\n",
      "Stochastic Gradient Descent(42237): loss=6.489860596841111\n",
      "Stochastic Gradient Descent(42238): loss=9.11997752319255\n",
      "Stochastic Gradient Descent(42239): loss=0.013188557291844904\n",
      "Stochastic Gradient Descent(42240): loss=35.182083931840914\n",
      "Stochastic Gradient Descent(42241): loss=1.6220938174040314\n",
      "Stochastic Gradient Descent(42242): loss=12.018662503732575\n",
      "Stochastic Gradient Descent(42243): loss=0.5511108635744445\n",
      "Stochastic Gradient Descent(42244): loss=0.5041250900140151\n",
      "Stochastic Gradient Descent(42245): loss=1.5036567903488565\n",
      "Stochastic Gradient Descent(42246): loss=1.9617756785067924\n",
      "Stochastic Gradient Descent(42247): loss=0.004640460818587471\n",
      "Stochastic Gradient Descent(42248): loss=1.0589104493778068\n",
      "Stochastic Gradient Descent(42249): loss=0.2594912055324482\n",
      "Stochastic Gradient Descent(42250): loss=1.5138175749674863\n",
      "Stochastic Gradient Descent(42251): loss=0.07798123149672362\n",
      "Stochastic Gradient Descent(42252): loss=0.6010492565550083\n",
      "Stochastic Gradient Descent(42253): loss=1.2132872026800192\n",
      "Stochastic Gradient Descent(42254): loss=6.281731563349956\n",
      "Stochastic Gradient Descent(42255): loss=0.45604820537446333\n",
      "Stochastic Gradient Descent(42256): loss=12.115021147527228\n",
      "Stochastic Gradient Descent(42257): loss=2.2186719226835154\n",
      "Stochastic Gradient Descent(42258): loss=8.980413967256666\n",
      "Stochastic Gradient Descent(42259): loss=0.06932088917072958\n",
      "Stochastic Gradient Descent(42260): loss=17.858267195049507\n",
      "Stochastic Gradient Descent(42261): loss=8.55807561947417\n",
      "Stochastic Gradient Descent(42262): loss=0.9935039224357296\n",
      "Stochastic Gradient Descent(42263): loss=17.77526350893374\n",
      "Stochastic Gradient Descent(42264): loss=6.235800187299738\n",
      "Stochastic Gradient Descent(42265): loss=0.8067012214207897\n",
      "Stochastic Gradient Descent(42266): loss=0.7117524208390065\n",
      "Stochastic Gradient Descent(42267): loss=24.94989473332505\n",
      "Stochastic Gradient Descent(42268): loss=0.02648657499783115\n",
      "Stochastic Gradient Descent(42269): loss=0.019831344766273882\n",
      "Stochastic Gradient Descent(42270): loss=0.09844756903052396\n",
      "Stochastic Gradient Descent(42271): loss=4.926068358847148\n",
      "Stochastic Gradient Descent(42272): loss=5.758273192874373\n",
      "Stochastic Gradient Descent(42273): loss=29.104900046563507\n",
      "Stochastic Gradient Descent(42274): loss=2.049801891898318\n",
      "Stochastic Gradient Descent(42275): loss=5.619129879868414\n",
      "Stochastic Gradient Descent(42276): loss=0.32966780608430424\n",
      "Stochastic Gradient Descent(42277): loss=0.049069129149448366\n",
      "Stochastic Gradient Descent(42278): loss=7.427752543075224\n",
      "Stochastic Gradient Descent(42279): loss=7.987742730455324\n",
      "Stochastic Gradient Descent(42280): loss=0.11256255812919044\n",
      "Stochastic Gradient Descent(42281): loss=0.24530622076023995\n",
      "Stochastic Gradient Descent(42282): loss=0.7534399794922161\n",
      "Stochastic Gradient Descent(42283): loss=21.71568918002503\n",
      "Stochastic Gradient Descent(42284): loss=0.1831509278129703\n",
      "Stochastic Gradient Descent(42285): loss=0.1743267028429806\n",
      "Stochastic Gradient Descent(42286): loss=1.4239853734257422\n",
      "Stochastic Gradient Descent(42287): loss=0.143932172030656\n",
      "Stochastic Gradient Descent(42288): loss=33.174405149965345\n",
      "Stochastic Gradient Descent(42289): loss=0.07518335495606981\n",
      "Stochastic Gradient Descent(42290): loss=1.3284489401237412\n",
      "Stochastic Gradient Descent(42291): loss=0.0922203202399528\n",
      "Stochastic Gradient Descent(42292): loss=0.09670588506504155\n",
      "Stochastic Gradient Descent(42293): loss=2.4407608577242557\n",
      "Stochastic Gradient Descent(42294): loss=1.9564279021248618\n",
      "Stochastic Gradient Descent(42295): loss=0.47722620206461314\n",
      "Stochastic Gradient Descent(42296): loss=0.09260481248537737\n",
      "Stochastic Gradient Descent(42297): loss=0.20629886353906365\n",
      "Stochastic Gradient Descent(42298): loss=14.715478860796834\n",
      "Stochastic Gradient Descent(42299): loss=11.897594194543176\n",
      "Stochastic Gradient Descent(42300): loss=0.0225581386731492\n",
      "Stochastic Gradient Descent(42301): loss=1.0933520839432471\n",
      "Stochastic Gradient Descent(42302): loss=1.0519897522523873\n",
      "Stochastic Gradient Descent(42303): loss=3.208917358852564\n",
      "Stochastic Gradient Descent(42304): loss=1.6688257039658636\n",
      "Stochastic Gradient Descent(42305): loss=2.2048345524312434\n",
      "Stochastic Gradient Descent(42306): loss=0.01792882637117842\n",
      "Stochastic Gradient Descent(42307): loss=0.9167342589429136\n",
      "Stochastic Gradient Descent(42308): loss=0.20822048240387545\n",
      "Stochastic Gradient Descent(42309): loss=42.08612986387111\n",
      "Stochastic Gradient Descent(42310): loss=2.790559242566294\n",
      "Stochastic Gradient Descent(42311): loss=5.601812708678605\n",
      "Stochastic Gradient Descent(42312): loss=0.7862886955060392\n",
      "Stochastic Gradient Descent(42313): loss=14.130812380763624\n",
      "Stochastic Gradient Descent(42314): loss=0.08694395807650991\n",
      "Stochastic Gradient Descent(42315): loss=6.834873456125406\n",
      "Stochastic Gradient Descent(42316): loss=0.5586567081915559\n",
      "Stochastic Gradient Descent(42317): loss=1.149989813742243\n",
      "Stochastic Gradient Descent(42318): loss=2.724106622691471\n",
      "Stochastic Gradient Descent(42319): loss=0.1696981474620752\n",
      "Stochastic Gradient Descent(42320): loss=0.2076526264083074\n",
      "Stochastic Gradient Descent(42321): loss=0.05397769868041808\n",
      "Stochastic Gradient Descent(42322): loss=7.142017164655387\n",
      "Stochastic Gradient Descent(42323): loss=13.942267147210938\n",
      "Stochastic Gradient Descent(42324): loss=7.925729743068741\n",
      "Stochastic Gradient Descent(42325): loss=11.921416384111707\n",
      "Stochastic Gradient Descent(42326): loss=2.811467968042775\n",
      "Stochastic Gradient Descent(42327): loss=0.998899246607094\n",
      "Stochastic Gradient Descent(42328): loss=13.470620865198725\n",
      "Stochastic Gradient Descent(42329): loss=0.35397973166414665\n",
      "Stochastic Gradient Descent(42330): loss=0.6487996619319586\n",
      "Stochastic Gradient Descent(42331): loss=0.02247721438126702\n",
      "Stochastic Gradient Descent(42332): loss=8.883517923165966\n",
      "Stochastic Gradient Descent(42333): loss=0.8617361175437308\n",
      "Stochastic Gradient Descent(42334): loss=0.0775902871740747\n",
      "Stochastic Gradient Descent(42335): loss=10.310244714055257\n",
      "Stochastic Gradient Descent(42336): loss=0.5766180190672061\n",
      "Stochastic Gradient Descent(42337): loss=0.12565172166987532\n",
      "Stochastic Gradient Descent(42338): loss=0.937829271792147\n",
      "Stochastic Gradient Descent(42339): loss=6.4731098137561185\n",
      "Stochastic Gradient Descent(42340): loss=2.559166370780489\n",
      "Stochastic Gradient Descent(42341): loss=0.08783578626091908\n",
      "Stochastic Gradient Descent(42342): loss=0.8645430212924156\n",
      "Stochastic Gradient Descent(42343): loss=1.1997153425540072\n",
      "Stochastic Gradient Descent(42344): loss=33.17398890798781\n",
      "Stochastic Gradient Descent(42345): loss=7.865169451903628\n",
      "Stochastic Gradient Descent(42346): loss=3.506979041612646\n",
      "Stochastic Gradient Descent(42347): loss=0.4524746396749835\n",
      "Stochastic Gradient Descent(42348): loss=0.2664229934210269\n",
      "Stochastic Gradient Descent(42349): loss=3.4164486539790717\n",
      "Stochastic Gradient Descent(42350): loss=4.704858151497192\n",
      "Stochastic Gradient Descent(42351): loss=1.1473693407763026\n",
      "Stochastic Gradient Descent(42352): loss=4.242177301068294\n",
      "Stochastic Gradient Descent(42353): loss=0.025406530527558892\n",
      "Stochastic Gradient Descent(42354): loss=37.384484573450735\n",
      "Stochastic Gradient Descent(42355): loss=40.08382795616549\n",
      "Stochastic Gradient Descent(42356): loss=29.00529963341307\n",
      "Stochastic Gradient Descent(42357): loss=0.11504228712430378\n",
      "Stochastic Gradient Descent(42358): loss=1.116668535260524\n",
      "Stochastic Gradient Descent(42359): loss=1.844158381176993\n",
      "Stochastic Gradient Descent(42360): loss=0.26417613252607763\n",
      "Stochastic Gradient Descent(42361): loss=9.286864047854989\n",
      "Stochastic Gradient Descent(42362): loss=2.4974521514886328\n",
      "Stochastic Gradient Descent(42363): loss=8.392824478955525\n",
      "Stochastic Gradient Descent(42364): loss=0.012852139850068887\n",
      "Stochastic Gradient Descent(42365): loss=4.439682021826063\n",
      "Stochastic Gradient Descent(42366): loss=0.03377870717303963\n",
      "Stochastic Gradient Descent(42367): loss=13.715706027815335\n",
      "Stochastic Gradient Descent(42368): loss=9.219346988491047\n",
      "Stochastic Gradient Descent(42369): loss=0.4206248367404653\n",
      "Stochastic Gradient Descent(42370): loss=0.012655697077472335\n",
      "Stochastic Gradient Descent(42371): loss=1.0913316352444231\n",
      "Stochastic Gradient Descent(42372): loss=3.8102800117167344\n",
      "Stochastic Gradient Descent(42373): loss=6.670008830726285\n",
      "Stochastic Gradient Descent(42374): loss=0.7982575372914974\n",
      "Stochastic Gradient Descent(42375): loss=6.99638129928791\n",
      "Stochastic Gradient Descent(42376): loss=0.08790883899940909\n",
      "Stochastic Gradient Descent(42377): loss=0.7467386554504393\n",
      "Stochastic Gradient Descent(42378): loss=0.01849910529795257\n",
      "Stochastic Gradient Descent(42379): loss=1.9043203964916238\n",
      "Stochastic Gradient Descent(42380): loss=3.9566732663758657\n",
      "Stochastic Gradient Descent(42381): loss=2.051545225007553\n",
      "Stochastic Gradient Descent(42382): loss=7.489366178360619\n",
      "Stochastic Gradient Descent(42383): loss=0.07735996890571278\n",
      "Stochastic Gradient Descent(42384): loss=0.47394549530217095\n",
      "Stochastic Gradient Descent(42385): loss=4.707371908969806\n",
      "Stochastic Gradient Descent(42386): loss=4.53811991082337\n",
      "Stochastic Gradient Descent(42387): loss=3.9179779996388326\n",
      "Stochastic Gradient Descent(42388): loss=4.785096721959139\n",
      "Stochastic Gradient Descent(42389): loss=0.8526503398519838\n",
      "Stochastic Gradient Descent(42390): loss=1.7941929284630709\n",
      "Stochastic Gradient Descent(42391): loss=11.17759898984141\n",
      "Stochastic Gradient Descent(42392): loss=0.29156678102531564\n",
      "Stochastic Gradient Descent(42393): loss=7.762496974474654\n",
      "Stochastic Gradient Descent(42394): loss=0.8294496768214925\n",
      "Stochastic Gradient Descent(42395): loss=4.489186196227517\n",
      "Stochastic Gradient Descent(42396): loss=6.91686948605737\n",
      "Stochastic Gradient Descent(42397): loss=26.83867873601858\n",
      "Stochastic Gradient Descent(42398): loss=4.080628218902898\n",
      "Stochastic Gradient Descent(42399): loss=12.517450388167859\n",
      "Stochastic Gradient Descent(42400): loss=15.840361792532471\n",
      "Stochastic Gradient Descent(42401): loss=0.010284983374113696\n",
      "Stochastic Gradient Descent(42402): loss=17.62285513617272\n",
      "Stochastic Gradient Descent(42403): loss=16.505002670844434\n",
      "Stochastic Gradient Descent(42404): loss=19.968028762935706\n",
      "Stochastic Gradient Descent(42405): loss=4.301350573494732\n",
      "Stochastic Gradient Descent(42406): loss=1.7720142654426712\n",
      "Stochastic Gradient Descent(42407): loss=2.083669657362939\n",
      "Stochastic Gradient Descent(42408): loss=6.013479277651214\n",
      "Stochastic Gradient Descent(42409): loss=10.364030246112572\n",
      "Stochastic Gradient Descent(42410): loss=0.6652112030461781\n",
      "Stochastic Gradient Descent(42411): loss=0.8367782531157746\n",
      "Stochastic Gradient Descent(42412): loss=0.027719997411925\n",
      "Stochastic Gradient Descent(42413): loss=0.059413381698770876\n",
      "Stochastic Gradient Descent(42414): loss=0.03409191623583542\n",
      "Stochastic Gradient Descent(42415): loss=1.4221046043761247\n",
      "Stochastic Gradient Descent(42416): loss=0.2029349856017692\n",
      "Stochastic Gradient Descent(42417): loss=0.015110479052194781\n",
      "Stochastic Gradient Descent(42418): loss=11.081383091596418\n",
      "Stochastic Gradient Descent(42419): loss=2.7491788689263985\n",
      "Stochastic Gradient Descent(42420): loss=0.957299563381942\n",
      "Stochastic Gradient Descent(42421): loss=5.497337783816273\n",
      "Stochastic Gradient Descent(42422): loss=13.77299888169312\n",
      "Stochastic Gradient Descent(42423): loss=1.5348178851184646\n",
      "Stochastic Gradient Descent(42424): loss=0.3581049537667295\n",
      "Stochastic Gradient Descent(42425): loss=7.654009237543414\n",
      "Stochastic Gradient Descent(42426): loss=0.22760965491812704\n",
      "Stochastic Gradient Descent(42427): loss=1.9449490159307965\n",
      "Stochastic Gradient Descent(42428): loss=1.4731072312020927\n",
      "Stochastic Gradient Descent(42429): loss=0.9712319944093205\n",
      "Stochastic Gradient Descent(42430): loss=18.251482083083996\n",
      "Stochastic Gradient Descent(42431): loss=6.567451380741572\n",
      "Stochastic Gradient Descent(42432): loss=1.3035974221483377\n",
      "Stochastic Gradient Descent(42433): loss=6.320379542000904\n",
      "Stochastic Gradient Descent(42434): loss=1.9959391031685014\n",
      "Stochastic Gradient Descent(42435): loss=3.379046209277225\n",
      "Stochastic Gradient Descent(42436): loss=1.2083653765765012\n",
      "Stochastic Gradient Descent(42437): loss=6.495677690420714\n",
      "Stochastic Gradient Descent(42438): loss=8.749266866061328\n",
      "Stochastic Gradient Descent(42439): loss=0.7384417969591603\n",
      "Stochastic Gradient Descent(42440): loss=1.1656666137096046\n",
      "Stochastic Gradient Descent(42441): loss=3.1893654485916425\n",
      "Stochastic Gradient Descent(42442): loss=1.2143873096503417\n",
      "Stochastic Gradient Descent(42443): loss=0.8063342384525675\n",
      "Stochastic Gradient Descent(42444): loss=1.043192968683257\n",
      "Stochastic Gradient Descent(42445): loss=2.4102981207903555\n",
      "Stochastic Gradient Descent(42446): loss=1.1493868799509774\n",
      "Stochastic Gradient Descent(42447): loss=2.02010571736729\n",
      "Stochastic Gradient Descent(42448): loss=1.2010521803448846\n",
      "Stochastic Gradient Descent(42449): loss=1.361395035613156\n",
      "Stochastic Gradient Descent(42450): loss=7.974915150292907\n",
      "Stochastic Gradient Descent(42451): loss=0.01062062672375626\n",
      "Stochastic Gradient Descent(42452): loss=3.160253922425951\n",
      "Stochastic Gradient Descent(42453): loss=4.739052571635055\n",
      "Stochastic Gradient Descent(42454): loss=0.02736750022203028\n",
      "Stochastic Gradient Descent(42455): loss=0.00499238425357929\n",
      "Stochastic Gradient Descent(42456): loss=4.2263913107458455\n",
      "Stochastic Gradient Descent(42457): loss=0.006425675424598891\n",
      "Stochastic Gradient Descent(42458): loss=2.352097311851492\n",
      "Stochastic Gradient Descent(42459): loss=16.05382495382383\n",
      "Stochastic Gradient Descent(42460): loss=13.806682951144813\n",
      "Stochastic Gradient Descent(42461): loss=0.2046213956677082\n",
      "Stochastic Gradient Descent(42462): loss=2.644173456806668\n",
      "Stochastic Gradient Descent(42463): loss=1.4900445883679208\n",
      "Stochastic Gradient Descent(42464): loss=5.812077535546414\n",
      "Stochastic Gradient Descent(42465): loss=0.009740396513527054\n",
      "Stochastic Gradient Descent(42466): loss=4.160560194043965\n",
      "Stochastic Gradient Descent(42467): loss=14.43263306097884\n",
      "Stochastic Gradient Descent(42468): loss=16.04882897771195\n",
      "Stochastic Gradient Descent(42469): loss=3.637700156346606\n",
      "Stochastic Gradient Descent(42470): loss=1.8562209873495585\n",
      "Stochastic Gradient Descent(42471): loss=3.745884011495343\n",
      "Stochastic Gradient Descent(42472): loss=4.345577837109991\n",
      "Stochastic Gradient Descent(42473): loss=4.0402396039489785\n",
      "Stochastic Gradient Descent(42474): loss=0.01307587110311642\n",
      "Stochastic Gradient Descent(42475): loss=0.026361587311672697\n",
      "Stochastic Gradient Descent(42476): loss=0.47685444334537985\n",
      "Stochastic Gradient Descent(42477): loss=2.0141926189843935\n",
      "Stochastic Gradient Descent(42478): loss=0.007247875310370548\n",
      "Stochastic Gradient Descent(42479): loss=0.1694281644214388\n",
      "Stochastic Gradient Descent(42480): loss=6.2995711813143735\n",
      "Stochastic Gradient Descent(42481): loss=1.455305521824796\n",
      "Stochastic Gradient Descent(42482): loss=1.5573201690629177\n",
      "Stochastic Gradient Descent(42483): loss=0.225864280262333\n",
      "Stochastic Gradient Descent(42484): loss=31.289909611714585\n",
      "Stochastic Gradient Descent(42485): loss=1.2346090896768864\n",
      "Stochastic Gradient Descent(42486): loss=0.6003838934335987\n",
      "Stochastic Gradient Descent(42487): loss=5.6556601523199195\n",
      "Stochastic Gradient Descent(42488): loss=0.23983945062442583\n",
      "Stochastic Gradient Descent(42489): loss=27.492462087672116\n",
      "Stochastic Gradient Descent(42490): loss=0.9461582359616649\n",
      "Stochastic Gradient Descent(42491): loss=3.189152182874834\n",
      "Stochastic Gradient Descent(42492): loss=4.168080305240661\n",
      "Stochastic Gradient Descent(42493): loss=7.863154680868532\n",
      "Stochastic Gradient Descent(42494): loss=0.2737627011539509\n",
      "Stochastic Gradient Descent(42495): loss=1.0594012208500965\n",
      "Stochastic Gradient Descent(42496): loss=0.8000773661079214\n",
      "Stochastic Gradient Descent(42497): loss=0.5928217854031596\n",
      "Stochastic Gradient Descent(42498): loss=0.19207754953988293\n",
      "Stochastic Gradient Descent(42499): loss=4.060345151238767\n",
      "Stochastic Gradient Descent(42500): loss=4.564973319639142\n",
      "Stochastic Gradient Descent(42501): loss=0.6080594613585247\n",
      "Stochastic Gradient Descent(42502): loss=6.160854894926045\n",
      "Stochastic Gradient Descent(42503): loss=2.3451297348409708\n",
      "Stochastic Gradient Descent(42504): loss=0.8677883106846287\n",
      "Stochastic Gradient Descent(42505): loss=0.06236699928467225\n",
      "Stochastic Gradient Descent(42506): loss=0.2799631718123344\n",
      "Stochastic Gradient Descent(42507): loss=0.025821818147165446\n",
      "Stochastic Gradient Descent(42508): loss=0.32394923892512\n",
      "Stochastic Gradient Descent(42509): loss=1.80493477943633\n",
      "Stochastic Gradient Descent(42510): loss=0.5017406119130531\n",
      "Stochastic Gradient Descent(42511): loss=0.07532554545273829\n",
      "Stochastic Gradient Descent(42512): loss=13.813426256153202\n",
      "Stochastic Gradient Descent(42513): loss=8.315094021433431\n",
      "Stochastic Gradient Descent(42514): loss=0.8337875885747462\n",
      "Stochastic Gradient Descent(42515): loss=3.4685115042197405\n",
      "Stochastic Gradient Descent(42516): loss=0.23536298789694957\n",
      "Stochastic Gradient Descent(42517): loss=0.030454917393657972\n",
      "Stochastic Gradient Descent(42518): loss=2.0022691846041325\n",
      "Stochastic Gradient Descent(42519): loss=2.2815031113543554\n",
      "Stochastic Gradient Descent(42520): loss=9.459566926387946\n",
      "Stochastic Gradient Descent(42521): loss=1.4301128536011594\n",
      "Stochastic Gradient Descent(42522): loss=4.536662681352912\n",
      "Stochastic Gradient Descent(42523): loss=6.613067857581166\n",
      "Stochastic Gradient Descent(42524): loss=2.400913540964258\n",
      "Stochastic Gradient Descent(42525): loss=6.64769236624367\n",
      "Stochastic Gradient Descent(42526): loss=0.34637218135354186\n",
      "Stochastic Gradient Descent(42527): loss=13.240417401925095\n",
      "Stochastic Gradient Descent(42528): loss=0.0829878643822892\n",
      "Stochastic Gradient Descent(42529): loss=1.3687707275884806\n",
      "Stochastic Gradient Descent(42530): loss=14.606166538457472\n",
      "Stochastic Gradient Descent(42531): loss=9.602131891200212\n",
      "Stochastic Gradient Descent(42532): loss=0.019562292327373325\n",
      "Stochastic Gradient Descent(42533): loss=0.7234392638017071\n",
      "Stochastic Gradient Descent(42534): loss=1.7793853164594722\n",
      "Stochastic Gradient Descent(42535): loss=1.379057074751436\n",
      "Stochastic Gradient Descent(42536): loss=0.00468752033718544\n",
      "Stochastic Gradient Descent(42537): loss=6.45238267562361\n",
      "Stochastic Gradient Descent(42538): loss=1.5320731495354862\n",
      "Stochastic Gradient Descent(42539): loss=0.7620431356967988\n",
      "Stochastic Gradient Descent(42540): loss=1.491438695769501\n",
      "Stochastic Gradient Descent(42541): loss=0.843470434623387\n",
      "Stochastic Gradient Descent(42542): loss=3.7659118616357103\n",
      "Stochastic Gradient Descent(42543): loss=1.5010188918743392\n",
      "Stochastic Gradient Descent(42544): loss=2.136491029396332\n",
      "Stochastic Gradient Descent(42545): loss=0.7081265751526437\n",
      "Stochastic Gradient Descent(42546): loss=2.580802655481898\n",
      "Stochastic Gradient Descent(42547): loss=5.188635036903989\n",
      "Stochastic Gradient Descent(42548): loss=2.1090965200384804\n",
      "Stochastic Gradient Descent(42549): loss=4.3685306542909155\n",
      "Stochastic Gradient Descent(42550): loss=1.3404467458597848\n",
      "Stochastic Gradient Descent(42551): loss=0.20509629318111508\n",
      "Stochastic Gradient Descent(42552): loss=1.9124451232793969\n",
      "Stochastic Gradient Descent(42553): loss=3.9900893340099146\n",
      "Stochastic Gradient Descent(42554): loss=0.04686816454993015\n",
      "Stochastic Gradient Descent(42555): loss=6.15579027511089\n",
      "Stochastic Gradient Descent(42556): loss=12.046339345323712\n",
      "Stochastic Gradient Descent(42557): loss=1.4859216970984503\n",
      "Stochastic Gradient Descent(42558): loss=14.96039614045491\n",
      "Stochastic Gradient Descent(42559): loss=6.017448563098072\n",
      "Stochastic Gradient Descent(42560): loss=1.715012986023583\n",
      "Stochastic Gradient Descent(42561): loss=6.607371110650277\n",
      "Stochastic Gradient Descent(42562): loss=9.518467397802608\n",
      "Stochastic Gradient Descent(42563): loss=0.11634311421278118\n",
      "Stochastic Gradient Descent(42564): loss=1.3366826740920796\n",
      "Stochastic Gradient Descent(42565): loss=16.06282666030599\n",
      "Stochastic Gradient Descent(42566): loss=1.160718867360203\n",
      "Stochastic Gradient Descent(42567): loss=1.6488886814568535\n",
      "Stochastic Gradient Descent(42568): loss=4.523256507307384\n",
      "Stochastic Gradient Descent(42569): loss=12.478155640262502\n",
      "Stochastic Gradient Descent(42570): loss=29.760817882028093\n",
      "Stochastic Gradient Descent(42571): loss=1.2771933153904496\n",
      "Stochastic Gradient Descent(42572): loss=0.05408137478821926\n",
      "Stochastic Gradient Descent(42573): loss=0.11122499684703233\n",
      "Stochastic Gradient Descent(42574): loss=0.16969521107474625\n",
      "Stochastic Gradient Descent(42575): loss=1.237800748084253\n",
      "Stochastic Gradient Descent(42576): loss=4.647388756392229\n",
      "Stochastic Gradient Descent(42577): loss=0.12490532941483883\n",
      "Stochastic Gradient Descent(42578): loss=0.15390445330056754\n",
      "Stochastic Gradient Descent(42579): loss=9.565441196429921\n",
      "Stochastic Gradient Descent(42580): loss=0.0014172691306696729\n",
      "Stochastic Gradient Descent(42581): loss=33.59730763973893\n",
      "Stochastic Gradient Descent(42582): loss=3.4594624439618937\n",
      "Stochastic Gradient Descent(42583): loss=0.7817972201084884\n",
      "Stochastic Gradient Descent(42584): loss=317.36092929222235\n",
      "Stochastic Gradient Descent(42585): loss=343.41511331389455\n",
      "Stochastic Gradient Descent(42586): loss=15.974117628047154\n",
      "Stochastic Gradient Descent(42587): loss=28.679154662037547\n",
      "Stochastic Gradient Descent(42588): loss=12.747400679867988\n",
      "Stochastic Gradient Descent(42589): loss=47.467159982851726\n",
      "Stochastic Gradient Descent(42590): loss=21.747661991181978\n",
      "Stochastic Gradient Descent(42591): loss=169.13572395868957\n",
      "Stochastic Gradient Descent(42592): loss=7.8979679877522395\n",
      "Stochastic Gradient Descent(42593): loss=53.624060447420725\n",
      "Stochastic Gradient Descent(42594): loss=0.6451883751391554\n",
      "Stochastic Gradient Descent(42595): loss=0.24941182530904193\n",
      "Stochastic Gradient Descent(42596): loss=0.3619881931543158\n",
      "Stochastic Gradient Descent(42597): loss=1.5840372139706984\n",
      "Stochastic Gradient Descent(42598): loss=6.027571195434582\n",
      "Stochastic Gradient Descent(42599): loss=5.285753532542208\n",
      "Stochastic Gradient Descent(42600): loss=0.6781282255649657\n",
      "Stochastic Gradient Descent(42601): loss=0.9945728803234332\n",
      "Stochastic Gradient Descent(42602): loss=4.912164516999064\n",
      "Stochastic Gradient Descent(42603): loss=8.766176084879246\n",
      "Stochastic Gradient Descent(42604): loss=0.9085665928819848\n",
      "Stochastic Gradient Descent(42605): loss=5.346689352808614\n",
      "Stochastic Gradient Descent(42606): loss=8.783995768431242e-07\n",
      "Stochastic Gradient Descent(42607): loss=0.8956223008403389\n",
      "Stochastic Gradient Descent(42608): loss=18.64069696761614\n",
      "Stochastic Gradient Descent(42609): loss=0.0009268747815746586\n",
      "Stochastic Gradient Descent(42610): loss=0.8317508596578962\n",
      "Stochastic Gradient Descent(42611): loss=38.76778185994984\n",
      "Stochastic Gradient Descent(42612): loss=5.1326342126233975\n",
      "Stochastic Gradient Descent(42613): loss=3.172027177736112\n",
      "Stochastic Gradient Descent(42614): loss=0.2674349928316995\n",
      "Stochastic Gradient Descent(42615): loss=2.4498766222904633\n",
      "Stochastic Gradient Descent(42616): loss=17.921405691937778\n",
      "Stochastic Gradient Descent(42617): loss=0.19380155852312447\n",
      "Stochastic Gradient Descent(42618): loss=3.7621619300275135\n",
      "Stochastic Gradient Descent(42619): loss=0.06482265949895438\n",
      "Stochastic Gradient Descent(42620): loss=3.144073772804155\n",
      "Stochastic Gradient Descent(42621): loss=0.0766529470730294\n",
      "Stochastic Gradient Descent(42622): loss=2.732394299108199\n",
      "Stochastic Gradient Descent(42623): loss=2.8489890058145457\n",
      "Stochastic Gradient Descent(42624): loss=0.7193851126138899\n",
      "Stochastic Gradient Descent(42625): loss=7.107620075733084\n",
      "Stochastic Gradient Descent(42626): loss=0.01769512017592797\n",
      "Stochastic Gradient Descent(42627): loss=12.505592706381348\n",
      "Stochastic Gradient Descent(42628): loss=3.0996679658196853\n",
      "Stochastic Gradient Descent(42629): loss=2.1175152025601434\n",
      "Stochastic Gradient Descent(42630): loss=7.164715177765004\n",
      "Stochastic Gradient Descent(42631): loss=5.286662410587752\n",
      "Stochastic Gradient Descent(42632): loss=1.35182488555443\n",
      "Stochastic Gradient Descent(42633): loss=14.429862827479692\n",
      "Stochastic Gradient Descent(42634): loss=1.5526395449558439\n",
      "Stochastic Gradient Descent(42635): loss=2.1844621307106302\n",
      "Stochastic Gradient Descent(42636): loss=3.1677923610048238\n",
      "Stochastic Gradient Descent(42637): loss=0.027908459627845474\n",
      "Stochastic Gradient Descent(42638): loss=5.069128025831167\n",
      "Stochastic Gradient Descent(42639): loss=0.7184413977700803\n",
      "Stochastic Gradient Descent(42640): loss=1.7478659041681741\n",
      "Stochastic Gradient Descent(42641): loss=2.154212738990078\n",
      "Stochastic Gradient Descent(42642): loss=1.183810855264267\n",
      "Stochastic Gradient Descent(42643): loss=7.057793758824049\n",
      "Stochastic Gradient Descent(42644): loss=4.826437870149829\n",
      "Stochastic Gradient Descent(42645): loss=9.73504451296683\n",
      "Stochastic Gradient Descent(42646): loss=20.10733592966776\n",
      "Stochastic Gradient Descent(42647): loss=2.1305282720307614\n",
      "Stochastic Gradient Descent(42648): loss=7.1417193241108174\n",
      "Stochastic Gradient Descent(42649): loss=0.7992210405956284\n",
      "Stochastic Gradient Descent(42650): loss=1.7680426396989184\n",
      "Stochastic Gradient Descent(42651): loss=15.497871786605801\n",
      "Stochastic Gradient Descent(42652): loss=6.395419454455647\n",
      "Stochastic Gradient Descent(42653): loss=6.502281052403957\n",
      "Stochastic Gradient Descent(42654): loss=10.57526981050803\n",
      "Stochastic Gradient Descent(42655): loss=4.48369880246105\n",
      "Stochastic Gradient Descent(42656): loss=8.062904830286293\n",
      "Stochastic Gradient Descent(42657): loss=2.5336705506485124\n",
      "Stochastic Gradient Descent(42658): loss=0.15826032937733447\n",
      "Stochastic Gradient Descent(42659): loss=8.77700007751673\n",
      "Stochastic Gradient Descent(42660): loss=3.353151037901101e-05\n",
      "Stochastic Gradient Descent(42661): loss=2.2968373744506088\n",
      "Stochastic Gradient Descent(42662): loss=11.499724987727229\n",
      "Stochastic Gradient Descent(42663): loss=2.60390589621514\n",
      "Stochastic Gradient Descent(42664): loss=13.291956602820445\n",
      "Stochastic Gradient Descent(42665): loss=0.19016207984855735\n",
      "Stochastic Gradient Descent(42666): loss=0.23589878225296462\n",
      "Stochastic Gradient Descent(42667): loss=7.165792685931929\n",
      "Stochastic Gradient Descent(42668): loss=0.14521798392818713\n",
      "Stochastic Gradient Descent(42669): loss=0.3455450237669298\n",
      "Stochastic Gradient Descent(42670): loss=6.720156735513218\n",
      "Stochastic Gradient Descent(42671): loss=4.380896486008675\n",
      "Stochastic Gradient Descent(42672): loss=1.433549279191972\n",
      "Stochastic Gradient Descent(42673): loss=0.031237122554832925\n",
      "Stochastic Gradient Descent(42674): loss=8.40946004178024\n",
      "Stochastic Gradient Descent(42675): loss=0.6239458435316696\n",
      "Stochastic Gradient Descent(42676): loss=0.23571869718100383\n",
      "Stochastic Gradient Descent(42677): loss=3.335497914437065\n",
      "Stochastic Gradient Descent(42678): loss=1.58478857277531\n",
      "Stochastic Gradient Descent(42679): loss=1.7672303505030529\n",
      "Stochastic Gradient Descent(42680): loss=0.27713263180868525\n",
      "Stochastic Gradient Descent(42681): loss=0.01757259003022446\n",
      "Stochastic Gradient Descent(42682): loss=0.026381643048746968\n",
      "Stochastic Gradient Descent(42683): loss=8.21603132702417\n",
      "Stochastic Gradient Descent(42684): loss=0.2559212136185321\n",
      "Stochastic Gradient Descent(42685): loss=2.2814910994517414\n",
      "Stochastic Gradient Descent(42686): loss=0.2378942994097634\n",
      "Stochastic Gradient Descent(42687): loss=6.467746455985264\n",
      "Stochastic Gradient Descent(42688): loss=24.932599141354405\n",
      "Stochastic Gradient Descent(42689): loss=0.01638164324389499\n",
      "Stochastic Gradient Descent(42690): loss=3.496339405586695\n",
      "Stochastic Gradient Descent(42691): loss=0.07120854850146385\n",
      "Stochastic Gradient Descent(42692): loss=0.022436875551912542\n",
      "Stochastic Gradient Descent(42693): loss=0.00297730739411232\n",
      "Stochastic Gradient Descent(42694): loss=3.2086611217532703\n",
      "Stochastic Gradient Descent(42695): loss=2.238619637051126\n",
      "Stochastic Gradient Descent(42696): loss=0.14090768913821264\n",
      "Stochastic Gradient Descent(42697): loss=6.891496264035943\n",
      "Stochastic Gradient Descent(42698): loss=6.928833470933484\n",
      "Stochastic Gradient Descent(42699): loss=4.2648626708066475\n",
      "Stochastic Gradient Descent(42700): loss=1.7813673493877908\n",
      "Stochastic Gradient Descent(42701): loss=0.8286645360264144\n",
      "Stochastic Gradient Descent(42702): loss=12.625360202896271\n",
      "Stochastic Gradient Descent(42703): loss=0.0061086698470316\n",
      "Stochastic Gradient Descent(42704): loss=1.8555348152467568\n",
      "Stochastic Gradient Descent(42705): loss=0.09652998835915463\n",
      "Stochastic Gradient Descent(42706): loss=11.644635887254033\n",
      "Stochastic Gradient Descent(42707): loss=3.726385653583891\n",
      "Stochastic Gradient Descent(42708): loss=1.7518418159765288\n",
      "Stochastic Gradient Descent(42709): loss=1.823921593107739\n",
      "Stochastic Gradient Descent(42710): loss=1.910562375814494\n",
      "Stochastic Gradient Descent(42711): loss=15.160069099107291\n",
      "Stochastic Gradient Descent(42712): loss=1.8959844184132684\n",
      "Stochastic Gradient Descent(42713): loss=0.8648479234115379\n",
      "Stochastic Gradient Descent(42714): loss=6.29557089740789\n",
      "Stochastic Gradient Descent(42715): loss=0.09252131361199523\n",
      "Stochastic Gradient Descent(42716): loss=17.757158169517613\n",
      "Stochastic Gradient Descent(42717): loss=3.0669622112669983\n",
      "Stochastic Gradient Descent(42718): loss=3.010314741920532\n",
      "Stochastic Gradient Descent(42719): loss=5.228469132788513\n",
      "Stochastic Gradient Descent(42720): loss=1.2991139211934726\n",
      "Stochastic Gradient Descent(42721): loss=8.69543634058891\n",
      "Stochastic Gradient Descent(42722): loss=0.036612881186819456\n",
      "Stochastic Gradient Descent(42723): loss=1.077428626521025\n",
      "Stochastic Gradient Descent(42724): loss=1.0199665606984758\n",
      "Stochastic Gradient Descent(42725): loss=0.0016701556537376521\n",
      "Stochastic Gradient Descent(42726): loss=0.4882809400765844\n",
      "Stochastic Gradient Descent(42727): loss=12.407173302632467\n",
      "Stochastic Gradient Descent(42728): loss=1.0656389396812183\n",
      "Stochastic Gradient Descent(42729): loss=5.7804170641276755\n",
      "Stochastic Gradient Descent(42730): loss=1.0480563664528755\n",
      "Stochastic Gradient Descent(42731): loss=1.8215850060665888\n",
      "Stochastic Gradient Descent(42732): loss=13.534135908426537\n",
      "Stochastic Gradient Descent(42733): loss=1.759970458313382\n",
      "Stochastic Gradient Descent(42734): loss=2.5976959174601193\n",
      "Stochastic Gradient Descent(42735): loss=7.215248414113078\n",
      "Stochastic Gradient Descent(42736): loss=1.5273171929125398\n",
      "Stochastic Gradient Descent(42737): loss=1.6055492571813503\n",
      "Stochastic Gradient Descent(42738): loss=77.71619648274259\n",
      "Stochastic Gradient Descent(42739): loss=0.0912245007513655\n",
      "Stochastic Gradient Descent(42740): loss=12.017935896767707\n",
      "Stochastic Gradient Descent(42741): loss=1.4613090476411468\n",
      "Stochastic Gradient Descent(42742): loss=0.04003604236483152\n",
      "Stochastic Gradient Descent(42743): loss=24.37695730421878\n",
      "Stochastic Gradient Descent(42744): loss=6.5172370522570855\n",
      "Stochastic Gradient Descent(42745): loss=7.543260107000454\n",
      "Stochastic Gradient Descent(42746): loss=15.758729548064862\n",
      "Stochastic Gradient Descent(42747): loss=17.014379096599818\n",
      "Stochastic Gradient Descent(42748): loss=39.08040550195012\n",
      "Stochastic Gradient Descent(42749): loss=11.9769355173766\n",
      "Stochastic Gradient Descent(42750): loss=0.16373457274895148\n",
      "Stochastic Gradient Descent(42751): loss=0.32730900849222117\n",
      "Stochastic Gradient Descent(42752): loss=4.631988017749905\n",
      "Stochastic Gradient Descent(42753): loss=0.009182126420456795\n",
      "Stochastic Gradient Descent(42754): loss=36.308323220269635\n",
      "Stochastic Gradient Descent(42755): loss=9.988448283795474\n",
      "Stochastic Gradient Descent(42756): loss=0.7153669763204576\n",
      "Stochastic Gradient Descent(42757): loss=85.88228205854823\n",
      "Stochastic Gradient Descent(42758): loss=26.317940961999383\n",
      "Stochastic Gradient Descent(42759): loss=0.20252026696626899\n",
      "Stochastic Gradient Descent(42760): loss=8.651252446971263\n",
      "Stochastic Gradient Descent(42761): loss=43.27895853055845\n",
      "Stochastic Gradient Descent(42762): loss=10.348048526507073\n",
      "Stochastic Gradient Descent(42763): loss=0.5968156635391899\n",
      "Stochastic Gradient Descent(42764): loss=0.6405409035666937\n",
      "Stochastic Gradient Descent(42765): loss=9.903438331858595\n",
      "Stochastic Gradient Descent(42766): loss=47.025417462039506\n",
      "Stochastic Gradient Descent(42767): loss=30.45151853847446\n",
      "Stochastic Gradient Descent(42768): loss=2.780405036222932\n",
      "Stochastic Gradient Descent(42769): loss=1.9759927421763421\n",
      "Stochastic Gradient Descent(42770): loss=3.7473481381092095\n",
      "Stochastic Gradient Descent(42771): loss=0.10617959006512175\n",
      "Stochastic Gradient Descent(42772): loss=0.8262674672336919\n",
      "Stochastic Gradient Descent(42773): loss=0.002921931090497821\n",
      "Stochastic Gradient Descent(42774): loss=1.8671158588976011\n",
      "Stochastic Gradient Descent(42775): loss=37.85594875728892\n",
      "Stochastic Gradient Descent(42776): loss=5.910262025700042\n",
      "Stochastic Gradient Descent(42777): loss=0.13145813565586517\n",
      "Stochastic Gradient Descent(42778): loss=18.058086312452076\n",
      "Stochastic Gradient Descent(42779): loss=8.929380122600309\n",
      "Stochastic Gradient Descent(42780): loss=4.50944549894565\n",
      "Stochastic Gradient Descent(42781): loss=7.990964116995628\n",
      "Stochastic Gradient Descent(42782): loss=13.959476650234455\n",
      "Stochastic Gradient Descent(42783): loss=0.007025161388429183\n",
      "Stochastic Gradient Descent(42784): loss=9.010244371189096\n",
      "Stochastic Gradient Descent(42785): loss=19.589234004838126\n",
      "Stochastic Gradient Descent(42786): loss=22.875572894870157\n",
      "Stochastic Gradient Descent(42787): loss=3.6423501118987227\n",
      "Stochastic Gradient Descent(42788): loss=2.8992409826551016\n",
      "Stochastic Gradient Descent(42789): loss=0.2765055485594805\n",
      "Stochastic Gradient Descent(42790): loss=1.1062323792009887\n",
      "Stochastic Gradient Descent(42791): loss=4.291496098181001\n",
      "Stochastic Gradient Descent(42792): loss=23.846076795514698\n",
      "Stochastic Gradient Descent(42793): loss=3.248299602020806\n",
      "Stochastic Gradient Descent(42794): loss=0.48977231277144073\n",
      "Stochastic Gradient Descent(42795): loss=1.366215182834847\n",
      "Stochastic Gradient Descent(42796): loss=5.3905898813218505\n",
      "Stochastic Gradient Descent(42797): loss=5.51773093951058\n",
      "Stochastic Gradient Descent(42798): loss=0.8822450408319341\n",
      "Stochastic Gradient Descent(42799): loss=0.0924844549316819\n",
      "Stochastic Gradient Descent(42800): loss=1.3983279354704146\n",
      "Stochastic Gradient Descent(42801): loss=3.0405712059596692\n",
      "Stochastic Gradient Descent(42802): loss=3.278866910828086\n",
      "Stochastic Gradient Descent(42803): loss=8.680589740165171\n",
      "Stochastic Gradient Descent(42804): loss=10.310767018611243\n",
      "Stochastic Gradient Descent(42805): loss=8.295604342109865\n",
      "Stochastic Gradient Descent(42806): loss=0.868738540382523\n",
      "Stochastic Gradient Descent(42807): loss=15.430293671313967\n",
      "Stochastic Gradient Descent(42808): loss=3.259458946653785\n",
      "Stochastic Gradient Descent(42809): loss=0.2841313922627755\n",
      "Stochastic Gradient Descent(42810): loss=0.12848955102645848\n",
      "Stochastic Gradient Descent(42811): loss=3.0648724046458953\n",
      "Stochastic Gradient Descent(42812): loss=0.08464293567573895\n",
      "Stochastic Gradient Descent(42813): loss=6.887653278221496\n",
      "Stochastic Gradient Descent(42814): loss=0.09694425659109056\n",
      "Stochastic Gradient Descent(42815): loss=2.6667334570638133\n",
      "Stochastic Gradient Descent(42816): loss=0.14773122348098805\n",
      "Stochastic Gradient Descent(42817): loss=0.5906822791425141\n",
      "Stochastic Gradient Descent(42818): loss=7.375082663718513\n",
      "Stochastic Gradient Descent(42819): loss=6.017967556390593\n",
      "Stochastic Gradient Descent(42820): loss=1.5921750747978256\n",
      "Stochastic Gradient Descent(42821): loss=5.584532202009718\n",
      "Stochastic Gradient Descent(42822): loss=12.67010145940324\n",
      "Stochastic Gradient Descent(42823): loss=4.688901103905287\n",
      "Stochastic Gradient Descent(42824): loss=0.386272622972794\n",
      "Stochastic Gradient Descent(42825): loss=0.016843968898675316\n",
      "Stochastic Gradient Descent(42826): loss=10.952379853456677\n",
      "Stochastic Gradient Descent(42827): loss=0.10469990947863474\n",
      "Stochastic Gradient Descent(42828): loss=3.3183119518294384\n",
      "Stochastic Gradient Descent(42829): loss=0.13927891258592554\n",
      "Stochastic Gradient Descent(42830): loss=0.005756314985693663\n",
      "Stochastic Gradient Descent(42831): loss=7.224148220651439\n",
      "Stochastic Gradient Descent(42832): loss=1.2589787946348854\n",
      "Stochastic Gradient Descent(42833): loss=1.252433294224367\n",
      "Stochastic Gradient Descent(42834): loss=0.0415229149517485\n",
      "Stochastic Gradient Descent(42835): loss=0.8904297231437026\n",
      "Stochastic Gradient Descent(42836): loss=1.3029060437026339\n",
      "Stochastic Gradient Descent(42837): loss=17.91573052703501\n",
      "Stochastic Gradient Descent(42838): loss=1.148774365268088\n",
      "Stochastic Gradient Descent(42839): loss=5.08456273378982\n",
      "Stochastic Gradient Descent(42840): loss=1.3573805600874653\n",
      "Stochastic Gradient Descent(42841): loss=13.671041888907613\n",
      "Stochastic Gradient Descent(42842): loss=10.737225208583597\n",
      "Stochastic Gradient Descent(42843): loss=0.9536876608979986\n",
      "Stochastic Gradient Descent(42844): loss=2.3024995250039857\n",
      "Stochastic Gradient Descent(42845): loss=1.869916429838025\n",
      "Stochastic Gradient Descent(42846): loss=0.002110348930080164\n",
      "Stochastic Gradient Descent(42847): loss=6.940823398059453\n",
      "Stochastic Gradient Descent(42848): loss=3.560839944660374\n",
      "Stochastic Gradient Descent(42849): loss=4.067306239597485\n",
      "Stochastic Gradient Descent(42850): loss=1.7326622826289693\n",
      "Stochastic Gradient Descent(42851): loss=1.0498027125229843\n",
      "Stochastic Gradient Descent(42852): loss=0.4945296886828291\n",
      "Stochastic Gradient Descent(42853): loss=2.3669547616671656\n",
      "Stochastic Gradient Descent(42854): loss=3.6065181375716073\n",
      "Stochastic Gradient Descent(42855): loss=1.7644334904172683\n",
      "Stochastic Gradient Descent(42856): loss=4.944453443132448\n",
      "Stochastic Gradient Descent(42857): loss=0.06447236557378093\n",
      "Stochastic Gradient Descent(42858): loss=6.507667434793196\n",
      "Stochastic Gradient Descent(42859): loss=0.004055111847433357\n",
      "Stochastic Gradient Descent(42860): loss=0.7193043720063554\n",
      "Stochastic Gradient Descent(42861): loss=9.288170022673718\n",
      "Stochastic Gradient Descent(42862): loss=2.468395902823264\n",
      "Stochastic Gradient Descent(42863): loss=2.1355360961788303\n",
      "Stochastic Gradient Descent(42864): loss=11.967154168196751\n",
      "Stochastic Gradient Descent(42865): loss=3.312237283886529\n",
      "Stochastic Gradient Descent(42866): loss=3.009281059712923\n",
      "Stochastic Gradient Descent(42867): loss=6.560034755534825\n",
      "Stochastic Gradient Descent(42868): loss=1.4884079587901793\n",
      "Stochastic Gradient Descent(42869): loss=0.007954213634294167\n",
      "Stochastic Gradient Descent(42870): loss=0.15263431615587889\n",
      "Stochastic Gradient Descent(42871): loss=0.2779288249524445\n",
      "Stochastic Gradient Descent(42872): loss=24.27241355559182\n",
      "Stochastic Gradient Descent(42873): loss=1.7794417311928443\n",
      "Stochastic Gradient Descent(42874): loss=27.6151453543513\n",
      "Stochastic Gradient Descent(42875): loss=0.557638752173688\n",
      "Stochastic Gradient Descent(42876): loss=0.9461262560043179\n",
      "Stochastic Gradient Descent(42877): loss=2.5071118949144786\n",
      "Stochastic Gradient Descent(42878): loss=8.110890174210297\n",
      "Stochastic Gradient Descent(42879): loss=4.577322159358986\n",
      "Stochastic Gradient Descent(42880): loss=0.3689569564679302\n",
      "Stochastic Gradient Descent(42881): loss=1.2174523447337835\n",
      "Stochastic Gradient Descent(42882): loss=1.564940516756413\n",
      "Stochastic Gradient Descent(42883): loss=0.8551699482509075\n",
      "Stochastic Gradient Descent(42884): loss=0.8310096286713222\n",
      "Stochastic Gradient Descent(42885): loss=3.117309418544018\n",
      "Stochastic Gradient Descent(42886): loss=0.18835935811714422\n",
      "Stochastic Gradient Descent(42887): loss=0.0015239491635416387\n",
      "Stochastic Gradient Descent(42888): loss=25.04031012810872\n",
      "Stochastic Gradient Descent(42889): loss=6.0309146150793715\n",
      "Stochastic Gradient Descent(42890): loss=2.540377856193999\n",
      "Stochastic Gradient Descent(42891): loss=18.53124565463751\n",
      "Stochastic Gradient Descent(42892): loss=0.0006775029186200414\n",
      "Stochastic Gradient Descent(42893): loss=1.3278289540471193\n",
      "Stochastic Gradient Descent(42894): loss=0.40305628573070224\n",
      "Stochastic Gradient Descent(42895): loss=41.962020986503184\n",
      "Stochastic Gradient Descent(42896): loss=2.631245024038313\n",
      "Stochastic Gradient Descent(42897): loss=0.13145161680753187\n",
      "Stochastic Gradient Descent(42898): loss=0.6095785165084778\n",
      "Stochastic Gradient Descent(42899): loss=1.9304292945745014\n",
      "Stochastic Gradient Descent(42900): loss=0.0008992021501446178\n",
      "Stochastic Gradient Descent(42901): loss=0.1198103330638678\n",
      "Stochastic Gradient Descent(42902): loss=18.311971436114394\n",
      "Stochastic Gradient Descent(42903): loss=0.008036051251662497\n",
      "Stochastic Gradient Descent(42904): loss=0.18701721693708176\n",
      "Stochastic Gradient Descent(42905): loss=2.128256339876135\n",
      "Stochastic Gradient Descent(42906): loss=3.007222702970629\n",
      "Stochastic Gradient Descent(42907): loss=1.6727183404079422\n",
      "Stochastic Gradient Descent(42908): loss=0.28541813070912664\n",
      "Stochastic Gradient Descent(42909): loss=0.23026815266147396\n",
      "Stochastic Gradient Descent(42910): loss=0.07252188228569448\n",
      "Stochastic Gradient Descent(42911): loss=2.1148416043550062\n",
      "Stochastic Gradient Descent(42912): loss=0.816222935266487\n",
      "Stochastic Gradient Descent(42913): loss=9.891745756177501\n",
      "Stochastic Gradient Descent(42914): loss=0.9862242409976958\n",
      "Stochastic Gradient Descent(42915): loss=9.673995114268996\n",
      "Stochastic Gradient Descent(42916): loss=30.287350046291703\n",
      "Stochastic Gradient Descent(42917): loss=1.3188542355902562\n",
      "Stochastic Gradient Descent(42918): loss=0.0004685959145863355\n",
      "Stochastic Gradient Descent(42919): loss=2.2587765651489877\n",
      "Stochastic Gradient Descent(42920): loss=72.87057106927433\n",
      "Stochastic Gradient Descent(42921): loss=0.058550398425737255\n",
      "Stochastic Gradient Descent(42922): loss=2.1185655533521954\n",
      "Stochastic Gradient Descent(42923): loss=0.3308514138622397\n",
      "Stochastic Gradient Descent(42924): loss=5.7332095054102465\n",
      "Stochastic Gradient Descent(42925): loss=1.7502865106809407\n",
      "Stochastic Gradient Descent(42926): loss=3.90293764659464\n",
      "Stochastic Gradient Descent(42927): loss=1.530326880459585\n",
      "Stochastic Gradient Descent(42928): loss=8.388483936006113\n",
      "Stochastic Gradient Descent(42929): loss=1.4053629729129455\n",
      "Stochastic Gradient Descent(42930): loss=0.6961441099790882\n",
      "Stochastic Gradient Descent(42931): loss=3.8175799635946372\n",
      "Stochastic Gradient Descent(42932): loss=0.055248167899528036\n",
      "Stochastic Gradient Descent(42933): loss=0.3109082076152741\n",
      "Stochastic Gradient Descent(42934): loss=1.1201831328119332\n",
      "Stochastic Gradient Descent(42935): loss=0.03978855823577127\n",
      "Stochastic Gradient Descent(42936): loss=0.07421300295289208\n",
      "Stochastic Gradient Descent(42937): loss=0.4966903276991649\n",
      "Stochastic Gradient Descent(42938): loss=2.628353604538795\n",
      "Stochastic Gradient Descent(42939): loss=4.175373161448885\n",
      "Stochastic Gradient Descent(42940): loss=0.5282383913419668\n",
      "Stochastic Gradient Descent(42941): loss=0.2281800264153429\n",
      "Stochastic Gradient Descent(42942): loss=0.5903913139415098\n",
      "Stochastic Gradient Descent(42943): loss=1.2781334366199708\n",
      "Stochastic Gradient Descent(42944): loss=0.317195063992989\n",
      "Stochastic Gradient Descent(42945): loss=0.1812136169492718\n",
      "Stochastic Gradient Descent(42946): loss=1.3168600621217676\n",
      "Stochastic Gradient Descent(42947): loss=0.606579483114721\n",
      "Stochastic Gradient Descent(42948): loss=4.982327619514607\n",
      "Stochastic Gradient Descent(42949): loss=0.03281560262160344\n",
      "Stochastic Gradient Descent(42950): loss=2.854819364332344\n",
      "Stochastic Gradient Descent(42951): loss=6.011426939387748\n",
      "Stochastic Gradient Descent(42952): loss=0.07140151972276927\n",
      "Stochastic Gradient Descent(42953): loss=1.331221121732061\n",
      "Stochastic Gradient Descent(42954): loss=4.00587487933342\n",
      "Stochastic Gradient Descent(42955): loss=0.21364453651479948\n",
      "Stochastic Gradient Descent(42956): loss=2.191656509484063\n",
      "Stochastic Gradient Descent(42957): loss=0.7774017511025376\n",
      "Stochastic Gradient Descent(42958): loss=8.428479678576588\n",
      "Stochastic Gradient Descent(42959): loss=0.7711045609725602\n",
      "Stochastic Gradient Descent(42960): loss=3.375915893874093\n",
      "Stochastic Gradient Descent(42961): loss=0.22299396731472768\n",
      "Stochastic Gradient Descent(42962): loss=4.812821289481\n",
      "Stochastic Gradient Descent(42963): loss=20.032267206936172\n",
      "Stochastic Gradient Descent(42964): loss=4.761283123445501\n",
      "Stochastic Gradient Descent(42965): loss=0.3160013933897843\n",
      "Stochastic Gradient Descent(42966): loss=0.1027023774851323\n",
      "Stochastic Gradient Descent(42967): loss=3.3229653255137968\n",
      "Stochastic Gradient Descent(42968): loss=2.961120992876649\n",
      "Stochastic Gradient Descent(42969): loss=0.29483946463542837\n",
      "Stochastic Gradient Descent(42970): loss=3.1249541005888313\n",
      "Stochastic Gradient Descent(42971): loss=0.9780158280484271\n",
      "Stochastic Gradient Descent(42972): loss=3.1402596857379654\n",
      "Stochastic Gradient Descent(42973): loss=0.5004181109113957\n",
      "Stochastic Gradient Descent(42974): loss=5.233195920566193\n",
      "Stochastic Gradient Descent(42975): loss=0.959164754307774\n",
      "Stochastic Gradient Descent(42976): loss=26.481534028520787\n",
      "Stochastic Gradient Descent(42977): loss=3.917747373194156\n",
      "Stochastic Gradient Descent(42978): loss=0.2576145884201423\n",
      "Stochastic Gradient Descent(42979): loss=1.8336990277461704\n",
      "Stochastic Gradient Descent(42980): loss=0.1471083189492091\n",
      "Stochastic Gradient Descent(42981): loss=3.025749843054508\n",
      "Stochastic Gradient Descent(42982): loss=1.5886640628169184\n",
      "Stochastic Gradient Descent(42983): loss=0.17898204824001335\n",
      "Stochastic Gradient Descent(42984): loss=8.689052936475791\n",
      "Stochastic Gradient Descent(42985): loss=0.6575230110530501\n",
      "Stochastic Gradient Descent(42986): loss=1.3422994017584893\n",
      "Stochastic Gradient Descent(42987): loss=11.563802256875857\n",
      "Stochastic Gradient Descent(42988): loss=2.7814915258334065\n",
      "Stochastic Gradient Descent(42989): loss=6.80621745378358\n",
      "Stochastic Gradient Descent(42990): loss=5.878216087011922\n",
      "Stochastic Gradient Descent(42991): loss=0.29868737869301976\n",
      "Stochastic Gradient Descent(42992): loss=2.085549780969338\n",
      "Stochastic Gradient Descent(42993): loss=1.643734831923297\n",
      "Stochastic Gradient Descent(42994): loss=17.20633567108033\n",
      "Stochastic Gradient Descent(42995): loss=0.12458976961628637\n",
      "Stochastic Gradient Descent(42996): loss=2.6469710375844766\n",
      "Stochastic Gradient Descent(42997): loss=0.12680352853854587\n",
      "Stochastic Gradient Descent(42998): loss=0.23276070144087377\n",
      "Stochastic Gradient Descent(42999): loss=11.986702637130815\n",
      "Stochastic Gradient Descent(43000): loss=0.03486889073789373\n",
      "Stochastic Gradient Descent(43001): loss=2.4159418788272067\n",
      "Stochastic Gradient Descent(43002): loss=0.021025145813927233\n",
      "Stochastic Gradient Descent(43003): loss=0.8690294715130199\n",
      "Stochastic Gradient Descent(43004): loss=0.5342236834666966\n",
      "Stochastic Gradient Descent(43005): loss=14.70642824841805\n",
      "Stochastic Gradient Descent(43006): loss=8.19937955971152\n",
      "Stochastic Gradient Descent(43007): loss=0.2767454285529491\n",
      "Stochastic Gradient Descent(43008): loss=3.3666686552789513\n",
      "Stochastic Gradient Descent(43009): loss=7.279578518963408\n",
      "Stochastic Gradient Descent(43010): loss=1.7305477817470283\n",
      "Stochastic Gradient Descent(43011): loss=0.06393643036948388\n",
      "Stochastic Gradient Descent(43012): loss=0.8945975417183902\n",
      "Stochastic Gradient Descent(43013): loss=1.8400606925830088\n",
      "Stochastic Gradient Descent(43014): loss=5.133732466062553\n",
      "Stochastic Gradient Descent(43015): loss=0.21476050676180697\n",
      "Stochastic Gradient Descent(43016): loss=7.49539245125439\n",
      "Stochastic Gradient Descent(43017): loss=7.877320166942809\n",
      "Stochastic Gradient Descent(43018): loss=0.9529326576038069\n",
      "Stochastic Gradient Descent(43019): loss=0.00014628102668938546\n",
      "Stochastic Gradient Descent(43020): loss=4.814599293924324\n",
      "Stochastic Gradient Descent(43021): loss=2.6634513432973694\n",
      "Stochastic Gradient Descent(43022): loss=4.970681622442992\n",
      "Stochastic Gradient Descent(43023): loss=4.2797168691501515\n",
      "Stochastic Gradient Descent(43024): loss=5.411554752830565\n",
      "Stochastic Gradient Descent(43025): loss=0.39548831175358345\n",
      "Stochastic Gradient Descent(43026): loss=5.547612228421001\n",
      "Stochastic Gradient Descent(43027): loss=0.8690622180700549\n",
      "Stochastic Gradient Descent(43028): loss=1.392831565817556\n",
      "Stochastic Gradient Descent(43029): loss=8.831404026860813\n",
      "Stochastic Gradient Descent(43030): loss=3.054024936873494\n",
      "Stochastic Gradient Descent(43031): loss=0.9976766186459044\n",
      "Stochastic Gradient Descent(43032): loss=8.354276851892624\n",
      "Stochastic Gradient Descent(43033): loss=0.0089199725199182\n",
      "Stochastic Gradient Descent(43034): loss=1.5518615638264426\n",
      "Stochastic Gradient Descent(43035): loss=8.324698371740023\n",
      "Stochastic Gradient Descent(43036): loss=0.39029038858549403\n",
      "Stochastic Gradient Descent(43037): loss=9.235443281443098\n",
      "Stochastic Gradient Descent(43038): loss=0.056846178262046215\n",
      "Stochastic Gradient Descent(43039): loss=0.4179849844447566\n",
      "Stochastic Gradient Descent(43040): loss=1.256317084487663\n",
      "Stochastic Gradient Descent(43041): loss=6.811778740881653\n",
      "Stochastic Gradient Descent(43042): loss=0.10672602355141023\n",
      "Stochastic Gradient Descent(43043): loss=0.13612644279936653\n",
      "Stochastic Gradient Descent(43044): loss=1.6947771564868013\n",
      "Stochastic Gradient Descent(43045): loss=12.971068858688328\n",
      "Stochastic Gradient Descent(43046): loss=0.013394937731857475\n",
      "Stochastic Gradient Descent(43047): loss=0.010460596417212562\n",
      "Stochastic Gradient Descent(43048): loss=0.7009346150897152\n",
      "Stochastic Gradient Descent(43049): loss=0.465154960921904\n",
      "Stochastic Gradient Descent(43050): loss=0.7193220682770524\n",
      "Stochastic Gradient Descent(43051): loss=1.4329560285393927\n",
      "Stochastic Gradient Descent(43052): loss=2.5523227513340445\n",
      "Stochastic Gradient Descent(43053): loss=0.8764433581393503\n",
      "Stochastic Gradient Descent(43054): loss=1.3375955573905005\n",
      "Stochastic Gradient Descent(43055): loss=0.03154007702010485\n",
      "Stochastic Gradient Descent(43056): loss=0.037325192561207546\n",
      "Stochastic Gradient Descent(43057): loss=1.125093527313327\n",
      "Stochastic Gradient Descent(43058): loss=0.7307297713481443\n",
      "Stochastic Gradient Descent(43059): loss=5.255303128371284\n",
      "Stochastic Gradient Descent(43060): loss=0.06941631990903809\n",
      "Stochastic Gradient Descent(43061): loss=4.49924933901476\n",
      "Stochastic Gradient Descent(43062): loss=1.2330533049085802e-05\n",
      "Stochastic Gradient Descent(43063): loss=0.6392428665766696\n",
      "Stochastic Gradient Descent(43064): loss=5.945307888807868\n",
      "Stochastic Gradient Descent(43065): loss=2.4114616386807493\n",
      "Stochastic Gradient Descent(43066): loss=0.12884269470700413\n",
      "Stochastic Gradient Descent(43067): loss=0.13024287075227214\n",
      "Stochastic Gradient Descent(43068): loss=1.2215171498836355\n",
      "Stochastic Gradient Descent(43069): loss=0.11425058481703004\n",
      "Stochastic Gradient Descent(43070): loss=0.4185081279638707\n",
      "Stochastic Gradient Descent(43071): loss=1.752200626195125\n",
      "Stochastic Gradient Descent(43072): loss=0.14058450961385247\n",
      "Stochastic Gradient Descent(43073): loss=1.6955451153185988\n",
      "Stochastic Gradient Descent(43074): loss=3.9273090836326365\n",
      "Stochastic Gradient Descent(43075): loss=5.457198822971504\n",
      "Stochastic Gradient Descent(43076): loss=18.635985190233047\n",
      "Stochastic Gradient Descent(43077): loss=0.0003415897038879746\n",
      "Stochastic Gradient Descent(43078): loss=5.071918007752626\n",
      "Stochastic Gradient Descent(43079): loss=0.011339753426238618\n",
      "Stochastic Gradient Descent(43080): loss=8.101665608988501\n",
      "Stochastic Gradient Descent(43081): loss=0.9210970506981526\n",
      "Stochastic Gradient Descent(43082): loss=1.4150891895064672\n",
      "Stochastic Gradient Descent(43083): loss=0.6598143235111643\n",
      "Stochastic Gradient Descent(43084): loss=8.043408859177287\n",
      "Stochastic Gradient Descent(43085): loss=0.012854459448806026\n",
      "Stochastic Gradient Descent(43086): loss=3.965811307578136\n",
      "Stochastic Gradient Descent(43087): loss=8.463377778716353\n",
      "Stochastic Gradient Descent(43088): loss=2.9974803458892016\n",
      "Stochastic Gradient Descent(43089): loss=2.899462876517714\n",
      "Stochastic Gradient Descent(43090): loss=0.0005149708235590258\n",
      "Stochastic Gradient Descent(43091): loss=2.941217192014614\n",
      "Stochastic Gradient Descent(43092): loss=14.70086323632274\n",
      "Stochastic Gradient Descent(43093): loss=13.170098343784222\n",
      "Stochastic Gradient Descent(43094): loss=11.233923469127351\n",
      "Stochastic Gradient Descent(43095): loss=5.0067224201532875e-05\n",
      "Stochastic Gradient Descent(43096): loss=0.03206646178948316\n",
      "Stochastic Gradient Descent(43097): loss=0.002225033111731868\n",
      "Stochastic Gradient Descent(43098): loss=3.269614969817011\n",
      "Stochastic Gradient Descent(43099): loss=10.160644662194613\n",
      "Stochastic Gradient Descent(43100): loss=3.777791948766863\n",
      "Stochastic Gradient Descent(43101): loss=19.999616204340867\n",
      "Stochastic Gradient Descent(43102): loss=14.804166048657107\n",
      "Stochastic Gradient Descent(43103): loss=4.186499492554756\n",
      "Stochastic Gradient Descent(43104): loss=9.503937491891756\n",
      "Stochastic Gradient Descent(43105): loss=1.0620680583019422\n",
      "Stochastic Gradient Descent(43106): loss=0.09130135839803953\n",
      "Stochastic Gradient Descent(43107): loss=0.14813451758075866\n",
      "Stochastic Gradient Descent(43108): loss=0.15121577336949696\n",
      "Stochastic Gradient Descent(43109): loss=0.431097114871564\n",
      "Stochastic Gradient Descent(43110): loss=2.844168479814493\n",
      "Stochastic Gradient Descent(43111): loss=2.6218661704157946\n",
      "Stochastic Gradient Descent(43112): loss=6.106921263942556\n",
      "Stochastic Gradient Descent(43113): loss=1.5072635620765709\n",
      "Stochastic Gradient Descent(43114): loss=5.14964205923951\n",
      "Stochastic Gradient Descent(43115): loss=0.588566142852652\n",
      "Stochastic Gradient Descent(43116): loss=4.62238806486785\n",
      "Stochastic Gradient Descent(43117): loss=8.785970049628348\n",
      "Stochastic Gradient Descent(43118): loss=3.987600864444741\n",
      "Stochastic Gradient Descent(43119): loss=1.215099616969509\n",
      "Stochastic Gradient Descent(43120): loss=0.26548275262651616\n",
      "Stochastic Gradient Descent(43121): loss=0.4745117043350092\n",
      "Stochastic Gradient Descent(43122): loss=0.0006603105562198715\n",
      "Stochastic Gradient Descent(43123): loss=9.884647478225299\n",
      "Stochastic Gradient Descent(43124): loss=1.1098924964546644\n",
      "Stochastic Gradient Descent(43125): loss=17.999247373613805\n",
      "Stochastic Gradient Descent(43126): loss=0.8709185323207461\n",
      "Stochastic Gradient Descent(43127): loss=5.28212159646253\n",
      "Stochastic Gradient Descent(43128): loss=4.913682844304889\n",
      "Stochastic Gradient Descent(43129): loss=1.3772099678610388\n",
      "Stochastic Gradient Descent(43130): loss=0.3075604387935848\n",
      "Stochastic Gradient Descent(43131): loss=2.2637503861544066\n",
      "Stochastic Gradient Descent(43132): loss=5.9484301872863075\n",
      "Stochastic Gradient Descent(43133): loss=5.316424662534399\n",
      "Stochastic Gradient Descent(43134): loss=0.8422218540967981\n",
      "Stochastic Gradient Descent(43135): loss=4.083336739897085\n",
      "Stochastic Gradient Descent(43136): loss=0.44090592556485686\n",
      "Stochastic Gradient Descent(43137): loss=0.013978951788600516\n",
      "Stochastic Gradient Descent(43138): loss=0.26459847662357044\n",
      "Stochastic Gradient Descent(43139): loss=1.1535132142423328\n",
      "Stochastic Gradient Descent(43140): loss=0.15696963437088024\n",
      "Stochastic Gradient Descent(43141): loss=0.9120065248936658\n",
      "Stochastic Gradient Descent(43142): loss=13.6281046211944\n",
      "Stochastic Gradient Descent(43143): loss=2.3805032517366964\n",
      "Stochastic Gradient Descent(43144): loss=2.1320571100519943\n",
      "Stochastic Gradient Descent(43145): loss=0.21842186792665555\n",
      "Stochastic Gradient Descent(43146): loss=2.913070707674725\n",
      "Stochastic Gradient Descent(43147): loss=0.36939495227773356\n",
      "Stochastic Gradient Descent(43148): loss=1.2318618396719918\n",
      "Stochastic Gradient Descent(43149): loss=0.009189741943843751\n",
      "Stochastic Gradient Descent(43150): loss=9.703251157654446\n",
      "Stochastic Gradient Descent(43151): loss=0.055713488859980405\n",
      "Stochastic Gradient Descent(43152): loss=8.224516579618939\n",
      "Stochastic Gradient Descent(43153): loss=0.16859666309649368\n",
      "Stochastic Gradient Descent(43154): loss=12.126073913185891\n",
      "Stochastic Gradient Descent(43155): loss=12.31281330248048\n",
      "Stochastic Gradient Descent(43156): loss=0.2582423606100786\n",
      "Stochastic Gradient Descent(43157): loss=2.4190941606147454\n",
      "Stochastic Gradient Descent(43158): loss=9.488944716434057\n",
      "Stochastic Gradient Descent(43159): loss=9.958772063241867\n",
      "Stochastic Gradient Descent(43160): loss=0.054333541422768444\n",
      "Stochastic Gradient Descent(43161): loss=2.986672047343114\n",
      "Stochastic Gradient Descent(43162): loss=8.47699293328616\n",
      "Stochastic Gradient Descent(43163): loss=3.497820640828804\n",
      "Stochastic Gradient Descent(43164): loss=1.2890242981601485\n",
      "Stochastic Gradient Descent(43165): loss=2.910449713832241\n",
      "Stochastic Gradient Descent(43166): loss=0.07856621230206924\n",
      "Stochastic Gradient Descent(43167): loss=3.6839779747680157\n",
      "Stochastic Gradient Descent(43168): loss=0.07331749570731726\n",
      "Stochastic Gradient Descent(43169): loss=4.63531120959903\n",
      "Stochastic Gradient Descent(43170): loss=0.0003092936752072982\n",
      "Stochastic Gradient Descent(43171): loss=7.617394411775411\n",
      "Stochastic Gradient Descent(43172): loss=9.403518236223698\n",
      "Stochastic Gradient Descent(43173): loss=16.523000399922573\n",
      "Stochastic Gradient Descent(43174): loss=3.4106387208302658\n",
      "Stochastic Gradient Descent(43175): loss=0.8107710711085431\n",
      "Stochastic Gradient Descent(43176): loss=1.4593448793372057\n",
      "Stochastic Gradient Descent(43177): loss=0.981733707815784\n",
      "Stochastic Gradient Descent(43178): loss=4.9858460723194975\n",
      "Stochastic Gradient Descent(43179): loss=0.6596253686966453\n",
      "Stochastic Gradient Descent(43180): loss=0.0795066649449478\n",
      "Stochastic Gradient Descent(43181): loss=0.20352310048458946\n",
      "Stochastic Gradient Descent(43182): loss=0.753351900350346\n",
      "Stochastic Gradient Descent(43183): loss=1.197684404210924\n",
      "Stochastic Gradient Descent(43184): loss=3.91528289644776\n",
      "Stochastic Gradient Descent(43185): loss=0.1986094222325911\n",
      "Stochastic Gradient Descent(43186): loss=0.0678579188318001\n",
      "Stochastic Gradient Descent(43187): loss=3.5759583855098858\n",
      "Stochastic Gradient Descent(43188): loss=7.194620486753149\n",
      "Stochastic Gradient Descent(43189): loss=0.0009176688703896442\n",
      "Stochastic Gradient Descent(43190): loss=0.0001153507464791473\n",
      "Stochastic Gradient Descent(43191): loss=0.0027231312244080648\n",
      "Stochastic Gradient Descent(43192): loss=0.5100382526648836\n",
      "Stochastic Gradient Descent(43193): loss=2.4268149893820614\n",
      "Stochastic Gradient Descent(43194): loss=25.992815300667715\n",
      "Stochastic Gradient Descent(43195): loss=0.4630251522030159\n",
      "Stochastic Gradient Descent(43196): loss=0.028502551964890755\n",
      "Stochastic Gradient Descent(43197): loss=50.948255563627924\n",
      "Stochastic Gradient Descent(43198): loss=0.004241951153054364\n",
      "Stochastic Gradient Descent(43199): loss=6.279222666960843\n",
      "Stochastic Gradient Descent(43200): loss=0.07256168646718511\n",
      "Stochastic Gradient Descent(43201): loss=0.11946771587014318\n",
      "Stochastic Gradient Descent(43202): loss=0.04172935246269884\n",
      "Stochastic Gradient Descent(43203): loss=0.5573823601113425\n",
      "Stochastic Gradient Descent(43204): loss=3.5563675636885503\n",
      "Stochastic Gradient Descent(43205): loss=4.4449430355073085\n",
      "Stochastic Gradient Descent(43206): loss=3.93109371023545\n",
      "Stochastic Gradient Descent(43207): loss=0.5922418140365032\n",
      "Stochastic Gradient Descent(43208): loss=7.25474165425299\n",
      "Stochastic Gradient Descent(43209): loss=3.8702657874522277\n",
      "Stochastic Gradient Descent(43210): loss=14.21404989862191\n",
      "Stochastic Gradient Descent(43211): loss=0.03337551411677901\n",
      "Stochastic Gradient Descent(43212): loss=0.036714735400116674\n",
      "Stochastic Gradient Descent(43213): loss=4.0115642407291014\n",
      "Stochastic Gradient Descent(43214): loss=7.284237732309674\n",
      "Stochastic Gradient Descent(43215): loss=6.814799444652943\n",
      "Stochastic Gradient Descent(43216): loss=9.062263117735306e-06\n",
      "Stochastic Gradient Descent(43217): loss=8.590164962977957\n",
      "Stochastic Gradient Descent(43218): loss=1.5546470082403527\n",
      "Stochastic Gradient Descent(43219): loss=3.863824137377753\n",
      "Stochastic Gradient Descent(43220): loss=0.23294401306742077\n",
      "Stochastic Gradient Descent(43221): loss=2.6392860201066535\n",
      "Stochastic Gradient Descent(43222): loss=0.02124966582816279\n",
      "Stochastic Gradient Descent(43223): loss=4.924060336039709\n",
      "Stochastic Gradient Descent(43224): loss=0.5464102784675775\n",
      "Stochastic Gradient Descent(43225): loss=2.7389248471814693\n",
      "Stochastic Gradient Descent(43226): loss=0.08653108974413705\n",
      "Stochastic Gradient Descent(43227): loss=0.09638414987933161\n",
      "Stochastic Gradient Descent(43228): loss=10.531533254357056\n",
      "Stochastic Gradient Descent(43229): loss=3.639150968680027\n",
      "Stochastic Gradient Descent(43230): loss=4.354836260545403\n",
      "Stochastic Gradient Descent(43231): loss=2.208588926201634\n",
      "Stochastic Gradient Descent(43232): loss=0.1815418167311318\n",
      "Stochastic Gradient Descent(43233): loss=3.4241958956475793\n",
      "Stochastic Gradient Descent(43234): loss=4.1673820705804285\n",
      "Stochastic Gradient Descent(43235): loss=3.031373522289717\n",
      "Stochastic Gradient Descent(43236): loss=1.8751148996797986\n",
      "Stochastic Gradient Descent(43237): loss=0.47209176692557275\n",
      "Stochastic Gradient Descent(43238): loss=0.0008115725800267373\n",
      "Stochastic Gradient Descent(43239): loss=0.3724575563460658\n",
      "Stochastic Gradient Descent(43240): loss=0.40072451877630716\n",
      "Stochastic Gradient Descent(43241): loss=19.470283794961873\n",
      "Stochastic Gradient Descent(43242): loss=3.1875693124524993\n",
      "Stochastic Gradient Descent(43243): loss=0.994534770789784\n",
      "Stochastic Gradient Descent(43244): loss=20.536143943577898\n",
      "Stochastic Gradient Descent(43245): loss=0.30064088341747464\n",
      "Stochastic Gradient Descent(43246): loss=0.08107683121714102\n",
      "Stochastic Gradient Descent(43247): loss=0.0014151530743435235\n",
      "Stochastic Gradient Descent(43248): loss=1.4726509434997794\n",
      "Stochastic Gradient Descent(43249): loss=4.378311299503441\n",
      "Stochastic Gradient Descent(43250): loss=3.582533063405449\n",
      "Stochastic Gradient Descent(43251): loss=0.04295290010668359\n",
      "Stochastic Gradient Descent(43252): loss=0.33595046490544095\n",
      "Stochastic Gradient Descent(43253): loss=2.1891312237753184\n",
      "Stochastic Gradient Descent(43254): loss=9.942087130614958\n",
      "Stochastic Gradient Descent(43255): loss=2.4699399462608866\n",
      "Stochastic Gradient Descent(43256): loss=28.754308226451634\n",
      "Stochastic Gradient Descent(43257): loss=30.557299168796675\n",
      "Stochastic Gradient Descent(43258): loss=1.2815227489852368\n",
      "Stochastic Gradient Descent(43259): loss=0.4090696630045066\n",
      "Stochastic Gradient Descent(43260): loss=6.6084688602288955\n",
      "Stochastic Gradient Descent(43261): loss=0.006701552903175421\n",
      "Stochastic Gradient Descent(43262): loss=2.4346212248571755\n",
      "Stochastic Gradient Descent(43263): loss=1.122196113081131\n",
      "Stochastic Gradient Descent(43264): loss=1.5672667431792882\n",
      "Stochastic Gradient Descent(43265): loss=12.545616139687139\n",
      "Stochastic Gradient Descent(43266): loss=5.437600191245353\n",
      "Stochastic Gradient Descent(43267): loss=0.5870994366413255\n",
      "Stochastic Gradient Descent(43268): loss=14.713551959411618\n",
      "Stochastic Gradient Descent(43269): loss=0.2725086367651054\n",
      "Stochastic Gradient Descent(43270): loss=7.319843185183284\n",
      "Stochastic Gradient Descent(43271): loss=3.4044991967254354\n",
      "Stochastic Gradient Descent(43272): loss=0.22578700828359727\n",
      "Stochastic Gradient Descent(43273): loss=0.8810749973763596\n",
      "Stochastic Gradient Descent(43274): loss=0.05295178585569434\n",
      "Stochastic Gradient Descent(43275): loss=0.6231261409737814\n",
      "Stochastic Gradient Descent(43276): loss=2.148088849720007\n",
      "Stochastic Gradient Descent(43277): loss=10.3600880429807\n",
      "Stochastic Gradient Descent(43278): loss=1.6308178400204734\n",
      "Stochastic Gradient Descent(43279): loss=0.4698903840870719\n",
      "Stochastic Gradient Descent(43280): loss=4.570466576194594\n",
      "Stochastic Gradient Descent(43281): loss=5.409498744548117\n",
      "Stochastic Gradient Descent(43282): loss=0.8334019943861992\n",
      "Stochastic Gradient Descent(43283): loss=0.1854752098190177\n",
      "Stochastic Gradient Descent(43284): loss=0.026868015646627195\n",
      "Stochastic Gradient Descent(43285): loss=0.07773098339273783\n",
      "Stochastic Gradient Descent(43286): loss=23.65340304186404\n",
      "Stochastic Gradient Descent(43287): loss=17.537776660884223\n",
      "Stochastic Gradient Descent(43288): loss=0.0401050600096196\n",
      "Stochastic Gradient Descent(43289): loss=0.12239043228710818\n",
      "Stochastic Gradient Descent(43290): loss=2.785558963907216\n",
      "Stochastic Gradient Descent(43291): loss=1.3385478148809369\n",
      "Stochastic Gradient Descent(43292): loss=9.397772317389414\n",
      "Stochastic Gradient Descent(43293): loss=0.24417213769421758\n",
      "Stochastic Gradient Descent(43294): loss=2.582801283338608\n",
      "Stochastic Gradient Descent(43295): loss=3.7057328556349898\n",
      "Stochastic Gradient Descent(43296): loss=14.374831461531633\n",
      "Stochastic Gradient Descent(43297): loss=0.9423937051118896\n",
      "Stochastic Gradient Descent(43298): loss=1.8505300248587904\n",
      "Stochastic Gradient Descent(43299): loss=0.09938975425421968\n",
      "Stochastic Gradient Descent(43300): loss=0.22447667773535313\n",
      "Stochastic Gradient Descent(43301): loss=4.673036817987041\n",
      "Stochastic Gradient Descent(43302): loss=1.0158934512185889\n",
      "Stochastic Gradient Descent(43303): loss=10.769283818452827\n",
      "Stochastic Gradient Descent(43304): loss=13.611584141715841\n",
      "Stochastic Gradient Descent(43305): loss=1.3449570553589936\n",
      "Stochastic Gradient Descent(43306): loss=1.1235855279547748\n",
      "Stochastic Gradient Descent(43307): loss=0.17496390279138282\n",
      "Stochastic Gradient Descent(43308): loss=1.1985608747760248\n",
      "Stochastic Gradient Descent(43309): loss=2.7730806450819183\n",
      "Stochastic Gradient Descent(43310): loss=7.455190850515841\n",
      "Stochastic Gradient Descent(43311): loss=0.0368454933818462\n",
      "Stochastic Gradient Descent(43312): loss=0.24708266504322907\n",
      "Stochastic Gradient Descent(43313): loss=0.1420902933088691\n",
      "Stochastic Gradient Descent(43314): loss=10.253118777237235\n",
      "Stochastic Gradient Descent(43315): loss=0.02264625799610326\n",
      "Stochastic Gradient Descent(43316): loss=4.305515505045607\n",
      "Stochastic Gradient Descent(43317): loss=0.05366340771346152\n",
      "Stochastic Gradient Descent(43318): loss=3.6808488695283996\n",
      "Stochastic Gradient Descent(43319): loss=0.19911401335018764\n",
      "Stochastic Gradient Descent(43320): loss=2.420707420432912\n",
      "Stochastic Gradient Descent(43321): loss=1.0205027595424134\n",
      "Stochastic Gradient Descent(43322): loss=2.745399402649931\n",
      "Stochastic Gradient Descent(43323): loss=2.4630270557852247\n",
      "Stochastic Gradient Descent(43324): loss=3.127831908355556\n",
      "Stochastic Gradient Descent(43325): loss=6.939640633141779\n",
      "Stochastic Gradient Descent(43326): loss=0.0013307300174419288\n",
      "Stochastic Gradient Descent(43327): loss=27.611001403062588\n",
      "Stochastic Gradient Descent(43328): loss=0.6612454931740782\n",
      "Stochastic Gradient Descent(43329): loss=2.7110566289804083\n",
      "Stochastic Gradient Descent(43330): loss=1.5558627421343518\n",
      "Stochastic Gradient Descent(43331): loss=5.504958785999973\n",
      "Stochastic Gradient Descent(43332): loss=10.649352697686073\n",
      "Stochastic Gradient Descent(43333): loss=0.17276575399123834\n",
      "Stochastic Gradient Descent(43334): loss=0.6040132178951996\n",
      "Stochastic Gradient Descent(43335): loss=0.3681221081470773\n",
      "Stochastic Gradient Descent(43336): loss=0.0014727986947212985\n",
      "Stochastic Gradient Descent(43337): loss=11.278512651414504\n",
      "Stochastic Gradient Descent(43338): loss=0.09032165885902176\n",
      "Stochastic Gradient Descent(43339): loss=0.00026021002512232856\n",
      "Stochastic Gradient Descent(43340): loss=0.6303161842241226\n",
      "Stochastic Gradient Descent(43341): loss=1.9511069406430173\n",
      "Stochastic Gradient Descent(43342): loss=0.09251515748137566\n",
      "Stochastic Gradient Descent(43343): loss=1.4335193220601616\n",
      "Stochastic Gradient Descent(43344): loss=4.77819421386656\n",
      "Stochastic Gradient Descent(43345): loss=10.78765834120126\n",
      "Stochastic Gradient Descent(43346): loss=2.366093689679461\n",
      "Stochastic Gradient Descent(43347): loss=0.8200733734295561\n",
      "Stochastic Gradient Descent(43348): loss=17.832704885004425\n",
      "Stochastic Gradient Descent(43349): loss=0.2324736076783031\n",
      "Stochastic Gradient Descent(43350): loss=1.4875413534041637\n",
      "Stochastic Gradient Descent(43351): loss=1.7605746465127845\n",
      "Stochastic Gradient Descent(43352): loss=5.0921661935579685\n",
      "Stochastic Gradient Descent(43353): loss=0.04616569146686312\n",
      "Stochastic Gradient Descent(43354): loss=0.37742572041883554\n",
      "Stochastic Gradient Descent(43355): loss=1.821775625569999\n",
      "Stochastic Gradient Descent(43356): loss=0.20753082583935958\n",
      "Stochastic Gradient Descent(43357): loss=4.69987340680424\n",
      "Stochastic Gradient Descent(43358): loss=7.876897002539803\n",
      "Stochastic Gradient Descent(43359): loss=2.726533649291228\n",
      "Stochastic Gradient Descent(43360): loss=3.3872388918188387\n",
      "Stochastic Gradient Descent(43361): loss=7.182858369224418\n",
      "Stochastic Gradient Descent(43362): loss=0.10501954820645583\n",
      "Stochastic Gradient Descent(43363): loss=0.9973233688058644\n",
      "Stochastic Gradient Descent(43364): loss=0.01049062336561366\n",
      "Stochastic Gradient Descent(43365): loss=2.157409393910728\n",
      "Stochastic Gradient Descent(43366): loss=9.113440523091846\n",
      "Stochastic Gradient Descent(43367): loss=5.054933639145995\n",
      "Stochastic Gradient Descent(43368): loss=10.6579415243532\n",
      "Stochastic Gradient Descent(43369): loss=0.3700354358505991\n",
      "Stochastic Gradient Descent(43370): loss=0.25299110447604034\n",
      "Stochastic Gradient Descent(43371): loss=1.93787764031516\n",
      "Stochastic Gradient Descent(43372): loss=0.00047029187089810435\n",
      "Stochastic Gradient Descent(43373): loss=1.3722412194588038\n",
      "Stochastic Gradient Descent(43374): loss=0.8209275531252301\n",
      "Stochastic Gradient Descent(43375): loss=0.8920832481691786\n",
      "Stochastic Gradient Descent(43376): loss=5.602002040827809\n",
      "Stochastic Gradient Descent(43377): loss=0.8269782915980188\n",
      "Stochastic Gradient Descent(43378): loss=20.776079938366895\n",
      "Stochastic Gradient Descent(43379): loss=8.514414509506882\n",
      "Stochastic Gradient Descent(43380): loss=1.4780864615100242\n",
      "Stochastic Gradient Descent(43381): loss=3.941513076395123\n",
      "Stochastic Gradient Descent(43382): loss=9.033108090186893\n",
      "Stochastic Gradient Descent(43383): loss=0.11020958920149453\n",
      "Stochastic Gradient Descent(43384): loss=1.728771763320005\n",
      "Stochastic Gradient Descent(43385): loss=8.919099850487894\n",
      "Stochastic Gradient Descent(43386): loss=1.2565327575521135\n",
      "Stochastic Gradient Descent(43387): loss=3.7112015316094875\n",
      "Stochastic Gradient Descent(43388): loss=4.388418383994784\n",
      "Stochastic Gradient Descent(43389): loss=13.383757493831556\n",
      "Stochastic Gradient Descent(43390): loss=0.30819351458344235\n",
      "Stochastic Gradient Descent(43391): loss=1.1735937138828165\n",
      "Stochastic Gradient Descent(43392): loss=7.828233256184679\n",
      "Stochastic Gradient Descent(43393): loss=0.5166108883666833\n",
      "Stochastic Gradient Descent(43394): loss=2.2261207979967943\n",
      "Stochastic Gradient Descent(43395): loss=17.088623500178496\n",
      "Stochastic Gradient Descent(43396): loss=0.009374602452726978\n",
      "Stochastic Gradient Descent(43397): loss=0.0013134432848596932\n",
      "Stochastic Gradient Descent(43398): loss=0.7808442640608847\n",
      "Stochastic Gradient Descent(43399): loss=0.5261047431559792\n",
      "Stochastic Gradient Descent(43400): loss=2.9482788470878787\n",
      "Stochastic Gradient Descent(43401): loss=0.11037037265749766\n",
      "Stochastic Gradient Descent(43402): loss=0.006579018621293584\n",
      "Stochastic Gradient Descent(43403): loss=26.225222123929075\n",
      "Stochastic Gradient Descent(43404): loss=31.836066050166753\n",
      "Stochastic Gradient Descent(43405): loss=1.2751684379214219\n",
      "Stochastic Gradient Descent(43406): loss=0.00094523537570871\n",
      "Stochastic Gradient Descent(43407): loss=1.2061337775693313\n",
      "Stochastic Gradient Descent(43408): loss=2.2980439019453933\n",
      "Stochastic Gradient Descent(43409): loss=10.491371323627103\n",
      "Stochastic Gradient Descent(43410): loss=1.5036917798744878\n",
      "Stochastic Gradient Descent(43411): loss=3.4199672788423867\n",
      "Stochastic Gradient Descent(43412): loss=2.6487633653111113\n",
      "Stochastic Gradient Descent(43413): loss=2.1462905672063317\n",
      "Stochastic Gradient Descent(43414): loss=7.761968540274916\n",
      "Stochastic Gradient Descent(43415): loss=12.271999139744178\n",
      "Stochastic Gradient Descent(43416): loss=5.974172223492384\n",
      "Stochastic Gradient Descent(43417): loss=3.6749526956265104\n",
      "Stochastic Gradient Descent(43418): loss=0.19943471676170657\n",
      "Stochastic Gradient Descent(43419): loss=1.6472827368792222\n",
      "Stochastic Gradient Descent(43420): loss=0.23995018221578665\n",
      "Stochastic Gradient Descent(43421): loss=2.130013766616651\n",
      "Stochastic Gradient Descent(43422): loss=3.590286144917469\n",
      "Stochastic Gradient Descent(43423): loss=0.015366197663659056\n",
      "Stochastic Gradient Descent(43424): loss=0.6921487532494925\n",
      "Stochastic Gradient Descent(43425): loss=0.051112608692486274\n",
      "Stochastic Gradient Descent(43426): loss=11.420777221667143\n",
      "Stochastic Gradient Descent(43427): loss=0.500270203743957\n",
      "Stochastic Gradient Descent(43428): loss=0.21504352700329654\n",
      "Stochastic Gradient Descent(43429): loss=6.736074287433835\n",
      "Stochastic Gradient Descent(43430): loss=11.41553731468499\n",
      "Stochastic Gradient Descent(43431): loss=1.2373731713476432\n",
      "Stochastic Gradient Descent(43432): loss=1.5518999488996206\n",
      "Stochastic Gradient Descent(43433): loss=11.199765278899868\n",
      "Stochastic Gradient Descent(43434): loss=1.2798258916781384\n",
      "Stochastic Gradient Descent(43435): loss=4.620015261237128\n",
      "Stochastic Gradient Descent(43436): loss=9.196365039740613\n",
      "Stochastic Gradient Descent(43437): loss=5.062570019882958\n",
      "Stochastic Gradient Descent(43438): loss=0.7441774134237275\n",
      "Stochastic Gradient Descent(43439): loss=3.0264808186061853\n",
      "Stochastic Gradient Descent(43440): loss=4.33038390206858\n",
      "Stochastic Gradient Descent(43441): loss=9.56425807458626\n",
      "Stochastic Gradient Descent(43442): loss=4.713340855584383\n",
      "Stochastic Gradient Descent(43443): loss=2.4713853817535494\n",
      "Stochastic Gradient Descent(43444): loss=4.590509901334439\n",
      "Stochastic Gradient Descent(43445): loss=0.19997016108479085\n",
      "Stochastic Gradient Descent(43446): loss=1.2962997254337678\n",
      "Stochastic Gradient Descent(43447): loss=0.13238685191475044\n",
      "Stochastic Gradient Descent(43448): loss=5.981839229570433\n",
      "Stochastic Gradient Descent(43449): loss=1.103454720026139\n",
      "Stochastic Gradient Descent(43450): loss=0.557790586416147\n",
      "Stochastic Gradient Descent(43451): loss=8.742998103563256\n",
      "Stochastic Gradient Descent(43452): loss=3.089907619752563\n",
      "Stochastic Gradient Descent(43453): loss=3.7228594170175957\n",
      "Stochastic Gradient Descent(43454): loss=1.3299628764942357\n",
      "Stochastic Gradient Descent(43455): loss=3.9949638244463923\n",
      "Stochastic Gradient Descent(43456): loss=18.414319798352413\n",
      "Stochastic Gradient Descent(43457): loss=0.9763463280224242\n",
      "Stochastic Gradient Descent(43458): loss=0.001237697806600969\n",
      "Stochastic Gradient Descent(43459): loss=0.8416161711092288\n",
      "Stochastic Gradient Descent(43460): loss=0.32393325590902755\n",
      "Stochastic Gradient Descent(43461): loss=0.7714086110421453\n",
      "Stochastic Gradient Descent(43462): loss=1.0642725555547212\n",
      "Stochastic Gradient Descent(43463): loss=13.269206383099318\n",
      "Stochastic Gradient Descent(43464): loss=0.1356165347848916\n",
      "Stochastic Gradient Descent(43465): loss=1.9187604513061978\n",
      "Stochastic Gradient Descent(43466): loss=21.99290565822354\n",
      "Stochastic Gradient Descent(43467): loss=8.581896524960872\n",
      "Stochastic Gradient Descent(43468): loss=1.56659336298223\n",
      "Stochastic Gradient Descent(43469): loss=0.778457972531573\n",
      "Stochastic Gradient Descent(43470): loss=36.82558995155335\n",
      "Stochastic Gradient Descent(43471): loss=0.23546538931165112\n",
      "Stochastic Gradient Descent(43472): loss=6.663799118218294\n",
      "Stochastic Gradient Descent(43473): loss=0.01705322037382503\n",
      "Stochastic Gradient Descent(43474): loss=4.90483700688067\n",
      "Stochastic Gradient Descent(43475): loss=0.028613215797909878\n",
      "Stochastic Gradient Descent(43476): loss=9.57207616029447\n",
      "Stochastic Gradient Descent(43477): loss=1.7093109730776124\n",
      "Stochastic Gradient Descent(43478): loss=6.896547611761416\n",
      "Stochastic Gradient Descent(43479): loss=6.244889735614051\n",
      "Stochastic Gradient Descent(43480): loss=1.755821528251767\n",
      "Stochastic Gradient Descent(43481): loss=0.07476016405826436\n",
      "Stochastic Gradient Descent(43482): loss=0.02636278669768513\n",
      "Stochastic Gradient Descent(43483): loss=7.395077896944702\n",
      "Stochastic Gradient Descent(43484): loss=18.067029933007717\n",
      "Stochastic Gradient Descent(43485): loss=0.7830037840443345\n",
      "Stochastic Gradient Descent(43486): loss=0.2948448988531959\n",
      "Stochastic Gradient Descent(43487): loss=7.904846833358948\n",
      "Stochastic Gradient Descent(43488): loss=4.512259961808288\n",
      "Stochastic Gradient Descent(43489): loss=1.4588125042870765\n",
      "Stochastic Gradient Descent(43490): loss=2.530487655213909\n",
      "Stochastic Gradient Descent(43491): loss=7.707547854255231\n",
      "Stochastic Gradient Descent(43492): loss=1.028634530329411\n",
      "Stochastic Gradient Descent(43493): loss=2.383973256801038\n",
      "Stochastic Gradient Descent(43494): loss=4.998784606780072\n",
      "Stochastic Gradient Descent(43495): loss=7.2487332801809865\n",
      "Stochastic Gradient Descent(43496): loss=5.339010968891499\n",
      "Stochastic Gradient Descent(43497): loss=4.9223820994993135\n",
      "Stochastic Gradient Descent(43498): loss=2.083075676774771\n",
      "Stochastic Gradient Descent(43499): loss=1.6053985291131323\n",
      "Stochastic Gradient Descent(43500): loss=0.3697303818262161\n",
      "Stochastic Gradient Descent(43501): loss=5.303246722317402\n",
      "Stochastic Gradient Descent(43502): loss=0.6156551492595211\n",
      "Stochastic Gradient Descent(43503): loss=0.736600930556529\n",
      "Stochastic Gradient Descent(43504): loss=4.767386039252573\n",
      "Stochastic Gradient Descent(43505): loss=0.7748154338274984\n",
      "Stochastic Gradient Descent(43506): loss=0.037752548405925096\n",
      "Stochastic Gradient Descent(43507): loss=4.384487375990203\n",
      "Stochastic Gradient Descent(43508): loss=2.0760238697386466\n",
      "Stochastic Gradient Descent(43509): loss=0.5109048999102185\n",
      "Stochastic Gradient Descent(43510): loss=4.0434478320102984\n",
      "Stochastic Gradient Descent(43511): loss=1.8513517694068826\n",
      "Stochastic Gradient Descent(43512): loss=2.022767464500113\n",
      "Stochastic Gradient Descent(43513): loss=3.4928840565841663\n",
      "Stochastic Gradient Descent(43514): loss=0.05816489367197658\n",
      "Stochastic Gradient Descent(43515): loss=10.243812597149793\n",
      "Stochastic Gradient Descent(43516): loss=10.320922808710996\n",
      "Stochastic Gradient Descent(43517): loss=2.6076996486841053\n",
      "Stochastic Gradient Descent(43518): loss=2.004270629782966\n",
      "Stochastic Gradient Descent(43519): loss=0.12393157957203692\n",
      "Stochastic Gradient Descent(43520): loss=1.0461374956851064\n",
      "Stochastic Gradient Descent(43521): loss=12.525087210080526\n",
      "Stochastic Gradient Descent(43522): loss=0.4076609730169259\n",
      "Stochastic Gradient Descent(43523): loss=2.7794708842076896\n",
      "Stochastic Gradient Descent(43524): loss=7.933710575423056\n",
      "Stochastic Gradient Descent(43525): loss=3.5264188889898214\n",
      "Stochastic Gradient Descent(43526): loss=0.002467177353960016\n",
      "Stochastic Gradient Descent(43527): loss=0.11041534268356627\n",
      "Stochastic Gradient Descent(43528): loss=0.2000644485870533\n",
      "Stochastic Gradient Descent(43529): loss=0.3786616410114163\n",
      "Stochastic Gradient Descent(43530): loss=2.188429628692577\n",
      "Stochastic Gradient Descent(43531): loss=0.028511811586641452\n",
      "Stochastic Gradient Descent(43532): loss=2.7352689617789507\n",
      "Stochastic Gradient Descent(43533): loss=0.8334212709482126\n",
      "Stochastic Gradient Descent(43534): loss=6.219962383226865\n",
      "Stochastic Gradient Descent(43535): loss=17.565763479201074\n",
      "Stochastic Gradient Descent(43536): loss=2.9718480331148505\n",
      "Stochastic Gradient Descent(43537): loss=2.0005987215166203\n",
      "Stochastic Gradient Descent(43538): loss=6.310205598538491\n",
      "Stochastic Gradient Descent(43539): loss=0.009595398966987065\n",
      "Stochastic Gradient Descent(43540): loss=2.472651300602719\n",
      "Stochastic Gradient Descent(43541): loss=1.8060388243565098\n",
      "Stochastic Gradient Descent(43542): loss=5.531386604727907\n",
      "Stochastic Gradient Descent(43543): loss=0.901261321687319\n",
      "Stochastic Gradient Descent(43544): loss=0.4777765973772744\n",
      "Stochastic Gradient Descent(43545): loss=0.46124158655456415\n",
      "Stochastic Gradient Descent(43546): loss=3.9793934548062775\n",
      "Stochastic Gradient Descent(43547): loss=0.48347595895377554\n",
      "Stochastic Gradient Descent(43548): loss=0.012599951576042403\n",
      "Stochastic Gradient Descent(43549): loss=23.836227995705585\n",
      "Stochastic Gradient Descent(43550): loss=52.43437598474991\n",
      "Stochastic Gradient Descent(43551): loss=1.4115769161189529\n",
      "Stochastic Gradient Descent(43552): loss=2.5017534779048205\n",
      "Stochastic Gradient Descent(43553): loss=23.286131205567695\n",
      "Stochastic Gradient Descent(43554): loss=5.332424255371022\n",
      "Stochastic Gradient Descent(43555): loss=2.0813614721690743\n",
      "Stochastic Gradient Descent(43556): loss=0.3213869569977097\n",
      "Stochastic Gradient Descent(43557): loss=1.761783468794327\n",
      "Stochastic Gradient Descent(43558): loss=0.05135503420368947\n",
      "Stochastic Gradient Descent(43559): loss=0.3737011922222742\n",
      "Stochastic Gradient Descent(43560): loss=1.7101391356498674\n",
      "Stochastic Gradient Descent(43561): loss=1.0838285451877947\n",
      "Stochastic Gradient Descent(43562): loss=4.919537233814699\n",
      "Stochastic Gradient Descent(43563): loss=2.9736781020840515\n",
      "Stochastic Gradient Descent(43564): loss=0.3180086147077653\n",
      "Stochastic Gradient Descent(43565): loss=10.11987625890653\n",
      "Stochastic Gradient Descent(43566): loss=1.2477174022410378\n",
      "Stochastic Gradient Descent(43567): loss=1.564407557087344\n",
      "Stochastic Gradient Descent(43568): loss=6.123618741648487e-05\n",
      "Stochastic Gradient Descent(43569): loss=6.272338955208725\n",
      "Stochastic Gradient Descent(43570): loss=0.4345315000536181\n",
      "Stochastic Gradient Descent(43571): loss=0.4204598476891816\n",
      "Stochastic Gradient Descent(43572): loss=4.020818570646514\n",
      "Stochastic Gradient Descent(43573): loss=0.029193498974162273\n",
      "Stochastic Gradient Descent(43574): loss=0.03281595006646534\n",
      "Stochastic Gradient Descent(43575): loss=6.652825075030861\n",
      "Stochastic Gradient Descent(43576): loss=1.6792417181258064\n",
      "Stochastic Gradient Descent(43577): loss=2.086689791465495\n",
      "Stochastic Gradient Descent(43578): loss=3.6589284675207554\n",
      "Stochastic Gradient Descent(43579): loss=0.4739245283805753\n",
      "Stochastic Gradient Descent(43580): loss=0.19548050075926324\n",
      "Stochastic Gradient Descent(43581): loss=0.09080027398235611\n",
      "Stochastic Gradient Descent(43582): loss=23.125935122173143\n",
      "Stochastic Gradient Descent(43583): loss=7.241757312214521\n",
      "Stochastic Gradient Descent(43584): loss=0.3942536068269246\n",
      "Stochastic Gradient Descent(43585): loss=4.76187194004898\n",
      "Stochastic Gradient Descent(43586): loss=0.706399742620886\n",
      "Stochastic Gradient Descent(43587): loss=1.6097635828748396\n",
      "Stochastic Gradient Descent(43588): loss=1.4342441592579904\n",
      "Stochastic Gradient Descent(43589): loss=0.09411861140297349\n",
      "Stochastic Gradient Descent(43590): loss=0.9617498203656476\n",
      "Stochastic Gradient Descent(43591): loss=0.02706358745380594\n",
      "Stochastic Gradient Descent(43592): loss=11.245104616131174\n",
      "Stochastic Gradient Descent(43593): loss=9.073677862146289\n",
      "Stochastic Gradient Descent(43594): loss=0.8654845591292927\n",
      "Stochastic Gradient Descent(43595): loss=3.556019946998128\n",
      "Stochastic Gradient Descent(43596): loss=11.34805249815644\n",
      "Stochastic Gradient Descent(43597): loss=0.039077711009638284\n",
      "Stochastic Gradient Descent(43598): loss=0.47025062141185825\n",
      "Stochastic Gradient Descent(43599): loss=4.38240486364655\n",
      "Stochastic Gradient Descent(43600): loss=2.509984680534867\n",
      "Stochastic Gradient Descent(43601): loss=0.0635254462275957\n",
      "Stochastic Gradient Descent(43602): loss=2.4193168847590454\n",
      "Stochastic Gradient Descent(43603): loss=1.3180006933742672\n",
      "Stochastic Gradient Descent(43604): loss=1.914870219660626\n",
      "Stochastic Gradient Descent(43605): loss=0.004261336724526601\n",
      "Stochastic Gradient Descent(43606): loss=4.149813741709662\n",
      "Stochastic Gradient Descent(43607): loss=4.515800473961683\n",
      "Stochastic Gradient Descent(43608): loss=0.0638612212727125\n",
      "Stochastic Gradient Descent(43609): loss=1.0490844437650393\n",
      "Stochastic Gradient Descent(43610): loss=2.483182816569401\n",
      "Stochastic Gradient Descent(43611): loss=0.8527720057046333\n",
      "Stochastic Gradient Descent(43612): loss=2.994322770603049\n",
      "Stochastic Gradient Descent(43613): loss=0.047511678993367286\n",
      "Stochastic Gradient Descent(43614): loss=0.23636803890308794\n",
      "Stochastic Gradient Descent(43615): loss=6.8555746646415034\n",
      "Stochastic Gradient Descent(43616): loss=1.0072472006525721\n",
      "Stochastic Gradient Descent(43617): loss=0.1105465821962431\n",
      "Stochastic Gradient Descent(43618): loss=1.289585112705713\n",
      "Stochastic Gradient Descent(43619): loss=4.48109934490474\n",
      "Stochastic Gradient Descent(43620): loss=0.8559590785843735\n",
      "Stochastic Gradient Descent(43621): loss=14.503960783184246\n",
      "Stochastic Gradient Descent(43622): loss=0.2189560523445482\n",
      "Stochastic Gradient Descent(43623): loss=1.6941955139137865\n",
      "Stochastic Gradient Descent(43624): loss=43.58102144073022\n",
      "Stochastic Gradient Descent(43625): loss=9.158704712769207\n",
      "Stochastic Gradient Descent(43626): loss=4.335940475267345\n",
      "Stochastic Gradient Descent(43627): loss=1.6569181055689088\n",
      "Stochastic Gradient Descent(43628): loss=3.1716622227687146e-05\n",
      "Stochastic Gradient Descent(43629): loss=0.22559174563321965\n",
      "Stochastic Gradient Descent(43630): loss=16.23259775770456\n",
      "Stochastic Gradient Descent(43631): loss=0.007755725799836396\n",
      "Stochastic Gradient Descent(43632): loss=0.6367713405402715\n",
      "Stochastic Gradient Descent(43633): loss=2.3357833920707347\n",
      "Stochastic Gradient Descent(43634): loss=0.6201092723850283\n",
      "Stochastic Gradient Descent(43635): loss=8.741125612259097\n",
      "Stochastic Gradient Descent(43636): loss=2.014446450611828\n",
      "Stochastic Gradient Descent(43637): loss=0.023224746251167598\n",
      "Stochastic Gradient Descent(43638): loss=0.6510578447634155\n",
      "Stochastic Gradient Descent(43639): loss=6.452185050887602\n",
      "Stochastic Gradient Descent(43640): loss=11.304220779799852\n",
      "Stochastic Gradient Descent(43641): loss=0.31487431451142245\n",
      "Stochastic Gradient Descent(43642): loss=4.206868012821352\n",
      "Stochastic Gradient Descent(43643): loss=0.17631192583439215\n",
      "Stochastic Gradient Descent(43644): loss=1.6421742183522676\n",
      "Stochastic Gradient Descent(43645): loss=3.6423217237917043\n",
      "Stochastic Gradient Descent(43646): loss=5.154262100279044\n",
      "Stochastic Gradient Descent(43647): loss=5.160506040768325\n",
      "Stochastic Gradient Descent(43648): loss=2.2070478502958357\n",
      "Stochastic Gradient Descent(43649): loss=3.0040714553984156\n",
      "Stochastic Gradient Descent(43650): loss=1.3867393581848246\n",
      "Stochastic Gradient Descent(43651): loss=2.244004181297825\n",
      "Stochastic Gradient Descent(43652): loss=16.870592133590147\n",
      "Stochastic Gradient Descent(43653): loss=8.620678988421774\n",
      "Stochastic Gradient Descent(43654): loss=15.91391622325905\n",
      "Stochastic Gradient Descent(43655): loss=0.04257867458165231\n",
      "Stochastic Gradient Descent(43656): loss=1.5799158295833928\n",
      "Stochastic Gradient Descent(43657): loss=0.06402803494592132\n",
      "Stochastic Gradient Descent(43658): loss=8.039029585682373\n",
      "Stochastic Gradient Descent(43659): loss=1.7075095620292595\n",
      "Stochastic Gradient Descent(43660): loss=3.6485814279226787\n",
      "Stochastic Gradient Descent(43661): loss=0.358024459901482\n",
      "Stochastic Gradient Descent(43662): loss=14.65285276514388\n",
      "Stochastic Gradient Descent(43663): loss=4.646585525224858\n",
      "Stochastic Gradient Descent(43664): loss=0.0618700737266874\n",
      "Stochastic Gradient Descent(43665): loss=10.208154670901564\n",
      "Stochastic Gradient Descent(43666): loss=8.946478730948131\n",
      "Stochastic Gradient Descent(43667): loss=0.5803530662593251\n",
      "Stochastic Gradient Descent(43668): loss=2.6651314733029774\n",
      "Stochastic Gradient Descent(43669): loss=0.9313128323399351\n",
      "Stochastic Gradient Descent(43670): loss=0.04081273308413612\n",
      "Stochastic Gradient Descent(43671): loss=0.009813105753568325\n",
      "Stochastic Gradient Descent(43672): loss=1.3399659120132645\n",
      "Stochastic Gradient Descent(43673): loss=0.002319577368219001\n",
      "Stochastic Gradient Descent(43674): loss=0.05787354430397979\n",
      "Stochastic Gradient Descent(43675): loss=1.2218957993318642\n",
      "Stochastic Gradient Descent(43676): loss=2.7317091529802022\n",
      "Stochastic Gradient Descent(43677): loss=0.15747295131449998\n",
      "Stochastic Gradient Descent(43678): loss=4.14474970775033\n",
      "Stochastic Gradient Descent(43679): loss=0.05017177935784086\n",
      "Stochastic Gradient Descent(43680): loss=11.50472794330258\n",
      "Stochastic Gradient Descent(43681): loss=8.082284051393003\n",
      "Stochastic Gradient Descent(43682): loss=5.127666211553038\n",
      "Stochastic Gradient Descent(43683): loss=10.908670740151575\n",
      "Stochastic Gradient Descent(43684): loss=0.41387688018267504\n",
      "Stochastic Gradient Descent(43685): loss=21.705262427337257\n",
      "Stochastic Gradient Descent(43686): loss=0.5435592887850831\n",
      "Stochastic Gradient Descent(43687): loss=2.056906945177158\n",
      "Stochastic Gradient Descent(43688): loss=0.01581582426053873\n",
      "Stochastic Gradient Descent(43689): loss=2.0663857229565745\n",
      "Stochastic Gradient Descent(43690): loss=0.0046741549266827885\n",
      "Stochastic Gradient Descent(43691): loss=2.457957493289164\n",
      "Stochastic Gradient Descent(43692): loss=5.849423665953582\n",
      "Stochastic Gradient Descent(43693): loss=6.453855831471119\n",
      "Stochastic Gradient Descent(43694): loss=1.3528343254142043\n",
      "Stochastic Gradient Descent(43695): loss=0.7004657298464413\n",
      "Stochastic Gradient Descent(43696): loss=0.040841583872031455\n",
      "Stochastic Gradient Descent(43697): loss=0.08184874712132013\n",
      "Stochastic Gradient Descent(43698): loss=1.861727006789939\n",
      "Stochastic Gradient Descent(43699): loss=6.543411057128198\n",
      "Stochastic Gradient Descent(43700): loss=12.496097977128345\n",
      "Stochastic Gradient Descent(43701): loss=6.163026570885275\n",
      "Stochastic Gradient Descent(43702): loss=41.49548872881733\n",
      "Stochastic Gradient Descent(43703): loss=0.034665063661052736\n",
      "Stochastic Gradient Descent(43704): loss=0.3586367640058024\n",
      "Stochastic Gradient Descent(43705): loss=0.7583796431977817\n",
      "Stochastic Gradient Descent(43706): loss=2.8310934966969907\n",
      "Stochastic Gradient Descent(43707): loss=0.334196497340465\n",
      "Stochastic Gradient Descent(43708): loss=0.22398759041605118\n",
      "Stochastic Gradient Descent(43709): loss=1.4537535711104244\n",
      "Stochastic Gradient Descent(43710): loss=1.8060768970742296\n",
      "Stochastic Gradient Descent(43711): loss=4.747470926155731\n",
      "Stochastic Gradient Descent(43712): loss=0.025811801949826328\n",
      "Stochastic Gradient Descent(43713): loss=0.0008553308578917025\n",
      "Stochastic Gradient Descent(43714): loss=14.159802346472354\n",
      "Stochastic Gradient Descent(43715): loss=10.000883121990817\n",
      "Stochastic Gradient Descent(43716): loss=0.4438279590953841\n",
      "Stochastic Gradient Descent(43717): loss=3.7281412720168734\n",
      "Stochastic Gradient Descent(43718): loss=14.08125007647676\n",
      "Stochastic Gradient Descent(43719): loss=3.890707461771558\n",
      "Stochastic Gradient Descent(43720): loss=4.682688155526934\n",
      "Stochastic Gradient Descent(43721): loss=3.813525708992435\n",
      "Stochastic Gradient Descent(43722): loss=0.7847435763859781\n",
      "Stochastic Gradient Descent(43723): loss=1.0629668875731155\n",
      "Stochastic Gradient Descent(43724): loss=1.1873348930517305\n",
      "Stochastic Gradient Descent(43725): loss=6.135277726373332\n",
      "Stochastic Gradient Descent(43726): loss=0.9007052741666708\n",
      "Stochastic Gradient Descent(43727): loss=6.357170842229075\n",
      "Stochastic Gradient Descent(43728): loss=4.599870175265332\n",
      "Stochastic Gradient Descent(43729): loss=2.6860288597931015\n",
      "Stochastic Gradient Descent(43730): loss=3.1638853677427683\n",
      "Stochastic Gradient Descent(43731): loss=3.4261403892659112\n",
      "Stochastic Gradient Descent(43732): loss=24.37919133206868\n",
      "Stochastic Gradient Descent(43733): loss=0.4394186224091024\n",
      "Stochastic Gradient Descent(43734): loss=5.09556575428453\n",
      "Stochastic Gradient Descent(43735): loss=4.499481752433012\n",
      "Stochastic Gradient Descent(43736): loss=0.05312262740663537\n",
      "Stochastic Gradient Descent(43737): loss=3.911658406680024\n",
      "Stochastic Gradient Descent(43738): loss=6.200347037559634\n",
      "Stochastic Gradient Descent(43739): loss=3.1755708896008668\n",
      "Stochastic Gradient Descent(43740): loss=33.21356317554168\n",
      "Stochastic Gradient Descent(43741): loss=0.3379878313103783\n",
      "Stochastic Gradient Descent(43742): loss=0.20035854184765378\n",
      "Stochastic Gradient Descent(43743): loss=0.30849882288708586\n",
      "Stochastic Gradient Descent(43744): loss=13.324426099175446\n",
      "Stochastic Gradient Descent(43745): loss=28.639831089835265\n",
      "Stochastic Gradient Descent(43746): loss=8.421632429787303\n",
      "Stochastic Gradient Descent(43747): loss=9.769280063837869\n",
      "Stochastic Gradient Descent(43748): loss=3.555896272466926\n",
      "Stochastic Gradient Descent(43749): loss=0.09060694529723598\n",
      "Stochastic Gradient Descent(43750): loss=30.856698675821082\n",
      "Stochastic Gradient Descent(43751): loss=8.755492230190205\n",
      "Stochastic Gradient Descent(43752): loss=0.629560320628863\n",
      "Stochastic Gradient Descent(43753): loss=10.987006591519119\n",
      "Stochastic Gradient Descent(43754): loss=2.84799509182441\n",
      "Stochastic Gradient Descent(43755): loss=6.167126398872346\n",
      "Stochastic Gradient Descent(43756): loss=0.22580403737176663\n",
      "Stochastic Gradient Descent(43757): loss=7.836083760736858\n",
      "Stochastic Gradient Descent(43758): loss=2.1997282444028747\n",
      "Stochastic Gradient Descent(43759): loss=4.079157793447577\n",
      "Stochastic Gradient Descent(43760): loss=3.3897689731522656\n",
      "Stochastic Gradient Descent(43761): loss=0.3128342997257726\n",
      "Stochastic Gradient Descent(43762): loss=4.1067307210961275\n",
      "Stochastic Gradient Descent(43763): loss=105.26008029536662\n",
      "Stochastic Gradient Descent(43764): loss=11.309970127293017\n",
      "Stochastic Gradient Descent(43765): loss=33.48480296499726\n",
      "Stochastic Gradient Descent(43766): loss=95.69563894331644\n",
      "Stochastic Gradient Descent(43767): loss=5.4182895025347895\n",
      "Stochastic Gradient Descent(43768): loss=14.132345128803161\n",
      "Stochastic Gradient Descent(43769): loss=5.233147857231436\n",
      "Stochastic Gradient Descent(43770): loss=0.4870704333730701\n",
      "Stochastic Gradient Descent(43771): loss=1.8154540170981899\n",
      "Stochastic Gradient Descent(43772): loss=0.01707184934726525\n",
      "Stochastic Gradient Descent(43773): loss=0.27849752529193156\n",
      "Stochastic Gradient Descent(43774): loss=12.75107240177554\n",
      "Stochastic Gradient Descent(43775): loss=0.06694321327959132\n",
      "Stochastic Gradient Descent(43776): loss=10.73424071050316\n",
      "Stochastic Gradient Descent(43777): loss=3.3499153684163816\n",
      "Stochastic Gradient Descent(43778): loss=0.47969597946812537\n",
      "Stochastic Gradient Descent(43779): loss=2.731996602827733\n",
      "Stochastic Gradient Descent(43780): loss=3.770954593124488\n",
      "Stochastic Gradient Descent(43781): loss=0.4197948665064357\n",
      "Stochastic Gradient Descent(43782): loss=0.8121299779911801\n",
      "Stochastic Gradient Descent(43783): loss=7.037898679765388\n",
      "Stochastic Gradient Descent(43784): loss=26.428869570882\n",
      "Stochastic Gradient Descent(43785): loss=0.3396776209318518\n",
      "Stochastic Gradient Descent(43786): loss=8.422956935464326\n",
      "Stochastic Gradient Descent(43787): loss=5.437311220482003\n",
      "Stochastic Gradient Descent(43788): loss=0.17596519044953193\n",
      "Stochastic Gradient Descent(43789): loss=1.1501297851919199\n",
      "Stochastic Gradient Descent(43790): loss=2.442921386829557\n",
      "Stochastic Gradient Descent(43791): loss=0.280669251519917\n",
      "Stochastic Gradient Descent(43792): loss=4.658220753116444\n",
      "Stochastic Gradient Descent(43793): loss=4.043785996094922\n",
      "Stochastic Gradient Descent(43794): loss=3.2127078129291\n",
      "Stochastic Gradient Descent(43795): loss=0.8759059982761617\n",
      "Stochastic Gradient Descent(43796): loss=0.014950942688425355\n",
      "Stochastic Gradient Descent(43797): loss=3.3144465504802274\n",
      "Stochastic Gradient Descent(43798): loss=0.209976957620469\n",
      "Stochastic Gradient Descent(43799): loss=0.9622302474452945\n",
      "Stochastic Gradient Descent(43800): loss=0.5139063636893182\n",
      "Stochastic Gradient Descent(43801): loss=1.120944781865195\n",
      "Stochastic Gradient Descent(43802): loss=4.336780928851059\n",
      "Stochastic Gradient Descent(43803): loss=0.8291333392396485\n",
      "Stochastic Gradient Descent(43804): loss=0.2275074999415096\n",
      "Stochastic Gradient Descent(43805): loss=1.687659147822709\n",
      "Stochastic Gradient Descent(43806): loss=5.310021251426116\n",
      "Stochastic Gradient Descent(43807): loss=0.5807023511827245\n",
      "Stochastic Gradient Descent(43808): loss=6.281102486334479\n",
      "Stochastic Gradient Descent(43809): loss=18.69893101315643\n",
      "Stochastic Gradient Descent(43810): loss=0.08774351613410397\n",
      "Stochastic Gradient Descent(43811): loss=0.16298437433274457\n",
      "Stochastic Gradient Descent(43812): loss=0.006467874772229534\n",
      "Stochastic Gradient Descent(43813): loss=6.701751957968191\n",
      "Stochastic Gradient Descent(43814): loss=2.767532838793049\n",
      "Stochastic Gradient Descent(43815): loss=1.6917821612095463\n",
      "Stochastic Gradient Descent(43816): loss=0.0012561107218793401\n",
      "Stochastic Gradient Descent(43817): loss=1.118164572689066\n",
      "Stochastic Gradient Descent(43818): loss=0.11957156440430534\n",
      "Stochastic Gradient Descent(43819): loss=5.874971513721059\n",
      "Stochastic Gradient Descent(43820): loss=0.0007946169247647505\n",
      "Stochastic Gradient Descent(43821): loss=0.06566910401733832\n",
      "Stochastic Gradient Descent(43822): loss=2.258960166086045\n",
      "Stochastic Gradient Descent(43823): loss=0.5599537744662298\n",
      "Stochastic Gradient Descent(43824): loss=2.1429605466084722\n",
      "Stochastic Gradient Descent(43825): loss=5.092590068883731\n",
      "Stochastic Gradient Descent(43826): loss=1.0352096541529767\n",
      "Stochastic Gradient Descent(43827): loss=0.5923755601110995\n",
      "Stochastic Gradient Descent(43828): loss=1.5493687763926585\n",
      "Stochastic Gradient Descent(43829): loss=0.0011291409931602431\n",
      "Stochastic Gradient Descent(43830): loss=0.3868186438027509\n",
      "Stochastic Gradient Descent(43831): loss=1.3043220521967447\n",
      "Stochastic Gradient Descent(43832): loss=0.06302047844588717\n",
      "Stochastic Gradient Descent(43833): loss=0.3722134537642031\n",
      "Stochastic Gradient Descent(43834): loss=2.5186865731033925\n",
      "Stochastic Gradient Descent(43835): loss=3.533215584023748\n",
      "Stochastic Gradient Descent(43836): loss=6.377170786886093\n",
      "Stochastic Gradient Descent(43837): loss=4.510431715879023\n",
      "Stochastic Gradient Descent(43838): loss=14.480989744467513\n",
      "Stochastic Gradient Descent(43839): loss=5.665657571722401\n",
      "Stochastic Gradient Descent(43840): loss=0.6722556661847878\n",
      "Stochastic Gradient Descent(43841): loss=18.68961789400389\n",
      "Stochastic Gradient Descent(43842): loss=0.4382178739656406\n",
      "Stochastic Gradient Descent(43843): loss=0.5724566741036063\n",
      "Stochastic Gradient Descent(43844): loss=8.979932075773833\n",
      "Stochastic Gradient Descent(43845): loss=0.007927740173153348\n",
      "Stochastic Gradient Descent(43846): loss=9.528352874280385\n",
      "Stochastic Gradient Descent(43847): loss=1.5951430069404566\n",
      "Stochastic Gradient Descent(43848): loss=2.8501907314055037\n",
      "Stochastic Gradient Descent(43849): loss=3.225462534352283\n",
      "Stochastic Gradient Descent(43850): loss=5.066471867501741e-06\n",
      "Stochastic Gradient Descent(43851): loss=0.39426010926041577\n",
      "Stochastic Gradient Descent(43852): loss=0.21952355536566495\n",
      "Stochastic Gradient Descent(43853): loss=0.22383700541646565\n",
      "Stochastic Gradient Descent(43854): loss=0.17498685632169278\n",
      "Stochastic Gradient Descent(43855): loss=0.002840861768930145\n",
      "Stochastic Gradient Descent(43856): loss=8.740744147890602\n",
      "Stochastic Gradient Descent(43857): loss=0.30357582605536226\n",
      "Stochastic Gradient Descent(43858): loss=1.722017632211772\n",
      "Stochastic Gradient Descent(43859): loss=10.919951630433498\n",
      "Stochastic Gradient Descent(43860): loss=0.10261379547961662\n",
      "Stochastic Gradient Descent(43861): loss=0.009391797687132424\n",
      "Stochastic Gradient Descent(43862): loss=1.046895406159931\n",
      "Stochastic Gradient Descent(43863): loss=0.15071553003750923\n",
      "Stochastic Gradient Descent(43864): loss=3.27635676254301\n",
      "Stochastic Gradient Descent(43865): loss=0.6279685535153885\n",
      "Stochastic Gradient Descent(43866): loss=4.662576603982355\n",
      "Stochastic Gradient Descent(43867): loss=8.367442914169956\n",
      "Stochastic Gradient Descent(43868): loss=0.9863109005871648\n",
      "Stochastic Gradient Descent(43869): loss=3.7005308144678524\n",
      "Stochastic Gradient Descent(43870): loss=2.7831104587265605\n",
      "Stochastic Gradient Descent(43871): loss=13.724574454560935\n",
      "Stochastic Gradient Descent(43872): loss=1.9100126762619003\n",
      "Stochastic Gradient Descent(43873): loss=3.780502846638662\n",
      "Stochastic Gradient Descent(43874): loss=3.013524866700087\n",
      "Stochastic Gradient Descent(43875): loss=0.021792719699946905\n",
      "Stochastic Gradient Descent(43876): loss=2.55993977509571\n",
      "Stochastic Gradient Descent(43877): loss=1.020497203963269\n",
      "Stochastic Gradient Descent(43878): loss=4.800824747405305\n",
      "Stochastic Gradient Descent(43879): loss=0.22518700897878766\n",
      "Stochastic Gradient Descent(43880): loss=3.624083551386523\n",
      "Stochastic Gradient Descent(43881): loss=6.272878330857143\n",
      "Stochastic Gradient Descent(43882): loss=0.17480797312699567\n",
      "Stochastic Gradient Descent(43883): loss=5.505322000958788\n",
      "Stochastic Gradient Descent(43884): loss=3.3524767927336074\n",
      "Stochastic Gradient Descent(43885): loss=0.40406490819910695\n",
      "Stochastic Gradient Descent(43886): loss=0.4206648847389485\n",
      "Stochastic Gradient Descent(43887): loss=1.146303284477285\n",
      "Stochastic Gradient Descent(43888): loss=0.29634040785743054\n",
      "Stochastic Gradient Descent(43889): loss=0.7941316102204472\n",
      "Stochastic Gradient Descent(43890): loss=1.0956381354179747\n",
      "Stochastic Gradient Descent(43891): loss=1.9492906365909586\n",
      "Stochastic Gradient Descent(43892): loss=0.16904190639873246\n",
      "Stochastic Gradient Descent(43893): loss=12.445320573907914\n",
      "Stochastic Gradient Descent(43894): loss=6.623067149887412\n",
      "Stochastic Gradient Descent(43895): loss=1.7947932398214361\n",
      "Stochastic Gradient Descent(43896): loss=13.992922967900611\n",
      "Stochastic Gradient Descent(43897): loss=3.4877042322317844\n",
      "Stochastic Gradient Descent(43898): loss=0.25499928679471906\n",
      "Stochastic Gradient Descent(43899): loss=1.7038300523503753\n",
      "Stochastic Gradient Descent(43900): loss=0.7112984852792458\n",
      "Stochastic Gradient Descent(43901): loss=3.8731462243192336\n",
      "Stochastic Gradient Descent(43902): loss=1.6997531141338889\n",
      "Stochastic Gradient Descent(43903): loss=3.0188717169753305\n",
      "Stochastic Gradient Descent(43904): loss=0.1429610417523402\n",
      "Stochastic Gradient Descent(43905): loss=1.7272749213749423\n",
      "Stochastic Gradient Descent(43906): loss=0.14715797725188232\n",
      "Stochastic Gradient Descent(43907): loss=1.1833913943894272\n",
      "Stochastic Gradient Descent(43908): loss=2.447998963474405\n",
      "Stochastic Gradient Descent(43909): loss=0.054099166711473176\n",
      "Stochastic Gradient Descent(43910): loss=14.009155926413655\n",
      "Stochastic Gradient Descent(43911): loss=0.25985173472995043\n",
      "Stochastic Gradient Descent(43912): loss=3.727676327571169\n",
      "Stochastic Gradient Descent(43913): loss=3.2528797402263043\n",
      "Stochastic Gradient Descent(43914): loss=2.8280491466749833\n",
      "Stochastic Gradient Descent(43915): loss=13.606782010241709\n",
      "Stochastic Gradient Descent(43916): loss=15.355418149486727\n",
      "Stochastic Gradient Descent(43917): loss=4.117169401015645\n",
      "Stochastic Gradient Descent(43918): loss=0.03835928025217832\n",
      "Stochastic Gradient Descent(43919): loss=0.6010445875327475\n",
      "Stochastic Gradient Descent(43920): loss=6.042529269440384\n",
      "Stochastic Gradient Descent(43921): loss=1.1467493977863399\n",
      "Stochastic Gradient Descent(43922): loss=0.24207149090721009\n",
      "Stochastic Gradient Descent(43923): loss=2.328409990684891\n",
      "Stochastic Gradient Descent(43924): loss=8.704337512768474\n",
      "Stochastic Gradient Descent(43925): loss=1.291441471865092\n",
      "Stochastic Gradient Descent(43926): loss=1.0212288373048553\n",
      "Stochastic Gradient Descent(43927): loss=0.030702693257100577\n",
      "Stochastic Gradient Descent(43928): loss=0.018627125414261776\n",
      "Stochastic Gradient Descent(43929): loss=4.406349297615458\n",
      "Stochastic Gradient Descent(43930): loss=0.1814280382870446\n",
      "Stochastic Gradient Descent(43931): loss=24.961431910590775\n",
      "Stochastic Gradient Descent(43932): loss=16.626774089121614\n",
      "Stochastic Gradient Descent(43933): loss=3.6948855934553158\n",
      "Stochastic Gradient Descent(43934): loss=0.009974708821964504\n",
      "Stochastic Gradient Descent(43935): loss=1.4735998783020272\n",
      "Stochastic Gradient Descent(43936): loss=2.5204758339819895\n",
      "Stochastic Gradient Descent(43937): loss=1.6480787294406793\n",
      "Stochastic Gradient Descent(43938): loss=0.9462296742467158\n",
      "Stochastic Gradient Descent(43939): loss=1.1892277940077323\n",
      "Stochastic Gradient Descent(43940): loss=0.04072302983488015\n",
      "Stochastic Gradient Descent(43941): loss=6.368145732848335\n",
      "Stochastic Gradient Descent(43942): loss=2.978981031893043\n",
      "Stochastic Gradient Descent(43943): loss=9.958067710234127\n",
      "Stochastic Gradient Descent(43944): loss=0.37536487816221153\n",
      "Stochastic Gradient Descent(43945): loss=0.13365250651972474\n",
      "Stochastic Gradient Descent(43946): loss=0.6477131439346422\n",
      "Stochastic Gradient Descent(43947): loss=0.012033279129852262\n",
      "Stochastic Gradient Descent(43948): loss=9.6581377355201\n",
      "Stochastic Gradient Descent(43949): loss=1.6334537965207274\n",
      "Stochastic Gradient Descent(43950): loss=0.00011301764269022082\n",
      "Stochastic Gradient Descent(43951): loss=2.672857957853471\n",
      "Stochastic Gradient Descent(43952): loss=1.682657251941341\n",
      "Stochastic Gradient Descent(43953): loss=0.7229670782790116\n",
      "Stochastic Gradient Descent(43954): loss=0.05008062768482392\n",
      "Stochastic Gradient Descent(43955): loss=0.07157256470133609\n",
      "Stochastic Gradient Descent(43956): loss=26.34440801175596\n",
      "Stochastic Gradient Descent(43957): loss=1.7915549310243646\n",
      "Stochastic Gradient Descent(43958): loss=3.2356035874885833\n",
      "Stochastic Gradient Descent(43959): loss=9.802682554870872\n",
      "Stochastic Gradient Descent(43960): loss=3.5168875529010077\n",
      "Stochastic Gradient Descent(43961): loss=1.6353189082023607\n",
      "Stochastic Gradient Descent(43962): loss=3.2013192411287408\n",
      "Stochastic Gradient Descent(43963): loss=10.520620422646848\n",
      "Stochastic Gradient Descent(43964): loss=0.3476275466863685\n",
      "Stochastic Gradient Descent(43965): loss=0.9818594603514121\n",
      "Stochastic Gradient Descent(43966): loss=26.99634205122884\n",
      "Stochastic Gradient Descent(43967): loss=21.704680019734795\n",
      "Stochastic Gradient Descent(43968): loss=8.90306240081796\n",
      "Stochastic Gradient Descent(43969): loss=24.125675501502677\n",
      "Stochastic Gradient Descent(43970): loss=48.82325717360092\n",
      "Stochastic Gradient Descent(43971): loss=81.28730387348249\n",
      "Stochastic Gradient Descent(43972): loss=9.513108428500233\n",
      "Stochastic Gradient Descent(43973): loss=0.04020234399489177\n",
      "Stochastic Gradient Descent(43974): loss=7.8217534595726494\n",
      "Stochastic Gradient Descent(43975): loss=21.535599804406047\n",
      "Stochastic Gradient Descent(43976): loss=0.056092271406081404\n",
      "Stochastic Gradient Descent(43977): loss=0.9271988289433053\n",
      "Stochastic Gradient Descent(43978): loss=3.005473754974542\n",
      "Stochastic Gradient Descent(43979): loss=0.050005552997823145\n",
      "Stochastic Gradient Descent(43980): loss=4.997275136139127\n",
      "Stochastic Gradient Descent(43981): loss=2.26172130926784\n",
      "Stochastic Gradient Descent(43982): loss=0.4661616807281944\n",
      "Stochastic Gradient Descent(43983): loss=2.2160611284492084\n",
      "Stochastic Gradient Descent(43984): loss=1.055652612035838\n",
      "Stochastic Gradient Descent(43985): loss=5.03648577798921\n",
      "Stochastic Gradient Descent(43986): loss=1.0901707865596053\n",
      "Stochastic Gradient Descent(43987): loss=12.726310123980415\n",
      "Stochastic Gradient Descent(43988): loss=1.0069733007665882\n",
      "Stochastic Gradient Descent(43989): loss=0.4615690912922238\n",
      "Stochastic Gradient Descent(43990): loss=11.585964103476798\n",
      "Stochastic Gradient Descent(43991): loss=0.9796986807100193\n",
      "Stochastic Gradient Descent(43992): loss=1.1458536581202219\n",
      "Stochastic Gradient Descent(43993): loss=2.2279928974489582\n",
      "Stochastic Gradient Descent(43994): loss=16.790492970048604\n",
      "Stochastic Gradient Descent(43995): loss=0.0036282437231200245\n",
      "Stochastic Gradient Descent(43996): loss=0.04070253688572832\n",
      "Stochastic Gradient Descent(43997): loss=8.134644246542791\n",
      "Stochastic Gradient Descent(43998): loss=0.09110062586322659\n",
      "Stochastic Gradient Descent(43999): loss=0.011811194956219831\n",
      "Stochastic Gradient Descent(44000): loss=2.17074251261591e-06\n",
      "Stochastic Gradient Descent(44001): loss=1.7038621706045076\n",
      "Stochastic Gradient Descent(44002): loss=2.5462647962738374\n",
      "Stochastic Gradient Descent(44003): loss=2.690141053534251\n",
      "Stochastic Gradient Descent(44004): loss=0.8413918686025458\n",
      "Stochastic Gradient Descent(44005): loss=0.09534173569968567\n",
      "Stochastic Gradient Descent(44006): loss=5.943071363433502\n",
      "Stochastic Gradient Descent(44007): loss=16.703177989215828\n",
      "Stochastic Gradient Descent(44008): loss=9.528902713412934\n",
      "Stochastic Gradient Descent(44009): loss=0.18648776921054397\n",
      "Stochastic Gradient Descent(44010): loss=0.15577625745334872\n",
      "Stochastic Gradient Descent(44011): loss=1.1963520688572764\n",
      "Stochastic Gradient Descent(44012): loss=7.871005748630166\n",
      "Stochastic Gradient Descent(44013): loss=4.581178867747232\n",
      "Stochastic Gradient Descent(44014): loss=10.89758165453798\n",
      "Stochastic Gradient Descent(44015): loss=0.11044341945980685\n",
      "Stochastic Gradient Descent(44016): loss=1.4436412637632126\n",
      "Stochastic Gradient Descent(44017): loss=17.785105941197372\n",
      "Stochastic Gradient Descent(44018): loss=1.5540516123652612\n",
      "Stochastic Gradient Descent(44019): loss=0.11264270963109393\n",
      "Stochastic Gradient Descent(44020): loss=1.2952783284325342\n",
      "Stochastic Gradient Descent(44021): loss=31.817702369481484\n",
      "Stochastic Gradient Descent(44022): loss=1.8600815080602446\n",
      "Stochastic Gradient Descent(44023): loss=1.1057581978961601\n",
      "Stochastic Gradient Descent(44024): loss=0.5854940086795468\n",
      "Stochastic Gradient Descent(44025): loss=6.06768485190938\n",
      "Stochastic Gradient Descent(44026): loss=2.9469717883512505\n",
      "Stochastic Gradient Descent(44027): loss=0.022671575568630965\n",
      "Stochastic Gradient Descent(44028): loss=1.154485955533759\n",
      "Stochastic Gradient Descent(44029): loss=4.022024839863816\n",
      "Stochastic Gradient Descent(44030): loss=11.930042230633001\n",
      "Stochastic Gradient Descent(44031): loss=2.6781189302902075\n",
      "Stochastic Gradient Descent(44032): loss=5.6587977067225435\n",
      "Stochastic Gradient Descent(44033): loss=0.0035185918472451184\n",
      "Stochastic Gradient Descent(44034): loss=0.36141844009692153\n",
      "Stochastic Gradient Descent(44035): loss=1.2523385748870892\n",
      "Stochastic Gradient Descent(44036): loss=53.24322815548504\n",
      "Stochastic Gradient Descent(44037): loss=0.3106806101442555\n",
      "Stochastic Gradient Descent(44038): loss=0.02120786893314763\n",
      "Stochastic Gradient Descent(44039): loss=3.8329474396370378\n",
      "Stochastic Gradient Descent(44040): loss=0.00032007721464990613\n",
      "Stochastic Gradient Descent(44041): loss=0.20515967331635057\n",
      "Stochastic Gradient Descent(44042): loss=0.6488715368369108\n",
      "Stochastic Gradient Descent(44043): loss=5.07232814968674\n",
      "Stochastic Gradient Descent(44044): loss=0.5065749643458466\n",
      "Stochastic Gradient Descent(44045): loss=1.734603197801506\n",
      "Stochastic Gradient Descent(44046): loss=1.2078322829682253\n",
      "Stochastic Gradient Descent(44047): loss=2.4688773964533417\n",
      "Stochastic Gradient Descent(44048): loss=1.2951687894834425\n",
      "Stochastic Gradient Descent(44049): loss=0.27884996049360083\n",
      "Stochastic Gradient Descent(44050): loss=2.290849752557722\n",
      "Stochastic Gradient Descent(44051): loss=14.408614712003516\n",
      "Stochastic Gradient Descent(44052): loss=8.283118379169887\n",
      "Stochastic Gradient Descent(44053): loss=8.375787283511295\n",
      "Stochastic Gradient Descent(44054): loss=0.4628169339978237\n",
      "Stochastic Gradient Descent(44055): loss=2.864633045999085\n",
      "Stochastic Gradient Descent(44056): loss=0.128249087774811\n",
      "Stochastic Gradient Descent(44057): loss=7.3280504953835095\n",
      "Stochastic Gradient Descent(44058): loss=0.12364240031661908\n",
      "Stochastic Gradient Descent(44059): loss=2.342083740195291\n",
      "Stochastic Gradient Descent(44060): loss=1.4029615455754363\n",
      "Stochastic Gradient Descent(44061): loss=0.40774078330615515\n",
      "Stochastic Gradient Descent(44062): loss=3.676575200865425\n",
      "Stochastic Gradient Descent(44063): loss=3.2424540542338702\n",
      "Stochastic Gradient Descent(44064): loss=0.0011987036913080876\n",
      "Stochastic Gradient Descent(44065): loss=6.418941040047249\n",
      "Stochastic Gradient Descent(44066): loss=0.9431196452781245\n",
      "Stochastic Gradient Descent(44067): loss=0.32070980597938903\n",
      "Stochastic Gradient Descent(44068): loss=0.688524832636326\n",
      "Stochastic Gradient Descent(44069): loss=3.6879877708502558\n",
      "Stochastic Gradient Descent(44070): loss=0.4441260382292309\n",
      "Stochastic Gradient Descent(44071): loss=0.892227601385336\n",
      "Stochastic Gradient Descent(44072): loss=1.225897468175273\n",
      "Stochastic Gradient Descent(44073): loss=3.7352258202278845\n",
      "Stochastic Gradient Descent(44074): loss=2.4553147120263663\n",
      "Stochastic Gradient Descent(44075): loss=0.8087690343108032\n",
      "Stochastic Gradient Descent(44076): loss=0.38671571337969096\n",
      "Stochastic Gradient Descent(44077): loss=2.908986057471666\n",
      "Stochastic Gradient Descent(44078): loss=1.9857320505741765\n",
      "Stochastic Gradient Descent(44079): loss=17.497326376322718\n",
      "Stochastic Gradient Descent(44080): loss=2.230387110779438\n",
      "Stochastic Gradient Descent(44081): loss=1.5440206479061347\n",
      "Stochastic Gradient Descent(44082): loss=0.3614739933005228\n",
      "Stochastic Gradient Descent(44083): loss=1.4047010895327654\n",
      "Stochastic Gradient Descent(44084): loss=6.666958060084082\n",
      "Stochastic Gradient Descent(44085): loss=3.270714563674518\n",
      "Stochastic Gradient Descent(44086): loss=4.476826782559938\n",
      "Stochastic Gradient Descent(44087): loss=8.675559083801307\n",
      "Stochastic Gradient Descent(44088): loss=5.325845971221861\n",
      "Stochastic Gradient Descent(44089): loss=7.122782066023313\n",
      "Stochastic Gradient Descent(44090): loss=0.47680759584337434\n",
      "Stochastic Gradient Descent(44091): loss=27.512190240019976\n",
      "Stochastic Gradient Descent(44092): loss=12.321000507554217\n",
      "Stochastic Gradient Descent(44093): loss=0.2815899628072907\n",
      "Stochastic Gradient Descent(44094): loss=56.51070651606554\n",
      "Stochastic Gradient Descent(44095): loss=2.295622312840433\n",
      "Stochastic Gradient Descent(44096): loss=5.957987766552956\n",
      "Stochastic Gradient Descent(44097): loss=9.982808364989856\n",
      "Stochastic Gradient Descent(44098): loss=11.131421387842336\n",
      "Stochastic Gradient Descent(44099): loss=0.16091153810590145\n",
      "Stochastic Gradient Descent(44100): loss=0.3497787195760099\n",
      "Stochastic Gradient Descent(44101): loss=2.7020423235419746\n",
      "Stochastic Gradient Descent(44102): loss=1.8764941281065115\n",
      "Stochastic Gradient Descent(44103): loss=0.6836134937821992\n",
      "Stochastic Gradient Descent(44104): loss=0.5049991697537416\n",
      "Stochastic Gradient Descent(44105): loss=0.34553528513172954\n",
      "Stochastic Gradient Descent(44106): loss=0.7714089790815798\n",
      "Stochastic Gradient Descent(44107): loss=0.023417721595325718\n",
      "Stochastic Gradient Descent(44108): loss=0.005268351915810575\n",
      "Stochastic Gradient Descent(44109): loss=3.6848833054932206\n",
      "Stochastic Gradient Descent(44110): loss=3.423801027226488\n",
      "Stochastic Gradient Descent(44111): loss=0.20827645490268995\n",
      "Stochastic Gradient Descent(44112): loss=0.34553980284141333\n",
      "Stochastic Gradient Descent(44113): loss=5.4225211225010295\n",
      "Stochastic Gradient Descent(44114): loss=0.8578310016248258\n",
      "Stochastic Gradient Descent(44115): loss=3.3522553228755325\n",
      "Stochastic Gradient Descent(44116): loss=1.9750096765836416\n",
      "Stochastic Gradient Descent(44117): loss=1.9219353599166447\n",
      "Stochastic Gradient Descent(44118): loss=3.8054802721356746\n",
      "Stochastic Gradient Descent(44119): loss=5.747752035865368\n",
      "Stochastic Gradient Descent(44120): loss=0.46036383904139194\n",
      "Stochastic Gradient Descent(44121): loss=6.998233237714032\n",
      "Stochastic Gradient Descent(44122): loss=1.4463906161212676\n",
      "Stochastic Gradient Descent(44123): loss=0.45892974436648537\n",
      "Stochastic Gradient Descent(44124): loss=2.0309438197565104\n",
      "Stochastic Gradient Descent(44125): loss=0.22681496024649278\n",
      "Stochastic Gradient Descent(44126): loss=0.5021942094305784\n",
      "Stochastic Gradient Descent(44127): loss=1.5867610997625912\n",
      "Stochastic Gradient Descent(44128): loss=5.79606618836094\n",
      "Stochastic Gradient Descent(44129): loss=8.611560420727777\n",
      "Stochastic Gradient Descent(44130): loss=3.3700872650954925\n",
      "Stochastic Gradient Descent(44131): loss=2.2073827018243377\n",
      "Stochastic Gradient Descent(44132): loss=0.36535528673258694\n",
      "Stochastic Gradient Descent(44133): loss=8.22897424854966\n",
      "Stochastic Gradient Descent(44134): loss=4.4193959023253395\n",
      "Stochastic Gradient Descent(44135): loss=4.932911650460818\n",
      "Stochastic Gradient Descent(44136): loss=0.1058941071930343\n",
      "Stochastic Gradient Descent(44137): loss=2.0546458002062447\n",
      "Stochastic Gradient Descent(44138): loss=0.09797926285921106\n",
      "Stochastic Gradient Descent(44139): loss=6.600227359538091\n",
      "Stochastic Gradient Descent(44140): loss=0.20908106665509607\n",
      "Stochastic Gradient Descent(44141): loss=0.28893956194380516\n",
      "Stochastic Gradient Descent(44142): loss=7.293170747296621\n",
      "Stochastic Gradient Descent(44143): loss=0.8193117189396113\n",
      "Stochastic Gradient Descent(44144): loss=5.129190105291238\n",
      "Stochastic Gradient Descent(44145): loss=3.2552603699754483\n",
      "Stochastic Gradient Descent(44146): loss=0.010729876068963085\n",
      "Stochastic Gradient Descent(44147): loss=0.6795151334388687\n",
      "Stochastic Gradient Descent(44148): loss=1.7260025744508545\n",
      "Stochastic Gradient Descent(44149): loss=0.8051547083903168\n",
      "Stochastic Gradient Descent(44150): loss=1.0079315790903576\n",
      "Stochastic Gradient Descent(44151): loss=1.6601221995543134\n",
      "Stochastic Gradient Descent(44152): loss=1.8578260837718807\n",
      "Stochastic Gradient Descent(44153): loss=1.354580331345388\n",
      "Stochastic Gradient Descent(44154): loss=15.079399835264402\n",
      "Stochastic Gradient Descent(44155): loss=1.525653964843316\n",
      "Stochastic Gradient Descent(44156): loss=16.200528465376628\n",
      "Stochastic Gradient Descent(44157): loss=0.7558344667719131\n",
      "Stochastic Gradient Descent(44158): loss=0.6418389668533356\n",
      "Stochastic Gradient Descent(44159): loss=5.768436795194024\n",
      "Stochastic Gradient Descent(44160): loss=10.058254516347393\n",
      "Stochastic Gradient Descent(44161): loss=10.346131057973711\n",
      "Stochastic Gradient Descent(44162): loss=0.18114617546480302\n",
      "Stochastic Gradient Descent(44163): loss=0.018501925569275376\n",
      "Stochastic Gradient Descent(44164): loss=2.2029509288138143\n",
      "Stochastic Gradient Descent(44165): loss=0.6401403846671324\n",
      "Stochastic Gradient Descent(44166): loss=2.675531087126179\n",
      "Stochastic Gradient Descent(44167): loss=0.6668392090798327\n",
      "Stochastic Gradient Descent(44168): loss=13.68028214539123\n",
      "Stochastic Gradient Descent(44169): loss=0.18407262388236084\n",
      "Stochastic Gradient Descent(44170): loss=0.20585727781097438\n",
      "Stochastic Gradient Descent(44171): loss=0.05663953662492015\n",
      "Stochastic Gradient Descent(44172): loss=2.143470517300818\n",
      "Stochastic Gradient Descent(44173): loss=0.011919677816319424\n",
      "Stochastic Gradient Descent(44174): loss=0.7693946222070405\n",
      "Stochastic Gradient Descent(44175): loss=0.039060128493124936\n",
      "Stochastic Gradient Descent(44176): loss=1.2175675182039392\n",
      "Stochastic Gradient Descent(44177): loss=0.005803004665125856\n",
      "Stochastic Gradient Descent(44178): loss=4.561909764796899\n",
      "Stochastic Gradient Descent(44179): loss=0.35245940521128727\n",
      "Stochastic Gradient Descent(44180): loss=1.4043985770695568\n",
      "Stochastic Gradient Descent(44181): loss=0.8141870251648836\n",
      "Stochastic Gradient Descent(44182): loss=4.198818155207627\n",
      "Stochastic Gradient Descent(44183): loss=0.04353380558407646\n",
      "Stochastic Gradient Descent(44184): loss=4.549615309358042\n",
      "Stochastic Gradient Descent(44185): loss=0.22599180414016656\n",
      "Stochastic Gradient Descent(44186): loss=24.199264238711624\n",
      "Stochastic Gradient Descent(44187): loss=32.53208654847058\n",
      "Stochastic Gradient Descent(44188): loss=2.2182493549913818\n",
      "Stochastic Gradient Descent(44189): loss=3.378157378172688\n",
      "Stochastic Gradient Descent(44190): loss=1.4913035089382285\n",
      "Stochastic Gradient Descent(44191): loss=2.8403182708461223\n",
      "Stochastic Gradient Descent(44192): loss=8.091473982541327\n",
      "Stochastic Gradient Descent(44193): loss=2.602859131267127\n",
      "Stochastic Gradient Descent(44194): loss=2.189732615187737\n",
      "Stochastic Gradient Descent(44195): loss=0.8977196202417062\n",
      "Stochastic Gradient Descent(44196): loss=0.31814556275316547\n",
      "Stochastic Gradient Descent(44197): loss=4.526207735193914\n",
      "Stochastic Gradient Descent(44198): loss=0.819137575414365\n",
      "Stochastic Gradient Descent(44199): loss=9.75645748414425\n",
      "Stochastic Gradient Descent(44200): loss=3.8890575746176426\n",
      "Stochastic Gradient Descent(44201): loss=4.364667233945345\n",
      "Stochastic Gradient Descent(44202): loss=0.002600780259700187\n",
      "Stochastic Gradient Descent(44203): loss=0.23860077945836797\n",
      "Stochastic Gradient Descent(44204): loss=0.0628631127979612\n",
      "Stochastic Gradient Descent(44205): loss=0.2268493551847581\n",
      "Stochastic Gradient Descent(44206): loss=0.03378810861376012\n",
      "Stochastic Gradient Descent(44207): loss=3.4692352073143535\n",
      "Stochastic Gradient Descent(44208): loss=4.595718925461031\n",
      "Stochastic Gradient Descent(44209): loss=6.647201851813875\n",
      "Stochastic Gradient Descent(44210): loss=0.05697918581607311\n",
      "Stochastic Gradient Descent(44211): loss=4.19016761584319\n",
      "Stochastic Gradient Descent(44212): loss=1.2320812757525825\n",
      "Stochastic Gradient Descent(44213): loss=0.08722739841605652\n",
      "Stochastic Gradient Descent(44214): loss=4.617595740585115\n",
      "Stochastic Gradient Descent(44215): loss=3.132994344299485\n",
      "Stochastic Gradient Descent(44216): loss=6.168846762770883\n",
      "Stochastic Gradient Descent(44217): loss=0.01604535022274475\n",
      "Stochastic Gradient Descent(44218): loss=0.022340451412861086\n",
      "Stochastic Gradient Descent(44219): loss=5.913106831201339\n",
      "Stochastic Gradient Descent(44220): loss=1.8621093221750395\n",
      "Stochastic Gradient Descent(44221): loss=1.3670407056224516\n",
      "Stochastic Gradient Descent(44222): loss=23.39233935570119\n",
      "Stochastic Gradient Descent(44223): loss=18.180778495471195\n",
      "Stochastic Gradient Descent(44224): loss=1.2475788899205005\n",
      "Stochastic Gradient Descent(44225): loss=0.10733040667928709\n",
      "Stochastic Gradient Descent(44226): loss=12.877493900432775\n",
      "Stochastic Gradient Descent(44227): loss=5.06021951856364\n",
      "Stochastic Gradient Descent(44228): loss=2.4030961532745514\n",
      "Stochastic Gradient Descent(44229): loss=0.45216393138366795\n",
      "Stochastic Gradient Descent(44230): loss=10.23612822765985\n",
      "Stochastic Gradient Descent(44231): loss=7.917746944546716e-05\n",
      "Stochastic Gradient Descent(44232): loss=0.11206726800096917\n",
      "Stochastic Gradient Descent(44233): loss=0.9547554336948918\n",
      "Stochastic Gradient Descent(44234): loss=14.85253510055624\n",
      "Stochastic Gradient Descent(44235): loss=12.027089729999046\n",
      "Stochastic Gradient Descent(44236): loss=5.46717254614753\n",
      "Stochastic Gradient Descent(44237): loss=0.5883252877287855\n",
      "Stochastic Gradient Descent(44238): loss=0.875660191652041\n",
      "Stochastic Gradient Descent(44239): loss=9.817640625102989\n",
      "Stochastic Gradient Descent(44240): loss=0.04313266106573534\n",
      "Stochastic Gradient Descent(44241): loss=3.494259895273756\n",
      "Stochastic Gradient Descent(44242): loss=21.414190546077304\n",
      "Stochastic Gradient Descent(44243): loss=0.7732766307911165\n",
      "Stochastic Gradient Descent(44244): loss=0.03955410830743006\n",
      "Stochastic Gradient Descent(44245): loss=5.384704309581803\n",
      "Stochastic Gradient Descent(44246): loss=4.603464105099266\n",
      "Stochastic Gradient Descent(44247): loss=0.5166817737074032\n",
      "Stochastic Gradient Descent(44248): loss=3.189949199964604\n",
      "Stochastic Gradient Descent(44249): loss=0.0031440100016545485\n",
      "Stochastic Gradient Descent(44250): loss=9.129444574369517\n",
      "Stochastic Gradient Descent(44251): loss=7.28247072831639\n",
      "Stochastic Gradient Descent(44252): loss=7.013695879819192\n",
      "Stochastic Gradient Descent(44253): loss=1.3261957877099977\n",
      "Stochastic Gradient Descent(44254): loss=1.6612682075561889\n",
      "Stochastic Gradient Descent(44255): loss=5.159792411328329\n",
      "Stochastic Gradient Descent(44256): loss=0.002915106320032537\n",
      "Stochastic Gradient Descent(44257): loss=1.1961147545515884\n",
      "Stochastic Gradient Descent(44258): loss=3.668326436769446\n",
      "Stochastic Gradient Descent(44259): loss=6.779082392564195\n",
      "Stochastic Gradient Descent(44260): loss=4.448494888375668\n",
      "Stochastic Gradient Descent(44261): loss=10.328601698176985\n",
      "Stochastic Gradient Descent(44262): loss=2.5819018337796638\n",
      "Stochastic Gradient Descent(44263): loss=7.241309381328345\n",
      "Stochastic Gradient Descent(44264): loss=2.026166474102056\n",
      "Stochastic Gradient Descent(44265): loss=0.1525036002321703\n",
      "Stochastic Gradient Descent(44266): loss=0.3481621595049531\n",
      "Stochastic Gradient Descent(44267): loss=2.055684688842948\n",
      "Stochastic Gradient Descent(44268): loss=2.382116893615813\n",
      "Stochastic Gradient Descent(44269): loss=1.9704090051298706\n",
      "Stochastic Gradient Descent(44270): loss=1.741573279647303\n",
      "Stochastic Gradient Descent(44271): loss=0.24408481186281766\n",
      "Stochastic Gradient Descent(44272): loss=10.469202391061774\n",
      "Stochastic Gradient Descent(44273): loss=0.4582807920432728\n",
      "Stochastic Gradient Descent(44274): loss=1.1608339856014382\n",
      "Stochastic Gradient Descent(44275): loss=10.123080149637888\n",
      "Stochastic Gradient Descent(44276): loss=6.20776374693216\n",
      "Stochastic Gradient Descent(44277): loss=0.3366294548719077\n",
      "Stochastic Gradient Descent(44278): loss=1.9513147016352843\n",
      "Stochastic Gradient Descent(44279): loss=0.02813500177402582\n",
      "Stochastic Gradient Descent(44280): loss=0.20639619643613477\n",
      "Stochastic Gradient Descent(44281): loss=0.44216210201212214\n",
      "Stochastic Gradient Descent(44282): loss=2.5649749009236698\n",
      "Stochastic Gradient Descent(44283): loss=0.3399113983250771\n",
      "Stochastic Gradient Descent(44284): loss=1.6901480123201726\n",
      "Stochastic Gradient Descent(44285): loss=0.6904313678193652\n",
      "Stochastic Gradient Descent(44286): loss=16.01882790244539\n",
      "Stochastic Gradient Descent(44287): loss=2.37997994878179\n",
      "Stochastic Gradient Descent(44288): loss=0.02373842647407314\n",
      "Stochastic Gradient Descent(44289): loss=4.364421585290487\n",
      "Stochastic Gradient Descent(44290): loss=0.37371472583445203\n",
      "Stochastic Gradient Descent(44291): loss=0.3708542145280378\n",
      "Stochastic Gradient Descent(44292): loss=4.714762523690598\n",
      "Stochastic Gradient Descent(44293): loss=8.459321537145335\n",
      "Stochastic Gradient Descent(44294): loss=8.467166259810586\n",
      "Stochastic Gradient Descent(44295): loss=0.055552870912262076\n",
      "Stochastic Gradient Descent(44296): loss=44.55373545528436\n",
      "Stochastic Gradient Descent(44297): loss=9.666555914003872\n",
      "Stochastic Gradient Descent(44298): loss=0.32122326896533804\n",
      "Stochastic Gradient Descent(44299): loss=0.8334368588826926\n",
      "Stochastic Gradient Descent(44300): loss=1.6172202151069461\n",
      "Stochastic Gradient Descent(44301): loss=9.17182716614477\n",
      "Stochastic Gradient Descent(44302): loss=11.86067852867551\n",
      "Stochastic Gradient Descent(44303): loss=3.363640405006383\n",
      "Stochastic Gradient Descent(44304): loss=0.010455865087746204\n",
      "Stochastic Gradient Descent(44305): loss=0.5067884788178039\n",
      "Stochastic Gradient Descent(44306): loss=0.5021058204392197\n",
      "Stochastic Gradient Descent(44307): loss=1.9382942800750942\n",
      "Stochastic Gradient Descent(44308): loss=2.603118475767105\n",
      "Stochastic Gradient Descent(44309): loss=2.0210320852856545\n",
      "Stochastic Gradient Descent(44310): loss=0.012586377850986935\n",
      "Stochastic Gradient Descent(44311): loss=3.5228876937429936\n",
      "Stochastic Gradient Descent(44312): loss=1.3382697903511531\n",
      "Stochastic Gradient Descent(44313): loss=0.07692065656654322\n",
      "Stochastic Gradient Descent(44314): loss=6.310973643011875\n",
      "Stochastic Gradient Descent(44315): loss=5.5633283110132705\n",
      "Stochastic Gradient Descent(44316): loss=1.6055765490678726\n",
      "Stochastic Gradient Descent(44317): loss=0.04181311892060539\n",
      "Stochastic Gradient Descent(44318): loss=2.1650971948944524\n",
      "Stochastic Gradient Descent(44319): loss=9.71683201989522\n",
      "Stochastic Gradient Descent(44320): loss=0.3186947443473391\n",
      "Stochastic Gradient Descent(44321): loss=1.186010464804985\n",
      "Stochastic Gradient Descent(44322): loss=5.986507443510804\n",
      "Stochastic Gradient Descent(44323): loss=0.5606123576255458\n",
      "Stochastic Gradient Descent(44324): loss=0.7619672687166561\n",
      "Stochastic Gradient Descent(44325): loss=0.35978420509097636\n",
      "Stochastic Gradient Descent(44326): loss=0.03773723205539508\n",
      "Stochastic Gradient Descent(44327): loss=0.10244511028717901\n",
      "Stochastic Gradient Descent(44328): loss=1.576669656190951\n",
      "Stochastic Gradient Descent(44329): loss=0.032489918199311296\n",
      "Stochastic Gradient Descent(44330): loss=3.310779173886569\n",
      "Stochastic Gradient Descent(44331): loss=0.020306302643496678\n",
      "Stochastic Gradient Descent(44332): loss=0.7905481728804508\n",
      "Stochastic Gradient Descent(44333): loss=0.04120867249253923\n",
      "Stochastic Gradient Descent(44334): loss=0.9114774727124486\n",
      "Stochastic Gradient Descent(44335): loss=0.36303423357929626\n",
      "Stochastic Gradient Descent(44336): loss=0.34108398230167103\n",
      "Stochastic Gradient Descent(44337): loss=2.1223694331716447\n",
      "Stochastic Gradient Descent(44338): loss=2.9656929295343795\n",
      "Stochastic Gradient Descent(44339): loss=3.736925925928399\n",
      "Stochastic Gradient Descent(44340): loss=6.433854684968427\n",
      "Stochastic Gradient Descent(44341): loss=3.8121223157728306\n",
      "Stochastic Gradient Descent(44342): loss=1.5349878039942042\n",
      "Stochastic Gradient Descent(44343): loss=9.527404818847057\n",
      "Stochastic Gradient Descent(44344): loss=10.849005915064861\n",
      "Stochastic Gradient Descent(44345): loss=6.4767511416221515\n",
      "Stochastic Gradient Descent(44346): loss=4.529476340036001\n",
      "Stochastic Gradient Descent(44347): loss=0.13728767953955248\n",
      "Stochastic Gradient Descent(44348): loss=2.3226995759507387\n",
      "Stochastic Gradient Descent(44349): loss=4.240983164577088\n",
      "Stochastic Gradient Descent(44350): loss=1.284902351613254\n",
      "Stochastic Gradient Descent(44351): loss=1.602411678258137\n",
      "Stochastic Gradient Descent(44352): loss=1.6635032664688187\n",
      "Stochastic Gradient Descent(44353): loss=0.0028560911191361817\n",
      "Stochastic Gradient Descent(44354): loss=0.4607766323532152\n",
      "Stochastic Gradient Descent(44355): loss=0.10439177624664045\n",
      "Stochastic Gradient Descent(44356): loss=7.030157720863397\n",
      "Stochastic Gradient Descent(44357): loss=2.082541814267571\n",
      "Stochastic Gradient Descent(44358): loss=0.9028088239609685\n",
      "Stochastic Gradient Descent(44359): loss=5.5559090013777785\n",
      "Stochastic Gradient Descent(44360): loss=0.5408680210199277\n",
      "Stochastic Gradient Descent(44361): loss=2.902200192185497\n",
      "Stochastic Gradient Descent(44362): loss=2.279343983712065\n",
      "Stochastic Gradient Descent(44363): loss=36.8412139930832\n",
      "Stochastic Gradient Descent(44364): loss=32.732717009207384\n",
      "Stochastic Gradient Descent(44365): loss=1.9085746045779568\n",
      "Stochastic Gradient Descent(44366): loss=0.285933529404102\n",
      "Stochastic Gradient Descent(44367): loss=0.021822018953972323\n",
      "Stochastic Gradient Descent(44368): loss=15.840137352908695\n",
      "Stochastic Gradient Descent(44369): loss=1.1035260831438176\n",
      "Stochastic Gradient Descent(44370): loss=4.613193650097773\n",
      "Stochastic Gradient Descent(44371): loss=6.579356451054964\n",
      "Stochastic Gradient Descent(44372): loss=0.004358066274069765\n",
      "Stochastic Gradient Descent(44373): loss=2.7334346437635975\n",
      "Stochastic Gradient Descent(44374): loss=6.685015196473217\n",
      "Stochastic Gradient Descent(44375): loss=5.620036070887629\n",
      "Stochastic Gradient Descent(44376): loss=7.290750308508598\n",
      "Stochastic Gradient Descent(44377): loss=3.8895689831040743\n",
      "Stochastic Gradient Descent(44378): loss=2.0618468606250464\n",
      "Stochastic Gradient Descent(44379): loss=7.260514913633134\n",
      "Stochastic Gradient Descent(44380): loss=4.753808553618577\n",
      "Stochastic Gradient Descent(44381): loss=0.21072312159387385\n",
      "Stochastic Gradient Descent(44382): loss=0.2820418857684859\n",
      "Stochastic Gradient Descent(44383): loss=0.04841957452890584\n",
      "Stochastic Gradient Descent(44384): loss=0.4500144113779084\n",
      "Stochastic Gradient Descent(44385): loss=5.074183353714987\n",
      "Stochastic Gradient Descent(44386): loss=19.160491852702815\n",
      "Stochastic Gradient Descent(44387): loss=0.40255296360801035\n",
      "Stochastic Gradient Descent(44388): loss=5.015935482747826\n",
      "Stochastic Gradient Descent(44389): loss=2.0257158779670927\n",
      "Stochastic Gradient Descent(44390): loss=0.5034707799099319\n",
      "Stochastic Gradient Descent(44391): loss=3.57099324198482\n",
      "Stochastic Gradient Descent(44392): loss=2.2941817354189395\n",
      "Stochastic Gradient Descent(44393): loss=2.4217221616722027\n",
      "Stochastic Gradient Descent(44394): loss=1.5812850335150925\n",
      "Stochastic Gradient Descent(44395): loss=0.007599432542336263\n",
      "Stochastic Gradient Descent(44396): loss=1.0666419496003132\n",
      "Stochastic Gradient Descent(44397): loss=2.299539697896198\n",
      "Stochastic Gradient Descent(44398): loss=0.624318237754917\n",
      "Stochastic Gradient Descent(44399): loss=9.223069330172166\n",
      "Stochastic Gradient Descent(44400): loss=1.8231044231311608\n",
      "Stochastic Gradient Descent(44401): loss=0.008589487394873916\n",
      "Stochastic Gradient Descent(44402): loss=7.225457638194512\n",
      "Stochastic Gradient Descent(44403): loss=0.15295666698924876\n",
      "Stochastic Gradient Descent(44404): loss=1.370260416708317\n",
      "Stochastic Gradient Descent(44405): loss=9.347274508825304\n",
      "Stochastic Gradient Descent(44406): loss=0.41364934464880754\n",
      "Stochastic Gradient Descent(44407): loss=0.6258992675207574\n",
      "Stochastic Gradient Descent(44408): loss=2.872327289853979\n",
      "Stochastic Gradient Descent(44409): loss=42.92125726950921\n",
      "Stochastic Gradient Descent(44410): loss=6.420786085272982\n",
      "Stochastic Gradient Descent(44411): loss=0.05786287041396031\n",
      "Stochastic Gradient Descent(44412): loss=0.7696035084216795\n",
      "Stochastic Gradient Descent(44413): loss=11.186037897872302\n",
      "Stochastic Gradient Descent(44414): loss=3.5720990236638754\n",
      "Stochastic Gradient Descent(44415): loss=17.61875328407304\n",
      "Stochastic Gradient Descent(44416): loss=0.9366432453928485\n",
      "Stochastic Gradient Descent(44417): loss=0.06800245825307208\n",
      "Stochastic Gradient Descent(44418): loss=0.08194153678325919\n",
      "Stochastic Gradient Descent(44419): loss=7.3765762487646604\n",
      "Stochastic Gradient Descent(44420): loss=0.3234091090381595\n",
      "Stochastic Gradient Descent(44421): loss=0.0058732260605132355\n",
      "Stochastic Gradient Descent(44422): loss=3.602612127508083\n",
      "Stochastic Gradient Descent(44423): loss=3.7921239152015733\n",
      "Stochastic Gradient Descent(44424): loss=0.573492349262258\n",
      "Stochastic Gradient Descent(44425): loss=3.9571451386101293\n",
      "Stochastic Gradient Descent(44426): loss=0.19518890964612526\n",
      "Stochastic Gradient Descent(44427): loss=0.14564782195237452\n",
      "Stochastic Gradient Descent(44428): loss=2.262326453047245\n",
      "Stochastic Gradient Descent(44429): loss=0.9583196258426758\n",
      "Stochastic Gradient Descent(44430): loss=5.340819596913124\n",
      "Stochastic Gradient Descent(44431): loss=1.2780116166548006\n",
      "Stochastic Gradient Descent(44432): loss=1.0218040548162628\n",
      "Stochastic Gradient Descent(44433): loss=0.561013912413253\n",
      "Stochastic Gradient Descent(44434): loss=6.3233121173405875\n",
      "Stochastic Gradient Descent(44435): loss=0.4327867141542205\n",
      "Stochastic Gradient Descent(44436): loss=1.2879440603076644\n",
      "Stochastic Gradient Descent(44437): loss=0.21555341998085523\n",
      "Stochastic Gradient Descent(44438): loss=32.52084096560867\n",
      "Stochastic Gradient Descent(44439): loss=3.530528439933817\n",
      "Stochastic Gradient Descent(44440): loss=1.347979781602243\n",
      "Stochastic Gradient Descent(44441): loss=12.61506071979045\n",
      "Stochastic Gradient Descent(44442): loss=7.036193154652061\n",
      "Stochastic Gradient Descent(44443): loss=51.71355083511955\n",
      "Stochastic Gradient Descent(44444): loss=0.3348161132578908\n",
      "Stochastic Gradient Descent(44445): loss=0.41095365743976503\n",
      "Stochastic Gradient Descent(44446): loss=2.7601416914409067\n",
      "Stochastic Gradient Descent(44447): loss=1.5533934626859691\n",
      "Stochastic Gradient Descent(44448): loss=0.08838003812467135\n",
      "Stochastic Gradient Descent(44449): loss=1.8722632198102642\n",
      "Stochastic Gradient Descent(44450): loss=4.196599291723118\n",
      "Stochastic Gradient Descent(44451): loss=0.3453831900252681\n",
      "Stochastic Gradient Descent(44452): loss=2.3448304169938936\n",
      "Stochastic Gradient Descent(44453): loss=2.3496786515004393\n",
      "Stochastic Gradient Descent(44454): loss=0.2501722841538352\n",
      "Stochastic Gradient Descent(44455): loss=0.1432679461774147\n",
      "Stochastic Gradient Descent(44456): loss=1.3187891675266603\n",
      "Stochastic Gradient Descent(44457): loss=5.71166995743551\n",
      "Stochastic Gradient Descent(44458): loss=12.388941502311422\n",
      "Stochastic Gradient Descent(44459): loss=15.56647723225936\n",
      "Stochastic Gradient Descent(44460): loss=7.117615722520768\n",
      "Stochastic Gradient Descent(44461): loss=1.329712380539384\n",
      "Stochastic Gradient Descent(44462): loss=2.012336736011202\n",
      "Stochastic Gradient Descent(44463): loss=5.953396422242669\n",
      "Stochastic Gradient Descent(44464): loss=1.4303624781121944\n",
      "Stochastic Gradient Descent(44465): loss=10.154498727200338\n",
      "Stochastic Gradient Descent(44466): loss=6.510471860907605\n",
      "Stochastic Gradient Descent(44467): loss=2.170684189204202\n",
      "Stochastic Gradient Descent(44468): loss=0.5407420277310656\n",
      "Stochastic Gradient Descent(44469): loss=3.211485376636137\n",
      "Stochastic Gradient Descent(44470): loss=5.87787849084801\n",
      "Stochastic Gradient Descent(44471): loss=9.62588041870492\n",
      "Stochastic Gradient Descent(44472): loss=1.950757325834973\n",
      "Stochastic Gradient Descent(44473): loss=0.8461334219806518\n",
      "Stochastic Gradient Descent(44474): loss=0.03181344470182171\n",
      "Stochastic Gradient Descent(44475): loss=0.058558332673043056\n",
      "Stochastic Gradient Descent(44476): loss=2.3759015873988374\n",
      "Stochastic Gradient Descent(44477): loss=0.0008134208028719041\n",
      "Stochastic Gradient Descent(44478): loss=0.5506229801270169\n",
      "Stochastic Gradient Descent(44479): loss=0.41557865440662456\n",
      "Stochastic Gradient Descent(44480): loss=7.297195618576912\n",
      "Stochastic Gradient Descent(44481): loss=0.9683172203236575\n",
      "Stochastic Gradient Descent(44482): loss=0.8951373794593839\n",
      "Stochastic Gradient Descent(44483): loss=0.6836433687079375\n",
      "Stochastic Gradient Descent(44484): loss=48.24433236895657\n",
      "Stochastic Gradient Descent(44485): loss=6.131293165142247\n",
      "Stochastic Gradient Descent(44486): loss=4.712851659902718\n",
      "Stochastic Gradient Descent(44487): loss=0.043214106661166984\n",
      "Stochastic Gradient Descent(44488): loss=0.14619381919787097\n",
      "Stochastic Gradient Descent(44489): loss=0.10390992918787492\n",
      "Stochastic Gradient Descent(44490): loss=1.0252361553928895\n",
      "Stochastic Gradient Descent(44491): loss=0.11774869601799001\n",
      "Stochastic Gradient Descent(44492): loss=4.445234454539488\n",
      "Stochastic Gradient Descent(44493): loss=2.6195115444623043\n",
      "Stochastic Gradient Descent(44494): loss=0.49815129749048\n",
      "Stochastic Gradient Descent(44495): loss=1.5186717343886056\n",
      "Stochastic Gradient Descent(44496): loss=4.535423675418171\n",
      "Stochastic Gradient Descent(44497): loss=14.489369192445682\n",
      "Stochastic Gradient Descent(44498): loss=0.32858457741896385\n",
      "Stochastic Gradient Descent(44499): loss=1.6257956954631518\n",
      "Stochastic Gradient Descent(44500): loss=0.08221735977006511\n",
      "Stochastic Gradient Descent(44501): loss=5.461935048170166\n",
      "Stochastic Gradient Descent(44502): loss=4.912287974200357\n",
      "Stochastic Gradient Descent(44503): loss=0.2671760918435939\n",
      "Stochastic Gradient Descent(44504): loss=2.990562799621112\n",
      "Stochastic Gradient Descent(44505): loss=0.1728695501720811\n",
      "Stochastic Gradient Descent(44506): loss=0.9639143851993879\n",
      "Stochastic Gradient Descent(44507): loss=0.44554940914264135\n",
      "Stochastic Gradient Descent(44508): loss=0.11487540199977425\n",
      "Stochastic Gradient Descent(44509): loss=0.0005790350413452207\n",
      "Stochastic Gradient Descent(44510): loss=0.38410988798463\n",
      "Stochastic Gradient Descent(44511): loss=0.19484069726719766\n",
      "Stochastic Gradient Descent(44512): loss=1.3731761514788805\n",
      "Stochastic Gradient Descent(44513): loss=6.415291662627475\n",
      "Stochastic Gradient Descent(44514): loss=0.46523036305132764\n",
      "Stochastic Gradient Descent(44515): loss=24.38123198263947\n",
      "Stochastic Gradient Descent(44516): loss=1.047371320739185\n",
      "Stochastic Gradient Descent(44517): loss=0.010771214401282732\n",
      "Stochastic Gradient Descent(44518): loss=2.218242823763539\n",
      "Stochastic Gradient Descent(44519): loss=2.2790373773080765\n",
      "Stochastic Gradient Descent(44520): loss=1.932581772448978\n",
      "Stochastic Gradient Descent(44521): loss=0.28786456603314026\n",
      "Stochastic Gradient Descent(44522): loss=0.0357332914316543\n",
      "Stochastic Gradient Descent(44523): loss=0.06924259833697322\n",
      "Stochastic Gradient Descent(44524): loss=0.31893596796539325\n",
      "Stochastic Gradient Descent(44525): loss=9.370763231465158\n",
      "Stochastic Gradient Descent(44526): loss=2.351577221882813\n",
      "Stochastic Gradient Descent(44527): loss=1.348091781029402\n",
      "Stochastic Gradient Descent(44528): loss=4.613835133915812\n",
      "Stochastic Gradient Descent(44529): loss=0.8486836458261883\n",
      "Stochastic Gradient Descent(44530): loss=2.8538715300135724\n",
      "Stochastic Gradient Descent(44531): loss=0.0008862602623663007\n",
      "Stochastic Gradient Descent(44532): loss=0.9395514240786602\n",
      "Stochastic Gradient Descent(44533): loss=0.436777391868319\n",
      "Stochastic Gradient Descent(44534): loss=3.2278811025238974\n",
      "Stochastic Gradient Descent(44535): loss=2.1881250470188354\n",
      "Stochastic Gradient Descent(44536): loss=25.093243082647888\n",
      "Stochastic Gradient Descent(44537): loss=0.03137609933419423\n",
      "Stochastic Gradient Descent(44538): loss=3.9186269274682544\n",
      "Stochastic Gradient Descent(44539): loss=0.2519012687281425\n",
      "Stochastic Gradient Descent(44540): loss=0.13453542595040016\n",
      "Stochastic Gradient Descent(44541): loss=0.44404817229921045\n",
      "Stochastic Gradient Descent(44542): loss=0.0315099623894674\n",
      "Stochastic Gradient Descent(44543): loss=0.6457048887293401\n",
      "Stochastic Gradient Descent(44544): loss=4.725449233707471\n",
      "Stochastic Gradient Descent(44545): loss=0.11498939952419146\n",
      "Stochastic Gradient Descent(44546): loss=7.357010009473742\n",
      "Stochastic Gradient Descent(44547): loss=0.7518977746987848\n",
      "Stochastic Gradient Descent(44548): loss=5.420400715144607\n",
      "Stochastic Gradient Descent(44549): loss=5.914622025314355\n",
      "Stochastic Gradient Descent(44550): loss=0.10428024466600173\n",
      "Stochastic Gradient Descent(44551): loss=0.4425259777629952\n",
      "Stochastic Gradient Descent(44552): loss=0.8425591346548598\n",
      "Stochastic Gradient Descent(44553): loss=2.693413056881957\n",
      "Stochastic Gradient Descent(44554): loss=1.129070675754063\n",
      "Stochastic Gradient Descent(44555): loss=0.332001535967153\n",
      "Stochastic Gradient Descent(44556): loss=1.3068504680217152\n",
      "Stochastic Gradient Descent(44557): loss=0.19108718759837925\n",
      "Stochastic Gradient Descent(44558): loss=0.3608349471911201\n",
      "Stochastic Gradient Descent(44559): loss=1.3440942645818128\n",
      "Stochastic Gradient Descent(44560): loss=0.5993599062670965\n",
      "Stochastic Gradient Descent(44561): loss=14.95973991063802\n",
      "Stochastic Gradient Descent(44562): loss=6.1911976776064614\n",
      "Stochastic Gradient Descent(44563): loss=0.03495431105642768\n",
      "Stochastic Gradient Descent(44564): loss=3.102748666278623\n",
      "Stochastic Gradient Descent(44565): loss=6.902683334169087\n",
      "Stochastic Gradient Descent(44566): loss=3.0203787358591447\n",
      "Stochastic Gradient Descent(44567): loss=0.06579820684041612\n",
      "Stochastic Gradient Descent(44568): loss=1.753257861850122\n",
      "Stochastic Gradient Descent(44569): loss=0.0019792598867734864\n",
      "Stochastic Gradient Descent(44570): loss=12.809726911095945\n",
      "Stochastic Gradient Descent(44571): loss=0.9531248826802655\n",
      "Stochastic Gradient Descent(44572): loss=0.35507507183255055\n",
      "Stochastic Gradient Descent(44573): loss=0.4837715741693718\n",
      "Stochastic Gradient Descent(44574): loss=0.023835274915010352\n",
      "Stochastic Gradient Descent(44575): loss=4.934843769989075\n",
      "Stochastic Gradient Descent(44576): loss=8.002904697784446\n",
      "Stochastic Gradient Descent(44577): loss=0.3299749044799582\n",
      "Stochastic Gradient Descent(44578): loss=3.619872932637975\n",
      "Stochastic Gradient Descent(44579): loss=7.804112518456336\n",
      "Stochastic Gradient Descent(44580): loss=0.20801001818873388\n",
      "Stochastic Gradient Descent(44581): loss=9.555028893147737\n",
      "Stochastic Gradient Descent(44582): loss=0.2113957571037799\n",
      "Stochastic Gradient Descent(44583): loss=1.3126609741655313\n",
      "Stochastic Gradient Descent(44584): loss=2.543928280217601\n",
      "Stochastic Gradient Descent(44585): loss=0.9002603906887802\n",
      "Stochastic Gradient Descent(44586): loss=9.506680790175073\n",
      "Stochastic Gradient Descent(44587): loss=3.204576675106577\n",
      "Stochastic Gradient Descent(44588): loss=0.0256719351029113\n",
      "Stochastic Gradient Descent(44589): loss=0.4852561261410966\n",
      "Stochastic Gradient Descent(44590): loss=5.547599958876439\n",
      "Stochastic Gradient Descent(44591): loss=1.7114295720023736\n",
      "Stochastic Gradient Descent(44592): loss=4.149153907086073\n",
      "Stochastic Gradient Descent(44593): loss=0.0680931994789213\n",
      "Stochastic Gradient Descent(44594): loss=0.2712196817604128\n",
      "Stochastic Gradient Descent(44595): loss=0.26613433729576175\n",
      "Stochastic Gradient Descent(44596): loss=0.9192193909198013\n",
      "Stochastic Gradient Descent(44597): loss=14.786001648795775\n",
      "Stochastic Gradient Descent(44598): loss=10.30245200906818\n",
      "Stochastic Gradient Descent(44599): loss=12.689837493280017\n",
      "Stochastic Gradient Descent(44600): loss=3.9453720512601946\n",
      "Stochastic Gradient Descent(44601): loss=0.7365182926854488\n",
      "Stochastic Gradient Descent(44602): loss=6.166428104540657\n",
      "Stochastic Gradient Descent(44603): loss=8.640056876623689\n",
      "Stochastic Gradient Descent(44604): loss=0.5273793115422271\n",
      "Stochastic Gradient Descent(44605): loss=1.0667932536765756\n",
      "Stochastic Gradient Descent(44606): loss=7.165205128622058\n",
      "Stochastic Gradient Descent(44607): loss=3.5023961457108364\n",
      "Stochastic Gradient Descent(44608): loss=8.117660331672909\n",
      "Stochastic Gradient Descent(44609): loss=1.7962952135095698\n",
      "Stochastic Gradient Descent(44610): loss=1.388208351312343\n",
      "Stochastic Gradient Descent(44611): loss=1.8333818332609026\n",
      "Stochastic Gradient Descent(44612): loss=1.8608216582617965\n",
      "Stochastic Gradient Descent(44613): loss=0.03623509298704943\n",
      "Stochastic Gradient Descent(44614): loss=1.2274650864954844\n",
      "Stochastic Gradient Descent(44615): loss=36.24577494624215\n",
      "Stochastic Gradient Descent(44616): loss=3.9312807595512274\n",
      "Stochastic Gradient Descent(44617): loss=1.157931221649745\n",
      "Stochastic Gradient Descent(44618): loss=7.319863123990994\n",
      "Stochastic Gradient Descent(44619): loss=0.09293840287417227\n",
      "Stochastic Gradient Descent(44620): loss=1.080828675656497\n",
      "Stochastic Gradient Descent(44621): loss=2.186110444153295\n",
      "Stochastic Gradient Descent(44622): loss=1.8385250112407396\n",
      "Stochastic Gradient Descent(44623): loss=3.130651289105324\n",
      "Stochastic Gradient Descent(44624): loss=1.1044147995672031\n",
      "Stochastic Gradient Descent(44625): loss=0.06776924679660369\n",
      "Stochastic Gradient Descent(44626): loss=14.287054674052676\n",
      "Stochastic Gradient Descent(44627): loss=0.07792526532255571\n",
      "Stochastic Gradient Descent(44628): loss=0.6729078794893423\n",
      "Stochastic Gradient Descent(44629): loss=0.0543312977330567\n",
      "Stochastic Gradient Descent(44630): loss=6.23761290557533\n",
      "Stochastic Gradient Descent(44631): loss=1.1141752545614152\n",
      "Stochastic Gradient Descent(44632): loss=0.22811691446829274\n",
      "Stochastic Gradient Descent(44633): loss=0.03320103443355676\n",
      "Stochastic Gradient Descent(44634): loss=1.2521226889679187\n",
      "Stochastic Gradient Descent(44635): loss=0.5154414951972641\n",
      "Stochastic Gradient Descent(44636): loss=0.18867874645282884\n",
      "Stochastic Gradient Descent(44637): loss=1.2121266060068374\n",
      "Stochastic Gradient Descent(44638): loss=6.142650036447294\n",
      "Stochastic Gradient Descent(44639): loss=0.44179567779309403\n",
      "Stochastic Gradient Descent(44640): loss=15.290834698727942\n",
      "Stochastic Gradient Descent(44641): loss=1.6314502283836627\n",
      "Stochastic Gradient Descent(44642): loss=0.7161934762230711\n",
      "Stochastic Gradient Descent(44643): loss=5.25354763638761\n",
      "Stochastic Gradient Descent(44644): loss=1.0540916663275788\n",
      "Stochastic Gradient Descent(44645): loss=6.485949700337748\n",
      "Stochastic Gradient Descent(44646): loss=0.10302176350757132\n",
      "Stochastic Gradient Descent(44647): loss=2.139625374376882\n",
      "Stochastic Gradient Descent(44648): loss=10.883527808349461\n",
      "Stochastic Gradient Descent(44649): loss=3.6839008520840917\n",
      "Stochastic Gradient Descent(44650): loss=0.8704927458179584\n",
      "Stochastic Gradient Descent(44651): loss=2.735579108635105\n",
      "Stochastic Gradient Descent(44652): loss=3.571824550523154\n",
      "Stochastic Gradient Descent(44653): loss=0.39102998749042317\n",
      "Stochastic Gradient Descent(44654): loss=2.646384024595015\n",
      "Stochastic Gradient Descent(44655): loss=1.0914704901197585\n",
      "Stochastic Gradient Descent(44656): loss=7.810738341925966\n",
      "Stochastic Gradient Descent(44657): loss=2.130240730290071\n",
      "Stochastic Gradient Descent(44658): loss=0.0009195419612586384\n",
      "Stochastic Gradient Descent(44659): loss=1.2033184612256485\n",
      "Stochastic Gradient Descent(44660): loss=0.08539860195881756\n",
      "Stochastic Gradient Descent(44661): loss=0.06264073659863943\n",
      "Stochastic Gradient Descent(44662): loss=9.467294270920021\n",
      "Stochastic Gradient Descent(44663): loss=2.067853571887004\n",
      "Stochastic Gradient Descent(44664): loss=0.34147368358054353\n",
      "Stochastic Gradient Descent(44665): loss=2.3792344505344203\n",
      "Stochastic Gradient Descent(44666): loss=0.334384261353536\n",
      "Stochastic Gradient Descent(44667): loss=6.524622740579996\n",
      "Stochastic Gradient Descent(44668): loss=9.572826641937043\n",
      "Stochastic Gradient Descent(44669): loss=0.6029550982941457\n",
      "Stochastic Gradient Descent(44670): loss=0.18192307039319505\n",
      "Stochastic Gradient Descent(44671): loss=0.768009540535913\n",
      "Stochastic Gradient Descent(44672): loss=7.900101892652974\n",
      "Stochastic Gradient Descent(44673): loss=1.2212187987766558\n",
      "Stochastic Gradient Descent(44674): loss=4.762047153914147\n",
      "Stochastic Gradient Descent(44675): loss=2.0747453658272024\n",
      "Stochastic Gradient Descent(44676): loss=4.830621723076299\n",
      "Stochastic Gradient Descent(44677): loss=1.185133079630713\n",
      "Stochastic Gradient Descent(44678): loss=2.7413196165062708\n",
      "Stochastic Gradient Descent(44679): loss=6.965393073618817\n",
      "Stochastic Gradient Descent(44680): loss=0.10881490863358403\n",
      "Stochastic Gradient Descent(44681): loss=9.01664665652622\n",
      "Stochastic Gradient Descent(44682): loss=0.10766267878341709\n",
      "Stochastic Gradient Descent(44683): loss=0.2607999553120677\n",
      "Stochastic Gradient Descent(44684): loss=0.8380513535119514\n",
      "Stochastic Gradient Descent(44685): loss=3.6599706369802347\n",
      "Stochastic Gradient Descent(44686): loss=1.0100924352569378\n",
      "Stochastic Gradient Descent(44687): loss=5.2110832172461174\n",
      "Stochastic Gradient Descent(44688): loss=0.09564754451051632\n",
      "Stochastic Gradient Descent(44689): loss=1.7105222445134936\n",
      "Stochastic Gradient Descent(44690): loss=3.2616602846641647\n",
      "Stochastic Gradient Descent(44691): loss=0.7216226858485794\n",
      "Stochastic Gradient Descent(44692): loss=17.30073597588729\n",
      "Stochastic Gradient Descent(44693): loss=2.1512862790299443\n",
      "Stochastic Gradient Descent(44694): loss=0.7278638629775823\n",
      "Stochastic Gradient Descent(44695): loss=3.2301894551742967\n",
      "Stochastic Gradient Descent(44696): loss=5.08164645537985\n",
      "Stochastic Gradient Descent(44697): loss=0.0007370354871620665\n",
      "Stochastic Gradient Descent(44698): loss=0.14308908223486144\n",
      "Stochastic Gradient Descent(44699): loss=0.984711575674604\n",
      "Stochastic Gradient Descent(44700): loss=3.3015587447644403\n",
      "Stochastic Gradient Descent(44701): loss=0.8763966139434637\n",
      "Stochastic Gradient Descent(44702): loss=0.563503551786721\n",
      "Stochastic Gradient Descent(44703): loss=1.8202859626383163\n",
      "Stochastic Gradient Descent(44704): loss=16.36725746592814\n",
      "Stochastic Gradient Descent(44705): loss=0.9906226686257692\n",
      "Stochastic Gradient Descent(44706): loss=0.16524438091729743\n",
      "Stochastic Gradient Descent(44707): loss=3.5573144937964316\n",
      "Stochastic Gradient Descent(44708): loss=0.8129846718628947\n",
      "Stochastic Gradient Descent(44709): loss=0.547141719958138\n",
      "Stochastic Gradient Descent(44710): loss=0.7561571590273839\n",
      "Stochastic Gradient Descent(44711): loss=0.1343292227024708\n",
      "Stochastic Gradient Descent(44712): loss=1.5879629031809637\n",
      "Stochastic Gradient Descent(44713): loss=0.9183298346093204\n",
      "Stochastic Gradient Descent(44714): loss=14.701399150497378\n",
      "Stochastic Gradient Descent(44715): loss=3.130732668275478\n",
      "Stochastic Gradient Descent(44716): loss=7.012117524204552\n",
      "Stochastic Gradient Descent(44717): loss=1.5048344629422834\n",
      "Stochastic Gradient Descent(44718): loss=3.282691861965062\n",
      "Stochastic Gradient Descent(44719): loss=1.4924472171681198\n",
      "Stochastic Gradient Descent(44720): loss=19.203117040852362\n",
      "Stochastic Gradient Descent(44721): loss=1.9242958592710793\n",
      "Stochastic Gradient Descent(44722): loss=0.07046774047743687\n",
      "Stochastic Gradient Descent(44723): loss=1.347391178523077\n",
      "Stochastic Gradient Descent(44724): loss=0.04183372026123011\n",
      "Stochastic Gradient Descent(44725): loss=1.4264341811274435\n",
      "Stochastic Gradient Descent(44726): loss=2.3807088136443686\n",
      "Stochastic Gradient Descent(44727): loss=2.524119888918599\n",
      "Stochastic Gradient Descent(44728): loss=0.18458456385481015\n",
      "Stochastic Gradient Descent(44729): loss=0.14829199304758542\n",
      "Stochastic Gradient Descent(44730): loss=1.2233093908517032\n",
      "Stochastic Gradient Descent(44731): loss=0.06940771436694569\n",
      "Stochastic Gradient Descent(44732): loss=0.3926974942935906\n",
      "Stochastic Gradient Descent(44733): loss=0.02746968755274272\n",
      "Stochastic Gradient Descent(44734): loss=7.2868920254070195\n",
      "Stochastic Gradient Descent(44735): loss=0.004779331204785768\n",
      "Stochastic Gradient Descent(44736): loss=0.28139522006286993\n",
      "Stochastic Gradient Descent(44737): loss=1.6397992166293138\n",
      "Stochastic Gradient Descent(44738): loss=0.9893613479158777\n",
      "Stochastic Gradient Descent(44739): loss=14.509115046561403\n",
      "Stochastic Gradient Descent(44740): loss=4.591232854948057\n",
      "Stochastic Gradient Descent(44741): loss=4.194398239308171\n",
      "Stochastic Gradient Descent(44742): loss=1.5077254281686603\n",
      "Stochastic Gradient Descent(44743): loss=0.009911744083588343\n",
      "Stochastic Gradient Descent(44744): loss=1.3564586106501904\n",
      "Stochastic Gradient Descent(44745): loss=0.013678810662752413\n",
      "Stochastic Gradient Descent(44746): loss=5.582105319734655\n",
      "Stochastic Gradient Descent(44747): loss=5.0824622598463565\n",
      "Stochastic Gradient Descent(44748): loss=0.7760421580133191\n",
      "Stochastic Gradient Descent(44749): loss=1.8139271904032797\n",
      "Stochastic Gradient Descent(44750): loss=1.8092482279199302e-05\n",
      "Stochastic Gradient Descent(44751): loss=0.9812999993413103\n",
      "Stochastic Gradient Descent(44752): loss=1.1913150020069994\n",
      "Stochastic Gradient Descent(44753): loss=4.86589447106001\n",
      "Stochastic Gradient Descent(44754): loss=1.9422845874376853\n",
      "Stochastic Gradient Descent(44755): loss=0.4006680666053556\n",
      "Stochastic Gradient Descent(44756): loss=16.68664826347418\n",
      "Stochastic Gradient Descent(44757): loss=24.228096589564778\n",
      "Stochastic Gradient Descent(44758): loss=12.786751969715997\n",
      "Stochastic Gradient Descent(44759): loss=0.006135937320820814\n",
      "Stochastic Gradient Descent(44760): loss=11.496483290646706\n",
      "Stochastic Gradient Descent(44761): loss=0.6116936452050289\n",
      "Stochastic Gradient Descent(44762): loss=3.6585648480477846\n",
      "Stochastic Gradient Descent(44763): loss=0.29021386071432675\n",
      "Stochastic Gradient Descent(44764): loss=0.2660099498910329\n",
      "Stochastic Gradient Descent(44765): loss=2.340946157659982\n",
      "Stochastic Gradient Descent(44766): loss=6.985516939334259\n",
      "Stochastic Gradient Descent(44767): loss=1.7229503918405802\n",
      "Stochastic Gradient Descent(44768): loss=2.386391282264862\n",
      "Stochastic Gradient Descent(44769): loss=0.785920687587865\n",
      "Stochastic Gradient Descent(44770): loss=3.924457430502696\n",
      "Stochastic Gradient Descent(44771): loss=0.5005168166005143\n",
      "Stochastic Gradient Descent(44772): loss=3.792428703332673\n",
      "Stochastic Gradient Descent(44773): loss=11.159392204958182\n",
      "Stochastic Gradient Descent(44774): loss=0.2822823832025708\n",
      "Stochastic Gradient Descent(44775): loss=5.576832562826523\n",
      "Stochastic Gradient Descent(44776): loss=0.9909601247134291\n",
      "Stochastic Gradient Descent(44777): loss=0.08184141072401876\n",
      "Stochastic Gradient Descent(44778): loss=5.8069442194947465\n",
      "Stochastic Gradient Descent(44779): loss=7.145642729940096\n",
      "Stochastic Gradient Descent(44780): loss=5.259315161396375\n",
      "Stochastic Gradient Descent(44781): loss=3.7459104056415957\n",
      "Stochastic Gradient Descent(44782): loss=9.438836084569266\n",
      "Stochastic Gradient Descent(44783): loss=0.08716656550942416\n",
      "Stochastic Gradient Descent(44784): loss=5.57336778171835\n",
      "Stochastic Gradient Descent(44785): loss=1.404699367507561\n",
      "Stochastic Gradient Descent(44786): loss=2.1349904412490326\n",
      "Stochastic Gradient Descent(44787): loss=2.5023683711236706\n",
      "Stochastic Gradient Descent(44788): loss=0.014216021251513184\n",
      "Stochastic Gradient Descent(44789): loss=8.863096110363298\n",
      "Stochastic Gradient Descent(44790): loss=5.034507078810675e-05\n",
      "Stochastic Gradient Descent(44791): loss=6.379736242385848\n",
      "Stochastic Gradient Descent(44792): loss=6.893006914655114\n",
      "Stochastic Gradient Descent(44793): loss=1.1310020886667913\n",
      "Stochastic Gradient Descent(44794): loss=0.28415799402718334\n",
      "Stochastic Gradient Descent(44795): loss=2.503012546816898\n",
      "Stochastic Gradient Descent(44796): loss=0.08588059020901842\n",
      "Stochastic Gradient Descent(44797): loss=0.045486588007869255\n",
      "Stochastic Gradient Descent(44798): loss=5.405770610278319\n",
      "Stochastic Gradient Descent(44799): loss=6.967744254466021\n",
      "Stochastic Gradient Descent(44800): loss=0.005325262655267185\n",
      "Stochastic Gradient Descent(44801): loss=0.3138276134634489\n",
      "Stochastic Gradient Descent(44802): loss=3.09199798626965\n",
      "Stochastic Gradient Descent(44803): loss=3.081961869536129\n",
      "Stochastic Gradient Descent(44804): loss=2.065998586550199\n",
      "Stochastic Gradient Descent(44805): loss=3.336785774472852\n",
      "Stochastic Gradient Descent(44806): loss=1.4313267282240556\n",
      "Stochastic Gradient Descent(44807): loss=3.8243101960720374\n",
      "Stochastic Gradient Descent(44808): loss=0.009666640077233765\n",
      "Stochastic Gradient Descent(44809): loss=0.1953371475962087\n",
      "Stochastic Gradient Descent(44810): loss=0.23189119489498333\n",
      "Stochastic Gradient Descent(44811): loss=0.6653818624435256\n",
      "Stochastic Gradient Descent(44812): loss=1.1148359741297829\n",
      "Stochastic Gradient Descent(44813): loss=6.140376397766608\n",
      "Stochastic Gradient Descent(44814): loss=4.035370243936877\n",
      "Stochastic Gradient Descent(44815): loss=0.6863873854321628\n",
      "Stochastic Gradient Descent(44816): loss=0.03852208814432025\n",
      "Stochastic Gradient Descent(44817): loss=5.5528809841146805\n",
      "Stochastic Gradient Descent(44818): loss=0.9191697967732523\n",
      "Stochastic Gradient Descent(44819): loss=3.3600020105879924\n",
      "Stochastic Gradient Descent(44820): loss=3.146183306326628\n",
      "Stochastic Gradient Descent(44821): loss=0.012196116755952684\n",
      "Stochastic Gradient Descent(44822): loss=6.75667317873153\n",
      "Stochastic Gradient Descent(44823): loss=3.813841204042609\n",
      "Stochastic Gradient Descent(44824): loss=9.684767473464111\n",
      "Stochastic Gradient Descent(44825): loss=10.497819094526577\n",
      "Stochastic Gradient Descent(44826): loss=1.9490289178924802\n",
      "Stochastic Gradient Descent(44827): loss=5.89880367401964\n",
      "Stochastic Gradient Descent(44828): loss=0.7127281643613538\n",
      "Stochastic Gradient Descent(44829): loss=23.10883626483725\n",
      "Stochastic Gradient Descent(44830): loss=1.3778632291144042\n",
      "Stochastic Gradient Descent(44831): loss=0.6036193098064631\n",
      "Stochastic Gradient Descent(44832): loss=0.16114744550453683\n",
      "Stochastic Gradient Descent(44833): loss=0.033898196095708544\n",
      "Stochastic Gradient Descent(44834): loss=0.8772536129456331\n",
      "Stochastic Gradient Descent(44835): loss=2.349412082854254\n",
      "Stochastic Gradient Descent(44836): loss=5.142040835763304\n",
      "Stochastic Gradient Descent(44837): loss=10.20903988247364\n",
      "Stochastic Gradient Descent(44838): loss=0.00020385789896051697\n",
      "Stochastic Gradient Descent(44839): loss=1.6591592654068061\n",
      "Stochastic Gradient Descent(44840): loss=2.0246527129669945\n",
      "Stochastic Gradient Descent(44841): loss=5.196726040110456\n",
      "Stochastic Gradient Descent(44842): loss=0.15854862599419084\n",
      "Stochastic Gradient Descent(44843): loss=0.8645443781875404\n",
      "Stochastic Gradient Descent(44844): loss=0.06793538928398432\n",
      "Stochastic Gradient Descent(44845): loss=0.4107620617198264\n",
      "Stochastic Gradient Descent(44846): loss=7.253183733978369\n",
      "Stochastic Gradient Descent(44847): loss=12.557175878516198\n",
      "Stochastic Gradient Descent(44848): loss=1.7461406451955386\n",
      "Stochastic Gradient Descent(44849): loss=0.29170630990608853\n",
      "Stochastic Gradient Descent(44850): loss=0.7823307050122005\n",
      "Stochastic Gradient Descent(44851): loss=0.066844190081301\n",
      "Stochastic Gradient Descent(44852): loss=1.5731063475049558\n",
      "Stochastic Gradient Descent(44853): loss=6.621685442492734\n",
      "Stochastic Gradient Descent(44854): loss=12.730554386057563\n",
      "Stochastic Gradient Descent(44855): loss=1.200176289995942\n",
      "Stochastic Gradient Descent(44856): loss=0.0006650360827495559\n",
      "Stochastic Gradient Descent(44857): loss=1.2847973702445907\n",
      "Stochastic Gradient Descent(44858): loss=0.0167288278622178\n",
      "Stochastic Gradient Descent(44859): loss=13.22518189068616\n",
      "Stochastic Gradient Descent(44860): loss=0.9615299183379014\n",
      "Stochastic Gradient Descent(44861): loss=7.532230593925078\n",
      "Stochastic Gradient Descent(44862): loss=1.1003002272274012\n",
      "Stochastic Gradient Descent(44863): loss=8.572257408398187\n",
      "Stochastic Gradient Descent(44864): loss=8.278593234625895\n",
      "Stochastic Gradient Descent(44865): loss=0.37366248015874426\n",
      "Stochastic Gradient Descent(44866): loss=0.09279295355418676\n",
      "Stochastic Gradient Descent(44867): loss=7.208816158811281\n",
      "Stochastic Gradient Descent(44868): loss=0.495700148363135\n",
      "Stochastic Gradient Descent(44869): loss=0.7710859804103706\n",
      "Stochastic Gradient Descent(44870): loss=1.995261306202937\n",
      "Stochastic Gradient Descent(44871): loss=14.193879549433792\n",
      "Stochastic Gradient Descent(44872): loss=3.678746698790314\n",
      "Stochastic Gradient Descent(44873): loss=4.031030239856815\n",
      "Stochastic Gradient Descent(44874): loss=3.7403374765478943\n",
      "Stochastic Gradient Descent(44875): loss=0.0656060982379342\n",
      "Stochastic Gradient Descent(44876): loss=0.30465790204227244\n",
      "Stochastic Gradient Descent(44877): loss=1.658239336909261\n",
      "Stochastic Gradient Descent(44878): loss=0.20683332209111477\n",
      "Stochastic Gradient Descent(44879): loss=0.43058770084213455\n",
      "Stochastic Gradient Descent(44880): loss=0.7091796061366735\n",
      "Stochastic Gradient Descent(44881): loss=0.4871783862313953\n",
      "Stochastic Gradient Descent(44882): loss=0.3509150723038776\n",
      "Stochastic Gradient Descent(44883): loss=0.014838212700596522\n",
      "Stochastic Gradient Descent(44884): loss=1.4755066222206836\n",
      "Stochastic Gradient Descent(44885): loss=0.5141458411435301\n",
      "Stochastic Gradient Descent(44886): loss=0.04389796961481561\n",
      "Stochastic Gradient Descent(44887): loss=2.46031278008789\n",
      "Stochastic Gradient Descent(44888): loss=0.0029111020963870776\n",
      "Stochastic Gradient Descent(44889): loss=2.296326559753582\n",
      "Stochastic Gradient Descent(44890): loss=1.8998676104041945\n",
      "Stochastic Gradient Descent(44891): loss=8.135332202301365\n",
      "Stochastic Gradient Descent(44892): loss=2.009659373168159\n",
      "Stochastic Gradient Descent(44893): loss=1.7029873896115486\n",
      "Stochastic Gradient Descent(44894): loss=2.1720579477314317\n",
      "Stochastic Gradient Descent(44895): loss=0.0013974834635969177\n",
      "Stochastic Gradient Descent(44896): loss=0.40973953876406516\n",
      "Stochastic Gradient Descent(44897): loss=0.659897668590965\n",
      "Stochastic Gradient Descent(44898): loss=3.5447951091726484\n",
      "Stochastic Gradient Descent(44899): loss=4.019077030827282\n",
      "Stochastic Gradient Descent(44900): loss=0.027187481739537608\n",
      "Stochastic Gradient Descent(44901): loss=0.190758665495842\n",
      "Stochastic Gradient Descent(44902): loss=33.0126355393614\n",
      "Stochastic Gradient Descent(44903): loss=26.327505100932957\n",
      "Stochastic Gradient Descent(44904): loss=9.237635717910738\n",
      "Stochastic Gradient Descent(44905): loss=4.356663418380169\n",
      "Stochastic Gradient Descent(44906): loss=3.765559146790186\n",
      "Stochastic Gradient Descent(44907): loss=0.6266277911327018\n",
      "Stochastic Gradient Descent(44908): loss=0.014692063958553308\n",
      "Stochastic Gradient Descent(44909): loss=14.782625964479143\n",
      "Stochastic Gradient Descent(44910): loss=1.3470809996653659\n",
      "Stochastic Gradient Descent(44911): loss=0.226722531760192\n",
      "Stochastic Gradient Descent(44912): loss=0.17149752457259837\n",
      "Stochastic Gradient Descent(44913): loss=0.4354990004329391\n",
      "Stochastic Gradient Descent(44914): loss=0.008623969554928043\n",
      "Stochastic Gradient Descent(44915): loss=0.02556312483363246\n",
      "Stochastic Gradient Descent(44916): loss=1.8981710886202179\n",
      "Stochastic Gradient Descent(44917): loss=1.2192424317633581\n",
      "Stochastic Gradient Descent(44918): loss=3.6303701932605246\n",
      "Stochastic Gradient Descent(44919): loss=0.001305579328272965\n",
      "Stochastic Gradient Descent(44920): loss=4.364371342471805\n",
      "Stochastic Gradient Descent(44921): loss=7.8710421413678135\n",
      "Stochastic Gradient Descent(44922): loss=10.349202505049805\n",
      "Stochastic Gradient Descent(44923): loss=9.488384577514346\n",
      "Stochastic Gradient Descent(44924): loss=1.1482995992439293\n",
      "Stochastic Gradient Descent(44925): loss=8.521718582648237\n",
      "Stochastic Gradient Descent(44926): loss=2.791132861162225\n",
      "Stochastic Gradient Descent(44927): loss=10.791005483440657\n",
      "Stochastic Gradient Descent(44928): loss=1.1208573373009811\n",
      "Stochastic Gradient Descent(44929): loss=12.841459448529273\n",
      "Stochastic Gradient Descent(44930): loss=0.018730804237077358\n",
      "Stochastic Gradient Descent(44931): loss=1.9015865118937199\n",
      "Stochastic Gradient Descent(44932): loss=1.3384792686080016\n",
      "Stochastic Gradient Descent(44933): loss=3.908848690364529\n",
      "Stochastic Gradient Descent(44934): loss=2.3664104089456734\n",
      "Stochastic Gradient Descent(44935): loss=1.329850013655731\n",
      "Stochastic Gradient Descent(44936): loss=3.185390820438991\n",
      "Stochastic Gradient Descent(44937): loss=0.5829140171949423\n",
      "Stochastic Gradient Descent(44938): loss=1.8614216607100518\n",
      "Stochastic Gradient Descent(44939): loss=1.452057195149368\n",
      "Stochastic Gradient Descent(44940): loss=3.402220345108478\n",
      "Stochastic Gradient Descent(44941): loss=1.2950124031033257\n",
      "Stochastic Gradient Descent(44942): loss=0.9128205711997347\n",
      "Stochastic Gradient Descent(44943): loss=2.1725015008356205\n",
      "Stochastic Gradient Descent(44944): loss=8.609912964532704\n",
      "Stochastic Gradient Descent(44945): loss=3.2940138599146622\n",
      "Stochastic Gradient Descent(44946): loss=22.50209214912629\n",
      "Stochastic Gradient Descent(44947): loss=0.03602882381797415\n",
      "Stochastic Gradient Descent(44948): loss=0.32352237944540574\n",
      "Stochastic Gradient Descent(44949): loss=2.5724885379401257\n",
      "Stochastic Gradient Descent(44950): loss=1.1130366442104134\n",
      "Stochastic Gradient Descent(44951): loss=1.6060763695078677\n",
      "Stochastic Gradient Descent(44952): loss=0.9374625143351306\n",
      "Stochastic Gradient Descent(44953): loss=0.00304758202152771\n",
      "Stochastic Gradient Descent(44954): loss=1.9512931553294937\n",
      "Stochastic Gradient Descent(44955): loss=0.9342665815036516\n",
      "Stochastic Gradient Descent(44956): loss=1.9782757041620902\n",
      "Stochastic Gradient Descent(44957): loss=15.854608524719977\n",
      "Stochastic Gradient Descent(44958): loss=0.6611196167435447\n",
      "Stochastic Gradient Descent(44959): loss=10.064133928555044\n",
      "Stochastic Gradient Descent(44960): loss=4.658114847372234\n",
      "Stochastic Gradient Descent(44961): loss=2.079267609043039\n",
      "Stochastic Gradient Descent(44962): loss=5.102729477764219\n",
      "Stochastic Gradient Descent(44963): loss=22.19777217315411\n",
      "Stochastic Gradient Descent(44964): loss=3.7520825890284484\n",
      "Stochastic Gradient Descent(44965): loss=10.584351519938856\n",
      "Stochastic Gradient Descent(44966): loss=12.009349907613082\n",
      "Stochastic Gradient Descent(44967): loss=2.819212098164394\n",
      "Stochastic Gradient Descent(44968): loss=0.5532055980132066\n",
      "Stochastic Gradient Descent(44969): loss=0.21208496018713247\n",
      "Stochastic Gradient Descent(44970): loss=0.40448522136416604\n",
      "Stochastic Gradient Descent(44971): loss=0.02473685049114868\n",
      "Stochastic Gradient Descent(44972): loss=13.112658259103148\n",
      "Stochastic Gradient Descent(44973): loss=2.1171120375119563\n",
      "Stochastic Gradient Descent(44974): loss=3.1924133574337237\n",
      "Stochastic Gradient Descent(44975): loss=6.802069347433032\n",
      "Stochastic Gradient Descent(44976): loss=0.14580732626238493\n",
      "Stochastic Gradient Descent(44977): loss=0.6588159793014263\n",
      "Stochastic Gradient Descent(44978): loss=0.0002286136974486159\n",
      "Stochastic Gradient Descent(44979): loss=2.3981094050796203\n",
      "Stochastic Gradient Descent(44980): loss=0.010160352134892433\n",
      "Stochastic Gradient Descent(44981): loss=1.526187478506805\n",
      "Stochastic Gradient Descent(44982): loss=2.958788582302792\n",
      "Stochastic Gradient Descent(44983): loss=0.8570821561379791\n",
      "Stochastic Gradient Descent(44984): loss=0.19130348286969315\n",
      "Stochastic Gradient Descent(44985): loss=0.4148783423406914\n",
      "Stochastic Gradient Descent(44986): loss=3.992543369731975\n",
      "Stochastic Gradient Descent(44987): loss=0.0034373413436192377\n",
      "Stochastic Gradient Descent(44988): loss=0.12135643531057867\n",
      "Stochastic Gradient Descent(44989): loss=0.4040072292996687\n",
      "Stochastic Gradient Descent(44990): loss=8.835587271487382\n",
      "Stochastic Gradient Descent(44991): loss=2.5315905794251745\n",
      "Stochastic Gradient Descent(44992): loss=0.11665214467911669\n",
      "Stochastic Gradient Descent(44993): loss=3.248808491985813\n",
      "Stochastic Gradient Descent(44994): loss=5.463895169232795\n",
      "Stochastic Gradient Descent(44995): loss=0.12087783343327885\n",
      "Stochastic Gradient Descent(44996): loss=6.535235747909102\n",
      "Stochastic Gradient Descent(44997): loss=12.385185886488264\n",
      "Stochastic Gradient Descent(44998): loss=0.05191843203659865\n",
      "Stochastic Gradient Descent(44999): loss=5.447560360913695\n",
      "Stochastic Gradient Descent(45000): loss=1.304906092794306\n",
      "Stochastic Gradient Descent(45001): loss=10.27620951109656\n",
      "Stochastic Gradient Descent(45002): loss=44.79053166444639\n",
      "Stochastic Gradient Descent(45003): loss=4.8312817888184085\n",
      "Stochastic Gradient Descent(45004): loss=0.05051951705656279\n",
      "Stochastic Gradient Descent(45005): loss=0.20077619686116552\n",
      "Stochastic Gradient Descent(45006): loss=3.7591373600859805\n",
      "Stochastic Gradient Descent(45007): loss=4.566799766144355\n",
      "Stochastic Gradient Descent(45008): loss=2.2280753787420258\n",
      "Stochastic Gradient Descent(45009): loss=0.12741406070492736\n",
      "Stochastic Gradient Descent(45010): loss=9.924695927755053\n",
      "Stochastic Gradient Descent(45011): loss=1.0693979097692836\n",
      "Stochastic Gradient Descent(45012): loss=0.054286743518673505\n",
      "Stochastic Gradient Descent(45013): loss=0.16781282623792548\n",
      "Stochastic Gradient Descent(45014): loss=3.0832802331997944\n",
      "Stochastic Gradient Descent(45015): loss=0.12573561414074852\n",
      "Stochastic Gradient Descent(45016): loss=0.8416272316886965\n",
      "Stochastic Gradient Descent(45017): loss=0.009017809767439997\n",
      "Stochastic Gradient Descent(45018): loss=0.16694750481585185\n",
      "Stochastic Gradient Descent(45019): loss=6.433718137820425\n",
      "Stochastic Gradient Descent(45020): loss=1.5824484596860262\n",
      "Stochastic Gradient Descent(45021): loss=0.0011017065169287492\n",
      "Stochastic Gradient Descent(45022): loss=0.4525684470385086\n",
      "Stochastic Gradient Descent(45023): loss=0.030082089785543453\n",
      "Stochastic Gradient Descent(45024): loss=4.028141875391997\n",
      "Stochastic Gradient Descent(45025): loss=2.5800649927349264\n",
      "Stochastic Gradient Descent(45026): loss=10.120255032821783\n",
      "Stochastic Gradient Descent(45027): loss=1.0415572055487745\n",
      "Stochastic Gradient Descent(45028): loss=0.02696189034556768\n",
      "Stochastic Gradient Descent(45029): loss=11.491409962191753\n",
      "Stochastic Gradient Descent(45030): loss=0.8901863350426624\n",
      "Stochastic Gradient Descent(45031): loss=4.391608707579739\n",
      "Stochastic Gradient Descent(45032): loss=18.43507635806742\n",
      "Stochastic Gradient Descent(45033): loss=0.015488143972925743\n",
      "Stochastic Gradient Descent(45034): loss=12.151084463439345\n",
      "Stochastic Gradient Descent(45035): loss=4.69379436169154\n",
      "Stochastic Gradient Descent(45036): loss=0.32222442311091226\n",
      "Stochastic Gradient Descent(45037): loss=0.6897575314187371\n",
      "Stochastic Gradient Descent(45038): loss=23.788095292285046\n",
      "Stochastic Gradient Descent(45039): loss=2.318677812390121\n",
      "Stochastic Gradient Descent(45040): loss=0.8134937434672697\n",
      "Stochastic Gradient Descent(45041): loss=21.201512690905442\n",
      "Stochastic Gradient Descent(45042): loss=0.8414740702462685\n",
      "Stochastic Gradient Descent(45043): loss=1.0306294779942826\n",
      "Stochastic Gradient Descent(45044): loss=0.038394881004537404\n",
      "Stochastic Gradient Descent(45045): loss=2.4185242929097788\n",
      "Stochastic Gradient Descent(45046): loss=1.593087523201818\n",
      "Stochastic Gradient Descent(45047): loss=8.801814984567681\n",
      "Stochastic Gradient Descent(45048): loss=0.9248679035127998\n",
      "Stochastic Gradient Descent(45049): loss=6.845120930975391\n",
      "Stochastic Gradient Descent(45050): loss=0.177284300454377\n",
      "Stochastic Gradient Descent(45051): loss=1.3209386584943994\n",
      "Stochastic Gradient Descent(45052): loss=9.237122270000498\n",
      "Stochastic Gradient Descent(45053): loss=1.5839571133591765\n",
      "Stochastic Gradient Descent(45054): loss=2.3096589248340087\n",
      "Stochastic Gradient Descent(45055): loss=0.0580847332908112\n",
      "Stochastic Gradient Descent(45056): loss=1.318388535916608\n",
      "Stochastic Gradient Descent(45057): loss=1.5567660923985092\n",
      "Stochastic Gradient Descent(45058): loss=0.04296911119888187\n",
      "Stochastic Gradient Descent(45059): loss=0.5592085467186106\n",
      "Stochastic Gradient Descent(45060): loss=0.4604816868149145\n",
      "Stochastic Gradient Descent(45061): loss=11.410088944899575\n",
      "Stochastic Gradient Descent(45062): loss=2.2542489474804057\n",
      "Stochastic Gradient Descent(45063): loss=5.1221641839551335\n",
      "Stochastic Gradient Descent(45064): loss=6.952790257918066\n",
      "Stochastic Gradient Descent(45065): loss=13.615081115153645\n",
      "Stochastic Gradient Descent(45066): loss=1.0014376145929642\n",
      "Stochastic Gradient Descent(45067): loss=0.19033228095538862\n",
      "Stochastic Gradient Descent(45068): loss=0.9726933350615052\n",
      "Stochastic Gradient Descent(45069): loss=0.31939790520040684\n",
      "Stochastic Gradient Descent(45070): loss=3.541107016282082\n",
      "Stochastic Gradient Descent(45071): loss=8.595702219099707\n",
      "Stochastic Gradient Descent(45072): loss=0.0120078796827691\n",
      "Stochastic Gradient Descent(45073): loss=10.809277002341013\n",
      "Stochastic Gradient Descent(45074): loss=6.044041380558803\n",
      "Stochastic Gradient Descent(45075): loss=7.538613480433324\n",
      "Stochastic Gradient Descent(45076): loss=6.365991073793319\n",
      "Stochastic Gradient Descent(45077): loss=0.34103267679863375\n",
      "Stochastic Gradient Descent(45078): loss=1.2505436121762565\n",
      "Stochastic Gradient Descent(45079): loss=1.7612463505578977\n",
      "Stochastic Gradient Descent(45080): loss=0.00037560881848271535\n",
      "Stochastic Gradient Descent(45081): loss=0.5729183202770693\n",
      "Stochastic Gradient Descent(45082): loss=0.005055313404250332\n",
      "Stochastic Gradient Descent(45083): loss=1.5327342643473787\n",
      "Stochastic Gradient Descent(45084): loss=0.4822942020144125\n",
      "Stochastic Gradient Descent(45085): loss=2.3485748398733706\n",
      "Stochastic Gradient Descent(45086): loss=5.326645208255186\n",
      "Stochastic Gradient Descent(45087): loss=3.067433123983011\n",
      "Stochastic Gradient Descent(45088): loss=6.686883882928807\n",
      "Stochastic Gradient Descent(45089): loss=3.559164255193832\n",
      "Stochastic Gradient Descent(45090): loss=11.881975965093377\n",
      "Stochastic Gradient Descent(45091): loss=10.229777591227707\n",
      "Stochastic Gradient Descent(45092): loss=0.12157025794399329\n",
      "Stochastic Gradient Descent(45093): loss=3.450545723205272\n",
      "Stochastic Gradient Descent(45094): loss=1.0047521544201625\n",
      "Stochastic Gradient Descent(45095): loss=2.4774925216052908\n",
      "Stochastic Gradient Descent(45096): loss=0.09536917227705212\n",
      "Stochastic Gradient Descent(45097): loss=1.2391511337883505\n",
      "Stochastic Gradient Descent(45098): loss=0.08991311728850758\n",
      "Stochastic Gradient Descent(45099): loss=0.10419820094672363\n",
      "Stochastic Gradient Descent(45100): loss=11.151012376402399\n",
      "Stochastic Gradient Descent(45101): loss=0.6873506791278546\n",
      "Stochastic Gradient Descent(45102): loss=25.276997246869694\n",
      "Stochastic Gradient Descent(45103): loss=4.666733802047248\n",
      "Stochastic Gradient Descent(45104): loss=0.8406361305523581\n",
      "Stochastic Gradient Descent(45105): loss=4.323686946980562\n",
      "Stochastic Gradient Descent(45106): loss=3.2902344125871976\n",
      "Stochastic Gradient Descent(45107): loss=0.9920274194515626\n",
      "Stochastic Gradient Descent(45108): loss=4.042075985813228\n",
      "Stochastic Gradient Descent(45109): loss=3.5992088806924856\n",
      "Stochastic Gradient Descent(45110): loss=7.820400411338024\n",
      "Stochastic Gradient Descent(45111): loss=0.34511500531935796\n",
      "Stochastic Gradient Descent(45112): loss=0.01109809116581575\n",
      "Stochastic Gradient Descent(45113): loss=2.038062838341812\n",
      "Stochastic Gradient Descent(45114): loss=0.004623386668673693\n",
      "Stochastic Gradient Descent(45115): loss=1.0038529303193513\n",
      "Stochastic Gradient Descent(45116): loss=0.021940352332268483\n",
      "Stochastic Gradient Descent(45117): loss=1.0152316819969944\n",
      "Stochastic Gradient Descent(45118): loss=1.1984226776238953\n",
      "Stochastic Gradient Descent(45119): loss=0.06457545376873088\n",
      "Stochastic Gradient Descent(45120): loss=0.020514965383704002\n",
      "Stochastic Gradient Descent(45121): loss=0.7240550074057763\n",
      "Stochastic Gradient Descent(45122): loss=2.0232810900792835\n",
      "Stochastic Gradient Descent(45123): loss=5.168592690457292\n",
      "Stochastic Gradient Descent(45124): loss=0.070560940928114\n",
      "Stochastic Gradient Descent(45125): loss=0.12398748528880729\n",
      "Stochastic Gradient Descent(45126): loss=0.001056238380793584\n",
      "Stochastic Gradient Descent(45127): loss=0.12694555352272643\n",
      "Stochastic Gradient Descent(45128): loss=1.9253531534896753\n",
      "Stochastic Gradient Descent(45129): loss=2.9983864534895086\n",
      "Stochastic Gradient Descent(45130): loss=0.011196282399246716\n",
      "Stochastic Gradient Descent(45131): loss=2.7782348309541445\n",
      "Stochastic Gradient Descent(45132): loss=10.702068724002837\n",
      "Stochastic Gradient Descent(45133): loss=3.2615787849949522\n",
      "Stochastic Gradient Descent(45134): loss=19.5007903225107\n",
      "Stochastic Gradient Descent(45135): loss=1.8373579118260097\n",
      "Stochastic Gradient Descent(45136): loss=8.667524823733823\n",
      "Stochastic Gradient Descent(45137): loss=0.04281472516155749\n",
      "Stochastic Gradient Descent(45138): loss=0.37754067865598995\n",
      "Stochastic Gradient Descent(45139): loss=0.30078077550332827\n",
      "Stochastic Gradient Descent(45140): loss=12.290499594965613\n",
      "Stochastic Gradient Descent(45141): loss=0.9146570630125201\n",
      "Stochastic Gradient Descent(45142): loss=0.03194021007293053\n",
      "Stochastic Gradient Descent(45143): loss=3.699605303096078\n",
      "Stochastic Gradient Descent(45144): loss=0.25049040079063034\n",
      "Stochastic Gradient Descent(45145): loss=6.585576889497293\n",
      "Stochastic Gradient Descent(45146): loss=0.2618154374827948\n",
      "Stochastic Gradient Descent(45147): loss=10.568138793884023\n",
      "Stochastic Gradient Descent(45148): loss=3.677014518911948\n",
      "Stochastic Gradient Descent(45149): loss=6.119820012474046\n",
      "Stochastic Gradient Descent(45150): loss=0.02107670149646448\n",
      "Stochastic Gradient Descent(45151): loss=0.4384893477027123\n",
      "Stochastic Gradient Descent(45152): loss=0.5111790368744152\n",
      "Stochastic Gradient Descent(45153): loss=6.438953216253083\n",
      "Stochastic Gradient Descent(45154): loss=0.18043953437047405\n",
      "Stochastic Gradient Descent(45155): loss=1.5007751742443673\n",
      "Stochastic Gradient Descent(45156): loss=1.075544436219236\n",
      "Stochastic Gradient Descent(45157): loss=7.112077528412766\n",
      "Stochastic Gradient Descent(45158): loss=15.824593484095715\n",
      "Stochastic Gradient Descent(45159): loss=1.50413981449559\n",
      "Stochastic Gradient Descent(45160): loss=30.823110043616808\n",
      "Stochastic Gradient Descent(45161): loss=0.9848790843422464\n",
      "Stochastic Gradient Descent(45162): loss=0.5951580476295334\n",
      "Stochastic Gradient Descent(45163): loss=1.6746778175456662\n",
      "Stochastic Gradient Descent(45164): loss=0.8295293345953757\n",
      "Stochastic Gradient Descent(45165): loss=0.009969360443966358\n",
      "Stochastic Gradient Descent(45166): loss=8.684209288406299\n",
      "Stochastic Gradient Descent(45167): loss=2.6014473536851095\n",
      "Stochastic Gradient Descent(45168): loss=2.8317456554144003\n",
      "Stochastic Gradient Descent(45169): loss=1.0426413732588115e-07\n",
      "Stochastic Gradient Descent(45170): loss=13.793726249808355\n",
      "Stochastic Gradient Descent(45171): loss=4.315753443060474\n",
      "Stochastic Gradient Descent(45172): loss=29.844947822479572\n",
      "Stochastic Gradient Descent(45173): loss=0.006991159292710709\n",
      "Stochastic Gradient Descent(45174): loss=0.5271103187787495\n",
      "Stochastic Gradient Descent(45175): loss=3.062764568429394\n",
      "Stochastic Gradient Descent(45176): loss=6.243413163302432\n",
      "Stochastic Gradient Descent(45177): loss=4.169215975462998\n",
      "Stochastic Gradient Descent(45178): loss=0.5079974157023156\n",
      "Stochastic Gradient Descent(45179): loss=6.898333383327209\n",
      "Stochastic Gradient Descent(45180): loss=0.380892801131127\n",
      "Stochastic Gradient Descent(45181): loss=1.6922745074000876\n",
      "Stochastic Gradient Descent(45182): loss=1.1709333210566752\n",
      "Stochastic Gradient Descent(45183): loss=0.04829990159762629\n",
      "Stochastic Gradient Descent(45184): loss=0.8412516601325936\n",
      "Stochastic Gradient Descent(45185): loss=13.625025749376382\n",
      "Stochastic Gradient Descent(45186): loss=79.41717036072444\n",
      "Stochastic Gradient Descent(45187): loss=14.016380107328485\n",
      "Stochastic Gradient Descent(45188): loss=11.585096534940542\n",
      "Stochastic Gradient Descent(45189): loss=6.4412571423746945\n",
      "Stochastic Gradient Descent(45190): loss=21.338921369122673\n",
      "Stochastic Gradient Descent(45191): loss=50.07284585040122\n",
      "Stochastic Gradient Descent(45192): loss=6.217713717855265\n",
      "Stochastic Gradient Descent(45193): loss=11.714016898649335\n",
      "Stochastic Gradient Descent(45194): loss=2.602264332532048\n",
      "Stochastic Gradient Descent(45195): loss=5.608888901435607\n",
      "Stochastic Gradient Descent(45196): loss=0.8726090695789198\n",
      "Stochastic Gradient Descent(45197): loss=4.314985196688505\n",
      "Stochastic Gradient Descent(45198): loss=0.16598512280983369\n",
      "Stochastic Gradient Descent(45199): loss=10.117438428844\n",
      "Stochastic Gradient Descent(45200): loss=7.257660361002197\n",
      "Stochastic Gradient Descent(45201): loss=1.5744129642168476\n",
      "Stochastic Gradient Descent(45202): loss=1.4651804048527823\n",
      "Stochastic Gradient Descent(45203): loss=2.2647694428471774\n",
      "Stochastic Gradient Descent(45204): loss=14.046850658250685\n",
      "Stochastic Gradient Descent(45205): loss=0.008933901153514223\n",
      "Stochastic Gradient Descent(45206): loss=0.009925825660760893\n",
      "Stochastic Gradient Descent(45207): loss=0.08047028324864884\n",
      "Stochastic Gradient Descent(45208): loss=17.97575598116436\n",
      "Stochastic Gradient Descent(45209): loss=6.42299928152425\n",
      "Stochastic Gradient Descent(45210): loss=0.8627078807692652\n",
      "Stochastic Gradient Descent(45211): loss=16.71274936290296\n",
      "Stochastic Gradient Descent(45212): loss=2.7341083907153725\n",
      "Stochastic Gradient Descent(45213): loss=0.8895735812498416\n",
      "Stochastic Gradient Descent(45214): loss=8.978533892667791\n",
      "Stochastic Gradient Descent(45215): loss=0.18677461760126107\n",
      "Stochastic Gradient Descent(45216): loss=10.685154230208264\n",
      "Stochastic Gradient Descent(45217): loss=2.6084356102132134\n",
      "Stochastic Gradient Descent(45218): loss=2.5282875257555375\n",
      "Stochastic Gradient Descent(45219): loss=17.452073316810193\n",
      "Stochastic Gradient Descent(45220): loss=0.23073279344728767\n",
      "Stochastic Gradient Descent(45221): loss=6.7292154906017245\n",
      "Stochastic Gradient Descent(45222): loss=7.756744454529493\n",
      "Stochastic Gradient Descent(45223): loss=0.0003615997523252648\n",
      "Stochastic Gradient Descent(45224): loss=0.2616298735825472\n",
      "Stochastic Gradient Descent(45225): loss=0.2646901017875889\n",
      "Stochastic Gradient Descent(45226): loss=0.6407503872834817\n",
      "Stochastic Gradient Descent(45227): loss=0.9195841475898247\n",
      "Stochastic Gradient Descent(45228): loss=2.6511579371021448\n",
      "Stochastic Gradient Descent(45229): loss=13.445186162217887\n",
      "Stochastic Gradient Descent(45230): loss=0.3966504484991531\n",
      "Stochastic Gradient Descent(45231): loss=0.9407451318738567\n",
      "Stochastic Gradient Descent(45232): loss=0.021164568676448847\n",
      "Stochastic Gradient Descent(45233): loss=5.1859096477709\n",
      "Stochastic Gradient Descent(45234): loss=2.485387570793303\n",
      "Stochastic Gradient Descent(45235): loss=0.6781835427535482\n",
      "Stochastic Gradient Descent(45236): loss=2.9874360737275096\n",
      "Stochastic Gradient Descent(45237): loss=6.729690329452114\n",
      "Stochastic Gradient Descent(45238): loss=0.12064517566299222\n",
      "Stochastic Gradient Descent(45239): loss=0.6304051334588355\n",
      "Stochastic Gradient Descent(45240): loss=10.682552563012846\n",
      "Stochastic Gradient Descent(45241): loss=0.29915818230651436\n",
      "Stochastic Gradient Descent(45242): loss=0.8562820958122724\n",
      "Stochastic Gradient Descent(45243): loss=0.2436894417731919\n",
      "Stochastic Gradient Descent(45244): loss=0.5876724859761997\n",
      "Stochastic Gradient Descent(45245): loss=4.1152528863996025\n",
      "Stochastic Gradient Descent(45246): loss=0.059091251723868476\n",
      "Stochastic Gradient Descent(45247): loss=1.998062026206314\n",
      "Stochastic Gradient Descent(45248): loss=0.7095034546347382\n",
      "Stochastic Gradient Descent(45249): loss=0.5747363490808372\n",
      "Stochastic Gradient Descent(45250): loss=23.265775834597488\n",
      "Stochastic Gradient Descent(45251): loss=3.125908609412431\n",
      "Stochastic Gradient Descent(45252): loss=11.382288789417139\n",
      "Stochastic Gradient Descent(45253): loss=0.025055837131953584\n",
      "Stochastic Gradient Descent(45254): loss=0.6982041699523427\n",
      "Stochastic Gradient Descent(45255): loss=0.5645533432092387\n",
      "Stochastic Gradient Descent(45256): loss=10.2710611178055\n",
      "Stochastic Gradient Descent(45257): loss=3.2186283814783825\n",
      "Stochastic Gradient Descent(45258): loss=1.6953186326326652\n",
      "Stochastic Gradient Descent(45259): loss=1.6074450130932165\n",
      "Stochastic Gradient Descent(45260): loss=1.0053834928287053\n",
      "Stochastic Gradient Descent(45261): loss=5.7957470864168394\n",
      "Stochastic Gradient Descent(45262): loss=3.2827687206925433\n",
      "Stochastic Gradient Descent(45263): loss=0.06362102844046402\n",
      "Stochastic Gradient Descent(45264): loss=0.6067873216119666\n",
      "Stochastic Gradient Descent(45265): loss=2.518831063776468\n",
      "Stochastic Gradient Descent(45266): loss=0.08460206352948597\n",
      "Stochastic Gradient Descent(45267): loss=5.39843765255168\n",
      "Stochastic Gradient Descent(45268): loss=19.15961039101419\n",
      "Stochastic Gradient Descent(45269): loss=0.035804826819185884\n",
      "Stochastic Gradient Descent(45270): loss=0.049480852870764834\n",
      "Stochastic Gradient Descent(45271): loss=13.415336491636229\n",
      "Stochastic Gradient Descent(45272): loss=0.2519480921913876\n",
      "Stochastic Gradient Descent(45273): loss=14.744563868512277\n",
      "Stochastic Gradient Descent(45274): loss=4.173420163088579\n",
      "Stochastic Gradient Descent(45275): loss=0.06162506925157652\n",
      "Stochastic Gradient Descent(45276): loss=1.574304072946577\n",
      "Stochastic Gradient Descent(45277): loss=12.254951991059535\n",
      "Stochastic Gradient Descent(45278): loss=0.016446416647831003\n",
      "Stochastic Gradient Descent(45279): loss=1.9093751945496016\n",
      "Stochastic Gradient Descent(45280): loss=2.2204474308742532\n",
      "Stochastic Gradient Descent(45281): loss=1.504967233702519\n",
      "Stochastic Gradient Descent(45282): loss=2.7469245232246307\n",
      "Stochastic Gradient Descent(45283): loss=8.193252243731513\n",
      "Stochastic Gradient Descent(45284): loss=0.5022024041438349\n",
      "Stochastic Gradient Descent(45285): loss=5.895310999011129\n",
      "Stochastic Gradient Descent(45286): loss=1.441102582316688\n",
      "Stochastic Gradient Descent(45287): loss=4.9303388464041715\n",
      "Stochastic Gradient Descent(45288): loss=7.4541174425211185\n",
      "Stochastic Gradient Descent(45289): loss=0.3892202574280546\n",
      "Stochastic Gradient Descent(45290): loss=0.3140654379517613\n",
      "Stochastic Gradient Descent(45291): loss=9.530646161073493\n",
      "Stochastic Gradient Descent(45292): loss=0.7537644990702089\n",
      "Stochastic Gradient Descent(45293): loss=0.648676654592014\n",
      "Stochastic Gradient Descent(45294): loss=0.2546913642607201\n",
      "Stochastic Gradient Descent(45295): loss=0.04331701293977351\n",
      "Stochastic Gradient Descent(45296): loss=1.4069091471877135\n",
      "Stochastic Gradient Descent(45297): loss=26.8957802615632\n",
      "Stochastic Gradient Descent(45298): loss=0.0008134276355573069\n",
      "Stochastic Gradient Descent(45299): loss=16.65408878076253\n",
      "Stochastic Gradient Descent(45300): loss=6.075718745580194\n",
      "Stochastic Gradient Descent(45301): loss=27.1062590582872\n",
      "Stochastic Gradient Descent(45302): loss=1.1762642182423464\n",
      "Stochastic Gradient Descent(45303): loss=0.7191445047158429\n",
      "Stochastic Gradient Descent(45304): loss=0.3041393630222069\n",
      "Stochastic Gradient Descent(45305): loss=1.4215207466741804\n",
      "Stochastic Gradient Descent(45306): loss=0.2294909724790207\n",
      "Stochastic Gradient Descent(45307): loss=8.598568086384578\n",
      "Stochastic Gradient Descent(45308): loss=2.5142561234578884\n",
      "Stochastic Gradient Descent(45309): loss=0.000985754703127018\n",
      "Stochastic Gradient Descent(45310): loss=17.267321449470693\n",
      "Stochastic Gradient Descent(45311): loss=0.299694160497278\n",
      "Stochastic Gradient Descent(45312): loss=0.2662962861444904\n",
      "Stochastic Gradient Descent(45313): loss=0.08029541053165559\n",
      "Stochastic Gradient Descent(45314): loss=10.456301200220903\n",
      "Stochastic Gradient Descent(45315): loss=0.6904242151900448\n",
      "Stochastic Gradient Descent(45316): loss=5.890068747551624\n",
      "Stochastic Gradient Descent(45317): loss=0.0003924169320092856\n",
      "Stochastic Gradient Descent(45318): loss=1.9036459213384012\n",
      "Stochastic Gradient Descent(45319): loss=4.65120953354411\n",
      "Stochastic Gradient Descent(45320): loss=8.92428559761019\n",
      "Stochastic Gradient Descent(45321): loss=0.43931438250939836\n",
      "Stochastic Gradient Descent(45322): loss=1.6093859614397539\n",
      "Stochastic Gradient Descent(45323): loss=0.019459796881501453\n",
      "Stochastic Gradient Descent(45324): loss=0.0164901775294994\n",
      "Stochastic Gradient Descent(45325): loss=0.28424089454682977\n",
      "Stochastic Gradient Descent(45326): loss=0.3392226135461336\n",
      "Stochastic Gradient Descent(45327): loss=3.375746209003866\n",
      "Stochastic Gradient Descent(45328): loss=5.0461148575163035\n",
      "Stochastic Gradient Descent(45329): loss=3.571257174021762\n",
      "Stochastic Gradient Descent(45330): loss=1.8659467170118769\n",
      "Stochastic Gradient Descent(45331): loss=2.6846201968883903\n",
      "Stochastic Gradient Descent(45332): loss=12.426224966840698\n",
      "Stochastic Gradient Descent(45333): loss=2.263513464720449\n",
      "Stochastic Gradient Descent(45334): loss=12.375071400463558\n",
      "Stochastic Gradient Descent(45335): loss=4.79470263604598\n",
      "Stochastic Gradient Descent(45336): loss=0.03619540676364249\n",
      "Stochastic Gradient Descent(45337): loss=6.44606582403075\n",
      "Stochastic Gradient Descent(45338): loss=8.85938172897697\n",
      "Stochastic Gradient Descent(45339): loss=0.21757436820572795\n",
      "Stochastic Gradient Descent(45340): loss=1.7083230819980812\n",
      "Stochastic Gradient Descent(45341): loss=1.5530396169897884\n",
      "Stochastic Gradient Descent(45342): loss=4.078063222735731\n",
      "Stochastic Gradient Descent(45343): loss=9.433146646567144\n",
      "Stochastic Gradient Descent(45344): loss=2.701622063416913\n",
      "Stochastic Gradient Descent(45345): loss=6.535683904000753\n",
      "Stochastic Gradient Descent(45346): loss=0.5215555540428952\n",
      "Stochastic Gradient Descent(45347): loss=5.490285453384153\n",
      "Stochastic Gradient Descent(45348): loss=4.516018465146538\n",
      "Stochastic Gradient Descent(45349): loss=5.228429346185803\n",
      "Stochastic Gradient Descent(45350): loss=0.6938825257617312\n",
      "Stochastic Gradient Descent(45351): loss=2.2010792176494745\n",
      "Stochastic Gradient Descent(45352): loss=5.691688010259527\n",
      "Stochastic Gradient Descent(45353): loss=3.238851021640293\n",
      "Stochastic Gradient Descent(45354): loss=0.24300378883344545\n",
      "Stochastic Gradient Descent(45355): loss=5.081894516877377e-05\n",
      "Stochastic Gradient Descent(45356): loss=0.7697276040613275\n",
      "Stochastic Gradient Descent(45357): loss=0.302368752595767\n",
      "Stochastic Gradient Descent(45358): loss=2.2284764585950265\n",
      "Stochastic Gradient Descent(45359): loss=0.06504955335792774\n",
      "Stochastic Gradient Descent(45360): loss=14.12823040959155\n",
      "Stochastic Gradient Descent(45361): loss=1.2340424855749546\n",
      "Stochastic Gradient Descent(45362): loss=0.5691948744171955\n",
      "Stochastic Gradient Descent(45363): loss=3.131125973606172\n",
      "Stochastic Gradient Descent(45364): loss=1.1578853533375129\n",
      "Stochastic Gradient Descent(45365): loss=0.01643950709203436\n",
      "Stochastic Gradient Descent(45366): loss=1.7536449490049237\n",
      "Stochastic Gradient Descent(45367): loss=2.253993613784782\n",
      "Stochastic Gradient Descent(45368): loss=30.482621822101404\n",
      "Stochastic Gradient Descent(45369): loss=166.50137633737864\n",
      "Stochastic Gradient Descent(45370): loss=0.13473892740529753\n",
      "Stochastic Gradient Descent(45371): loss=0.089641798710361\n",
      "Stochastic Gradient Descent(45372): loss=0.8553244288593371\n",
      "Stochastic Gradient Descent(45373): loss=0.03133023129291575\n",
      "Stochastic Gradient Descent(45374): loss=1.1398124857005936\n",
      "Stochastic Gradient Descent(45375): loss=1.4176089905486073\n",
      "Stochastic Gradient Descent(45376): loss=6.810216615626695\n",
      "Stochastic Gradient Descent(45377): loss=1.0521423714177285\n",
      "Stochastic Gradient Descent(45378): loss=0.04835848269889218\n",
      "Stochastic Gradient Descent(45379): loss=0.009252916402043537\n",
      "Stochastic Gradient Descent(45380): loss=0.2219572584653406\n",
      "Stochastic Gradient Descent(45381): loss=5.668048885391108\n",
      "Stochastic Gradient Descent(45382): loss=12.929283976437079\n",
      "Stochastic Gradient Descent(45383): loss=1.6812304664765834\n",
      "Stochastic Gradient Descent(45384): loss=10.05831622162763\n",
      "Stochastic Gradient Descent(45385): loss=2.162825495001282\n",
      "Stochastic Gradient Descent(45386): loss=1.2760627778222935\n",
      "Stochastic Gradient Descent(45387): loss=49.68346973044952\n",
      "Stochastic Gradient Descent(45388): loss=4.184395772838723\n",
      "Stochastic Gradient Descent(45389): loss=0.03957456115070514\n",
      "Stochastic Gradient Descent(45390): loss=1.0734607218044236\n",
      "Stochastic Gradient Descent(45391): loss=0.4917563341067442\n",
      "Stochastic Gradient Descent(45392): loss=10.602586970622797\n",
      "Stochastic Gradient Descent(45393): loss=76.67057909681905\n",
      "Stochastic Gradient Descent(45394): loss=2.4602634628208997\n",
      "Stochastic Gradient Descent(45395): loss=52.514940616756874\n",
      "Stochastic Gradient Descent(45396): loss=5.543929828853704\n",
      "Stochastic Gradient Descent(45397): loss=7.815531389017386\n",
      "Stochastic Gradient Descent(45398): loss=55.14551771710538\n",
      "Stochastic Gradient Descent(45399): loss=25.141532911714652\n",
      "Stochastic Gradient Descent(45400): loss=8.91891060665376\n",
      "Stochastic Gradient Descent(45401): loss=0.5065938638543315\n",
      "Stochastic Gradient Descent(45402): loss=7.694148254648549\n",
      "Stochastic Gradient Descent(45403): loss=0.7425416902013421\n",
      "Stochastic Gradient Descent(45404): loss=2.3158378717515085\n",
      "Stochastic Gradient Descent(45405): loss=0.44867198041030404\n",
      "Stochastic Gradient Descent(45406): loss=1.169975986078201\n",
      "Stochastic Gradient Descent(45407): loss=0.2298923083030358\n",
      "Stochastic Gradient Descent(45408): loss=10.715538570468302\n",
      "Stochastic Gradient Descent(45409): loss=1.4527843999183854\n",
      "Stochastic Gradient Descent(45410): loss=13.396665170572996\n",
      "Stochastic Gradient Descent(45411): loss=0.08943959978658422\n",
      "Stochastic Gradient Descent(45412): loss=2.773955775559468\n",
      "Stochastic Gradient Descent(45413): loss=1.9694295922797465\n",
      "Stochastic Gradient Descent(45414): loss=6.796235276902433\n",
      "Stochastic Gradient Descent(45415): loss=0.14146155401869162\n",
      "Stochastic Gradient Descent(45416): loss=0.010372416999349286\n",
      "Stochastic Gradient Descent(45417): loss=0.14841200211809172\n",
      "Stochastic Gradient Descent(45418): loss=6.03982480407131\n",
      "Stochastic Gradient Descent(45419): loss=8.813792565662505\n",
      "Stochastic Gradient Descent(45420): loss=9.911458872516016\n",
      "Stochastic Gradient Descent(45421): loss=12.219455455106075\n",
      "Stochastic Gradient Descent(45422): loss=2.8293350465332208\n",
      "Stochastic Gradient Descent(45423): loss=1.8070890365115326\n",
      "Stochastic Gradient Descent(45424): loss=20.329042481099787\n",
      "Stochastic Gradient Descent(45425): loss=18.799598935728074\n",
      "Stochastic Gradient Descent(45426): loss=1.7294835552097474\n",
      "Stochastic Gradient Descent(45427): loss=0.2259946750521295\n",
      "Stochastic Gradient Descent(45428): loss=4.827288847520665\n",
      "Stochastic Gradient Descent(45429): loss=1.2967090113788962\n",
      "Stochastic Gradient Descent(45430): loss=0.0961029490858604\n",
      "Stochastic Gradient Descent(45431): loss=0.022478767186035885\n",
      "Stochastic Gradient Descent(45432): loss=0.8969626195160167\n",
      "Stochastic Gradient Descent(45433): loss=2.075284339197082\n",
      "Stochastic Gradient Descent(45434): loss=2.4661124433960833\n",
      "Stochastic Gradient Descent(45435): loss=6.223188178645724\n",
      "Stochastic Gradient Descent(45436): loss=8.348251679037212\n",
      "Stochastic Gradient Descent(45437): loss=0.18118093024763712\n",
      "Stochastic Gradient Descent(45438): loss=16.997189456964467\n",
      "Stochastic Gradient Descent(45439): loss=0.6171417819894672\n",
      "Stochastic Gradient Descent(45440): loss=0.0006141331863911604\n",
      "Stochastic Gradient Descent(45441): loss=0.026992546064126603\n",
      "Stochastic Gradient Descent(45442): loss=1.370367433228038\n",
      "Stochastic Gradient Descent(45443): loss=6.375551389876687\n",
      "Stochastic Gradient Descent(45444): loss=0.3591517521065118\n",
      "Stochastic Gradient Descent(45445): loss=2.562131483310466\n",
      "Stochastic Gradient Descent(45446): loss=0.0020622722612351732\n",
      "Stochastic Gradient Descent(45447): loss=13.255791244778846\n",
      "Stochastic Gradient Descent(45448): loss=0.7560386374450284\n",
      "Stochastic Gradient Descent(45449): loss=9.694985829087022\n",
      "Stochastic Gradient Descent(45450): loss=1.0300884232748804\n",
      "Stochastic Gradient Descent(45451): loss=2.1811364002279165\n",
      "Stochastic Gradient Descent(45452): loss=0.1707308427346308\n",
      "Stochastic Gradient Descent(45453): loss=2.938185266202903\n",
      "Stochastic Gradient Descent(45454): loss=3.161643258490848\n",
      "Stochastic Gradient Descent(45455): loss=8.960338402654653\n",
      "Stochastic Gradient Descent(45456): loss=0.7088666964899295\n",
      "Stochastic Gradient Descent(45457): loss=0.01620601762786917\n",
      "Stochastic Gradient Descent(45458): loss=12.050172308092137\n",
      "Stochastic Gradient Descent(45459): loss=4.553477537376764\n",
      "Stochastic Gradient Descent(45460): loss=2.4646152876186096\n",
      "Stochastic Gradient Descent(45461): loss=15.946753001921573\n",
      "Stochastic Gradient Descent(45462): loss=0.986873425697217\n",
      "Stochastic Gradient Descent(45463): loss=6.006776899383266\n",
      "Stochastic Gradient Descent(45464): loss=6.738084599038615\n",
      "Stochastic Gradient Descent(45465): loss=13.406898490811022\n",
      "Stochastic Gradient Descent(45466): loss=7.071703473399733\n",
      "Stochastic Gradient Descent(45467): loss=0.7489052987130043\n",
      "Stochastic Gradient Descent(45468): loss=3.4785608958184184\n",
      "Stochastic Gradient Descent(45469): loss=0.04390323286206011\n",
      "Stochastic Gradient Descent(45470): loss=8.57485504123474\n",
      "Stochastic Gradient Descent(45471): loss=1.5067462996055239\n",
      "Stochastic Gradient Descent(45472): loss=0.014551578726002854\n",
      "Stochastic Gradient Descent(45473): loss=0.21387381811893832\n",
      "Stochastic Gradient Descent(45474): loss=0.14389484670302413\n",
      "Stochastic Gradient Descent(45475): loss=0.1578355899606357\n",
      "Stochastic Gradient Descent(45476): loss=2.3324392502049203\n",
      "Stochastic Gradient Descent(45477): loss=4.675041969510982\n",
      "Stochastic Gradient Descent(45478): loss=0.08925142777192407\n",
      "Stochastic Gradient Descent(45479): loss=0.02099442360249939\n",
      "Stochastic Gradient Descent(45480): loss=7.83570989232325\n",
      "Stochastic Gradient Descent(45481): loss=0.6345026102121379\n",
      "Stochastic Gradient Descent(45482): loss=2.62267570417059\n",
      "Stochastic Gradient Descent(45483): loss=0.092485748320596\n",
      "Stochastic Gradient Descent(45484): loss=1.218181026201092\n",
      "Stochastic Gradient Descent(45485): loss=15.932307776411886\n",
      "Stochastic Gradient Descent(45486): loss=1.4355218645051597\n",
      "Stochastic Gradient Descent(45487): loss=6.445219087470615\n",
      "Stochastic Gradient Descent(45488): loss=0.002046706617960874\n",
      "Stochastic Gradient Descent(45489): loss=0.3349882099986674\n",
      "Stochastic Gradient Descent(45490): loss=0.22508824753465184\n",
      "Stochastic Gradient Descent(45491): loss=4.6849860811214725\n",
      "Stochastic Gradient Descent(45492): loss=1.8463909764721647\n",
      "Stochastic Gradient Descent(45493): loss=0.006045087691318399\n",
      "Stochastic Gradient Descent(45494): loss=3.012009274969958\n",
      "Stochastic Gradient Descent(45495): loss=4.185729084592957\n",
      "Stochastic Gradient Descent(45496): loss=14.734021607519452\n",
      "Stochastic Gradient Descent(45497): loss=0.6689569580391257\n",
      "Stochastic Gradient Descent(45498): loss=42.63560221991748\n",
      "Stochastic Gradient Descent(45499): loss=2.1078758056234066\n",
      "Stochastic Gradient Descent(45500): loss=2.7872940815785308\n",
      "Stochastic Gradient Descent(45501): loss=0.8145929841335254\n",
      "Stochastic Gradient Descent(45502): loss=10.025645702058583\n",
      "Stochastic Gradient Descent(45503): loss=35.52697508101954\n",
      "Stochastic Gradient Descent(45504): loss=0.8420956813318131\n",
      "Stochastic Gradient Descent(45505): loss=0.023792700839670305\n",
      "Stochastic Gradient Descent(45506): loss=0.0013683002103366268\n",
      "Stochastic Gradient Descent(45507): loss=1.075971425614845\n",
      "Stochastic Gradient Descent(45508): loss=2.908027212538036\n",
      "Stochastic Gradient Descent(45509): loss=9.027581193443742\n",
      "Stochastic Gradient Descent(45510): loss=2.1284612760251953\n",
      "Stochastic Gradient Descent(45511): loss=6.424028269938264\n",
      "Stochastic Gradient Descent(45512): loss=0.9343095974923862\n",
      "Stochastic Gradient Descent(45513): loss=0.29030549903417396\n",
      "Stochastic Gradient Descent(45514): loss=4.540627697962835\n",
      "Stochastic Gradient Descent(45515): loss=3.4011886811969836\n",
      "Stochastic Gradient Descent(45516): loss=23.154297794635557\n",
      "Stochastic Gradient Descent(45517): loss=8.546082648353506\n",
      "Stochastic Gradient Descent(45518): loss=1.709199496236921\n",
      "Stochastic Gradient Descent(45519): loss=0.2748382361612517\n",
      "Stochastic Gradient Descent(45520): loss=0.4603185705804209\n",
      "Stochastic Gradient Descent(45521): loss=0.07148558411032249\n",
      "Stochastic Gradient Descent(45522): loss=0.0012456352880178183\n",
      "Stochastic Gradient Descent(45523): loss=0.3925435168493805\n",
      "Stochastic Gradient Descent(45524): loss=9.74031085556811\n",
      "Stochastic Gradient Descent(45525): loss=1.78273499768867\n",
      "Stochastic Gradient Descent(45526): loss=21.730484580469042\n",
      "Stochastic Gradient Descent(45527): loss=0.5978094351764955\n",
      "Stochastic Gradient Descent(45528): loss=5.181504054356254\n",
      "Stochastic Gradient Descent(45529): loss=0.3425825711105128\n",
      "Stochastic Gradient Descent(45530): loss=5.820025379136905\n",
      "Stochastic Gradient Descent(45531): loss=1.2136121544324694\n",
      "Stochastic Gradient Descent(45532): loss=1.7507998041458626\n",
      "Stochastic Gradient Descent(45533): loss=0.15084408111262362\n",
      "Stochastic Gradient Descent(45534): loss=0.3105402069594203\n",
      "Stochastic Gradient Descent(45535): loss=1.264412530314534\n",
      "Stochastic Gradient Descent(45536): loss=0.7996778406379145\n",
      "Stochastic Gradient Descent(45537): loss=0.006545702630282432\n",
      "Stochastic Gradient Descent(45538): loss=0.04690870009027644\n",
      "Stochastic Gradient Descent(45539): loss=29.28880318808139\n",
      "Stochastic Gradient Descent(45540): loss=0.5038886616572065\n",
      "Stochastic Gradient Descent(45541): loss=0.21059159526598076\n",
      "Stochastic Gradient Descent(45542): loss=0.313198262020524\n",
      "Stochastic Gradient Descent(45543): loss=3.6789225970031607\n",
      "Stochastic Gradient Descent(45544): loss=8.059248379480412\n",
      "Stochastic Gradient Descent(45545): loss=0.7326162594840133\n",
      "Stochastic Gradient Descent(45546): loss=0.0037151031961931945\n",
      "Stochastic Gradient Descent(45547): loss=0.06857989558328885\n",
      "Stochastic Gradient Descent(45548): loss=2.4537538974156434\n",
      "Stochastic Gradient Descent(45549): loss=2.1193538658049427\n",
      "Stochastic Gradient Descent(45550): loss=14.001297789091703\n",
      "Stochastic Gradient Descent(45551): loss=1.0986936234512783\n",
      "Stochastic Gradient Descent(45552): loss=4.221051700679321\n",
      "Stochastic Gradient Descent(45553): loss=0.3355367117431602\n",
      "Stochastic Gradient Descent(45554): loss=1.0562954320800821\n",
      "Stochastic Gradient Descent(45555): loss=0.6055456899243703\n",
      "Stochastic Gradient Descent(45556): loss=2.532941579034126\n",
      "Stochastic Gradient Descent(45557): loss=14.657780663878931\n",
      "Stochastic Gradient Descent(45558): loss=0.7309959519533814\n",
      "Stochastic Gradient Descent(45559): loss=2.673560053297655\n",
      "Stochastic Gradient Descent(45560): loss=1.9080201731418962\n",
      "Stochastic Gradient Descent(45561): loss=3.614560409544494\n",
      "Stochastic Gradient Descent(45562): loss=18.878718822725805\n",
      "Stochastic Gradient Descent(45563): loss=1.8462781893314038\n",
      "Stochastic Gradient Descent(45564): loss=1.6631456900686128\n",
      "Stochastic Gradient Descent(45565): loss=11.139699949303266\n",
      "Stochastic Gradient Descent(45566): loss=3.7933548157937236\n",
      "Stochastic Gradient Descent(45567): loss=0.05252677571331792\n",
      "Stochastic Gradient Descent(45568): loss=1.8745290574569895\n",
      "Stochastic Gradient Descent(45569): loss=0.9438542562922754\n",
      "Stochastic Gradient Descent(45570): loss=2.704239558771203\n",
      "Stochastic Gradient Descent(45571): loss=9.12226848057564\n",
      "Stochastic Gradient Descent(45572): loss=23.177081099417897\n",
      "Stochastic Gradient Descent(45573): loss=2.4438247569460856\n",
      "Stochastic Gradient Descent(45574): loss=0.03498605659971133\n",
      "Stochastic Gradient Descent(45575): loss=1.6331706276126055\n",
      "Stochastic Gradient Descent(45576): loss=0.0023674210272442635\n",
      "Stochastic Gradient Descent(45577): loss=5.499722472223862\n",
      "Stochastic Gradient Descent(45578): loss=4.767135548118726\n",
      "Stochastic Gradient Descent(45579): loss=15.820001838941458\n",
      "Stochastic Gradient Descent(45580): loss=12.388645267319621\n",
      "Stochastic Gradient Descent(45581): loss=4.640861423021191\n",
      "Stochastic Gradient Descent(45582): loss=1.4321551587168442\n",
      "Stochastic Gradient Descent(45583): loss=6.527843046464759\n",
      "Stochastic Gradient Descent(45584): loss=0.021227710312705633\n",
      "Stochastic Gradient Descent(45585): loss=6.731730990636548\n",
      "Stochastic Gradient Descent(45586): loss=2.4365424712634276\n",
      "Stochastic Gradient Descent(45587): loss=0.016551975326164564\n",
      "Stochastic Gradient Descent(45588): loss=2.5705395278534127\n",
      "Stochastic Gradient Descent(45589): loss=10.775634934154201\n",
      "Stochastic Gradient Descent(45590): loss=0.5180899719426153\n",
      "Stochastic Gradient Descent(45591): loss=16.406334652759465\n",
      "Stochastic Gradient Descent(45592): loss=13.172432503372285\n",
      "Stochastic Gradient Descent(45593): loss=0.06597184096025122\n",
      "Stochastic Gradient Descent(45594): loss=1.8785625414351945\n",
      "Stochastic Gradient Descent(45595): loss=2.5314970333350177\n",
      "Stochastic Gradient Descent(45596): loss=0.3076990634273636\n",
      "Stochastic Gradient Descent(45597): loss=0.106329724263682\n",
      "Stochastic Gradient Descent(45598): loss=4.933453686287891\n",
      "Stochastic Gradient Descent(45599): loss=4.384920158592154\n",
      "Stochastic Gradient Descent(45600): loss=4.386668867562573\n",
      "Stochastic Gradient Descent(45601): loss=0.28146623549799854\n",
      "Stochastic Gradient Descent(45602): loss=37.000544910311845\n",
      "Stochastic Gradient Descent(45603): loss=0.013334021373525063\n",
      "Stochastic Gradient Descent(45604): loss=0.3300703794620086\n",
      "Stochastic Gradient Descent(45605): loss=1.7074732034254267\n",
      "Stochastic Gradient Descent(45606): loss=0.004943064352782103\n",
      "Stochastic Gradient Descent(45607): loss=2.648833455078323\n",
      "Stochastic Gradient Descent(45608): loss=0.18981521809916627\n",
      "Stochastic Gradient Descent(45609): loss=0.5521501096035015\n",
      "Stochastic Gradient Descent(45610): loss=3.3486519552954066\n",
      "Stochastic Gradient Descent(45611): loss=0.13729223490504217\n",
      "Stochastic Gradient Descent(45612): loss=0.004911107431310951\n",
      "Stochastic Gradient Descent(45613): loss=0.0850124385745241\n",
      "Stochastic Gradient Descent(45614): loss=9.257781678178098\n",
      "Stochastic Gradient Descent(45615): loss=0.33733458687093026\n",
      "Stochastic Gradient Descent(45616): loss=0.9677310154003632\n",
      "Stochastic Gradient Descent(45617): loss=0.4955169505850993\n",
      "Stochastic Gradient Descent(45618): loss=0.14953889359604003\n",
      "Stochastic Gradient Descent(45619): loss=3.7088224109672487\n",
      "Stochastic Gradient Descent(45620): loss=8.65120727802268\n",
      "Stochastic Gradient Descent(45621): loss=0.7736818702190872\n",
      "Stochastic Gradient Descent(45622): loss=0.6145846345554116\n",
      "Stochastic Gradient Descent(45623): loss=0.2536622530206442\n",
      "Stochastic Gradient Descent(45624): loss=0.8749674018207175\n",
      "Stochastic Gradient Descent(45625): loss=2.8625399247779266\n",
      "Stochastic Gradient Descent(45626): loss=5.276796864815607\n",
      "Stochastic Gradient Descent(45627): loss=1.0861331782552481\n",
      "Stochastic Gradient Descent(45628): loss=2.434907006349575\n",
      "Stochastic Gradient Descent(45629): loss=9.353452273626965\n",
      "Stochastic Gradient Descent(45630): loss=6.470808494444156\n",
      "Stochastic Gradient Descent(45631): loss=9.4307969907757\n",
      "Stochastic Gradient Descent(45632): loss=0.00017697959802565595\n",
      "Stochastic Gradient Descent(45633): loss=0.12179802656641053\n",
      "Stochastic Gradient Descent(45634): loss=11.082097048345632\n",
      "Stochastic Gradient Descent(45635): loss=0.548040178169491\n",
      "Stochastic Gradient Descent(45636): loss=0.5140597409995996\n",
      "Stochastic Gradient Descent(45637): loss=0.9252671818360191\n",
      "Stochastic Gradient Descent(45638): loss=12.819910581032492\n",
      "Stochastic Gradient Descent(45639): loss=0.4593137958607387\n",
      "Stochastic Gradient Descent(45640): loss=7.210266344466431\n",
      "Stochastic Gradient Descent(45641): loss=0.19363787600079335\n",
      "Stochastic Gradient Descent(45642): loss=3.0450630115670645\n",
      "Stochastic Gradient Descent(45643): loss=20.751542247266045\n",
      "Stochastic Gradient Descent(45644): loss=7.311403845840248\n",
      "Stochastic Gradient Descent(45645): loss=3.1734064700890046\n",
      "Stochastic Gradient Descent(45646): loss=3.8770466910614627\n",
      "Stochastic Gradient Descent(45647): loss=7.436081640106714\n",
      "Stochastic Gradient Descent(45648): loss=2.151897466818319\n",
      "Stochastic Gradient Descent(45649): loss=0.3229973810539093\n",
      "Stochastic Gradient Descent(45650): loss=0.7693389506819509\n",
      "Stochastic Gradient Descent(45651): loss=2.054225999670282\n",
      "Stochastic Gradient Descent(45652): loss=0.001057247250815651\n",
      "Stochastic Gradient Descent(45653): loss=0.944660599833274\n",
      "Stochastic Gradient Descent(45654): loss=0.055674085309038654\n",
      "Stochastic Gradient Descent(45655): loss=1.8887399166267143\n",
      "Stochastic Gradient Descent(45656): loss=0.2587929970693493\n",
      "Stochastic Gradient Descent(45657): loss=3.082645774173184\n",
      "Stochastic Gradient Descent(45658): loss=1.8584323902799482\n",
      "Stochastic Gradient Descent(45659): loss=3.559359915702244\n",
      "Stochastic Gradient Descent(45660): loss=0.2579165842102641\n",
      "Stochastic Gradient Descent(45661): loss=0.6141826476153451\n",
      "Stochastic Gradient Descent(45662): loss=6.831172262818187\n",
      "Stochastic Gradient Descent(45663): loss=2.870838872555887\n",
      "Stochastic Gradient Descent(45664): loss=0.41787302556124084\n",
      "Stochastic Gradient Descent(45665): loss=3.895791781643341\n",
      "Stochastic Gradient Descent(45666): loss=15.95296815852417\n",
      "Stochastic Gradient Descent(45667): loss=0.24579280222754205\n",
      "Stochastic Gradient Descent(45668): loss=0.02207703318538311\n",
      "Stochastic Gradient Descent(45669): loss=1.0032701260909276\n",
      "Stochastic Gradient Descent(45670): loss=2.508584072548488\n",
      "Stochastic Gradient Descent(45671): loss=11.746590849331024\n",
      "Stochastic Gradient Descent(45672): loss=4.203697804371089\n",
      "Stochastic Gradient Descent(45673): loss=1.305441873686472\n",
      "Stochastic Gradient Descent(45674): loss=0.013025343712489463\n",
      "Stochastic Gradient Descent(45675): loss=1.2251983865237723\n",
      "Stochastic Gradient Descent(45676): loss=1.8194444671161647\n",
      "Stochastic Gradient Descent(45677): loss=2.285544865620868\n",
      "Stochastic Gradient Descent(45678): loss=2.010356802166272\n",
      "Stochastic Gradient Descent(45679): loss=3.4539409719839664\n",
      "Stochastic Gradient Descent(45680): loss=0.05024435530891295\n",
      "Stochastic Gradient Descent(45681): loss=2.5769053158058206\n",
      "Stochastic Gradient Descent(45682): loss=1.4164358467555485\n",
      "Stochastic Gradient Descent(45683): loss=1.2463013230600613\n",
      "Stochastic Gradient Descent(45684): loss=11.02406047600354\n",
      "Stochastic Gradient Descent(45685): loss=0.9368467300952051\n",
      "Stochastic Gradient Descent(45686): loss=4.369688650507629\n",
      "Stochastic Gradient Descent(45687): loss=0.5149661409614847\n",
      "Stochastic Gradient Descent(45688): loss=0.4676800159095917\n",
      "Stochastic Gradient Descent(45689): loss=6.955879552635258\n",
      "Stochastic Gradient Descent(45690): loss=0.1260382011949826\n",
      "Stochastic Gradient Descent(45691): loss=0.6378911114992023\n",
      "Stochastic Gradient Descent(45692): loss=4.201845918673449\n",
      "Stochastic Gradient Descent(45693): loss=10.707276943929468\n",
      "Stochastic Gradient Descent(45694): loss=3.397882213421919\n",
      "Stochastic Gradient Descent(45695): loss=1.5354303488293215\n",
      "Stochastic Gradient Descent(45696): loss=2.638955838919431\n",
      "Stochastic Gradient Descent(45697): loss=2.5803099517473154\n",
      "Stochastic Gradient Descent(45698): loss=6.484165868853073\n",
      "Stochastic Gradient Descent(45699): loss=5.826844954366549\n",
      "Stochastic Gradient Descent(45700): loss=4.6726870703131675\n",
      "Stochastic Gradient Descent(45701): loss=0.42939228425116716\n",
      "Stochastic Gradient Descent(45702): loss=0.39553310272098086\n",
      "Stochastic Gradient Descent(45703): loss=4.789479297637592\n",
      "Stochastic Gradient Descent(45704): loss=0.04206113432947328\n",
      "Stochastic Gradient Descent(45705): loss=1.1845717404655034\n",
      "Stochastic Gradient Descent(45706): loss=0.2262118538963829\n",
      "Stochastic Gradient Descent(45707): loss=0.19754639376869942\n",
      "Stochastic Gradient Descent(45708): loss=0.006541002712498544\n",
      "Stochastic Gradient Descent(45709): loss=8.533175573428437\n",
      "Stochastic Gradient Descent(45710): loss=0.4999883202178923\n",
      "Stochastic Gradient Descent(45711): loss=0.03443011437141333\n",
      "Stochastic Gradient Descent(45712): loss=2.833073778932366\n",
      "Stochastic Gradient Descent(45713): loss=0.00022851831281412726\n",
      "Stochastic Gradient Descent(45714): loss=7.475378975823809\n",
      "Stochastic Gradient Descent(45715): loss=0.3326889874520555\n",
      "Stochastic Gradient Descent(45716): loss=4.436030311208916\n",
      "Stochastic Gradient Descent(45717): loss=7.429872859560924\n",
      "Stochastic Gradient Descent(45718): loss=9.689283818966194\n",
      "Stochastic Gradient Descent(45719): loss=5.310581162905244\n",
      "Stochastic Gradient Descent(45720): loss=9.263587032964633\n",
      "Stochastic Gradient Descent(45721): loss=1.7967105807876984\n",
      "Stochastic Gradient Descent(45722): loss=0.1550000458936877\n",
      "Stochastic Gradient Descent(45723): loss=0.11297440640585671\n",
      "Stochastic Gradient Descent(45724): loss=0.5286087685266646\n",
      "Stochastic Gradient Descent(45725): loss=0.4227327508769407\n",
      "Stochastic Gradient Descent(45726): loss=6.763930032078775\n",
      "Stochastic Gradient Descent(45727): loss=1.4044569048204096\n",
      "Stochastic Gradient Descent(45728): loss=1.6067532088339749\n",
      "Stochastic Gradient Descent(45729): loss=2.0583840688127273\n",
      "Stochastic Gradient Descent(45730): loss=1.9994358703605186\n",
      "Stochastic Gradient Descent(45731): loss=1.4961991078727928\n",
      "Stochastic Gradient Descent(45732): loss=1.1828346477861909\n",
      "Stochastic Gradient Descent(45733): loss=0.956279661379358\n",
      "Stochastic Gradient Descent(45734): loss=1.0423443155008816\n",
      "Stochastic Gradient Descent(45735): loss=3.4165660538747877\n",
      "Stochastic Gradient Descent(45736): loss=0.047814233690499586\n",
      "Stochastic Gradient Descent(45737): loss=0.2797186972306029\n",
      "Stochastic Gradient Descent(45738): loss=1.4253072261407036\n",
      "Stochastic Gradient Descent(45739): loss=6.477924178477644\n",
      "Stochastic Gradient Descent(45740): loss=13.678517810286232\n",
      "Stochastic Gradient Descent(45741): loss=14.940403589712231\n",
      "Stochastic Gradient Descent(45742): loss=0.026774988360258097\n",
      "Stochastic Gradient Descent(45743): loss=3.640830534132432\n",
      "Stochastic Gradient Descent(45744): loss=9.788257169969139\n",
      "Stochastic Gradient Descent(45745): loss=2.311616609089677\n",
      "Stochastic Gradient Descent(45746): loss=3.599532158133447e-05\n",
      "Stochastic Gradient Descent(45747): loss=25.659398832406872\n",
      "Stochastic Gradient Descent(45748): loss=18.95392732702814\n",
      "Stochastic Gradient Descent(45749): loss=0.459896911871194\n",
      "Stochastic Gradient Descent(45750): loss=0.0921579145356023\n",
      "Stochastic Gradient Descent(45751): loss=35.4536751050185\n",
      "Stochastic Gradient Descent(45752): loss=0.6103602327941013\n",
      "Stochastic Gradient Descent(45753): loss=2.1566325388240295\n",
      "Stochastic Gradient Descent(45754): loss=5.523104003115564\n",
      "Stochastic Gradient Descent(45755): loss=0.45036110132078994\n",
      "Stochastic Gradient Descent(45756): loss=0.31159569114605085\n",
      "Stochastic Gradient Descent(45757): loss=0.0010370017206564285\n",
      "Stochastic Gradient Descent(45758): loss=4.107820958040724\n",
      "Stochastic Gradient Descent(45759): loss=0.01353026216700807\n",
      "Stochastic Gradient Descent(45760): loss=0.09467820160200674\n",
      "Stochastic Gradient Descent(45761): loss=0.003715693901446073\n",
      "Stochastic Gradient Descent(45762): loss=1.751498177745452\n",
      "Stochastic Gradient Descent(45763): loss=2.2096433031740648\n",
      "Stochastic Gradient Descent(45764): loss=2.3367146637654606\n",
      "Stochastic Gradient Descent(45765): loss=1.388005282788588\n",
      "Stochastic Gradient Descent(45766): loss=2.4250040884506183\n",
      "Stochastic Gradient Descent(45767): loss=1.5820975407707278\n",
      "Stochastic Gradient Descent(45768): loss=4.871680113569812\n",
      "Stochastic Gradient Descent(45769): loss=14.97244762904054\n",
      "Stochastic Gradient Descent(45770): loss=0.18341563028466099\n",
      "Stochastic Gradient Descent(45771): loss=0.14040431709380982\n",
      "Stochastic Gradient Descent(45772): loss=15.109007820714814\n",
      "Stochastic Gradient Descent(45773): loss=2.949714144864438\n",
      "Stochastic Gradient Descent(45774): loss=0.581701433411514\n",
      "Stochastic Gradient Descent(45775): loss=12.592762583919535\n",
      "Stochastic Gradient Descent(45776): loss=0.9203087084765987\n",
      "Stochastic Gradient Descent(45777): loss=0.5589351054921401\n",
      "Stochastic Gradient Descent(45778): loss=0.22137380099941917\n",
      "Stochastic Gradient Descent(45779): loss=0.17769630633897504\n",
      "Stochastic Gradient Descent(45780): loss=3.366333836717216\n",
      "Stochastic Gradient Descent(45781): loss=0.7077436960688241\n",
      "Stochastic Gradient Descent(45782): loss=5.798295532512223\n",
      "Stochastic Gradient Descent(45783): loss=0.3435256766611346\n",
      "Stochastic Gradient Descent(45784): loss=1.1980235139569682\n",
      "Stochastic Gradient Descent(45785): loss=0.41858765736929426\n",
      "Stochastic Gradient Descent(45786): loss=0.5668761217635357\n",
      "Stochastic Gradient Descent(45787): loss=1.2312260213079596\n",
      "Stochastic Gradient Descent(45788): loss=5.784218717987481\n",
      "Stochastic Gradient Descent(45789): loss=0.8101098595388296\n",
      "Stochastic Gradient Descent(45790): loss=0.8495087154855704\n",
      "Stochastic Gradient Descent(45791): loss=2.2649104859658276\n",
      "Stochastic Gradient Descent(45792): loss=3.8101342560777507\n",
      "Stochastic Gradient Descent(45793): loss=3.2982211167825084\n",
      "Stochastic Gradient Descent(45794): loss=4.236880746695741\n",
      "Stochastic Gradient Descent(45795): loss=0.42721126611959515\n",
      "Stochastic Gradient Descent(45796): loss=1.0856773744779245\n",
      "Stochastic Gradient Descent(45797): loss=7.639184896587414\n",
      "Stochastic Gradient Descent(45798): loss=16.597018458355326\n",
      "Stochastic Gradient Descent(45799): loss=3.947310507164578\n",
      "Stochastic Gradient Descent(45800): loss=0.12398009748951287\n",
      "Stochastic Gradient Descent(45801): loss=6.450891975684054e-05\n",
      "Stochastic Gradient Descent(45802): loss=9.758473902554286\n",
      "Stochastic Gradient Descent(45803): loss=0.07148296981889149\n",
      "Stochastic Gradient Descent(45804): loss=15.346588902923445\n",
      "Stochastic Gradient Descent(45805): loss=0.8515669617435959\n",
      "Stochastic Gradient Descent(45806): loss=4.6343111111938216\n",
      "Stochastic Gradient Descent(45807): loss=0.009690545366082506\n",
      "Stochastic Gradient Descent(45808): loss=0.12798796889579386\n",
      "Stochastic Gradient Descent(45809): loss=10.238064955279661\n",
      "Stochastic Gradient Descent(45810): loss=1.4697000129555429\n",
      "Stochastic Gradient Descent(45811): loss=10.320974326073738\n",
      "Stochastic Gradient Descent(45812): loss=7.9349231690668525\n",
      "Stochastic Gradient Descent(45813): loss=4.003496919954857\n",
      "Stochastic Gradient Descent(45814): loss=0.06139249426135218\n",
      "Stochastic Gradient Descent(45815): loss=1.6256415629720624\n",
      "Stochastic Gradient Descent(45816): loss=12.196895900620493\n",
      "Stochastic Gradient Descent(45817): loss=4.056763918000926\n",
      "Stochastic Gradient Descent(45818): loss=0.17031814340816326\n",
      "Stochastic Gradient Descent(45819): loss=14.814643434990376\n",
      "Stochastic Gradient Descent(45820): loss=0.6026164764404827\n",
      "Stochastic Gradient Descent(45821): loss=5.542326824797299\n",
      "Stochastic Gradient Descent(45822): loss=2.97367261267876\n",
      "Stochastic Gradient Descent(45823): loss=4.709909050416244\n",
      "Stochastic Gradient Descent(45824): loss=16.77435913683352\n",
      "Stochastic Gradient Descent(45825): loss=8.942979584872825\n",
      "Stochastic Gradient Descent(45826): loss=1.55136932274928\n",
      "Stochastic Gradient Descent(45827): loss=9.114335015716023\n",
      "Stochastic Gradient Descent(45828): loss=3.379609546369485\n",
      "Stochastic Gradient Descent(45829): loss=4.010288932033778\n",
      "Stochastic Gradient Descent(45830): loss=1.0813129928745575\n",
      "Stochastic Gradient Descent(45831): loss=0.3841557870873344\n",
      "Stochastic Gradient Descent(45832): loss=0.020339371304565593\n",
      "Stochastic Gradient Descent(45833): loss=0.025211001005767293\n",
      "Stochastic Gradient Descent(45834): loss=0.06670898726956158\n",
      "Stochastic Gradient Descent(45835): loss=1.264465589957929\n",
      "Stochastic Gradient Descent(45836): loss=2.5712025381334582\n",
      "Stochastic Gradient Descent(45837): loss=0.5091394648581102\n",
      "Stochastic Gradient Descent(45838): loss=2.9238415753746905\n",
      "Stochastic Gradient Descent(45839): loss=8.532450425640734\n",
      "Stochastic Gradient Descent(45840): loss=0.2523202982525759\n",
      "Stochastic Gradient Descent(45841): loss=6.436749513258475\n",
      "Stochastic Gradient Descent(45842): loss=0.017214250255279645\n",
      "Stochastic Gradient Descent(45843): loss=0.1055252638526604\n",
      "Stochastic Gradient Descent(45844): loss=2.8191243293874493\n",
      "Stochastic Gradient Descent(45845): loss=0.05384973612671175\n",
      "Stochastic Gradient Descent(45846): loss=4.005868501200537\n",
      "Stochastic Gradient Descent(45847): loss=2.8052602494987853\n",
      "Stochastic Gradient Descent(45848): loss=5.466144553025414\n",
      "Stochastic Gradient Descent(45849): loss=0.4494361173148739\n",
      "Stochastic Gradient Descent(45850): loss=0.8896898453678856\n",
      "Stochastic Gradient Descent(45851): loss=2.0209554851172626\n",
      "Stochastic Gradient Descent(45852): loss=10.239460987956596\n",
      "Stochastic Gradient Descent(45853): loss=0.044496006890425335\n",
      "Stochastic Gradient Descent(45854): loss=2.8539084129596555\n",
      "Stochastic Gradient Descent(45855): loss=14.901651667611628\n",
      "Stochastic Gradient Descent(45856): loss=4.473231346465496\n",
      "Stochastic Gradient Descent(45857): loss=21.710559636911057\n",
      "Stochastic Gradient Descent(45858): loss=2.7046141209722765\n",
      "Stochastic Gradient Descent(45859): loss=9.384696690290559\n",
      "Stochastic Gradient Descent(45860): loss=37.24502208580728\n",
      "Stochastic Gradient Descent(45861): loss=0.34264670512050655\n",
      "Stochastic Gradient Descent(45862): loss=7.256225122995453\n",
      "Stochastic Gradient Descent(45863): loss=1.396624570542357\n",
      "Stochastic Gradient Descent(45864): loss=0.4203308933197763\n",
      "Stochastic Gradient Descent(45865): loss=5.542976213581248\n",
      "Stochastic Gradient Descent(45866): loss=2.7034516541230564\n",
      "Stochastic Gradient Descent(45867): loss=1.8785832146969141\n",
      "Stochastic Gradient Descent(45868): loss=10.960381465937369\n",
      "Stochastic Gradient Descent(45869): loss=0.0007277040264671333\n",
      "Stochastic Gradient Descent(45870): loss=0.08975933854651057\n",
      "Stochastic Gradient Descent(45871): loss=3.2459280778942223\n",
      "Stochastic Gradient Descent(45872): loss=0.03461085762835869\n",
      "Stochastic Gradient Descent(45873): loss=22.186919687284817\n",
      "Stochastic Gradient Descent(45874): loss=2.9216769019577833\n",
      "Stochastic Gradient Descent(45875): loss=4.607483181881783\n",
      "Stochastic Gradient Descent(45876): loss=4.602719618206754\n",
      "Stochastic Gradient Descent(45877): loss=3.2746891175971697\n",
      "Stochastic Gradient Descent(45878): loss=3.1567947001680747\n",
      "Stochastic Gradient Descent(45879): loss=0.049152639211887575\n",
      "Stochastic Gradient Descent(45880): loss=6.764645518646833\n",
      "Stochastic Gradient Descent(45881): loss=3.355287043439254\n",
      "Stochastic Gradient Descent(45882): loss=4.7487319740233405\n",
      "Stochastic Gradient Descent(45883): loss=2.974696868973795\n",
      "Stochastic Gradient Descent(45884): loss=0.3263863820831932\n",
      "Stochastic Gradient Descent(45885): loss=14.34333029369235\n",
      "Stochastic Gradient Descent(45886): loss=1.8625343521607927\n",
      "Stochastic Gradient Descent(45887): loss=0.13423230330244063\n",
      "Stochastic Gradient Descent(45888): loss=3.853311691592913\n",
      "Stochastic Gradient Descent(45889): loss=2.7042199694435034\n",
      "Stochastic Gradient Descent(45890): loss=13.931430706466125\n",
      "Stochastic Gradient Descent(45891): loss=1.7119661167802682\n",
      "Stochastic Gradient Descent(45892): loss=1.4700198363359067\n",
      "Stochastic Gradient Descent(45893): loss=4.141262592214948\n",
      "Stochastic Gradient Descent(45894): loss=1.8237402312590174\n",
      "Stochastic Gradient Descent(45895): loss=1.243638379291363\n",
      "Stochastic Gradient Descent(45896): loss=4.227716607984647\n",
      "Stochastic Gradient Descent(45897): loss=1.618695348797669\n",
      "Stochastic Gradient Descent(45898): loss=0.8263702835969859\n",
      "Stochastic Gradient Descent(45899): loss=0.49010141370499666\n",
      "Stochastic Gradient Descent(45900): loss=6.349135329162743\n",
      "Stochastic Gradient Descent(45901): loss=0.2417391560905162\n",
      "Stochastic Gradient Descent(45902): loss=3.041528174588945\n",
      "Stochastic Gradient Descent(45903): loss=0.0007737866814187444\n",
      "Stochastic Gradient Descent(45904): loss=11.182460112467743\n",
      "Stochastic Gradient Descent(45905): loss=3.5075334724831335\n",
      "Stochastic Gradient Descent(45906): loss=5.910848195754208\n",
      "Stochastic Gradient Descent(45907): loss=3.8797076486851223\n",
      "Stochastic Gradient Descent(45908): loss=5.84421010257158\n",
      "Stochastic Gradient Descent(45909): loss=0.22974804458362602\n",
      "Stochastic Gradient Descent(45910): loss=0.5631707166061144\n",
      "Stochastic Gradient Descent(45911): loss=0.8474640557948458\n",
      "Stochastic Gradient Descent(45912): loss=4.609763569483703\n",
      "Stochastic Gradient Descent(45913): loss=4.631024232487877\n",
      "Stochastic Gradient Descent(45914): loss=1.942612506986437\n",
      "Stochastic Gradient Descent(45915): loss=7.337673560801783\n",
      "Stochastic Gradient Descent(45916): loss=10.647587684914516\n",
      "Stochastic Gradient Descent(45917): loss=0.10617944402638181\n",
      "Stochastic Gradient Descent(45918): loss=2.33731191547988\n",
      "Stochastic Gradient Descent(45919): loss=14.984331373501064\n",
      "Stochastic Gradient Descent(45920): loss=4.852088747209162\n",
      "Stochastic Gradient Descent(45921): loss=0.6516078420151289\n",
      "Stochastic Gradient Descent(45922): loss=1.967793466073101\n",
      "Stochastic Gradient Descent(45923): loss=0.012036547647766028\n",
      "Stochastic Gradient Descent(45924): loss=2.814542396286534\n",
      "Stochastic Gradient Descent(45925): loss=2.848765356409038\n",
      "Stochastic Gradient Descent(45926): loss=7.365196955503685\n",
      "Stochastic Gradient Descent(45927): loss=0.005294370679732763\n",
      "Stochastic Gradient Descent(45928): loss=3.2592223444493666\n",
      "Stochastic Gradient Descent(45929): loss=14.48602569802071\n",
      "Stochastic Gradient Descent(45930): loss=0.0004392259599915982\n",
      "Stochastic Gradient Descent(45931): loss=1.4018004531337978\n",
      "Stochastic Gradient Descent(45932): loss=6.827905775945153\n",
      "Stochastic Gradient Descent(45933): loss=2.893383340100683\n",
      "Stochastic Gradient Descent(45934): loss=1.5095565776292\n",
      "Stochastic Gradient Descent(45935): loss=0.010976013641477243\n",
      "Stochastic Gradient Descent(45936): loss=14.69460244193207\n",
      "Stochastic Gradient Descent(45937): loss=5.295547362934882\n",
      "Stochastic Gradient Descent(45938): loss=16.6000619042227\n",
      "Stochastic Gradient Descent(45939): loss=0.5527063430063764\n",
      "Stochastic Gradient Descent(45940): loss=2.6703120489625536\n",
      "Stochastic Gradient Descent(45941): loss=0.3549033960186688\n",
      "Stochastic Gradient Descent(45942): loss=0.0011733605481251117\n",
      "Stochastic Gradient Descent(45943): loss=0.19067686285903293\n",
      "Stochastic Gradient Descent(45944): loss=8.38287851348807\n",
      "Stochastic Gradient Descent(45945): loss=11.407650418215988\n",
      "Stochastic Gradient Descent(45946): loss=0.6219673504339898\n",
      "Stochastic Gradient Descent(45947): loss=2.1821454291535805\n",
      "Stochastic Gradient Descent(45948): loss=7.397496920906912\n",
      "Stochastic Gradient Descent(45949): loss=0.04365925774524145\n",
      "Stochastic Gradient Descent(45950): loss=0.22095057905602614\n",
      "Stochastic Gradient Descent(45951): loss=0.32003005934457396\n",
      "Stochastic Gradient Descent(45952): loss=5.167563666097072\n",
      "Stochastic Gradient Descent(45953): loss=0.2948418637657551\n",
      "Stochastic Gradient Descent(45954): loss=2.1949928321973466\n",
      "Stochastic Gradient Descent(45955): loss=0.8687593307905495\n",
      "Stochastic Gradient Descent(45956): loss=0.03024050318532092\n",
      "Stochastic Gradient Descent(45957): loss=0.13242919841917541\n",
      "Stochastic Gradient Descent(45958): loss=1.6707048467186487\n",
      "Stochastic Gradient Descent(45959): loss=0.021406899770229314\n",
      "Stochastic Gradient Descent(45960): loss=5.168307275956559\n",
      "Stochastic Gradient Descent(45961): loss=2.444383516136531\n",
      "Stochastic Gradient Descent(45962): loss=1.388235328681241\n",
      "Stochastic Gradient Descent(45963): loss=0.34937612163647247\n",
      "Stochastic Gradient Descent(45964): loss=7.35574368545855\n",
      "Stochastic Gradient Descent(45965): loss=1.280153458431653\n",
      "Stochastic Gradient Descent(45966): loss=0.4744921000261699\n",
      "Stochastic Gradient Descent(45967): loss=0.007121087651873828\n",
      "Stochastic Gradient Descent(45968): loss=12.67507834214205\n",
      "Stochastic Gradient Descent(45969): loss=0.2712971419283273\n",
      "Stochastic Gradient Descent(45970): loss=6.615074634951981\n",
      "Stochastic Gradient Descent(45971): loss=6.399461954332981\n",
      "Stochastic Gradient Descent(45972): loss=1.9041899334639385\n",
      "Stochastic Gradient Descent(45973): loss=0.8258566893247214\n",
      "Stochastic Gradient Descent(45974): loss=0.5870261069340316\n",
      "Stochastic Gradient Descent(45975): loss=33.556985753717655\n",
      "Stochastic Gradient Descent(45976): loss=45.81914389735336\n",
      "Stochastic Gradient Descent(45977): loss=2.1536479262321735\n",
      "Stochastic Gradient Descent(45978): loss=23.05769333791739\n",
      "Stochastic Gradient Descent(45979): loss=0.018980042928436795\n",
      "Stochastic Gradient Descent(45980): loss=0.9904852178948207\n",
      "Stochastic Gradient Descent(45981): loss=0.43299702086384334\n",
      "Stochastic Gradient Descent(45982): loss=0.44249145737878903\n",
      "Stochastic Gradient Descent(45983): loss=0.0076225113906334165\n",
      "Stochastic Gradient Descent(45984): loss=1.5555784235623706\n",
      "Stochastic Gradient Descent(45985): loss=0.15190842227149715\n",
      "Stochastic Gradient Descent(45986): loss=2.850428363227965\n",
      "Stochastic Gradient Descent(45987): loss=5.784858281652105\n",
      "Stochastic Gradient Descent(45988): loss=6.70024499473667\n",
      "Stochastic Gradient Descent(45989): loss=0.4993695380917425\n",
      "Stochastic Gradient Descent(45990): loss=0.8848752531820123\n",
      "Stochastic Gradient Descent(45991): loss=32.52019245640555\n",
      "Stochastic Gradient Descent(45992): loss=10.936429361263844\n",
      "Stochastic Gradient Descent(45993): loss=0.2351749792636627\n",
      "Stochastic Gradient Descent(45994): loss=2.1593705136661243\n",
      "Stochastic Gradient Descent(45995): loss=14.100990261513092\n",
      "Stochastic Gradient Descent(45996): loss=8.02430251232617\n",
      "Stochastic Gradient Descent(45997): loss=1.150373996305663\n",
      "Stochastic Gradient Descent(45998): loss=2.2373148599033255\n",
      "Stochastic Gradient Descent(45999): loss=3.6044093015930083\n",
      "Stochastic Gradient Descent(46000): loss=0.5106843018404259\n",
      "Stochastic Gradient Descent(46001): loss=0.8805786046925214\n",
      "Stochastic Gradient Descent(46002): loss=0.4364882446164996\n",
      "Stochastic Gradient Descent(46003): loss=6.644799339510856\n",
      "Stochastic Gradient Descent(46004): loss=3.3716450747073154\n",
      "Stochastic Gradient Descent(46005): loss=7.963343248923561\n",
      "Stochastic Gradient Descent(46006): loss=1.707088200283006\n",
      "Stochastic Gradient Descent(46007): loss=0.9932692640984017\n",
      "Stochastic Gradient Descent(46008): loss=9.710754122658557\n",
      "Stochastic Gradient Descent(46009): loss=0.0436018267202401\n",
      "Stochastic Gradient Descent(46010): loss=9.143605310608152\n",
      "Stochastic Gradient Descent(46011): loss=4.056982366376201\n",
      "Stochastic Gradient Descent(46012): loss=2.8231536585769454\n",
      "Stochastic Gradient Descent(46013): loss=0.7791589556908546\n",
      "Stochastic Gradient Descent(46014): loss=1.5093197659988353\n",
      "Stochastic Gradient Descent(46015): loss=8.054410911529148\n",
      "Stochastic Gradient Descent(46016): loss=7.676819750834402\n",
      "Stochastic Gradient Descent(46017): loss=5.607433597801747\n",
      "Stochastic Gradient Descent(46018): loss=2.904304895251116\n",
      "Stochastic Gradient Descent(46019): loss=4.428072656886763\n",
      "Stochastic Gradient Descent(46020): loss=0.0033691342511338895\n",
      "Stochastic Gradient Descent(46021): loss=0.30378193333772047\n",
      "Stochastic Gradient Descent(46022): loss=2.7418855673010247\n",
      "Stochastic Gradient Descent(46023): loss=18.683309986913414\n",
      "Stochastic Gradient Descent(46024): loss=0.14837740274888517\n",
      "Stochastic Gradient Descent(46025): loss=0.11710203511826241\n",
      "Stochastic Gradient Descent(46026): loss=0.3841183098512052\n",
      "Stochastic Gradient Descent(46027): loss=0.25778309572567004\n",
      "Stochastic Gradient Descent(46028): loss=6.667208756930374\n",
      "Stochastic Gradient Descent(46029): loss=0.7915304105879145\n",
      "Stochastic Gradient Descent(46030): loss=0.17719536948003967\n",
      "Stochastic Gradient Descent(46031): loss=13.217163726532014\n",
      "Stochastic Gradient Descent(46032): loss=0.5189123432466288\n",
      "Stochastic Gradient Descent(46033): loss=13.438702720134346\n",
      "Stochastic Gradient Descent(46034): loss=6.2982651738409245\n",
      "Stochastic Gradient Descent(46035): loss=3.0801379389567214\n",
      "Stochastic Gradient Descent(46036): loss=1.2994624459848672\n",
      "Stochastic Gradient Descent(46037): loss=1.8374460488401974\n",
      "Stochastic Gradient Descent(46038): loss=0.027176498828664707\n",
      "Stochastic Gradient Descent(46039): loss=14.729987738813174\n",
      "Stochastic Gradient Descent(46040): loss=2.5113337757846277\n",
      "Stochastic Gradient Descent(46041): loss=14.700434217877524\n",
      "Stochastic Gradient Descent(46042): loss=3.549851135001176\n",
      "Stochastic Gradient Descent(46043): loss=8.07969837537001\n",
      "Stochastic Gradient Descent(46044): loss=0.38163498093586923\n",
      "Stochastic Gradient Descent(46045): loss=0.15934657305244657\n",
      "Stochastic Gradient Descent(46046): loss=5.020017786417903\n",
      "Stochastic Gradient Descent(46047): loss=0.8928433699882974\n",
      "Stochastic Gradient Descent(46048): loss=0.001961773560134147\n",
      "Stochastic Gradient Descent(46049): loss=0.40815053610558644\n",
      "Stochastic Gradient Descent(46050): loss=4.519483420303779\n",
      "Stochastic Gradient Descent(46051): loss=1.8497463790880706\n",
      "Stochastic Gradient Descent(46052): loss=0.008893668356851877\n",
      "Stochastic Gradient Descent(46053): loss=0.09776190172358269\n",
      "Stochastic Gradient Descent(46054): loss=1.9511480304044457\n",
      "Stochastic Gradient Descent(46055): loss=0.28744669222936436\n",
      "Stochastic Gradient Descent(46056): loss=0.2291092895115785\n",
      "Stochastic Gradient Descent(46057): loss=0.11167625772934292\n",
      "Stochastic Gradient Descent(46058): loss=5.751056537967476\n",
      "Stochastic Gradient Descent(46059): loss=1.6122722362106374\n",
      "Stochastic Gradient Descent(46060): loss=21.282488140280293\n",
      "Stochastic Gradient Descent(46061): loss=4.092442092694504\n",
      "Stochastic Gradient Descent(46062): loss=0.8705724330857588\n",
      "Stochastic Gradient Descent(46063): loss=2.300705567251653\n",
      "Stochastic Gradient Descent(46064): loss=0.6294932694951422\n",
      "Stochastic Gradient Descent(46065): loss=0.004969879308409263\n",
      "Stochastic Gradient Descent(46066): loss=2.18612485860342\n",
      "Stochastic Gradient Descent(46067): loss=8.182441995693361\n",
      "Stochastic Gradient Descent(46068): loss=1.168261791689742\n",
      "Stochastic Gradient Descent(46069): loss=3.184331720251131\n",
      "Stochastic Gradient Descent(46070): loss=0.005356980595175211\n",
      "Stochastic Gradient Descent(46071): loss=0.33776511205865656\n",
      "Stochastic Gradient Descent(46072): loss=23.697446407815235\n",
      "Stochastic Gradient Descent(46073): loss=1.537982542991771\n",
      "Stochastic Gradient Descent(46074): loss=0.39299730440467123\n",
      "Stochastic Gradient Descent(46075): loss=0.057609362323805986\n",
      "Stochastic Gradient Descent(46076): loss=14.002581597004271\n",
      "Stochastic Gradient Descent(46077): loss=0.045398009798427445\n",
      "Stochastic Gradient Descent(46078): loss=0.3800040117763697\n",
      "Stochastic Gradient Descent(46079): loss=1.7442742457426865\n",
      "Stochastic Gradient Descent(46080): loss=4.028357161747152\n",
      "Stochastic Gradient Descent(46081): loss=2.182273681281178\n",
      "Stochastic Gradient Descent(46082): loss=1.0329127506715086\n",
      "Stochastic Gradient Descent(46083): loss=5.672786913865063\n",
      "Stochastic Gradient Descent(46084): loss=2.987468970969462\n",
      "Stochastic Gradient Descent(46085): loss=0.22590709501753498\n",
      "Stochastic Gradient Descent(46086): loss=0.12242987926569239\n",
      "Stochastic Gradient Descent(46087): loss=4.3062692055670855\n",
      "Stochastic Gradient Descent(46088): loss=0.6480823232493146\n",
      "Stochastic Gradient Descent(46089): loss=30.05289710083554\n",
      "Stochastic Gradient Descent(46090): loss=12.921558459911754\n",
      "Stochastic Gradient Descent(46091): loss=6.1017268300433205\n",
      "Stochastic Gradient Descent(46092): loss=12.490603865270439\n",
      "Stochastic Gradient Descent(46093): loss=0.021130595247895154\n",
      "Stochastic Gradient Descent(46094): loss=2.4682829506603654\n",
      "Stochastic Gradient Descent(46095): loss=6.806305698929811\n",
      "Stochastic Gradient Descent(46096): loss=4.937247790088058\n",
      "Stochastic Gradient Descent(46097): loss=2.0228995858708476\n",
      "Stochastic Gradient Descent(46098): loss=5.388249618868007\n",
      "Stochastic Gradient Descent(46099): loss=0.23491411885360106\n",
      "Stochastic Gradient Descent(46100): loss=7.608668542024611\n",
      "Stochastic Gradient Descent(46101): loss=1.7490249839150631\n",
      "Stochastic Gradient Descent(46102): loss=0.0985086721481932\n",
      "Stochastic Gradient Descent(46103): loss=0.9744459112942833\n",
      "Stochastic Gradient Descent(46104): loss=2.3514764312331735\n",
      "Stochastic Gradient Descent(46105): loss=1.8204185864696452\n",
      "Stochastic Gradient Descent(46106): loss=0.9955147658840328\n",
      "Stochastic Gradient Descent(46107): loss=6.182073997386825\n",
      "Stochastic Gradient Descent(46108): loss=0.08053847993359563\n",
      "Stochastic Gradient Descent(46109): loss=1.5697477629435985\n",
      "Stochastic Gradient Descent(46110): loss=17.832491080021658\n",
      "Stochastic Gradient Descent(46111): loss=0.035325829408982164\n",
      "Stochastic Gradient Descent(46112): loss=0.6213003784344472\n",
      "Stochastic Gradient Descent(46113): loss=0.748710799381926\n",
      "Stochastic Gradient Descent(46114): loss=0.7154491739191144\n",
      "Stochastic Gradient Descent(46115): loss=2.9561362022710096\n",
      "Stochastic Gradient Descent(46116): loss=2.383900770854682\n",
      "Stochastic Gradient Descent(46117): loss=0.0002563278652421092\n",
      "Stochastic Gradient Descent(46118): loss=0.22906397076392204\n",
      "Stochastic Gradient Descent(46119): loss=1.798547649746239\n",
      "Stochastic Gradient Descent(46120): loss=0.1549184212997974\n",
      "Stochastic Gradient Descent(46121): loss=2.9777102216479605\n",
      "Stochastic Gradient Descent(46122): loss=0.34327994468939577\n",
      "Stochastic Gradient Descent(46123): loss=3.7513864323039776\n",
      "Stochastic Gradient Descent(46124): loss=3.3832807265584317\n",
      "Stochastic Gradient Descent(46125): loss=0.13205576149461937\n",
      "Stochastic Gradient Descent(46126): loss=0.8946642887268912\n",
      "Stochastic Gradient Descent(46127): loss=15.929682102105915\n",
      "Stochastic Gradient Descent(46128): loss=0.5530107642862602\n",
      "Stochastic Gradient Descent(46129): loss=3.030680709987443\n",
      "Stochastic Gradient Descent(46130): loss=4.974526030332235\n",
      "Stochastic Gradient Descent(46131): loss=1.0926852514628143\n",
      "Stochastic Gradient Descent(46132): loss=5.096713129909435\n",
      "Stochastic Gradient Descent(46133): loss=0.09799904228934275\n",
      "Stochastic Gradient Descent(46134): loss=0.39513083961670165\n",
      "Stochastic Gradient Descent(46135): loss=0.0003021580369252628\n",
      "Stochastic Gradient Descent(46136): loss=0.07219517727915888\n",
      "Stochastic Gradient Descent(46137): loss=0.35793939819271253\n",
      "Stochastic Gradient Descent(46138): loss=0.9439775438384549\n",
      "Stochastic Gradient Descent(46139): loss=6.336124273241878\n",
      "Stochastic Gradient Descent(46140): loss=15.189439478745145\n",
      "Stochastic Gradient Descent(46141): loss=9.945645611530468\n",
      "Stochastic Gradient Descent(46142): loss=4.7735279436353\n",
      "Stochastic Gradient Descent(46143): loss=2.049993946708263\n",
      "Stochastic Gradient Descent(46144): loss=4.066230082670344\n",
      "Stochastic Gradient Descent(46145): loss=2.2254171218414984\n",
      "Stochastic Gradient Descent(46146): loss=2.789206756365348\n",
      "Stochastic Gradient Descent(46147): loss=7.259112943247937\n",
      "Stochastic Gradient Descent(46148): loss=3.9817998782311084\n",
      "Stochastic Gradient Descent(46149): loss=8.226101406551162\n",
      "Stochastic Gradient Descent(46150): loss=0.07064685993438333\n",
      "Stochastic Gradient Descent(46151): loss=3.9215048954945897\n",
      "Stochastic Gradient Descent(46152): loss=1.4882674644908922\n",
      "Stochastic Gradient Descent(46153): loss=2.533261622897856\n",
      "Stochastic Gradient Descent(46154): loss=1.2253949210844997\n",
      "Stochastic Gradient Descent(46155): loss=0.0473462367701915\n",
      "Stochastic Gradient Descent(46156): loss=5.760583675575016\n",
      "Stochastic Gradient Descent(46157): loss=0.5350318643731603\n",
      "Stochastic Gradient Descent(46158): loss=1.8073266489131306\n",
      "Stochastic Gradient Descent(46159): loss=11.563420903268476\n",
      "Stochastic Gradient Descent(46160): loss=0.6960028332723648\n",
      "Stochastic Gradient Descent(46161): loss=6.613466629821208\n",
      "Stochastic Gradient Descent(46162): loss=0.8737338686042478\n",
      "Stochastic Gradient Descent(46163): loss=1.7456717954551013\n",
      "Stochastic Gradient Descent(46164): loss=10.550136843606959\n",
      "Stochastic Gradient Descent(46165): loss=0.6230578980277536\n",
      "Stochastic Gradient Descent(46166): loss=0.003980640403167561\n",
      "Stochastic Gradient Descent(46167): loss=5.138675374533238\n",
      "Stochastic Gradient Descent(46168): loss=0.0017532227187675505\n",
      "Stochastic Gradient Descent(46169): loss=6.405499174552315\n",
      "Stochastic Gradient Descent(46170): loss=0.6883299433644131\n",
      "Stochastic Gradient Descent(46171): loss=0.04268224736328673\n",
      "Stochastic Gradient Descent(46172): loss=0.000876756724326475\n",
      "Stochastic Gradient Descent(46173): loss=0.0029477463390519738\n",
      "Stochastic Gradient Descent(46174): loss=1.4446725234456612\n",
      "Stochastic Gradient Descent(46175): loss=0.007680354220544655\n",
      "Stochastic Gradient Descent(46176): loss=0.09583561629051293\n",
      "Stochastic Gradient Descent(46177): loss=0.3173152987248767\n",
      "Stochastic Gradient Descent(46178): loss=0.1600573931842377\n",
      "Stochastic Gradient Descent(46179): loss=1.4271197264987976\n",
      "Stochastic Gradient Descent(46180): loss=0.0003342212675475142\n",
      "Stochastic Gradient Descent(46181): loss=1.5612613127549602\n",
      "Stochastic Gradient Descent(46182): loss=1.6805903486721496\n",
      "Stochastic Gradient Descent(46183): loss=1.5813029647230281\n",
      "Stochastic Gradient Descent(46184): loss=13.565457273902913\n",
      "Stochastic Gradient Descent(46185): loss=0.37338046253020957\n",
      "Stochastic Gradient Descent(46186): loss=23.31590304788487\n",
      "Stochastic Gradient Descent(46187): loss=0.15550737242936488\n",
      "Stochastic Gradient Descent(46188): loss=0.03557205867189073\n",
      "Stochastic Gradient Descent(46189): loss=1.0056205200389585\n",
      "Stochastic Gradient Descent(46190): loss=0.23387174707307629\n",
      "Stochastic Gradient Descent(46191): loss=7.269654691219782\n",
      "Stochastic Gradient Descent(46192): loss=1.2967889815334286\n",
      "Stochastic Gradient Descent(46193): loss=6.908257276329113\n",
      "Stochastic Gradient Descent(46194): loss=1.4807093430229332\n",
      "Stochastic Gradient Descent(46195): loss=3.0402310642903205\n",
      "Stochastic Gradient Descent(46196): loss=0.060524285079969056\n",
      "Stochastic Gradient Descent(46197): loss=8.406624330969873\n",
      "Stochastic Gradient Descent(46198): loss=3.145044791501672\n",
      "Stochastic Gradient Descent(46199): loss=0.732668078439131\n",
      "Stochastic Gradient Descent(46200): loss=1.1273080753806897\n",
      "Stochastic Gradient Descent(46201): loss=1.4966416515506356\n",
      "Stochastic Gradient Descent(46202): loss=0.8633159314198842\n",
      "Stochastic Gradient Descent(46203): loss=8.953637385201626\n",
      "Stochastic Gradient Descent(46204): loss=1.3856737976330795\n",
      "Stochastic Gradient Descent(46205): loss=16.34621492270178\n",
      "Stochastic Gradient Descent(46206): loss=0.002074829223591902\n",
      "Stochastic Gradient Descent(46207): loss=5.053183054350318\n",
      "Stochastic Gradient Descent(46208): loss=7.04880230408267\n",
      "Stochastic Gradient Descent(46209): loss=1.7195587963540064\n",
      "Stochastic Gradient Descent(46210): loss=24.603732362024097\n",
      "Stochastic Gradient Descent(46211): loss=3.8401980305067807\n",
      "Stochastic Gradient Descent(46212): loss=4.08405438295618\n",
      "Stochastic Gradient Descent(46213): loss=2.8377101555538182\n",
      "Stochastic Gradient Descent(46214): loss=22.90883575298348\n",
      "Stochastic Gradient Descent(46215): loss=0.2258585191822029\n",
      "Stochastic Gradient Descent(46216): loss=0.7929101979288369\n",
      "Stochastic Gradient Descent(46217): loss=7.7369463924168285\n",
      "Stochastic Gradient Descent(46218): loss=3.1459400672978597\n",
      "Stochastic Gradient Descent(46219): loss=0.9374530582159498\n",
      "Stochastic Gradient Descent(46220): loss=6.55376284080025\n",
      "Stochastic Gradient Descent(46221): loss=3.3920195913810405\n",
      "Stochastic Gradient Descent(46222): loss=1.6374124219465598\n",
      "Stochastic Gradient Descent(46223): loss=8.346493493397643\n",
      "Stochastic Gradient Descent(46224): loss=0.19208386803238325\n",
      "Stochastic Gradient Descent(46225): loss=6.86987727296187\n",
      "Stochastic Gradient Descent(46226): loss=3.4947251666622035\n",
      "Stochastic Gradient Descent(46227): loss=0.010829070593920253\n",
      "Stochastic Gradient Descent(46228): loss=3.235892381192315\n",
      "Stochastic Gradient Descent(46229): loss=3.1195507553975013\n",
      "Stochastic Gradient Descent(46230): loss=1.2255770007597178\n",
      "Stochastic Gradient Descent(46231): loss=0.38324048503340136\n",
      "Stochastic Gradient Descent(46232): loss=6.921626048469402\n",
      "Stochastic Gradient Descent(46233): loss=1.365686468116222\n",
      "Stochastic Gradient Descent(46234): loss=0.5381756361346541\n",
      "Stochastic Gradient Descent(46235): loss=0.32855912857254727\n",
      "Stochastic Gradient Descent(46236): loss=0.681168186851239\n",
      "Stochastic Gradient Descent(46237): loss=0.5324572211058144\n",
      "Stochastic Gradient Descent(46238): loss=3.3818378083036027\n",
      "Stochastic Gradient Descent(46239): loss=2.2973305238067514\n",
      "Stochastic Gradient Descent(46240): loss=8.893634406390603\n",
      "Stochastic Gradient Descent(46241): loss=0.0014956101532656895\n",
      "Stochastic Gradient Descent(46242): loss=7.671403069664236\n",
      "Stochastic Gradient Descent(46243): loss=4.152857197235052\n",
      "Stochastic Gradient Descent(46244): loss=1.8788480960140561\n",
      "Stochastic Gradient Descent(46245): loss=0.1904053034055215\n",
      "Stochastic Gradient Descent(46246): loss=6.748907320557363\n",
      "Stochastic Gradient Descent(46247): loss=0.044541853692855626\n",
      "Stochastic Gradient Descent(46248): loss=0.8407101647245548\n",
      "Stochastic Gradient Descent(46249): loss=3.3991789687078895\n",
      "Stochastic Gradient Descent(46250): loss=0.09747088606411801\n",
      "Stochastic Gradient Descent(46251): loss=0.44615079659405027\n",
      "Stochastic Gradient Descent(46252): loss=4.435161696438419\n",
      "Stochastic Gradient Descent(46253): loss=0.5256234673964679\n",
      "Stochastic Gradient Descent(46254): loss=2.8940041112125954\n",
      "Stochastic Gradient Descent(46255): loss=1.0654991591097025\n",
      "Stochastic Gradient Descent(46256): loss=2.7273395670366805\n",
      "Stochastic Gradient Descent(46257): loss=0.7705002552002501\n",
      "Stochastic Gradient Descent(46258): loss=7.569225511880797\n",
      "Stochastic Gradient Descent(46259): loss=6.916249243258713\n",
      "Stochastic Gradient Descent(46260): loss=9.4769105290022\n",
      "Stochastic Gradient Descent(46261): loss=3.8444084374273753\n",
      "Stochastic Gradient Descent(46262): loss=1.5418866638312807\n",
      "Stochastic Gradient Descent(46263): loss=0.09049265123298988\n",
      "Stochastic Gradient Descent(46264): loss=2.4698635731337273\n",
      "Stochastic Gradient Descent(46265): loss=10.275933501046504\n",
      "Stochastic Gradient Descent(46266): loss=0.013370387828735512\n",
      "Stochastic Gradient Descent(46267): loss=1.0795009677790364\n",
      "Stochastic Gradient Descent(46268): loss=0.0871282723918582\n",
      "Stochastic Gradient Descent(46269): loss=3.9370646973274175\n",
      "Stochastic Gradient Descent(46270): loss=8.229504681015724\n",
      "Stochastic Gradient Descent(46271): loss=1.9664471495300422\n",
      "Stochastic Gradient Descent(46272): loss=14.859182703534243\n",
      "Stochastic Gradient Descent(46273): loss=7.697559348903139\n",
      "Stochastic Gradient Descent(46274): loss=8.910658646133987\n",
      "Stochastic Gradient Descent(46275): loss=6.1745417241563745\n",
      "Stochastic Gradient Descent(46276): loss=0.6830367281354125\n",
      "Stochastic Gradient Descent(46277): loss=0.3006281027364984\n",
      "Stochastic Gradient Descent(46278): loss=9.267300512274323\n",
      "Stochastic Gradient Descent(46279): loss=1.6057697685977803\n",
      "Stochastic Gradient Descent(46280): loss=1.2688491409784768\n",
      "Stochastic Gradient Descent(46281): loss=0.02379762952070054\n",
      "Stochastic Gradient Descent(46282): loss=6.019889022824539\n",
      "Stochastic Gradient Descent(46283): loss=9.069391355707108\n",
      "Stochastic Gradient Descent(46284): loss=14.84572699198266\n",
      "Stochastic Gradient Descent(46285): loss=7.803670244359155\n",
      "Stochastic Gradient Descent(46286): loss=12.74314564421886\n",
      "Stochastic Gradient Descent(46287): loss=1.7704145685972597\n",
      "Stochastic Gradient Descent(46288): loss=1.1555423942817342\n",
      "Stochastic Gradient Descent(46289): loss=5.63004716654178\n",
      "Stochastic Gradient Descent(46290): loss=0.11589950131223481\n",
      "Stochastic Gradient Descent(46291): loss=0.7096640740993393\n",
      "Stochastic Gradient Descent(46292): loss=3.107980838793031\n",
      "Stochastic Gradient Descent(46293): loss=1.2568026433254196\n",
      "Stochastic Gradient Descent(46294): loss=9.052619308158551\n",
      "Stochastic Gradient Descent(46295): loss=0.0009489186713067259\n",
      "Stochastic Gradient Descent(46296): loss=2.364131492298658\n",
      "Stochastic Gradient Descent(46297): loss=0.4839902482442025\n",
      "Stochastic Gradient Descent(46298): loss=0.2414879746134898\n",
      "Stochastic Gradient Descent(46299): loss=0.19935617728180127\n",
      "Stochastic Gradient Descent(46300): loss=0.6829270771425772\n",
      "Stochastic Gradient Descent(46301): loss=1.37827100751147\n",
      "Stochastic Gradient Descent(46302): loss=1.2152899415460099\n",
      "Stochastic Gradient Descent(46303): loss=1.9942998149449154\n",
      "Stochastic Gradient Descent(46304): loss=3.6837650876521795\n",
      "Stochastic Gradient Descent(46305): loss=0.011058972350095378\n",
      "Stochastic Gradient Descent(46306): loss=0.011247402957103633\n",
      "Stochastic Gradient Descent(46307): loss=0.9342332360214726\n",
      "Stochastic Gradient Descent(46308): loss=0.39777573496538415\n",
      "Stochastic Gradient Descent(46309): loss=1.5642393514533746\n",
      "Stochastic Gradient Descent(46310): loss=1.5063014099269505\n",
      "Stochastic Gradient Descent(46311): loss=2.968904016797135\n",
      "Stochastic Gradient Descent(46312): loss=0.3107450218589237\n",
      "Stochastic Gradient Descent(46313): loss=0.00025482875163081864\n",
      "Stochastic Gradient Descent(46314): loss=0.35883560073968007\n",
      "Stochastic Gradient Descent(46315): loss=20.206591698727856\n",
      "Stochastic Gradient Descent(46316): loss=1.0065869595747927\n",
      "Stochastic Gradient Descent(46317): loss=2.557245729458437\n",
      "Stochastic Gradient Descent(46318): loss=0.038147207531675595\n",
      "Stochastic Gradient Descent(46319): loss=1.08195576220341\n",
      "Stochastic Gradient Descent(46320): loss=0.09117806591277855\n",
      "Stochastic Gradient Descent(46321): loss=1.4997843544794873\n",
      "Stochastic Gradient Descent(46322): loss=10.974471783440555\n",
      "Stochastic Gradient Descent(46323): loss=0.07658586870295943\n",
      "Stochastic Gradient Descent(46324): loss=4.173187567328475\n",
      "Stochastic Gradient Descent(46325): loss=3.4542875983372685\n",
      "Stochastic Gradient Descent(46326): loss=1.0800406186964773\n",
      "Stochastic Gradient Descent(46327): loss=5.4861855270965\n",
      "Stochastic Gradient Descent(46328): loss=0.7121000455759391\n",
      "Stochastic Gradient Descent(46329): loss=0.0642059132721306\n",
      "Stochastic Gradient Descent(46330): loss=3.131362156451376\n",
      "Stochastic Gradient Descent(46331): loss=13.954899005261206\n",
      "Stochastic Gradient Descent(46332): loss=4.633800029468475\n",
      "Stochastic Gradient Descent(46333): loss=0.05207094364872022\n",
      "Stochastic Gradient Descent(46334): loss=11.932817911070815\n",
      "Stochastic Gradient Descent(46335): loss=0.5640812293435267\n",
      "Stochastic Gradient Descent(46336): loss=0.21455438685532563\n",
      "Stochastic Gradient Descent(46337): loss=0.37104694765706214\n",
      "Stochastic Gradient Descent(46338): loss=0.050638782355689825\n",
      "Stochastic Gradient Descent(46339): loss=3.4404165931898882\n",
      "Stochastic Gradient Descent(46340): loss=0.3426504532038186\n",
      "Stochastic Gradient Descent(46341): loss=0.5893473039369507\n",
      "Stochastic Gradient Descent(46342): loss=1.0201161477549507\n",
      "Stochastic Gradient Descent(46343): loss=1.6073783842589338\n",
      "Stochastic Gradient Descent(46344): loss=0.5200692378501889\n",
      "Stochastic Gradient Descent(46345): loss=21.016726622906692\n",
      "Stochastic Gradient Descent(46346): loss=2.1279780074540686\n",
      "Stochastic Gradient Descent(46347): loss=6.3060388849006\n",
      "Stochastic Gradient Descent(46348): loss=0.3962713660946422\n",
      "Stochastic Gradient Descent(46349): loss=1.5367283184415597\n",
      "Stochastic Gradient Descent(46350): loss=2.1598699562812786\n",
      "Stochastic Gradient Descent(46351): loss=15.188841061980298\n",
      "Stochastic Gradient Descent(46352): loss=0.025128421708668462\n",
      "Stochastic Gradient Descent(46353): loss=10.586255198432198\n",
      "Stochastic Gradient Descent(46354): loss=0.6844732773354553\n",
      "Stochastic Gradient Descent(46355): loss=8.950764312780013\n",
      "Stochastic Gradient Descent(46356): loss=0.24086304579398862\n",
      "Stochastic Gradient Descent(46357): loss=75.32373392511798\n",
      "Stochastic Gradient Descent(46358): loss=27.244729740409106\n",
      "Stochastic Gradient Descent(46359): loss=0.3328923653849765\n",
      "Stochastic Gradient Descent(46360): loss=12.259776000602557\n",
      "Stochastic Gradient Descent(46361): loss=6.761421801966263\n",
      "Stochastic Gradient Descent(46362): loss=0.361167881010307\n",
      "Stochastic Gradient Descent(46363): loss=0.7961447437888756\n",
      "Stochastic Gradient Descent(46364): loss=4.569680601462774\n",
      "Stochastic Gradient Descent(46365): loss=9.982911270221669\n",
      "Stochastic Gradient Descent(46366): loss=0.12140625085478486\n",
      "Stochastic Gradient Descent(46367): loss=0.15899584574620215\n",
      "Stochastic Gradient Descent(46368): loss=2.3983626127855304\n",
      "Stochastic Gradient Descent(46369): loss=4.9448114530926315\n",
      "Stochastic Gradient Descent(46370): loss=4.088026746960096\n",
      "Stochastic Gradient Descent(46371): loss=53.90546874489162\n",
      "Stochastic Gradient Descent(46372): loss=16.611801646589093\n",
      "Stochastic Gradient Descent(46373): loss=0.6282875344655025\n",
      "Stochastic Gradient Descent(46374): loss=8.254003890291784\n",
      "Stochastic Gradient Descent(46375): loss=12.424670092444176\n",
      "Stochastic Gradient Descent(46376): loss=0.6098427180045165\n",
      "Stochastic Gradient Descent(46377): loss=0.5483496850357129\n",
      "Stochastic Gradient Descent(46378): loss=0.0021629992365162967\n",
      "Stochastic Gradient Descent(46379): loss=0.28364403283650463\n",
      "Stochastic Gradient Descent(46380): loss=2.0035564773659185\n",
      "Stochastic Gradient Descent(46381): loss=0.38842377039945286\n",
      "Stochastic Gradient Descent(46382): loss=0.38092456743809894\n",
      "Stochastic Gradient Descent(46383): loss=0.07314011686077976\n",
      "Stochastic Gradient Descent(46384): loss=5.210406528501657\n",
      "Stochastic Gradient Descent(46385): loss=0.1464945401882914\n",
      "Stochastic Gradient Descent(46386): loss=2.125302447818048\n",
      "Stochastic Gradient Descent(46387): loss=0.9639298954671875\n",
      "Stochastic Gradient Descent(46388): loss=0.6852719490014617\n",
      "Stochastic Gradient Descent(46389): loss=0.7369358068311564\n",
      "Stochastic Gradient Descent(46390): loss=0.7347581235116792\n",
      "Stochastic Gradient Descent(46391): loss=9.053921928167911\n",
      "Stochastic Gradient Descent(46392): loss=9.792142866101024\n",
      "Stochastic Gradient Descent(46393): loss=4.2046115078805135\n",
      "Stochastic Gradient Descent(46394): loss=1.6412810962003497\n",
      "Stochastic Gradient Descent(46395): loss=6.938490412393185\n",
      "Stochastic Gradient Descent(46396): loss=2.956421997368161\n",
      "Stochastic Gradient Descent(46397): loss=0.370225983365767\n",
      "Stochastic Gradient Descent(46398): loss=5.275266660485009\n",
      "Stochastic Gradient Descent(46399): loss=2.537113903856863\n",
      "Stochastic Gradient Descent(46400): loss=4.073206914319452\n",
      "Stochastic Gradient Descent(46401): loss=4.823800269955354\n",
      "Stochastic Gradient Descent(46402): loss=1.017405994491863\n",
      "Stochastic Gradient Descent(46403): loss=2.6204134157139167\n",
      "Stochastic Gradient Descent(46404): loss=1.47945169919838\n",
      "Stochastic Gradient Descent(46405): loss=0.6727946352392133\n",
      "Stochastic Gradient Descent(46406): loss=2.1764432010682038\n",
      "Stochastic Gradient Descent(46407): loss=4.217800787304898\n",
      "Stochastic Gradient Descent(46408): loss=59.261520733137786\n",
      "Stochastic Gradient Descent(46409): loss=0.0574114640188534\n",
      "Stochastic Gradient Descent(46410): loss=32.18085729396436\n",
      "Stochastic Gradient Descent(46411): loss=0.08494829125811695\n",
      "Stochastic Gradient Descent(46412): loss=12.398461815106364\n",
      "Stochastic Gradient Descent(46413): loss=0.4926553928289772\n",
      "Stochastic Gradient Descent(46414): loss=0.09917399545294259\n",
      "Stochastic Gradient Descent(46415): loss=10.989115539636718\n",
      "Stochastic Gradient Descent(46416): loss=2.3597856155643386\n",
      "Stochastic Gradient Descent(46417): loss=0.5824861523665222\n",
      "Stochastic Gradient Descent(46418): loss=2.924285330165383\n",
      "Stochastic Gradient Descent(46419): loss=0.1374706482823124\n",
      "Stochastic Gradient Descent(46420): loss=0.45391004728765366\n",
      "Stochastic Gradient Descent(46421): loss=0.7995455187237325\n",
      "Stochastic Gradient Descent(46422): loss=1.675095636875978\n",
      "Stochastic Gradient Descent(46423): loss=0.794792164123982\n",
      "Stochastic Gradient Descent(46424): loss=0.8777161075920251\n",
      "Stochastic Gradient Descent(46425): loss=1.6259923729154537\n",
      "Stochastic Gradient Descent(46426): loss=8.15930669515382\n",
      "Stochastic Gradient Descent(46427): loss=23.018184708984258\n",
      "Stochastic Gradient Descent(46428): loss=8.730828708973755\n",
      "Stochastic Gradient Descent(46429): loss=0.14804285470799966\n",
      "Stochastic Gradient Descent(46430): loss=0.37827164110556377\n",
      "Stochastic Gradient Descent(46431): loss=0.01245551979096841\n",
      "Stochastic Gradient Descent(46432): loss=14.842593917854808\n",
      "Stochastic Gradient Descent(46433): loss=2.411276439781024\n",
      "Stochastic Gradient Descent(46434): loss=16.541029007926852\n",
      "Stochastic Gradient Descent(46435): loss=0.35091346954851477\n",
      "Stochastic Gradient Descent(46436): loss=0.46522480706843955\n",
      "Stochastic Gradient Descent(46437): loss=0.6788303285319353\n",
      "Stochastic Gradient Descent(46438): loss=2.72603098994703\n",
      "Stochastic Gradient Descent(46439): loss=0.001983931265059963\n",
      "Stochastic Gradient Descent(46440): loss=1.6378090145287225\n",
      "Stochastic Gradient Descent(46441): loss=15.093517169015024\n",
      "Stochastic Gradient Descent(46442): loss=2.737916265921667\n",
      "Stochastic Gradient Descent(46443): loss=7.067063778129657\n",
      "Stochastic Gradient Descent(46444): loss=0.06502339475689274\n",
      "Stochastic Gradient Descent(46445): loss=12.928449436022811\n",
      "Stochastic Gradient Descent(46446): loss=0.517447502461576\n",
      "Stochastic Gradient Descent(46447): loss=0.31479425732356503\n",
      "Stochastic Gradient Descent(46448): loss=2.008042142755438\n",
      "Stochastic Gradient Descent(46449): loss=5.216890573364196\n",
      "Stochastic Gradient Descent(46450): loss=0.5205663738131722\n",
      "Stochastic Gradient Descent(46451): loss=2.1811147265860305\n",
      "Stochastic Gradient Descent(46452): loss=1.237670426804998\n",
      "Stochastic Gradient Descent(46453): loss=0.24920162025477058\n",
      "Stochastic Gradient Descent(46454): loss=0.9046428761093588\n",
      "Stochastic Gradient Descent(46455): loss=0.9153047268766988\n",
      "Stochastic Gradient Descent(46456): loss=0.3486710490736866\n",
      "Stochastic Gradient Descent(46457): loss=2.40774259901488\n",
      "Stochastic Gradient Descent(46458): loss=2.929074754027857\n",
      "Stochastic Gradient Descent(46459): loss=0.18110334089132235\n",
      "Stochastic Gradient Descent(46460): loss=0.011887547036357405\n",
      "Stochastic Gradient Descent(46461): loss=0.622744652638462\n",
      "Stochastic Gradient Descent(46462): loss=0.00806974678046902\n",
      "Stochastic Gradient Descent(46463): loss=0.5882249001547272\n",
      "Stochastic Gradient Descent(46464): loss=5.993899340789288\n",
      "Stochastic Gradient Descent(46465): loss=2.8842453895500784\n",
      "Stochastic Gradient Descent(46466): loss=0.8475273755458416\n",
      "Stochastic Gradient Descent(46467): loss=5.2521813232170285\n",
      "Stochastic Gradient Descent(46468): loss=0.5961594621998193\n",
      "Stochastic Gradient Descent(46469): loss=0.04387581074692214\n",
      "Stochastic Gradient Descent(46470): loss=0.48680198356140764\n",
      "Stochastic Gradient Descent(46471): loss=0.9086739580461966\n",
      "Stochastic Gradient Descent(46472): loss=2.128423518174563\n",
      "Stochastic Gradient Descent(46473): loss=4.825109346698524\n",
      "Stochastic Gradient Descent(46474): loss=0.23407752792748046\n",
      "Stochastic Gradient Descent(46475): loss=0.05864939572742894\n",
      "Stochastic Gradient Descent(46476): loss=0.04903654590107545\n",
      "Stochastic Gradient Descent(46477): loss=3.1854122628682116\n",
      "Stochastic Gradient Descent(46478): loss=0.3142526222387478\n",
      "Stochastic Gradient Descent(46479): loss=0.0528780516345827\n",
      "Stochastic Gradient Descent(46480): loss=4.145647121114278\n",
      "Stochastic Gradient Descent(46481): loss=0.7931828546951178\n",
      "Stochastic Gradient Descent(46482): loss=39.15375798005685\n",
      "Stochastic Gradient Descent(46483): loss=16.333286214733178\n",
      "Stochastic Gradient Descent(46484): loss=19.120012693351455\n",
      "Stochastic Gradient Descent(46485): loss=0.07974332164781082\n",
      "Stochastic Gradient Descent(46486): loss=5.78417481579544\n",
      "Stochastic Gradient Descent(46487): loss=2.229180028789182\n",
      "Stochastic Gradient Descent(46488): loss=3.549055517145847\n",
      "Stochastic Gradient Descent(46489): loss=2.290275042577765\n",
      "Stochastic Gradient Descent(46490): loss=0.119584055673655\n",
      "Stochastic Gradient Descent(46491): loss=24.685407716199958\n",
      "Stochastic Gradient Descent(46492): loss=7.914767638403591\n",
      "Stochastic Gradient Descent(46493): loss=0.4422135009293476\n",
      "Stochastic Gradient Descent(46494): loss=1.025479034953314\n",
      "Stochastic Gradient Descent(46495): loss=0.46337342265537496\n",
      "Stochastic Gradient Descent(46496): loss=2.61119053036943\n",
      "Stochastic Gradient Descent(46497): loss=1.5673178768994398\n",
      "Stochastic Gradient Descent(46498): loss=0.12442133531715831\n",
      "Stochastic Gradient Descent(46499): loss=10.979407182786122\n",
      "Stochastic Gradient Descent(46500): loss=0.408094331746937\n",
      "Stochastic Gradient Descent(46501): loss=0.9749825669334741\n",
      "Stochastic Gradient Descent(46502): loss=2.1406118545781987\n",
      "Stochastic Gradient Descent(46503): loss=1.1417713620307648\n",
      "Stochastic Gradient Descent(46504): loss=0.0020417805783589974\n",
      "Stochastic Gradient Descent(46505): loss=0.8354656339365\n",
      "Stochastic Gradient Descent(46506): loss=2.3798313752930302\n",
      "Stochastic Gradient Descent(46507): loss=2.0588668029743933\n",
      "Stochastic Gradient Descent(46508): loss=2.0596239570722847\n",
      "Stochastic Gradient Descent(46509): loss=1.483099515324759\n",
      "Stochastic Gradient Descent(46510): loss=0.21600793061082169\n",
      "Stochastic Gradient Descent(46511): loss=3.3361602785234217\n",
      "Stochastic Gradient Descent(46512): loss=5.5740581324914595\n",
      "Stochastic Gradient Descent(46513): loss=0.555826207659779\n",
      "Stochastic Gradient Descent(46514): loss=1.9476242529515384\n",
      "Stochastic Gradient Descent(46515): loss=1.503025339065491\n",
      "Stochastic Gradient Descent(46516): loss=2.231074188954365\n",
      "Stochastic Gradient Descent(46517): loss=0.23321940776197309\n",
      "Stochastic Gradient Descent(46518): loss=18.317787552941596\n",
      "Stochastic Gradient Descent(46519): loss=1.1943106149945935\n",
      "Stochastic Gradient Descent(46520): loss=3.5889807795173585\n",
      "Stochastic Gradient Descent(46521): loss=7.716520442859246\n",
      "Stochastic Gradient Descent(46522): loss=9.624511561509012\n",
      "Stochastic Gradient Descent(46523): loss=3.093230948922697\n",
      "Stochastic Gradient Descent(46524): loss=14.524295552677232\n",
      "Stochastic Gradient Descent(46525): loss=1.1415331872566508\n",
      "Stochastic Gradient Descent(46526): loss=4.808149578807036\n",
      "Stochastic Gradient Descent(46527): loss=4.035597259850305\n",
      "Stochastic Gradient Descent(46528): loss=0.17646174143516702\n",
      "Stochastic Gradient Descent(46529): loss=1.2821194357345171\n",
      "Stochastic Gradient Descent(46530): loss=22.250484081014264\n",
      "Stochastic Gradient Descent(46531): loss=0.16661189788119285\n",
      "Stochastic Gradient Descent(46532): loss=1.8352057386736556\n",
      "Stochastic Gradient Descent(46533): loss=1.6500625932212387\n",
      "Stochastic Gradient Descent(46534): loss=1.5854863913485782\n",
      "Stochastic Gradient Descent(46535): loss=1.9233213437561731\n",
      "Stochastic Gradient Descent(46536): loss=11.40927821451367\n",
      "Stochastic Gradient Descent(46537): loss=0.43460269402819884\n",
      "Stochastic Gradient Descent(46538): loss=5.725604500852587\n",
      "Stochastic Gradient Descent(46539): loss=0.00012064797970082635\n",
      "Stochastic Gradient Descent(46540): loss=1.1813682481727654\n",
      "Stochastic Gradient Descent(46541): loss=0.45545518874424396\n",
      "Stochastic Gradient Descent(46542): loss=0.0682039556690987\n",
      "Stochastic Gradient Descent(46543): loss=0.0008836444148475718\n",
      "Stochastic Gradient Descent(46544): loss=0.20234108625562502\n",
      "Stochastic Gradient Descent(46545): loss=4.496870655011829\n",
      "Stochastic Gradient Descent(46546): loss=2.391197664693511\n",
      "Stochastic Gradient Descent(46547): loss=0.18348213623958054\n",
      "Stochastic Gradient Descent(46548): loss=16.24979119135726\n",
      "Stochastic Gradient Descent(46549): loss=0.02027495400212769\n",
      "Stochastic Gradient Descent(46550): loss=5.362252607755388\n",
      "Stochastic Gradient Descent(46551): loss=4.576115648234458\n",
      "Stochastic Gradient Descent(46552): loss=0.03472812752012106\n",
      "Stochastic Gradient Descent(46553): loss=4.124618833993618\n",
      "Stochastic Gradient Descent(46554): loss=11.068507976978832\n",
      "Stochastic Gradient Descent(46555): loss=4.344505745645314\n",
      "Stochastic Gradient Descent(46556): loss=0.3993828340393\n",
      "Stochastic Gradient Descent(46557): loss=1.4952264569738498\n",
      "Stochastic Gradient Descent(46558): loss=43.40539885322651\n",
      "Stochastic Gradient Descent(46559): loss=48.20955250101923\n",
      "Stochastic Gradient Descent(46560): loss=0.4609278918856298\n",
      "Stochastic Gradient Descent(46561): loss=1.2767127731905394\n",
      "Stochastic Gradient Descent(46562): loss=0.7675941597360494\n",
      "Stochastic Gradient Descent(46563): loss=0.22121235056319224\n",
      "Stochastic Gradient Descent(46564): loss=1.124701173503443\n",
      "Stochastic Gradient Descent(46565): loss=4.810209859955468\n",
      "Stochastic Gradient Descent(46566): loss=0.15497581028989765\n",
      "Stochastic Gradient Descent(46567): loss=9.146484347861502\n",
      "Stochastic Gradient Descent(46568): loss=1.2732877952803954\n",
      "Stochastic Gradient Descent(46569): loss=0.1406399718417672\n",
      "Stochastic Gradient Descent(46570): loss=2.332031250930177\n",
      "Stochastic Gradient Descent(46571): loss=15.06136299169461\n",
      "Stochastic Gradient Descent(46572): loss=11.415236596012043\n",
      "Stochastic Gradient Descent(46573): loss=7.887917414896762\n",
      "Stochastic Gradient Descent(46574): loss=10.705945135328335\n",
      "Stochastic Gradient Descent(46575): loss=0.09312500341693718\n",
      "Stochastic Gradient Descent(46576): loss=1.8540303261168753\n",
      "Stochastic Gradient Descent(46577): loss=0.04111578214423258\n",
      "Stochastic Gradient Descent(46578): loss=0.8819077935889009\n",
      "Stochastic Gradient Descent(46579): loss=1.4964745453660186\n",
      "Stochastic Gradient Descent(46580): loss=5.25428068319955\n",
      "Stochastic Gradient Descent(46581): loss=1.953840970324151\n",
      "Stochastic Gradient Descent(46582): loss=3.2424105394910816\n",
      "Stochastic Gradient Descent(46583): loss=0.024210571543017256\n",
      "Stochastic Gradient Descent(46584): loss=6.826860590926789\n",
      "Stochastic Gradient Descent(46585): loss=0.9625504005335017\n",
      "Stochastic Gradient Descent(46586): loss=0.3932165004976602\n",
      "Stochastic Gradient Descent(46587): loss=0.3876906655787013\n",
      "Stochastic Gradient Descent(46588): loss=0.5946666127627512\n",
      "Stochastic Gradient Descent(46589): loss=0.5106293680204053\n",
      "Stochastic Gradient Descent(46590): loss=0.7176924450085205\n",
      "Stochastic Gradient Descent(46591): loss=12.861876052158678\n",
      "Stochastic Gradient Descent(46592): loss=0.5849322932502488\n",
      "Stochastic Gradient Descent(46593): loss=8.131937487936336\n",
      "Stochastic Gradient Descent(46594): loss=0.4091399494791532\n",
      "Stochastic Gradient Descent(46595): loss=0.3607395541723932\n",
      "Stochastic Gradient Descent(46596): loss=1.0288384551245082\n",
      "Stochastic Gradient Descent(46597): loss=0.2330654106063963\n",
      "Stochastic Gradient Descent(46598): loss=1.0571042066540732\n",
      "Stochastic Gradient Descent(46599): loss=3.4631600986315823\n",
      "Stochastic Gradient Descent(46600): loss=3.9623679753759022\n",
      "Stochastic Gradient Descent(46601): loss=0.056797890902490364\n",
      "Stochastic Gradient Descent(46602): loss=2.524829076411102\n",
      "Stochastic Gradient Descent(46603): loss=2.787001104686335\n",
      "Stochastic Gradient Descent(46604): loss=0.5859861329146803\n",
      "Stochastic Gradient Descent(46605): loss=1.7988773323584453\n",
      "Stochastic Gradient Descent(46606): loss=0.1992025825879821\n",
      "Stochastic Gradient Descent(46607): loss=0.0013508563371582097\n",
      "Stochastic Gradient Descent(46608): loss=5.922451347956552\n",
      "Stochastic Gradient Descent(46609): loss=1.0301981647605807\n",
      "Stochastic Gradient Descent(46610): loss=0.026635807916617057\n",
      "Stochastic Gradient Descent(46611): loss=1.313828986113159\n",
      "Stochastic Gradient Descent(46612): loss=12.346504480448397\n",
      "Stochastic Gradient Descent(46613): loss=0.24173572133806923\n",
      "Stochastic Gradient Descent(46614): loss=1.419494071848795\n",
      "Stochastic Gradient Descent(46615): loss=2.012878409901129\n",
      "Stochastic Gradient Descent(46616): loss=4.705219392156599\n",
      "Stochastic Gradient Descent(46617): loss=4.7219823310118505\n",
      "Stochastic Gradient Descent(46618): loss=1.655879774509955\n",
      "Stochastic Gradient Descent(46619): loss=7.828725047435237\n",
      "Stochastic Gradient Descent(46620): loss=7.25531655217186\n",
      "Stochastic Gradient Descent(46621): loss=0.02508560250459622\n",
      "Stochastic Gradient Descent(46622): loss=5.689609424859972\n",
      "Stochastic Gradient Descent(46623): loss=0.05695316232533966\n",
      "Stochastic Gradient Descent(46624): loss=0.5794808566424425\n",
      "Stochastic Gradient Descent(46625): loss=0.0011017515875247394\n",
      "Stochastic Gradient Descent(46626): loss=0.6344715258842372\n",
      "Stochastic Gradient Descent(46627): loss=0.14646292543940284\n",
      "Stochastic Gradient Descent(46628): loss=0.39434938509292444\n",
      "Stochastic Gradient Descent(46629): loss=1.4790176322488588\n",
      "Stochastic Gradient Descent(46630): loss=6.119168020535448\n",
      "Stochastic Gradient Descent(46631): loss=0.06769591119341742\n",
      "Stochastic Gradient Descent(46632): loss=1.2706943792852128\n",
      "Stochastic Gradient Descent(46633): loss=0.048765278009265174\n",
      "Stochastic Gradient Descent(46634): loss=2.2953238526713915\n",
      "Stochastic Gradient Descent(46635): loss=0.07393113429691155\n",
      "Stochastic Gradient Descent(46636): loss=0.04503812505906331\n",
      "Stochastic Gradient Descent(46637): loss=1.7414198065479363\n",
      "Stochastic Gradient Descent(46638): loss=0.2425734690890081\n",
      "Stochastic Gradient Descent(46639): loss=0.22735339669133892\n",
      "Stochastic Gradient Descent(46640): loss=1.609510138357695\n",
      "Stochastic Gradient Descent(46641): loss=8.631303235501216\n",
      "Stochastic Gradient Descent(46642): loss=0.36025297941023987\n",
      "Stochastic Gradient Descent(46643): loss=2.930133211497948\n",
      "Stochastic Gradient Descent(46644): loss=3.012801202579654\n",
      "Stochastic Gradient Descent(46645): loss=19.231431034080995\n",
      "Stochastic Gradient Descent(46646): loss=0.7531859743798325\n",
      "Stochastic Gradient Descent(46647): loss=0.012131615845022732\n",
      "Stochastic Gradient Descent(46648): loss=26.826425988005894\n",
      "Stochastic Gradient Descent(46649): loss=15.69715523693631\n",
      "Stochastic Gradient Descent(46650): loss=6.377981577887676\n",
      "Stochastic Gradient Descent(46651): loss=6.155699553087397\n",
      "Stochastic Gradient Descent(46652): loss=11.62463942194662\n",
      "Stochastic Gradient Descent(46653): loss=0.374802761081273\n",
      "Stochastic Gradient Descent(46654): loss=0.10986934885445336\n",
      "Stochastic Gradient Descent(46655): loss=18.30653807452802\n",
      "Stochastic Gradient Descent(46656): loss=0.5899363316177665\n",
      "Stochastic Gradient Descent(46657): loss=0.2838177981822638\n",
      "Stochastic Gradient Descent(46658): loss=0.4157852404828712\n",
      "Stochastic Gradient Descent(46659): loss=0.9501908295968567\n",
      "Stochastic Gradient Descent(46660): loss=5.08455520984113\n",
      "Stochastic Gradient Descent(46661): loss=2.472597071352255\n",
      "Stochastic Gradient Descent(46662): loss=0.4798847976839262\n",
      "Stochastic Gradient Descent(46663): loss=61.431841721651516\n",
      "Stochastic Gradient Descent(46664): loss=2.4973158381986087\n",
      "Stochastic Gradient Descent(46665): loss=26.213740628256225\n",
      "Stochastic Gradient Descent(46666): loss=55.39809733244195\n",
      "Stochastic Gradient Descent(46667): loss=69.98870843866511\n",
      "Stochastic Gradient Descent(46668): loss=0.6732190211373389\n",
      "Stochastic Gradient Descent(46669): loss=68.1355378841725\n",
      "Stochastic Gradient Descent(46670): loss=11.596948182592\n",
      "Stochastic Gradient Descent(46671): loss=5.601999835902927\n",
      "Stochastic Gradient Descent(46672): loss=1.9744117943809443\n",
      "Stochastic Gradient Descent(46673): loss=0.4842256593071584\n",
      "Stochastic Gradient Descent(46674): loss=2.289672366211227\n",
      "Stochastic Gradient Descent(46675): loss=0.053104643031877584\n",
      "Stochastic Gradient Descent(46676): loss=0.20758816586809933\n",
      "Stochastic Gradient Descent(46677): loss=0.5764542833819993\n",
      "Stochastic Gradient Descent(46678): loss=4.530287828793175\n",
      "Stochastic Gradient Descent(46679): loss=6.98204468897176\n",
      "Stochastic Gradient Descent(46680): loss=5.098225819008046\n",
      "Stochastic Gradient Descent(46681): loss=6.424340590843927\n",
      "Stochastic Gradient Descent(46682): loss=2.467692797693673\n",
      "Stochastic Gradient Descent(46683): loss=0.29639265917688645\n",
      "Stochastic Gradient Descent(46684): loss=13.228472534243789\n",
      "Stochastic Gradient Descent(46685): loss=0.10968555238002987\n",
      "Stochastic Gradient Descent(46686): loss=9.27518746074788\n",
      "Stochastic Gradient Descent(46687): loss=1.5180563809793493\n",
      "Stochastic Gradient Descent(46688): loss=9.037155209325155\n",
      "Stochastic Gradient Descent(46689): loss=0.47289744325808925\n",
      "Stochastic Gradient Descent(46690): loss=16.731631413701983\n",
      "Stochastic Gradient Descent(46691): loss=7.608455256645727e-05\n",
      "Stochastic Gradient Descent(46692): loss=15.87681885530844\n",
      "Stochastic Gradient Descent(46693): loss=0.9289792188207641\n",
      "Stochastic Gradient Descent(46694): loss=0.9273941803577813\n",
      "Stochastic Gradient Descent(46695): loss=0.5124377601963189\n",
      "Stochastic Gradient Descent(46696): loss=0.001205726139834411\n",
      "Stochastic Gradient Descent(46697): loss=0.03893882106059503\n",
      "Stochastic Gradient Descent(46698): loss=0.7997240932855652\n",
      "Stochastic Gradient Descent(46699): loss=1.039394701279397\n",
      "Stochastic Gradient Descent(46700): loss=1.0244105630995826\n",
      "Stochastic Gradient Descent(46701): loss=8.047228230306263\n",
      "Stochastic Gradient Descent(46702): loss=3.5532370838444627\n",
      "Stochastic Gradient Descent(46703): loss=0.647693829195728\n",
      "Stochastic Gradient Descent(46704): loss=11.771202760946402\n",
      "Stochastic Gradient Descent(46705): loss=0.02316735721009738\n",
      "Stochastic Gradient Descent(46706): loss=10.585136605890177\n",
      "Stochastic Gradient Descent(46707): loss=7.070432546717326\n",
      "Stochastic Gradient Descent(46708): loss=2.973769648027339\n",
      "Stochastic Gradient Descent(46709): loss=5.326720553001308\n",
      "Stochastic Gradient Descent(46710): loss=1.6625342948827346\n",
      "Stochastic Gradient Descent(46711): loss=0.6843882955703047\n",
      "Stochastic Gradient Descent(46712): loss=3.3119770337442023\n",
      "Stochastic Gradient Descent(46713): loss=1.5052249621913578\n",
      "Stochastic Gradient Descent(46714): loss=0.5816998120716228\n",
      "Stochastic Gradient Descent(46715): loss=6.667535287351245\n",
      "Stochastic Gradient Descent(46716): loss=11.8999066000446\n",
      "Stochastic Gradient Descent(46717): loss=3.6680624698660425\n",
      "Stochastic Gradient Descent(46718): loss=0.47275363354583855\n",
      "Stochastic Gradient Descent(46719): loss=0.274376275177721\n",
      "Stochastic Gradient Descent(46720): loss=0.5457883691332829\n",
      "Stochastic Gradient Descent(46721): loss=0.93366416534486\n",
      "Stochastic Gradient Descent(46722): loss=14.06809263234532\n",
      "Stochastic Gradient Descent(46723): loss=0.366250597175743\n",
      "Stochastic Gradient Descent(46724): loss=7.608542494224083\n",
      "Stochastic Gradient Descent(46725): loss=9.548665934086483\n",
      "Stochastic Gradient Descent(46726): loss=0.20086071309222434\n",
      "Stochastic Gradient Descent(46727): loss=0.11780815721625473\n",
      "Stochastic Gradient Descent(46728): loss=1.8456329899535229\n",
      "Stochastic Gradient Descent(46729): loss=5.060283373905652\n",
      "Stochastic Gradient Descent(46730): loss=0.0007648085670573398\n",
      "Stochastic Gradient Descent(46731): loss=1.5554779890179073\n",
      "Stochastic Gradient Descent(46732): loss=0.006458833166186285\n",
      "Stochastic Gradient Descent(46733): loss=4.786005848677761\n",
      "Stochastic Gradient Descent(46734): loss=5.875501821112344\n",
      "Stochastic Gradient Descent(46735): loss=15.195710820688399\n",
      "Stochastic Gradient Descent(46736): loss=0.399408445330603\n",
      "Stochastic Gradient Descent(46737): loss=2.746111270580165\n",
      "Stochastic Gradient Descent(46738): loss=5.409808030243689\n",
      "Stochastic Gradient Descent(46739): loss=5.305698170991021\n",
      "Stochastic Gradient Descent(46740): loss=0.059186104690214236\n",
      "Stochastic Gradient Descent(46741): loss=0.26107692138354444\n",
      "Stochastic Gradient Descent(46742): loss=1.2437357474086765\n",
      "Stochastic Gradient Descent(46743): loss=0.3811863765341192\n",
      "Stochastic Gradient Descent(46744): loss=0.1767309643287061\n",
      "Stochastic Gradient Descent(46745): loss=7.2329559502851986\n",
      "Stochastic Gradient Descent(46746): loss=2.607706958783646\n",
      "Stochastic Gradient Descent(46747): loss=0.5695930387791976\n",
      "Stochastic Gradient Descent(46748): loss=2.0580048570529192\n",
      "Stochastic Gradient Descent(46749): loss=0.16969146715725708\n",
      "Stochastic Gradient Descent(46750): loss=20.785289871353402\n",
      "Stochastic Gradient Descent(46751): loss=2.9746000577450102\n",
      "Stochastic Gradient Descent(46752): loss=4.090947828408811\n",
      "Stochastic Gradient Descent(46753): loss=9.916946628798673\n",
      "Stochastic Gradient Descent(46754): loss=4.6811882994403105\n",
      "Stochastic Gradient Descent(46755): loss=0.37802525222563427\n",
      "Stochastic Gradient Descent(46756): loss=0.31648752728118235\n",
      "Stochastic Gradient Descent(46757): loss=0.11112835413861602\n",
      "Stochastic Gradient Descent(46758): loss=4.07770412842284\n",
      "Stochastic Gradient Descent(46759): loss=0.036972613691212804\n",
      "Stochastic Gradient Descent(46760): loss=1.0703574474243052\n",
      "Stochastic Gradient Descent(46761): loss=3.071959948333495\n",
      "Stochastic Gradient Descent(46762): loss=11.934092632830746\n",
      "Stochastic Gradient Descent(46763): loss=0.0634593336455005\n",
      "Stochastic Gradient Descent(46764): loss=1.6224303138321505\n",
      "Stochastic Gradient Descent(46765): loss=0.2652654094554119\n",
      "Stochastic Gradient Descent(46766): loss=0.5246220125116215\n",
      "Stochastic Gradient Descent(46767): loss=0.8338135399344072\n",
      "Stochastic Gradient Descent(46768): loss=0.06753634925386061\n",
      "Stochastic Gradient Descent(46769): loss=0.645337398796871\n",
      "Stochastic Gradient Descent(46770): loss=1.4040023118469767\n",
      "Stochastic Gradient Descent(46771): loss=15.181589910737538\n",
      "Stochastic Gradient Descent(46772): loss=0.014317766086362892\n",
      "Stochastic Gradient Descent(46773): loss=3.649788632534924\n",
      "Stochastic Gradient Descent(46774): loss=0.12490175715934934\n",
      "Stochastic Gradient Descent(46775): loss=3.485421676581214\n",
      "Stochastic Gradient Descent(46776): loss=7.063523428757876\n",
      "Stochastic Gradient Descent(46777): loss=0.02936741527833508\n",
      "Stochastic Gradient Descent(46778): loss=1.1318777435522411\n",
      "Stochastic Gradient Descent(46779): loss=1.8726158008849954\n",
      "Stochastic Gradient Descent(46780): loss=0.46784024427524407\n",
      "Stochastic Gradient Descent(46781): loss=3.3724501835655465\n",
      "Stochastic Gradient Descent(46782): loss=15.358365733235097\n",
      "Stochastic Gradient Descent(46783): loss=5.073833818442591\n",
      "Stochastic Gradient Descent(46784): loss=0.6955353894074914\n",
      "Stochastic Gradient Descent(46785): loss=4.317498232476726\n",
      "Stochastic Gradient Descent(46786): loss=0.02537739564143685\n",
      "Stochastic Gradient Descent(46787): loss=2.907489158381274\n",
      "Stochastic Gradient Descent(46788): loss=0.00013841056506261475\n",
      "Stochastic Gradient Descent(46789): loss=0.8058360404401234\n",
      "Stochastic Gradient Descent(46790): loss=0.29052562686307576\n",
      "Stochastic Gradient Descent(46791): loss=0.04162214278248808\n",
      "Stochastic Gradient Descent(46792): loss=7.950907281891569\n",
      "Stochastic Gradient Descent(46793): loss=0.0016849424405677546\n",
      "Stochastic Gradient Descent(46794): loss=0.5343531242716728\n",
      "Stochastic Gradient Descent(46795): loss=1.0135754901728047\n",
      "Stochastic Gradient Descent(46796): loss=12.439898638825689\n",
      "Stochastic Gradient Descent(46797): loss=11.664905838608334\n",
      "Stochastic Gradient Descent(46798): loss=7.826988179610085\n",
      "Stochastic Gradient Descent(46799): loss=0.7771526785121402\n",
      "Stochastic Gradient Descent(46800): loss=0.68110699852707\n",
      "Stochastic Gradient Descent(46801): loss=5.864039660582196\n",
      "Stochastic Gradient Descent(46802): loss=0.8414372832322595\n",
      "Stochastic Gradient Descent(46803): loss=0.3743331308403837\n",
      "Stochastic Gradient Descent(46804): loss=0.00011882202795766118\n",
      "Stochastic Gradient Descent(46805): loss=0.4219508151183623\n",
      "Stochastic Gradient Descent(46806): loss=0.03627398876174903\n",
      "Stochastic Gradient Descent(46807): loss=2.32817136729605\n",
      "Stochastic Gradient Descent(46808): loss=0.09682964173619728\n",
      "Stochastic Gradient Descent(46809): loss=6.716875878760241\n",
      "Stochastic Gradient Descent(46810): loss=0.6869853510542412\n",
      "Stochastic Gradient Descent(46811): loss=3.5361509576878882\n",
      "Stochastic Gradient Descent(46812): loss=0.2376424908325968\n",
      "Stochastic Gradient Descent(46813): loss=13.138702942956739\n",
      "Stochastic Gradient Descent(46814): loss=0.7684955290734989\n",
      "Stochastic Gradient Descent(46815): loss=5.889075277565481\n",
      "Stochastic Gradient Descent(46816): loss=2.733421096598574\n",
      "Stochastic Gradient Descent(46817): loss=5.516083355100531\n",
      "Stochastic Gradient Descent(46818): loss=3.58653196233413\n",
      "Stochastic Gradient Descent(46819): loss=3.8774833793849885\n",
      "Stochastic Gradient Descent(46820): loss=5.283238084002614\n",
      "Stochastic Gradient Descent(46821): loss=7.942991789157049\n",
      "Stochastic Gradient Descent(46822): loss=7.759908376881789\n",
      "Stochastic Gradient Descent(46823): loss=20.532978064790555\n",
      "Stochastic Gradient Descent(46824): loss=1.1402055749228022\n",
      "Stochastic Gradient Descent(46825): loss=2.5524484165305514\n",
      "Stochastic Gradient Descent(46826): loss=5.8256909773253165\n",
      "Stochastic Gradient Descent(46827): loss=0.13059071211135637\n",
      "Stochastic Gradient Descent(46828): loss=16.237477165234903\n",
      "Stochastic Gradient Descent(46829): loss=0.0761998079627008\n",
      "Stochastic Gradient Descent(46830): loss=0.7413893591413523\n",
      "Stochastic Gradient Descent(46831): loss=29.719122623294027\n",
      "Stochastic Gradient Descent(46832): loss=2.8364656928234955\n",
      "Stochastic Gradient Descent(46833): loss=1.3832091250674983\n",
      "Stochastic Gradient Descent(46834): loss=12.898791899788577\n",
      "Stochastic Gradient Descent(46835): loss=3.973654972465563\n",
      "Stochastic Gradient Descent(46836): loss=8.214541850185348\n",
      "Stochastic Gradient Descent(46837): loss=0.7549287900315598\n",
      "Stochastic Gradient Descent(46838): loss=13.43376228701582\n",
      "Stochastic Gradient Descent(46839): loss=1.749298383317796\n",
      "Stochastic Gradient Descent(46840): loss=1.6013877148855147\n",
      "Stochastic Gradient Descent(46841): loss=10.664665195870995\n",
      "Stochastic Gradient Descent(46842): loss=0.05224146848638346\n",
      "Stochastic Gradient Descent(46843): loss=1.2384848460859033\n",
      "Stochastic Gradient Descent(46844): loss=0.525206081711535\n",
      "Stochastic Gradient Descent(46845): loss=0.16069631713634544\n",
      "Stochastic Gradient Descent(46846): loss=3.0162158733531435\n",
      "Stochastic Gradient Descent(46847): loss=2.428580968527864\n",
      "Stochastic Gradient Descent(46848): loss=2.5097055698331427\n",
      "Stochastic Gradient Descent(46849): loss=5.617296312291743\n",
      "Stochastic Gradient Descent(46850): loss=4.149967389488243\n",
      "Stochastic Gradient Descent(46851): loss=10.25516721300381\n",
      "Stochastic Gradient Descent(46852): loss=0.0021592334119092996\n",
      "Stochastic Gradient Descent(46853): loss=2.6767155414238024\n",
      "Stochastic Gradient Descent(46854): loss=13.593571746699357\n",
      "Stochastic Gradient Descent(46855): loss=6.754691333567841\n",
      "Stochastic Gradient Descent(46856): loss=0.6078631866898825\n",
      "Stochastic Gradient Descent(46857): loss=0.12997411076340668\n",
      "Stochastic Gradient Descent(46858): loss=5.949862287200673\n",
      "Stochastic Gradient Descent(46859): loss=5.801145691666472\n",
      "Stochastic Gradient Descent(46860): loss=0.26608443337082405\n",
      "Stochastic Gradient Descent(46861): loss=5.329888848445267\n",
      "Stochastic Gradient Descent(46862): loss=1.651174506084575\n",
      "Stochastic Gradient Descent(46863): loss=19.518071663993332\n",
      "Stochastic Gradient Descent(46864): loss=0.00033468888135795583\n",
      "Stochastic Gradient Descent(46865): loss=25.89305350756259\n",
      "Stochastic Gradient Descent(46866): loss=4.068198104674246\n",
      "Stochastic Gradient Descent(46867): loss=7.426141003713898\n",
      "Stochastic Gradient Descent(46868): loss=0.512684085841598\n",
      "Stochastic Gradient Descent(46869): loss=0.1422205264998626\n",
      "Stochastic Gradient Descent(46870): loss=0.02960125789168808\n",
      "Stochastic Gradient Descent(46871): loss=0.9844523979794811\n",
      "Stochastic Gradient Descent(46872): loss=0.21866524267979628\n",
      "Stochastic Gradient Descent(46873): loss=0.6609192005314974\n",
      "Stochastic Gradient Descent(46874): loss=10.5536072441142\n",
      "Stochastic Gradient Descent(46875): loss=0.3110438989622114\n",
      "Stochastic Gradient Descent(46876): loss=1.5275663557144734\n",
      "Stochastic Gradient Descent(46877): loss=0.5994234324581661\n",
      "Stochastic Gradient Descent(46878): loss=0.18321235215566012\n",
      "Stochastic Gradient Descent(46879): loss=0.0836237142688092\n",
      "Stochastic Gradient Descent(46880): loss=2.854034863437494\n",
      "Stochastic Gradient Descent(46881): loss=9.261402944229072\n",
      "Stochastic Gradient Descent(46882): loss=1.2618862991926751\n",
      "Stochastic Gradient Descent(46883): loss=5.72640024832125\n",
      "Stochastic Gradient Descent(46884): loss=0.6737135220508126\n",
      "Stochastic Gradient Descent(46885): loss=8.472597981061964\n",
      "Stochastic Gradient Descent(46886): loss=7.709500323937523\n",
      "Stochastic Gradient Descent(46887): loss=0.03028601778059036\n",
      "Stochastic Gradient Descent(46888): loss=0.24693086381176738\n",
      "Stochastic Gradient Descent(46889): loss=3.2173713011535106\n",
      "Stochastic Gradient Descent(46890): loss=0.12995523914796797\n",
      "Stochastic Gradient Descent(46891): loss=0.4672512541624211\n",
      "Stochastic Gradient Descent(46892): loss=5.281071194718947\n",
      "Stochastic Gradient Descent(46893): loss=1.6556839564428656\n",
      "Stochastic Gradient Descent(46894): loss=4.765875948734519\n",
      "Stochastic Gradient Descent(46895): loss=16.369670458782863\n",
      "Stochastic Gradient Descent(46896): loss=3.166300888307632\n",
      "Stochastic Gradient Descent(46897): loss=0.109940150407352\n",
      "Stochastic Gradient Descent(46898): loss=1.655357504124359\n",
      "Stochastic Gradient Descent(46899): loss=7.631959438790585\n",
      "Stochastic Gradient Descent(46900): loss=6.319305099126473\n",
      "Stochastic Gradient Descent(46901): loss=2.600029452831828\n",
      "Stochastic Gradient Descent(46902): loss=2.430782511243161\n",
      "Stochastic Gradient Descent(46903): loss=0.06607937887492497\n",
      "Stochastic Gradient Descent(46904): loss=9.371380295587155\n",
      "Stochastic Gradient Descent(46905): loss=0.16574040943397442\n",
      "Stochastic Gradient Descent(46906): loss=4.134397465075988\n",
      "Stochastic Gradient Descent(46907): loss=0.4101365994321682\n",
      "Stochastic Gradient Descent(46908): loss=0.0009132265687400529\n",
      "Stochastic Gradient Descent(46909): loss=6.250946765187895\n",
      "Stochastic Gradient Descent(46910): loss=1.602265687703556\n",
      "Stochastic Gradient Descent(46911): loss=0.865424563424134\n",
      "Stochastic Gradient Descent(46912): loss=5.253276712538783\n",
      "Stochastic Gradient Descent(46913): loss=3.5605118031763916\n",
      "Stochastic Gradient Descent(46914): loss=1.8329859205157635\n",
      "Stochastic Gradient Descent(46915): loss=0.6376828331195952\n",
      "Stochastic Gradient Descent(46916): loss=1.0449609051279327\n",
      "Stochastic Gradient Descent(46917): loss=14.073020754344414\n",
      "Stochastic Gradient Descent(46918): loss=1.665755959340465\n",
      "Stochastic Gradient Descent(46919): loss=40.165598215844895\n",
      "Stochastic Gradient Descent(46920): loss=22.977698007808225\n",
      "Stochastic Gradient Descent(46921): loss=6.49811794726084\n",
      "Stochastic Gradient Descent(46922): loss=55.509781918543084\n",
      "Stochastic Gradient Descent(46923): loss=2.0219929377251304\n",
      "Stochastic Gradient Descent(46924): loss=0.0006537365260834505\n",
      "Stochastic Gradient Descent(46925): loss=0.14671019573595176\n",
      "Stochastic Gradient Descent(46926): loss=2.3710639265618716\n",
      "Stochastic Gradient Descent(46927): loss=0.22666150371129623\n",
      "Stochastic Gradient Descent(46928): loss=0.06133382148076272\n",
      "Stochastic Gradient Descent(46929): loss=3.287182850470788\n",
      "Stochastic Gradient Descent(46930): loss=0.062240714528477695\n",
      "Stochastic Gradient Descent(46931): loss=1.8897380221893914\n",
      "Stochastic Gradient Descent(46932): loss=0.039062902293347204\n",
      "Stochastic Gradient Descent(46933): loss=0.9117951086853336\n",
      "Stochastic Gradient Descent(46934): loss=2.9767404981517678\n",
      "Stochastic Gradient Descent(46935): loss=0.09767732450543763\n",
      "Stochastic Gradient Descent(46936): loss=10.11393051882298\n",
      "Stochastic Gradient Descent(46937): loss=0.0008531218163571462\n",
      "Stochastic Gradient Descent(46938): loss=7.119462758521042\n",
      "Stochastic Gradient Descent(46939): loss=0.9471568477700407\n",
      "Stochastic Gradient Descent(46940): loss=7.8579603660045\n",
      "Stochastic Gradient Descent(46941): loss=2.1626900516488785\n",
      "Stochastic Gradient Descent(46942): loss=0.07673654511300056\n",
      "Stochastic Gradient Descent(46943): loss=0.897331252672243\n",
      "Stochastic Gradient Descent(46944): loss=0.8258685319004602\n",
      "Stochastic Gradient Descent(46945): loss=1.694582625447139\n",
      "Stochastic Gradient Descent(46946): loss=1.975569065341771\n",
      "Stochastic Gradient Descent(46947): loss=0.008903250996433614\n",
      "Stochastic Gradient Descent(46948): loss=4.322321038298968\n",
      "Stochastic Gradient Descent(46949): loss=1.0676301679929672\n",
      "Stochastic Gradient Descent(46950): loss=3.232823980132665\n",
      "Stochastic Gradient Descent(46951): loss=0.007025309490831224\n",
      "Stochastic Gradient Descent(46952): loss=0.945388231537026\n",
      "Stochastic Gradient Descent(46953): loss=2.5903793136089104\n",
      "Stochastic Gradient Descent(46954): loss=3.5177635125392035\n",
      "Stochastic Gradient Descent(46955): loss=7.533186092557005\n",
      "Stochastic Gradient Descent(46956): loss=10.239344251932922\n",
      "Stochastic Gradient Descent(46957): loss=0.36424306953088226\n",
      "Stochastic Gradient Descent(46958): loss=6.677555020617389\n",
      "Stochastic Gradient Descent(46959): loss=1.5878988055260528\n",
      "Stochastic Gradient Descent(46960): loss=11.476185194219658\n",
      "Stochastic Gradient Descent(46961): loss=0.0018791617470613148\n",
      "Stochastic Gradient Descent(46962): loss=3.1079193640477434\n",
      "Stochastic Gradient Descent(46963): loss=0.06279255034689862\n",
      "Stochastic Gradient Descent(46964): loss=1.2357190075158186\n",
      "Stochastic Gradient Descent(46965): loss=0.1257827092524816\n",
      "Stochastic Gradient Descent(46966): loss=0.04493982592713406\n",
      "Stochastic Gradient Descent(46967): loss=4.69946118560271\n",
      "Stochastic Gradient Descent(46968): loss=5.739966845702443\n",
      "Stochastic Gradient Descent(46969): loss=4.042236298252568\n",
      "Stochastic Gradient Descent(46970): loss=3.792766054677023\n",
      "Stochastic Gradient Descent(46971): loss=0.07527139123886889\n",
      "Stochastic Gradient Descent(46972): loss=0.00816573757202908\n",
      "Stochastic Gradient Descent(46973): loss=0.4940753674216221\n",
      "Stochastic Gradient Descent(46974): loss=0.36366465430405304\n",
      "Stochastic Gradient Descent(46975): loss=1.1614895728607817e-05\n",
      "Stochastic Gradient Descent(46976): loss=2.60201686570335\n",
      "Stochastic Gradient Descent(46977): loss=0.19024052728085386\n",
      "Stochastic Gradient Descent(46978): loss=7.586210481804258\n",
      "Stochastic Gradient Descent(46979): loss=6.7264974831047555\n",
      "Stochastic Gradient Descent(46980): loss=3.916815773732003\n",
      "Stochastic Gradient Descent(46981): loss=1.8449756409114664e-05\n",
      "Stochastic Gradient Descent(46982): loss=4.092145450413803\n",
      "Stochastic Gradient Descent(46983): loss=2.466063991056034\n",
      "Stochastic Gradient Descent(46984): loss=2.11316821067512\n",
      "Stochastic Gradient Descent(46985): loss=0.4911797893763596\n",
      "Stochastic Gradient Descent(46986): loss=0.02354650965506342\n",
      "Stochastic Gradient Descent(46987): loss=0.8569178144777114\n",
      "Stochastic Gradient Descent(46988): loss=2.6232373476862674\n",
      "Stochastic Gradient Descent(46989): loss=0.08771684455147577\n",
      "Stochastic Gradient Descent(46990): loss=3.3177396868178355\n",
      "Stochastic Gradient Descent(46991): loss=0.003728071624754098\n",
      "Stochastic Gradient Descent(46992): loss=0.5186357875324451\n",
      "Stochastic Gradient Descent(46993): loss=6.064707303722968\n",
      "Stochastic Gradient Descent(46994): loss=0.028892792983366073\n",
      "Stochastic Gradient Descent(46995): loss=8.065278544070127\n",
      "Stochastic Gradient Descent(46996): loss=0.5006274673676183\n",
      "Stochastic Gradient Descent(46997): loss=17.25001646184908\n",
      "Stochastic Gradient Descent(46998): loss=10.28381401916279\n",
      "Stochastic Gradient Descent(46999): loss=15.66905929389861\n",
      "Stochastic Gradient Descent(47000): loss=8.254146086722509\n",
      "Stochastic Gradient Descent(47001): loss=3.8648121427634448\n",
      "Stochastic Gradient Descent(47002): loss=1.0804199310719904\n",
      "Stochastic Gradient Descent(47003): loss=1.9261157707183143\n",
      "Stochastic Gradient Descent(47004): loss=15.476254171656148\n",
      "Stochastic Gradient Descent(47005): loss=0.2725115845676807\n",
      "Stochastic Gradient Descent(47006): loss=22.96773809466616\n",
      "Stochastic Gradient Descent(47007): loss=1.464930522129333\n",
      "Stochastic Gradient Descent(47008): loss=4.141560961587986\n",
      "Stochastic Gradient Descent(47009): loss=0.5282118586035623\n",
      "Stochastic Gradient Descent(47010): loss=3.93815851787309\n",
      "Stochastic Gradient Descent(47011): loss=3.903394716624645\n",
      "Stochastic Gradient Descent(47012): loss=8.45734776318794\n",
      "Stochastic Gradient Descent(47013): loss=5.015120449974845\n",
      "Stochastic Gradient Descent(47014): loss=10.010621033292992\n",
      "Stochastic Gradient Descent(47015): loss=3.561084704754469\n",
      "Stochastic Gradient Descent(47016): loss=11.876760524789384\n",
      "Stochastic Gradient Descent(47017): loss=2.0065180203001165\n",
      "Stochastic Gradient Descent(47018): loss=0.020047706422756013\n",
      "Stochastic Gradient Descent(47019): loss=16.24419448316873\n",
      "Stochastic Gradient Descent(47020): loss=1.6731178045336574\n",
      "Stochastic Gradient Descent(47021): loss=0.10143937352995486\n",
      "Stochastic Gradient Descent(47022): loss=11.042556086592176\n",
      "Stochastic Gradient Descent(47023): loss=2.872000832770238\n",
      "Stochastic Gradient Descent(47024): loss=0.5170416916828087\n",
      "Stochastic Gradient Descent(47025): loss=6.8190436118788345\n",
      "Stochastic Gradient Descent(47026): loss=5.012882560983479\n",
      "Stochastic Gradient Descent(47027): loss=3.0503927641469524\n",
      "Stochastic Gradient Descent(47028): loss=5.666329075507693\n",
      "Stochastic Gradient Descent(47029): loss=0.6993496411761083\n",
      "Stochastic Gradient Descent(47030): loss=24.72753688803507\n",
      "Stochastic Gradient Descent(47031): loss=0.015387548801530409\n",
      "Stochastic Gradient Descent(47032): loss=0.6726117660954919\n",
      "Stochastic Gradient Descent(47033): loss=0.0034012549570505398\n",
      "Stochastic Gradient Descent(47034): loss=0.0026945231759838775\n",
      "Stochastic Gradient Descent(47035): loss=2.273485032863244\n",
      "Stochastic Gradient Descent(47036): loss=3.2060276815545077\n",
      "Stochastic Gradient Descent(47037): loss=0.006493188179247334\n",
      "Stochastic Gradient Descent(47038): loss=0.1021712955500084\n",
      "Stochastic Gradient Descent(47039): loss=0.02278007198144678\n",
      "Stochastic Gradient Descent(47040): loss=1.847986132464312\n",
      "Stochastic Gradient Descent(47041): loss=3.8990665621560208\n",
      "Stochastic Gradient Descent(47042): loss=3.1420392719873047\n",
      "Stochastic Gradient Descent(47043): loss=53.47208453648034\n",
      "Stochastic Gradient Descent(47044): loss=0.5331275238427611\n",
      "Stochastic Gradient Descent(47045): loss=25.42820108860713\n",
      "Stochastic Gradient Descent(47046): loss=0.959025896201667\n",
      "Stochastic Gradient Descent(47047): loss=19.705128608532753\n",
      "Stochastic Gradient Descent(47048): loss=5.3191966752620665\n",
      "Stochastic Gradient Descent(47049): loss=6.949046187480409\n",
      "Stochastic Gradient Descent(47050): loss=0.6834313305773105\n",
      "Stochastic Gradient Descent(47051): loss=0.28627423308983363\n",
      "Stochastic Gradient Descent(47052): loss=3.663923639577538\n",
      "Stochastic Gradient Descent(47053): loss=0.7280508829809462\n",
      "Stochastic Gradient Descent(47054): loss=8.98597654186924\n",
      "Stochastic Gradient Descent(47055): loss=8.038202936987155\n",
      "Stochastic Gradient Descent(47056): loss=1.8118186771805191\n",
      "Stochastic Gradient Descent(47057): loss=1.775889348029014\n",
      "Stochastic Gradient Descent(47058): loss=3.951428978122044\n",
      "Stochastic Gradient Descent(47059): loss=0.06401513877021843\n",
      "Stochastic Gradient Descent(47060): loss=4.6167726699746\n",
      "Stochastic Gradient Descent(47061): loss=0.9377606123264611\n",
      "Stochastic Gradient Descent(47062): loss=8.266232238217071\n",
      "Stochastic Gradient Descent(47063): loss=16.632193468236476\n",
      "Stochastic Gradient Descent(47064): loss=1.9399768133258868\n",
      "Stochastic Gradient Descent(47065): loss=12.947038345855425\n",
      "Stochastic Gradient Descent(47066): loss=8.260132220354098\n",
      "Stochastic Gradient Descent(47067): loss=11.962441377036788\n",
      "Stochastic Gradient Descent(47068): loss=0.6523844546374075\n",
      "Stochastic Gradient Descent(47069): loss=9.052492404040002\n",
      "Stochastic Gradient Descent(47070): loss=8.965379127495554\n",
      "Stochastic Gradient Descent(47071): loss=2.5614618096633945\n",
      "Stochastic Gradient Descent(47072): loss=1.2298935079206303\n",
      "Stochastic Gradient Descent(47073): loss=1.52755526822273\n",
      "Stochastic Gradient Descent(47074): loss=0.6929130459427932\n",
      "Stochastic Gradient Descent(47075): loss=0.28458321252594754\n",
      "Stochastic Gradient Descent(47076): loss=0.09501669945117915\n",
      "Stochastic Gradient Descent(47077): loss=0.3786958505299776\n",
      "Stochastic Gradient Descent(47078): loss=12.089033530886745\n",
      "Stochastic Gradient Descent(47079): loss=5.970816626282778\n",
      "Stochastic Gradient Descent(47080): loss=3.6150506085679845\n",
      "Stochastic Gradient Descent(47081): loss=0.7735832225226653\n",
      "Stochastic Gradient Descent(47082): loss=3.936126873253713\n",
      "Stochastic Gradient Descent(47083): loss=0.7784644460535548\n",
      "Stochastic Gradient Descent(47084): loss=0.49498676378150713\n",
      "Stochastic Gradient Descent(47085): loss=6.349799112029435\n",
      "Stochastic Gradient Descent(47086): loss=5.705741810027754\n",
      "Stochastic Gradient Descent(47087): loss=4.918672607183613\n",
      "Stochastic Gradient Descent(47088): loss=0.2792940677059327\n",
      "Stochastic Gradient Descent(47089): loss=4.902208364232189\n",
      "Stochastic Gradient Descent(47090): loss=0.799954864948536\n",
      "Stochastic Gradient Descent(47091): loss=0.6092242950966431\n",
      "Stochastic Gradient Descent(47092): loss=1.0046061742610326\n",
      "Stochastic Gradient Descent(47093): loss=7.8169466622574335\n",
      "Stochastic Gradient Descent(47094): loss=0.5679053044533235\n",
      "Stochastic Gradient Descent(47095): loss=0.006778147360642111\n",
      "Stochastic Gradient Descent(47096): loss=5.569586976251459\n",
      "Stochastic Gradient Descent(47097): loss=0.23363015918606483\n",
      "Stochastic Gradient Descent(47098): loss=1.5794757859692936\n",
      "Stochastic Gradient Descent(47099): loss=0.3619524797792109\n",
      "Stochastic Gradient Descent(47100): loss=1.7614933150590353\n",
      "Stochastic Gradient Descent(47101): loss=11.395226503188026\n",
      "Stochastic Gradient Descent(47102): loss=0.30110937910299873\n",
      "Stochastic Gradient Descent(47103): loss=2.4855035935609378\n",
      "Stochastic Gradient Descent(47104): loss=1.084962634778547\n",
      "Stochastic Gradient Descent(47105): loss=0.8230447522938384\n",
      "Stochastic Gradient Descent(47106): loss=2.382166756932355\n",
      "Stochastic Gradient Descent(47107): loss=0.07316031677422252\n",
      "Stochastic Gradient Descent(47108): loss=1.8986326831607263\n",
      "Stochastic Gradient Descent(47109): loss=1.5435327269376409\n",
      "Stochastic Gradient Descent(47110): loss=1.2883144819146364\n",
      "Stochastic Gradient Descent(47111): loss=0.00034090790649656503\n",
      "Stochastic Gradient Descent(47112): loss=0.4265884239758462\n",
      "Stochastic Gradient Descent(47113): loss=0.6910525536246046\n",
      "Stochastic Gradient Descent(47114): loss=3.4602725302850352\n",
      "Stochastic Gradient Descent(47115): loss=23.7646490055188\n",
      "Stochastic Gradient Descent(47116): loss=0.0026800993726004405\n",
      "Stochastic Gradient Descent(47117): loss=1.2983059658451535\n",
      "Stochastic Gradient Descent(47118): loss=1.180167170047264\n",
      "Stochastic Gradient Descent(47119): loss=0.7877161386619345\n",
      "Stochastic Gradient Descent(47120): loss=1.7083317507188724\n",
      "Stochastic Gradient Descent(47121): loss=4.033573971043765\n",
      "Stochastic Gradient Descent(47122): loss=3.268894442577579\n",
      "Stochastic Gradient Descent(47123): loss=0.483234648836551\n",
      "Stochastic Gradient Descent(47124): loss=0.0605703821350036\n",
      "Stochastic Gradient Descent(47125): loss=0.13619513288515012\n",
      "Stochastic Gradient Descent(47126): loss=1.0092282136261468\n",
      "Stochastic Gradient Descent(47127): loss=1.4645143188535314\n",
      "Stochastic Gradient Descent(47128): loss=22.98579201123587\n",
      "Stochastic Gradient Descent(47129): loss=1.9124131700495886\n",
      "Stochastic Gradient Descent(47130): loss=2.707676528034894\n",
      "Stochastic Gradient Descent(47131): loss=26.326299608600568\n",
      "Stochastic Gradient Descent(47132): loss=0.00888658973801841\n",
      "Stochastic Gradient Descent(47133): loss=4.526766854945421\n",
      "Stochastic Gradient Descent(47134): loss=2.7875250881470297\n",
      "Stochastic Gradient Descent(47135): loss=7.7026380980184115\n",
      "Stochastic Gradient Descent(47136): loss=0.0014366672196766289\n",
      "Stochastic Gradient Descent(47137): loss=0.52360765420728\n",
      "Stochastic Gradient Descent(47138): loss=1.9804855800760925\n",
      "Stochastic Gradient Descent(47139): loss=5.224431311834041\n",
      "Stochastic Gradient Descent(47140): loss=37.99922478195505\n",
      "Stochastic Gradient Descent(47141): loss=3.965484761704004\n",
      "Stochastic Gradient Descent(47142): loss=0.402676924522821\n",
      "Stochastic Gradient Descent(47143): loss=5.7529246290938945\n",
      "Stochastic Gradient Descent(47144): loss=9.719827847012674\n",
      "Stochastic Gradient Descent(47145): loss=3.712726391356482\n",
      "Stochastic Gradient Descent(47146): loss=0.03722390599806289\n",
      "Stochastic Gradient Descent(47147): loss=0.6796222941475798\n",
      "Stochastic Gradient Descent(47148): loss=0.951125877958842\n",
      "Stochastic Gradient Descent(47149): loss=0.6156236369898037\n",
      "Stochastic Gradient Descent(47150): loss=2.070340581562298\n",
      "Stochastic Gradient Descent(47151): loss=0.3831413810705687\n",
      "Stochastic Gradient Descent(47152): loss=3.3996983456857657\n",
      "Stochastic Gradient Descent(47153): loss=0.29015752697482355\n",
      "Stochastic Gradient Descent(47154): loss=0.07690883746074426\n",
      "Stochastic Gradient Descent(47155): loss=5.210968231957697\n",
      "Stochastic Gradient Descent(47156): loss=8.645539880063716\n",
      "Stochastic Gradient Descent(47157): loss=0.40055924574680274\n",
      "Stochastic Gradient Descent(47158): loss=18.565721760730995\n",
      "Stochastic Gradient Descent(47159): loss=9.799321751825559\n",
      "Stochastic Gradient Descent(47160): loss=36.33526786832669\n",
      "Stochastic Gradient Descent(47161): loss=1.316804754577896\n",
      "Stochastic Gradient Descent(47162): loss=20.338699546096123\n",
      "Stochastic Gradient Descent(47163): loss=3.194454697354113\n",
      "Stochastic Gradient Descent(47164): loss=4.232804433436263\n",
      "Stochastic Gradient Descent(47165): loss=0.46116113165010547\n",
      "Stochastic Gradient Descent(47166): loss=0.09256116437330897\n",
      "Stochastic Gradient Descent(47167): loss=11.837914675292081\n",
      "Stochastic Gradient Descent(47168): loss=10.434600820810905\n",
      "Stochastic Gradient Descent(47169): loss=5.050451571574876\n",
      "Stochastic Gradient Descent(47170): loss=1.1808608969193006\n",
      "Stochastic Gradient Descent(47171): loss=0.04883458757066693\n",
      "Stochastic Gradient Descent(47172): loss=3.3769062072346854\n",
      "Stochastic Gradient Descent(47173): loss=1.0090070672159368\n",
      "Stochastic Gradient Descent(47174): loss=12.43564595232117\n",
      "Stochastic Gradient Descent(47175): loss=2.0944586348900156\n",
      "Stochastic Gradient Descent(47176): loss=6.757693318685614\n",
      "Stochastic Gradient Descent(47177): loss=0.11114468678090537\n",
      "Stochastic Gradient Descent(47178): loss=8.64621040537721\n",
      "Stochastic Gradient Descent(47179): loss=0.8996629507042782\n",
      "Stochastic Gradient Descent(47180): loss=3.2436477566718183\n",
      "Stochastic Gradient Descent(47181): loss=0.816147059932274\n",
      "Stochastic Gradient Descent(47182): loss=11.918597202035842\n",
      "Stochastic Gradient Descent(47183): loss=0.33476802345240453\n",
      "Stochastic Gradient Descent(47184): loss=4.654956612801538\n",
      "Stochastic Gradient Descent(47185): loss=0.09563589032763051\n",
      "Stochastic Gradient Descent(47186): loss=10.74450130224798\n",
      "Stochastic Gradient Descent(47187): loss=1.0673640858264672\n",
      "Stochastic Gradient Descent(47188): loss=0.8917206438593628\n",
      "Stochastic Gradient Descent(47189): loss=1.5827485121732507\n",
      "Stochastic Gradient Descent(47190): loss=1.6588666215150667\n",
      "Stochastic Gradient Descent(47191): loss=0.1380899678858898\n",
      "Stochastic Gradient Descent(47192): loss=1.2135380155723168\n",
      "Stochastic Gradient Descent(47193): loss=0.5155651749888616\n",
      "Stochastic Gradient Descent(47194): loss=2.2576316439008446\n",
      "Stochastic Gradient Descent(47195): loss=5.252595044995295\n",
      "Stochastic Gradient Descent(47196): loss=21.30199046762368\n",
      "Stochastic Gradient Descent(47197): loss=2.4548587682956233\n",
      "Stochastic Gradient Descent(47198): loss=2.9427161541475186\n",
      "Stochastic Gradient Descent(47199): loss=1.6747192635894343\n",
      "Stochastic Gradient Descent(47200): loss=3.6175381197488523\n",
      "Stochastic Gradient Descent(47201): loss=4.970310020058136\n",
      "Stochastic Gradient Descent(47202): loss=3.0388380922832856\n",
      "Stochastic Gradient Descent(47203): loss=0.5285915495195822\n",
      "Stochastic Gradient Descent(47204): loss=4.2125494982467675\n",
      "Stochastic Gradient Descent(47205): loss=0.6745706028978287\n",
      "Stochastic Gradient Descent(47206): loss=0.023084403647815603\n",
      "Stochastic Gradient Descent(47207): loss=11.12057469127533\n",
      "Stochastic Gradient Descent(47208): loss=0.5786635183900221\n",
      "Stochastic Gradient Descent(47209): loss=2.991050300482223\n",
      "Stochastic Gradient Descent(47210): loss=3.246988181277201\n",
      "Stochastic Gradient Descent(47211): loss=0.004711381983787493\n",
      "Stochastic Gradient Descent(47212): loss=3.8811396728693035\n",
      "Stochastic Gradient Descent(47213): loss=2.0375444413000015\n",
      "Stochastic Gradient Descent(47214): loss=2.344444608157446\n",
      "Stochastic Gradient Descent(47215): loss=0.8278068624235163\n",
      "Stochastic Gradient Descent(47216): loss=1.8956673992538333\n",
      "Stochastic Gradient Descent(47217): loss=2.9169524249260084\n",
      "Stochastic Gradient Descent(47218): loss=1.5800174561683764\n",
      "Stochastic Gradient Descent(47219): loss=0.10675104077522785\n",
      "Stochastic Gradient Descent(47220): loss=0.3975572667850392\n",
      "Stochastic Gradient Descent(47221): loss=0.693010507892545\n",
      "Stochastic Gradient Descent(47222): loss=0.0004215004902014769\n",
      "Stochastic Gradient Descent(47223): loss=0.436229286205068\n",
      "Stochastic Gradient Descent(47224): loss=0.6300894568361332\n",
      "Stochastic Gradient Descent(47225): loss=2.356978643607842\n",
      "Stochastic Gradient Descent(47226): loss=3.6682476267987236\n",
      "Stochastic Gradient Descent(47227): loss=0.34153456231416884\n",
      "Stochastic Gradient Descent(47228): loss=0.24778663908450363\n",
      "Stochastic Gradient Descent(47229): loss=1.1998049999678082\n",
      "Stochastic Gradient Descent(47230): loss=0.22789388382492876\n",
      "Stochastic Gradient Descent(47231): loss=0.7198976737271647\n",
      "Stochastic Gradient Descent(47232): loss=0.4466671277490027\n",
      "Stochastic Gradient Descent(47233): loss=11.508146000889864\n",
      "Stochastic Gradient Descent(47234): loss=0.2530762429658748\n",
      "Stochastic Gradient Descent(47235): loss=2.9536613526211135\n",
      "Stochastic Gradient Descent(47236): loss=5.064239949075502\n",
      "Stochastic Gradient Descent(47237): loss=34.740933372298834\n",
      "Stochastic Gradient Descent(47238): loss=56.861507673192676\n",
      "Stochastic Gradient Descent(47239): loss=7.461043530701993\n",
      "Stochastic Gradient Descent(47240): loss=0.7723169195436188\n",
      "Stochastic Gradient Descent(47241): loss=0.010850158855626254\n",
      "Stochastic Gradient Descent(47242): loss=3.6739401308294144\n",
      "Stochastic Gradient Descent(47243): loss=1.189482873023147\n",
      "Stochastic Gradient Descent(47244): loss=0.551579476447073\n",
      "Stochastic Gradient Descent(47245): loss=3.5194897147506325\n",
      "Stochastic Gradient Descent(47246): loss=3.2019651408490972\n",
      "Stochastic Gradient Descent(47247): loss=1.6801419256196892\n",
      "Stochastic Gradient Descent(47248): loss=3.5361285700797804\n",
      "Stochastic Gradient Descent(47249): loss=1.3781069428210797\n",
      "Stochastic Gradient Descent(47250): loss=5.488652441012365\n",
      "Stochastic Gradient Descent(47251): loss=0.03151401503748253\n",
      "Stochastic Gradient Descent(47252): loss=1.8729164717497373\n",
      "Stochastic Gradient Descent(47253): loss=0.06359640196331369\n",
      "Stochastic Gradient Descent(47254): loss=2.7460331691836184\n",
      "Stochastic Gradient Descent(47255): loss=1.5217441201605493\n",
      "Stochastic Gradient Descent(47256): loss=2.3842603460064873\n",
      "Stochastic Gradient Descent(47257): loss=0.20770492717921196\n",
      "Stochastic Gradient Descent(47258): loss=22.979047786664257\n",
      "Stochastic Gradient Descent(47259): loss=4.914761383430113\n",
      "Stochastic Gradient Descent(47260): loss=9.952489051079873\n",
      "Stochastic Gradient Descent(47261): loss=6.145246402842575\n",
      "Stochastic Gradient Descent(47262): loss=3.029954218165127\n",
      "Stochastic Gradient Descent(47263): loss=2.6976589879950392\n",
      "Stochastic Gradient Descent(47264): loss=0.2999001202020242\n",
      "Stochastic Gradient Descent(47265): loss=5.038311399591667\n",
      "Stochastic Gradient Descent(47266): loss=0.04819769045510961\n",
      "Stochastic Gradient Descent(47267): loss=0.08859794400921263\n",
      "Stochastic Gradient Descent(47268): loss=1.658514151932695\n",
      "Stochastic Gradient Descent(47269): loss=3.5177999364403734\n",
      "Stochastic Gradient Descent(47270): loss=1.4748369545629694\n",
      "Stochastic Gradient Descent(47271): loss=0.006125323063312181\n",
      "Stochastic Gradient Descent(47272): loss=0.4913033259691556\n",
      "Stochastic Gradient Descent(47273): loss=0.002923871760346647\n",
      "Stochastic Gradient Descent(47274): loss=0.0009880406290046918\n",
      "Stochastic Gradient Descent(47275): loss=15.882724795570702\n",
      "Stochastic Gradient Descent(47276): loss=0.22448211552547395\n",
      "Stochastic Gradient Descent(47277): loss=5.220092468225197\n",
      "Stochastic Gradient Descent(47278): loss=3.6059510242495616\n",
      "Stochastic Gradient Descent(47279): loss=2.885663223443348\n",
      "Stochastic Gradient Descent(47280): loss=13.211379134507649\n",
      "Stochastic Gradient Descent(47281): loss=0.5584390332630991\n",
      "Stochastic Gradient Descent(47282): loss=12.536087722206663\n",
      "Stochastic Gradient Descent(47283): loss=0.0025850981441480256\n",
      "Stochastic Gradient Descent(47284): loss=4.327140754408224\n",
      "Stochastic Gradient Descent(47285): loss=2.4663948189064544\n",
      "Stochastic Gradient Descent(47286): loss=1.651985269746739\n",
      "Stochastic Gradient Descent(47287): loss=13.45381209697548\n",
      "Stochastic Gradient Descent(47288): loss=5.432354874387798\n",
      "Stochastic Gradient Descent(47289): loss=8.052842923147084\n",
      "Stochastic Gradient Descent(47290): loss=0.441522758340634\n",
      "Stochastic Gradient Descent(47291): loss=7.291185893953086\n",
      "Stochastic Gradient Descent(47292): loss=0.5022193724304376\n",
      "Stochastic Gradient Descent(47293): loss=0.4528532586316186\n",
      "Stochastic Gradient Descent(47294): loss=0.2989224092996621\n",
      "Stochastic Gradient Descent(47295): loss=2.9112095020136426\n",
      "Stochastic Gradient Descent(47296): loss=1.2859566909407967\n",
      "Stochastic Gradient Descent(47297): loss=0.00019576554413714624\n",
      "Stochastic Gradient Descent(47298): loss=13.619265611312946\n",
      "Stochastic Gradient Descent(47299): loss=1.027412244780625\n",
      "Stochastic Gradient Descent(47300): loss=5.015818839205048\n",
      "Stochastic Gradient Descent(47301): loss=4.076521464960353\n",
      "Stochastic Gradient Descent(47302): loss=5.92346427292214\n",
      "Stochastic Gradient Descent(47303): loss=0.6025607296654002\n",
      "Stochastic Gradient Descent(47304): loss=1.654866508139068\n",
      "Stochastic Gradient Descent(47305): loss=20.761033536799626\n",
      "Stochastic Gradient Descent(47306): loss=3.3247616812086425\n",
      "Stochastic Gradient Descent(47307): loss=1.2295995242948397\n",
      "Stochastic Gradient Descent(47308): loss=0.31430920025530734\n",
      "Stochastic Gradient Descent(47309): loss=2.6539281138750916\n",
      "Stochastic Gradient Descent(47310): loss=4.740876668367554\n",
      "Stochastic Gradient Descent(47311): loss=0.998276012081793\n",
      "Stochastic Gradient Descent(47312): loss=1.797905109522156\n",
      "Stochastic Gradient Descent(47313): loss=0.6370978196879103\n",
      "Stochastic Gradient Descent(47314): loss=2.7982746007396955\n",
      "Stochastic Gradient Descent(47315): loss=1.5427404145531147\n",
      "Stochastic Gradient Descent(47316): loss=0.12994756998202708\n",
      "Stochastic Gradient Descent(47317): loss=0.2993448714163214\n",
      "Stochastic Gradient Descent(47318): loss=0.8524838062307863\n",
      "Stochastic Gradient Descent(47319): loss=4.161643373573113\n",
      "Stochastic Gradient Descent(47320): loss=11.363193824006556\n",
      "Stochastic Gradient Descent(47321): loss=1.5979978272449558\n",
      "Stochastic Gradient Descent(47322): loss=6.302123932060682\n",
      "Stochastic Gradient Descent(47323): loss=0.40610354848704355\n",
      "Stochastic Gradient Descent(47324): loss=6.841469139905072\n",
      "Stochastic Gradient Descent(47325): loss=0.46621542445586917\n",
      "Stochastic Gradient Descent(47326): loss=0.6667712011015875\n",
      "Stochastic Gradient Descent(47327): loss=18.7694012322192\n",
      "Stochastic Gradient Descent(47328): loss=2.3047275601806976\n",
      "Stochastic Gradient Descent(47329): loss=0.866974552960839\n",
      "Stochastic Gradient Descent(47330): loss=0.9724102385400061\n",
      "Stochastic Gradient Descent(47331): loss=0.748323896204373\n",
      "Stochastic Gradient Descent(47332): loss=0.048719769174740246\n",
      "Stochastic Gradient Descent(47333): loss=7.327440506124585\n",
      "Stochastic Gradient Descent(47334): loss=0.2896811666761109\n",
      "Stochastic Gradient Descent(47335): loss=6.031232303927473\n",
      "Stochastic Gradient Descent(47336): loss=3.093316889140513\n",
      "Stochastic Gradient Descent(47337): loss=12.784645002414075\n",
      "Stochastic Gradient Descent(47338): loss=0.3144864263962992\n",
      "Stochastic Gradient Descent(47339): loss=2.6592937619748707\n",
      "Stochastic Gradient Descent(47340): loss=0.7335255298833898\n",
      "Stochastic Gradient Descent(47341): loss=0.8925188118577679\n",
      "Stochastic Gradient Descent(47342): loss=0.5374259214580976\n",
      "Stochastic Gradient Descent(47343): loss=1.791482825674942\n",
      "Stochastic Gradient Descent(47344): loss=0.6222814065383336\n",
      "Stochastic Gradient Descent(47345): loss=0.27570282307891236\n",
      "Stochastic Gradient Descent(47346): loss=0.5029766939471403\n",
      "Stochastic Gradient Descent(47347): loss=3.2565026002133877\n",
      "Stochastic Gradient Descent(47348): loss=2.6045160279663584\n",
      "Stochastic Gradient Descent(47349): loss=1.9930693891708635\n",
      "Stochastic Gradient Descent(47350): loss=7.990007602347756\n",
      "Stochastic Gradient Descent(47351): loss=7.679238050914422\n",
      "Stochastic Gradient Descent(47352): loss=2.5442514673624825\n",
      "Stochastic Gradient Descent(47353): loss=4.184599896375836\n",
      "Stochastic Gradient Descent(47354): loss=3.125072942137279\n",
      "Stochastic Gradient Descent(47355): loss=1.6176662410300975\n",
      "Stochastic Gradient Descent(47356): loss=4.733734998542376\n",
      "Stochastic Gradient Descent(47357): loss=0.009737102429291933\n",
      "Stochastic Gradient Descent(47358): loss=0.04536014933456853\n",
      "Stochastic Gradient Descent(47359): loss=0.6758350949589711\n",
      "Stochastic Gradient Descent(47360): loss=1.0488593865460456\n",
      "Stochastic Gradient Descent(47361): loss=9.565475402219258\n",
      "Stochastic Gradient Descent(47362): loss=1.8943075559366755\n",
      "Stochastic Gradient Descent(47363): loss=2.5581637311565215\n",
      "Stochastic Gradient Descent(47364): loss=0.1320865678415073\n",
      "Stochastic Gradient Descent(47365): loss=9.160133095295237\n",
      "Stochastic Gradient Descent(47366): loss=1.7886600717559589\n",
      "Stochastic Gradient Descent(47367): loss=3.6678756611149526\n",
      "Stochastic Gradient Descent(47368): loss=3.8168134457487284\n",
      "Stochastic Gradient Descent(47369): loss=0.03624684829158533\n",
      "Stochastic Gradient Descent(47370): loss=0.6346777254086845\n",
      "Stochastic Gradient Descent(47371): loss=19.207849841828974\n",
      "Stochastic Gradient Descent(47372): loss=0.38118827653115556\n",
      "Stochastic Gradient Descent(47373): loss=0.030964360508761668\n",
      "Stochastic Gradient Descent(47374): loss=0.030248537215141716\n",
      "Stochastic Gradient Descent(47375): loss=7.485120240298761\n",
      "Stochastic Gradient Descent(47376): loss=1.8743345143664767\n",
      "Stochastic Gradient Descent(47377): loss=1.0111427798208694\n",
      "Stochastic Gradient Descent(47378): loss=7.250880366860084\n",
      "Stochastic Gradient Descent(47379): loss=0.44053334028134544\n",
      "Stochastic Gradient Descent(47380): loss=13.67505656077575\n",
      "Stochastic Gradient Descent(47381): loss=0.2996646999395403\n",
      "Stochastic Gradient Descent(47382): loss=2.2270144822649467e-05\n",
      "Stochastic Gradient Descent(47383): loss=0.38926194812839376\n",
      "Stochastic Gradient Descent(47384): loss=0.31325053292424526\n",
      "Stochastic Gradient Descent(47385): loss=3.2137919212890527\n",
      "Stochastic Gradient Descent(47386): loss=1.9361250517188286\n",
      "Stochastic Gradient Descent(47387): loss=5.243843524444069\n",
      "Stochastic Gradient Descent(47388): loss=0.5632305671211096\n",
      "Stochastic Gradient Descent(47389): loss=0.06448342383396216\n",
      "Stochastic Gradient Descent(47390): loss=4.379724439043046\n",
      "Stochastic Gradient Descent(47391): loss=3.7816335450749827\n",
      "Stochastic Gradient Descent(47392): loss=1.401983652970325\n",
      "Stochastic Gradient Descent(47393): loss=2.801422708252759\n",
      "Stochastic Gradient Descent(47394): loss=0.1915874892611065\n",
      "Stochastic Gradient Descent(47395): loss=10.704446865280948\n",
      "Stochastic Gradient Descent(47396): loss=0.0076863313152715634\n",
      "Stochastic Gradient Descent(47397): loss=0.6822645513906445\n",
      "Stochastic Gradient Descent(47398): loss=8.257513124108577\n",
      "Stochastic Gradient Descent(47399): loss=0.6753036924760885\n",
      "Stochastic Gradient Descent(47400): loss=0.7125535254413299\n",
      "Stochastic Gradient Descent(47401): loss=0.000311106614060914\n",
      "Stochastic Gradient Descent(47402): loss=5.096855091796838\n",
      "Stochastic Gradient Descent(47403): loss=6.539505402327637\n",
      "Stochastic Gradient Descent(47404): loss=11.299798910067103\n",
      "Stochastic Gradient Descent(47405): loss=3.4765320421476633\n",
      "Stochastic Gradient Descent(47406): loss=0.13032736471784817\n",
      "Stochastic Gradient Descent(47407): loss=0.038479526125454695\n",
      "Stochastic Gradient Descent(47408): loss=0.10917751088896278\n",
      "Stochastic Gradient Descent(47409): loss=1.948768037609566\n",
      "Stochastic Gradient Descent(47410): loss=0.011040983938785767\n",
      "Stochastic Gradient Descent(47411): loss=2.4942606779058907\n",
      "Stochastic Gradient Descent(47412): loss=10.683132653036328\n",
      "Stochastic Gradient Descent(47413): loss=4.108492409289788\n",
      "Stochastic Gradient Descent(47414): loss=1.1306464315336615\n",
      "Stochastic Gradient Descent(47415): loss=0.3166926719356909\n",
      "Stochastic Gradient Descent(47416): loss=3.095137349184043\n",
      "Stochastic Gradient Descent(47417): loss=2.0364501625744027\n",
      "Stochastic Gradient Descent(47418): loss=0.1109577427835123\n",
      "Stochastic Gradient Descent(47419): loss=7.408386574375794\n",
      "Stochastic Gradient Descent(47420): loss=0.40424839899324594\n",
      "Stochastic Gradient Descent(47421): loss=0.5283170672872525\n",
      "Stochastic Gradient Descent(47422): loss=1.09684011469679\n",
      "Stochastic Gradient Descent(47423): loss=0.024265343235561943\n",
      "Stochastic Gradient Descent(47424): loss=1.0178424595212194\n",
      "Stochastic Gradient Descent(47425): loss=10.205465489298122\n",
      "Stochastic Gradient Descent(47426): loss=0.20556870961216148\n",
      "Stochastic Gradient Descent(47427): loss=8.968001523561613\n",
      "Stochastic Gradient Descent(47428): loss=2.736732899426453\n",
      "Stochastic Gradient Descent(47429): loss=1.5446664430377748\n",
      "Stochastic Gradient Descent(47430): loss=0.04485872582812575\n",
      "Stochastic Gradient Descent(47431): loss=2.127897357480424\n",
      "Stochastic Gradient Descent(47432): loss=4.522169873777885\n",
      "Stochastic Gradient Descent(47433): loss=0.06333184042718419\n",
      "Stochastic Gradient Descent(47434): loss=1.9872801064673693\n",
      "Stochastic Gradient Descent(47435): loss=0.05922225995256386\n",
      "Stochastic Gradient Descent(47436): loss=0.21388728705956744\n",
      "Stochastic Gradient Descent(47437): loss=3.627404354604069\n",
      "Stochastic Gradient Descent(47438): loss=0.3369320229876491\n",
      "Stochastic Gradient Descent(47439): loss=0.019770966273194433\n",
      "Stochastic Gradient Descent(47440): loss=1.0156521437644621\n",
      "Stochastic Gradient Descent(47441): loss=0.013461552752528796\n",
      "Stochastic Gradient Descent(47442): loss=8.509106556833862\n",
      "Stochastic Gradient Descent(47443): loss=0.002687005859357012\n",
      "Stochastic Gradient Descent(47444): loss=4.7686859345412\n",
      "Stochastic Gradient Descent(47445): loss=0.008644094759756955\n",
      "Stochastic Gradient Descent(47446): loss=0.46215543608710896\n",
      "Stochastic Gradient Descent(47447): loss=1.0872720099333921\n",
      "Stochastic Gradient Descent(47448): loss=9.68529516432804\n",
      "Stochastic Gradient Descent(47449): loss=0.7042158283217863\n",
      "Stochastic Gradient Descent(47450): loss=4.778720764972403\n",
      "Stochastic Gradient Descent(47451): loss=10.406983569086073\n",
      "Stochastic Gradient Descent(47452): loss=3.573589749918535\n",
      "Stochastic Gradient Descent(47453): loss=0.00260502489325778\n",
      "Stochastic Gradient Descent(47454): loss=0.7420282558654294\n",
      "Stochastic Gradient Descent(47455): loss=2.2090680217580245\n",
      "Stochastic Gradient Descent(47456): loss=0.6201844996725273\n",
      "Stochastic Gradient Descent(47457): loss=1.8672378793721207\n",
      "Stochastic Gradient Descent(47458): loss=6.481255679315713\n",
      "Stochastic Gradient Descent(47459): loss=6.6605678968685575\n",
      "Stochastic Gradient Descent(47460): loss=2.6673379157616752\n",
      "Stochastic Gradient Descent(47461): loss=0.32152623646071543\n",
      "Stochastic Gradient Descent(47462): loss=0.04765795275380559\n",
      "Stochastic Gradient Descent(47463): loss=0.5131736094276445\n",
      "Stochastic Gradient Descent(47464): loss=4.583275434717104\n",
      "Stochastic Gradient Descent(47465): loss=0.34726239660340114\n",
      "Stochastic Gradient Descent(47466): loss=0.45534039258316966\n",
      "Stochastic Gradient Descent(47467): loss=3.80565514117363\n",
      "Stochastic Gradient Descent(47468): loss=3.1671192437292017\n",
      "Stochastic Gradient Descent(47469): loss=0.13418097410844518\n",
      "Stochastic Gradient Descent(47470): loss=9.694335043588218\n",
      "Stochastic Gradient Descent(47471): loss=6.258744655413246\n",
      "Stochastic Gradient Descent(47472): loss=7.922623895485108\n",
      "Stochastic Gradient Descent(47473): loss=9.547548762540764\n",
      "Stochastic Gradient Descent(47474): loss=0.6646624211248068\n",
      "Stochastic Gradient Descent(47475): loss=3.2172622112424025\n",
      "Stochastic Gradient Descent(47476): loss=10.11555980488759\n",
      "Stochastic Gradient Descent(47477): loss=8.977097271979838\n",
      "Stochastic Gradient Descent(47478): loss=7.097294775577163\n",
      "Stochastic Gradient Descent(47479): loss=1.9246935754876229\n",
      "Stochastic Gradient Descent(47480): loss=5.65208305064641\n",
      "Stochastic Gradient Descent(47481): loss=0.13645586683333813\n",
      "Stochastic Gradient Descent(47482): loss=2.4921249244311723e-08\n",
      "Stochastic Gradient Descent(47483): loss=0.06960579420785291\n",
      "Stochastic Gradient Descent(47484): loss=13.369568151845808\n",
      "Stochastic Gradient Descent(47485): loss=0.19978955832700832\n",
      "Stochastic Gradient Descent(47486): loss=0.16133115574264048\n",
      "Stochastic Gradient Descent(47487): loss=0.0017356541991786361\n",
      "Stochastic Gradient Descent(47488): loss=6.84694576144198\n",
      "Stochastic Gradient Descent(47489): loss=0.2539117404573048\n",
      "Stochastic Gradient Descent(47490): loss=3.2516058974316775\n",
      "Stochastic Gradient Descent(47491): loss=0.11090219751340494\n",
      "Stochastic Gradient Descent(47492): loss=7.921968938258722\n",
      "Stochastic Gradient Descent(47493): loss=0.03561981297849732\n",
      "Stochastic Gradient Descent(47494): loss=3.8419834690143926\n",
      "Stochastic Gradient Descent(47495): loss=0.7489307284233057\n",
      "Stochastic Gradient Descent(47496): loss=0.20624474667286657\n",
      "Stochastic Gradient Descent(47497): loss=2.2076328459642705\n",
      "Stochastic Gradient Descent(47498): loss=3.262946512415629\n",
      "Stochastic Gradient Descent(47499): loss=0.7864414221190087\n",
      "Stochastic Gradient Descent(47500): loss=0.6383611228973566\n",
      "Stochastic Gradient Descent(47501): loss=2.002582921973213\n",
      "Stochastic Gradient Descent(47502): loss=9.405638675998135\n",
      "Stochastic Gradient Descent(47503): loss=25.97260869264501\n",
      "Stochastic Gradient Descent(47504): loss=24.39763260141927\n",
      "Stochastic Gradient Descent(47505): loss=0.5865462702328986\n",
      "Stochastic Gradient Descent(47506): loss=0.4846713615983311\n",
      "Stochastic Gradient Descent(47507): loss=5.478130093566517\n",
      "Stochastic Gradient Descent(47508): loss=0.939895969226564\n",
      "Stochastic Gradient Descent(47509): loss=0.48641320710893504\n",
      "Stochastic Gradient Descent(47510): loss=2.900085002504143\n",
      "Stochastic Gradient Descent(47511): loss=10.528415039235167\n",
      "Stochastic Gradient Descent(47512): loss=6.683973709296008\n",
      "Stochastic Gradient Descent(47513): loss=5.178985721395869\n",
      "Stochastic Gradient Descent(47514): loss=0.6091253391280326\n",
      "Stochastic Gradient Descent(47515): loss=0.3654775471992916\n",
      "Stochastic Gradient Descent(47516): loss=0.4195988290895766\n",
      "Stochastic Gradient Descent(47517): loss=6.410749838200445\n",
      "Stochastic Gradient Descent(47518): loss=1.280542749794254\n",
      "Stochastic Gradient Descent(47519): loss=1.5385434919650463\n",
      "Stochastic Gradient Descent(47520): loss=5.574255273104045\n",
      "Stochastic Gradient Descent(47521): loss=1.6493738504173006\n",
      "Stochastic Gradient Descent(47522): loss=2.7027256760177756\n",
      "Stochastic Gradient Descent(47523): loss=0.41182209478163484\n",
      "Stochastic Gradient Descent(47524): loss=5.8496084377964275\n",
      "Stochastic Gradient Descent(47525): loss=5.12798786530092\n",
      "Stochastic Gradient Descent(47526): loss=0.6330822276749271\n",
      "Stochastic Gradient Descent(47527): loss=0.034121992303119124\n",
      "Stochastic Gradient Descent(47528): loss=0.07622790457929036\n",
      "Stochastic Gradient Descent(47529): loss=0.9227269607476192\n",
      "Stochastic Gradient Descent(47530): loss=0.1489764337727727\n",
      "Stochastic Gradient Descent(47531): loss=0.1664789727942176\n",
      "Stochastic Gradient Descent(47532): loss=0.0704806736689125\n",
      "Stochastic Gradient Descent(47533): loss=1.2084912048031937\n",
      "Stochastic Gradient Descent(47534): loss=0.6412908802099117\n",
      "Stochastic Gradient Descent(47535): loss=5.239268994611731\n",
      "Stochastic Gradient Descent(47536): loss=4.937689828160135\n",
      "Stochastic Gradient Descent(47537): loss=1.3958951336056873\n",
      "Stochastic Gradient Descent(47538): loss=0.009385667802423326\n",
      "Stochastic Gradient Descent(47539): loss=1.1448379173679302\n",
      "Stochastic Gradient Descent(47540): loss=7.118297507172544\n",
      "Stochastic Gradient Descent(47541): loss=0.25774381369671095\n",
      "Stochastic Gradient Descent(47542): loss=7.264367558622098\n",
      "Stochastic Gradient Descent(47543): loss=6.941610563483456\n",
      "Stochastic Gradient Descent(47544): loss=2.93358268339511\n",
      "Stochastic Gradient Descent(47545): loss=1.3277156176473324\n",
      "Stochastic Gradient Descent(47546): loss=5.872528517268209\n",
      "Stochastic Gradient Descent(47547): loss=4.8555430671680195\n",
      "Stochastic Gradient Descent(47548): loss=4.890914704728035\n",
      "Stochastic Gradient Descent(47549): loss=0.02795598948426601\n",
      "Stochastic Gradient Descent(47550): loss=2.3646501423883444\n",
      "Stochastic Gradient Descent(47551): loss=4.496629730637104\n",
      "Stochastic Gradient Descent(47552): loss=1.5508298190063023\n",
      "Stochastic Gradient Descent(47553): loss=0.355194854147292\n",
      "Stochastic Gradient Descent(47554): loss=0.2843973239217961\n",
      "Stochastic Gradient Descent(47555): loss=0.8939544238405834\n",
      "Stochastic Gradient Descent(47556): loss=0.027692057416635756\n",
      "Stochastic Gradient Descent(47557): loss=0.15062599320203676\n",
      "Stochastic Gradient Descent(47558): loss=0.15258550561233528\n",
      "Stochastic Gradient Descent(47559): loss=1.0053997540323423\n",
      "Stochastic Gradient Descent(47560): loss=1.6317163098105685\n",
      "Stochastic Gradient Descent(47561): loss=12.249959912426327\n",
      "Stochastic Gradient Descent(47562): loss=13.324429024005354\n",
      "Stochastic Gradient Descent(47563): loss=1.9537087827634452\n",
      "Stochastic Gradient Descent(47564): loss=0.09625251723696923\n",
      "Stochastic Gradient Descent(47565): loss=1.1806756510928658\n",
      "Stochastic Gradient Descent(47566): loss=3.645277978357179\n",
      "Stochastic Gradient Descent(47567): loss=0.005397126587015109\n",
      "Stochastic Gradient Descent(47568): loss=7.30944037919057\n",
      "Stochastic Gradient Descent(47569): loss=0.3608593215467696\n",
      "Stochastic Gradient Descent(47570): loss=18.06339445669748\n",
      "Stochastic Gradient Descent(47571): loss=0.31296949632953613\n",
      "Stochastic Gradient Descent(47572): loss=42.81878600264589\n",
      "Stochastic Gradient Descent(47573): loss=3.22585227460841\n",
      "Stochastic Gradient Descent(47574): loss=0.8952778728464494\n",
      "Stochastic Gradient Descent(47575): loss=11.345873725237803\n",
      "Stochastic Gradient Descent(47576): loss=0.6969023201252482\n",
      "Stochastic Gradient Descent(47577): loss=2.1885211256344963\n",
      "Stochastic Gradient Descent(47578): loss=3.1925759942343426\n",
      "Stochastic Gradient Descent(47579): loss=0.1955686533893107\n",
      "Stochastic Gradient Descent(47580): loss=5.590315219477972\n",
      "Stochastic Gradient Descent(47581): loss=4.167263072092981\n",
      "Stochastic Gradient Descent(47582): loss=0.06503745759832717\n",
      "Stochastic Gradient Descent(47583): loss=0.10980668977684013\n",
      "Stochastic Gradient Descent(47584): loss=0.2650230050697034\n",
      "Stochastic Gradient Descent(47585): loss=0.059372916167992784\n",
      "Stochastic Gradient Descent(47586): loss=0.2736135144733787\n",
      "Stochastic Gradient Descent(47587): loss=1.5482824877818189\n",
      "Stochastic Gradient Descent(47588): loss=1.860279591160777\n",
      "Stochastic Gradient Descent(47589): loss=12.36540532027415\n",
      "Stochastic Gradient Descent(47590): loss=3.3957846081187744\n",
      "Stochastic Gradient Descent(47591): loss=1.3544957996576064\n",
      "Stochastic Gradient Descent(47592): loss=3.8975221600612584\n",
      "Stochastic Gradient Descent(47593): loss=2.9857732653117597\n",
      "Stochastic Gradient Descent(47594): loss=9.25207996179969\n",
      "Stochastic Gradient Descent(47595): loss=4.716288760049342\n",
      "Stochastic Gradient Descent(47596): loss=7.003336433570841\n",
      "Stochastic Gradient Descent(47597): loss=2.7426332748634934\n",
      "Stochastic Gradient Descent(47598): loss=7.786702022378611\n",
      "Stochastic Gradient Descent(47599): loss=5.473158980546837\n",
      "Stochastic Gradient Descent(47600): loss=4.410293047276208\n",
      "Stochastic Gradient Descent(47601): loss=5.333175586810362\n",
      "Stochastic Gradient Descent(47602): loss=6.499354611697992\n",
      "Stochastic Gradient Descent(47603): loss=0.6782561454005424\n",
      "Stochastic Gradient Descent(47604): loss=0.033286868404759695\n",
      "Stochastic Gradient Descent(47605): loss=23.822691237319322\n",
      "Stochastic Gradient Descent(47606): loss=1.8662133851125902\n",
      "Stochastic Gradient Descent(47607): loss=0.16960243095728006\n",
      "Stochastic Gradient Descent(47608): loss=3.3811203626738315\n",
      "Stochastic Gradient Descent(47609): loss=0.5289559819618245\n",
      "Stochastic Gradient Descent(47610): loss=0.00024171183578512143\n",
      "Stochastic Gradient Descent(47611): loss=3.716467868175691\n",
      "Stochastic Gradient Descent(47612): loss=3.8904663587892747\n",
      "Stochastic Gradient Descent(47613): loss=4.571604218026007\n",
      "Stochastic Gradient Descent(47614): loss=1.8924714235603697\n",
      "Stochastic Gradient Descent(47615): loss=0.05507968943120489\n",
      "Stochastic Gradient Descent(47616): loss=37.58622137703408\n",
      "Stochastic Gradient Descent(47617): loss=0.11156847481482032\n",
      "Stochastic Gradient Descent(47618): loss=14.582593215801465\n",
      "Stochastic Gradient Descent(47619): loss=0.5846942844433952\n",
      "Stochastic Gradient Descent(47620): loss=0.3363355309994297\n",
      "Stochastic Gradient Descent(47621): loss=3.847498987450699\n",
      "Stochastic Gradient Descent(47622): loss=0.41349684832506517\n",
      "Stochastic Gradient Descent(47623): loss=1.839430260250776\n",
      "Stochastic Gradient Descent(47624): loss=0.07203447965613373\n",
      "Stochastic Gradient Descent(47625): loss=0.6905092337782699\n",
      "Stochastic Gradient Descent(47626): loss=3.001888408678672\n",
      "Stochastic Gradient Descent(47627): loss=16.41636565443396\n",
      "Stochastic Gradient Descent(47628): loss=4.309693845253175\n",
      "Stochastic Gradient Descent(47629): loss=3.1829876887922466\n",
      "Stochastic Gradient Descent(47630): loss=0.1916944783408419\n",
      "Stochastic Gradient Descent(47631): loss=3.8952886761132586\n",
      "Stochastic Gradient Descent(47632): loss=27.02858065094369\n",
      "Stochastic Gradient Descent(47633): loss=2.9530397770596206\n",
      "Stochastic Gradient Descent(47634): loss=0.512949244505996\n",
      "Stochastic Gradient Descent(47635): loss=0.12801341936514704\n",
      "Stochastic Gradient Descent(47636): loss=0.5547807385055475\n",
      "Stochastic Gradient Descent(47637): loss=8.439496055340781\n",
      "Stochastic Gradient Descent(47638): loss=5.5261607686412075\n",
      "Stochastic Gradient Descent(47639): loss=26.153746411575128\n",
      "Stochastic Gradient Descent(47640): loss=0.5965876188547603\n",
      "Stochastic Gradient Descent(47641): loss=0.012254306630255273\n",
      "Stochastic Gradient Descent(47642): loss=2.0778974500427347\n",
      "Stochastic Gradient Descent(47643): loss=0.006041201215032712\n",
      "Stochastic Gradient Descent(47644): loss=2.3031273379358095\n",
      "Stochastic Gradient Descent(47645): loss=13.48306735492057\n",
      "Stochastic Gradient Descent(47646): loss=3.708812049604399\n",
      "Stochastic Gradient Descent(47647): loss=3.7294941538934445\n",
      "Stochastic Gradient Descent(47648): loss=9.197084244739422\n",
      "Stochastic Gradient Descent(47649): loss=1.603999385095457\n",
      "Stochastic Gradient Descent(47650): loss=2.6681726993100026\n",
      "Stochastic Gradient Descent(47651): loss=1.0243077524820234\n",
      "Stochastic Gradient Descent(47652): loss=2.9043286535670854\n",
      "Stochastic Gradient Descent(47653): loss=0.38501164869197574\n",
      "Stochastic Gradient Descent(47654): loss=2.7363362674374323\n",
      "Stochastic Gradient Descent(47655): loss=5.753633019491059\n",
      "Stochastic Gradient Descent(47656): loss=1.6023265406710139\n",
      "Stochastic Gradient Descent(47657): loss=7.80031006511314\n",
      "Stochastic Gradient Descent(47658): loss=5.239473092073036\n",
      "Stochastic Gradient Descent(47659): loss=1.4065896656948125\n",
      "Stochastic Gradient Descent(47660): loss=1.7818537041814062\n",
      "Stochastic Gradient Descent(47661): loss=18.759787558811322\n",
      "Stochastic Gradient Descent(47662): loss=0.10018459637923381\n",
      "Stochastic Gradient Descent(47663): loss=0.12209489741628592\n",
      "Stochastic Gradient Descent(47664): loss=8.15677734341241\n",
      "Stochastic Gradient Descent(47665): loss=0.26981181654630326\n",
      "Stochastic Gradient Descent(47666): loss=2.5485405768656637\n",
      "Stochastic Gradient Descent(47667): loss=0.023877552844608664\n",
      "Stochastic Gradient Descent(47668): loss=3.4530387959145132\n",
      "Stochastic Gradient Descent(47669): loss=1.6529834671275085\n",
      "Stochastic Gradient Descent(47670): loss=1.8293678100633342\n",
      "Stochastic Gradient Descent(47671): loss=3.754826504694486\n",
      "Stochastic Gradient Descent(47672): loss=9.464691167228779\n",
      "Stochastic Gradient Descent(47673): loss=40.74617585196968\n",
      "Stochastic Gradient Descent(47674): loss=2.637628696864469\n",
      "Stochastic Gradient Descent(47675): loss=0.36883697709954916\n",
      "Stochastic Gradient Descent(47676): loss=3.1287414064680674\n",
      "Stochastic Gradient Descent(47677): loss=23.083200950972632\n",
      "Stochastic Gradient Descent(47678): loss=3.3233812789260617\n",
      "Stochastic Gradient Descent(47679): loss=0.6384166656202225\n",
      "Stochastic Gradient Descent(47680): loss=0.29616710944120256\n",
      "Stochastic Gradient Descent(47681): loss=12.910411389925391\n",
      "Stochastic Gradient Descent(47682): loss=8.791099847011887\n",
      "Stochastic Gradient Descent(47683): loss=2.7011358844188735\n",
      "Stochastic Gradient Descent(47684): loss=3.2324959107285838\n",
      "Stochastic Gradient Descent(47685): loss=13.931844610171956\n",
      "Stochastic Gradient Descent(47686): loss=0.0001504895250624727\n",
      "Stochastic Gradient Descent(47687): loss=0.34231569415179564\n",
      "Stochastic Gradient Descent(47688): loss=0.23748688900370504\n",
      "Stochastic Gradient Descent(47689): loss=6.438862325983246\n",
      "Stochastic Gradient Descent(47690): loss=3.272167005186511\n",
      "Stochastic Gradient Descent(47691): loss=2.1780810373196906\n",
      "Stochastic Gradient Descent(47692): loss=3.4565432000168514\n",
      "Stochastic Gradient Descent(47693): loss=16.537696936942638\n",
      "Stochastic Gradient Descent(47694): loss=3.5155882601425517\n",
      "Stochastic Gradient Descent(47695): loss=1.4227673904794234\n",
      "Stochastic Gradient Descent(47696): loss=0.43330249755083344\n",
      "Stochastic Gradient Descent(47697): loss=3.323881375434262\n",
      "Stochastic Gradient Descent(47698): loss=4.4528839706078776\n",
      "Stochastic Gradient Descent(47699): loss=1.794310048982875\n",
      "Stochastic Gradient Descent(47700): loss=0.762543929251306\n",
      "Stochastic Gradient Descent(47701): loss=0.6501829843512729\n",
      "Stochastic Gradient Descent(47702): loss=0.532120335066259\n",
      "Stochastic Gradient Descent(47703): loss=2.3376593755828097\n",
      "Stochastic Gradient Descent(47704): loss=2.459927346407899\n",
      "Stochastic Gradient Descent(47705): loss=2.9652477533340837\n",
      "Stochastic Gradient Descent(47706): loss=6.200495515356729\n",
      "Stochastic Gradient Descent(47707): loss=18.80261842767347\n",
      "Stochastic Gradient Descent(47708): loss=0.12664394380323912\n",
      "Stochastic Gradient Descent(47709): loss=2.021634511909522\n",
      "Stochastic Gradient Descent(47710): loss=23.87993840962583\n",
      "Stochastic Gradient Descent(47711): loss=1.4119792558742768\n",
      "Stochastic Gradient Descent(47712): loss=0.02708751165925868\n",
      "Stochastic Gradient Descent(47713): loss=10.046461598451092\n",
      "Stochastic Gradient Descent(47714): loss=1.678248189848505\n",
      "Stochastic Gradient Descent(47715): loss=3.609686030122203\n",
      "Stochastic Gradient Descent(47716): loss=4.44376643199993\n",
      "Stochastic Gradient Descent(47717): loss=0.7793191449365957\n",
      "Stochastic Gradient Descent(47718): loss=1.5012620951305182\n",
      "Stochastic Gradient Descent(47719): loss=3.34082539065288\n",
      "Stochastic Gradient Descent(47720): loss=1.455694999061495\n",
      "Stochastic Gradient Descent(47721): loss=0.05988750388579075\n",
      "Stochastic Gradient Descent(47722): loss=0.43403875894853133\n",
      "Stochastic Gradient Descent(47723): loss=1.014621430403648\n",
      "Stochastic Gradient Descent(47724): loss=1.5467981663604695\n",
      "Stochastic Gradient Descent(47725): loss=0.06886464589597242\n",
      "Stochastic Gradient Descent(47726): loss=0.04396487827234938\n",
      "Stochastic Gradient Descent(47727): loss=9.156472680913396\n",
      "Stochastic Gradient Descent(47728): loss=0.9903600726373885\n",
      "Stochastic Gradient Descent(47729): loss=2.0546560154525655\n",
      "Stochastic Gradient Descent(47730): loss=0.0009022299917719184\n",
      "Stochastic Gradient Descent(47731): loss=0.06981704230432231\n",
      "Stochastic Gradient Descent(47732): loss=29.34941193462452\n",
      "Stochastic Gradient Descent(47733): loss=1.9625608520756626\n",
      "Stochastic Gradient Descent(47734): loss=0.06790781601744411\n",
      "Stochastic Gradient Descent(47735): loss=0.8393192139877512\n",
      "Stochastic Gradient Descent(47736): loss=1.1264957466005\n",
      "Stochastic Gradient Descent(47737): loss=0.4487943749556982\n",
      "Stochastic Gradient Descent(47738): loss=2.5377807471160274\n",
      "Stochastic Gradient Descent(47739): loss=0.6460129252931914\n",
      "Stochastic Gradient Descent(47740): loss=1.4557066037395037\n",
      "Stochastic Gradient Descent(47741): loss=4.5488944433967715\n",
      "Stochastic Gradient Descent(47742): loss=2.781166977495181\n",
      "Stochastic Gradient Descent(47743): loss=19.517628782481847\n",
      "Stochastic Gradient Descent(47744): loss=0.01717579029464663\n",
      "Stochastic Gradient Descent(47745): loss=1.940003952491038\n",
      "Stochastic Gradient Descent(47746): loss=0.6101372740347876\n",
      "Stochastic Gradient Descent(47747): loss=0.3506980156164488\n",
      "Stochastic Gradient Descent(47748): loss=2.185622218365014\n",
      "Stochastic Gradient Descent(47749): loss=0.7427047272904435\n",
      "Stochastic Gradient Descent(47750): loss=0.40697606773015244\n",
      "Stochastic Gradient Descent(47751): loss=0.7669248222071795\n",
      "Stochastic Gradient Descent(47752): loss=0.5038877662509839\n",
      "Stochastic Gradient Descent(47753): loss=0.44620777765277964\n",
      "Stochastic Gradient Descent(47754): loss=2.8233461156821416\n",
      "Stochastic Gradient Descent(47755): loss=1.7247036791775778\n",
      "Stochastic Gradient Descent(47756): loss=1.9487752019529336\n",
      "Stochastic Gradient Descent(47757): loss=3.5955738943468845\n",
      "Stochastic Gradient Descent(47758): loss=22.263437985807375\n",
      "Stochastic Gradient Descent(47759): loss=0.46752203742364196\n",
      "Stochastic Gradient Descent(47760): loss=1.710948454271702\n",
      "Stochastic Gradient Descent(47761): loss=1.5897738831477888\n",
      "Stochastic Gradient Descent(47762): loss=1.0333834003193223\n",
      "Stochastic Gradient Descent(47763): loss=1.4896938610581663\n",
      "Stochastic Gradient Descent(47764): loss=22.562040993548028\n",
      "Stochastic Gradient Descent(47765): loss=0.8937648341381297\n",
      "Stochastic Gradient Descent(47766): loss=1.0995253046510247\n",
      "Stochastic Gradient Descent(47767): loss=4.12020851865756\n",
      "Stochastic Gradient Descent(47768): loss=20.788490365019797\n",
      "Stochastic Gradient Descent(47769): loss=6.5241546465903815\n",
      "Stochastic Gradient Descent(47770): loss=2.1127571290221336\n",
      "Stochastic Gradient Descent(47771): loss=1.499470292693908\n",
      "Stochastic Gradient Descent(47772): loss=1.3425498844457813\n",
      "Stochastic Gradient Descent(47773): loss=0.1195724730358571\n",
      "Stochastic Gradient Descent(47774): loss=0.48992091533323084\n",
      "Stochastic Gradient Descent(47775): loss=8.039807948319663\n",
      "Stochastic Gradient Descent(47776): loss=1.5002465746708704\n",
      "Stochastic Gradient Descent(47777): loss=7.226135159821693\n",
      "Stochastic Gradient Descent(47778): loss=1.005856729488519\n",
      "Stochastic Gradient Descent(47779): loss=5.930883317396381\n",
      "Stochastic Gradient Descent(47780): loss=1.0591470654395003\n",
      "Stochastic Gradient Descent(47781): loss=0.6086556393634649\n",
      "Stochastic Gradient Descent(47782): loss=0.2752526334020604\n",
      "Stochastic Gradient Descent(47783): loss=2.487087806735034\n",
      "Stochastic Gradient Descent(47784): loss=3.380063321374312\n",
      "Stochastic Gradient Descent(47785): loss=0.42466009857273507\n",
      "Stochastic Gradient Descent(47786): loss=15.41359305651634\n",
      "Stochastic Gradient Descent(47787): loss=6.073308857707456\n",
      "Stochastic Gradient Descent(47788): loss=0.45500534817722493\n",
      "Stochastic Gradient Descent(47789): loss=0.9162143873613796\n",
      "Stochastic Gradient Descent(47790): loss=0.0011752079780066459\n",
      "Stochastic Gradient Descent(47791): loss=0.8329042317273914\n",
      "Stochastic Gradient Descent(47792): loss=11.021983041942141\n",
      "Stochastic Gradient Descent(47793): loss=12.031738035038687\n",
      "Stochastic Gradient Descent(47794): loss=5.366214927854494\n",
      "Stochastic Gradient Descent(47795): loss=0.008475632572600601\n",
      "Stochastic Gradient Descent(47796): loss=16.33906517679281\n",
      "Stochastic Gradient Descent(47797): loss=7.268151751730228\n",
      "Stochastic Gradient Descent(47798): loss=8.942633389455816\n",
      "Stochastic Gradient Descent(47799): loss=1.167367843066434\n",
      "Stochastic Gradient Descent(47800): loss=4.0867763020349335\n",
      "Stochastic Gradient Descent(47801): loss=1.8247143179276322\n",
      "Stochastic Gradient Descent(47802): loss=0.34844355675199773\n",
      "Stochastic Gradient Descent(47803): loss=5.605901755402024\n",
      "Stochastic Gradient Descent(47804): loss=0.07509651275640788\n",
      "Stochastic Gradient Descent(47805): loss=0.08490100200577053\n",
      "Stochastic Gradient Descent(47806): loss=11.039543628901733\n",
      "Stochastic Gradient Descent(47807): loss=12.641916704418065\n",
      "Stochastic Gradient Descent(47808): loss=11.06725694725249\n",
      "Stochastic Gradient Descent(47809): loss=5.483488145963371\n",
      "Stochastic Gradient Descent(47810): loss=0.47569486803649985\n",
      "Stochastic Gradient Descent(47811): loss=1.0089337996855514\n",
      "Stochastic Gradient Descent(47812): loss=0.4004846914425692\n",
      "Stochastic Gradient Descent(47813): loss=0.0030075307877883077\n",
      "Stochastic Gradient Descent(47814): loss=0.8925069986177554\n",
      "Stochastic Gradient Descent(47815): loss=0.5482306561727914\n",
      "Stochastic Gradient Descent(47816): loss=7.00887037104264\n",
      "Stochastic Gradient Descent(47817): loss=0.057964321336340774\n",
      "Stochastic Gradient Descent(47818): loss=10.85633724799185\n",
      "Stochastic Gradient Descent(47819): loss=1.0514919930520537\n",
      "Stochastic Gradient Descent(47820): loss=0.2402810683813879\n",
      "Stochastic Gradient Descent(47821): loss=3.5353665829474012\n",
      "Stochastic Gradient Descent(47822): loss=1.2259380862917055\n",
      "Stochastic Gradient Descent(47823): loss=0.009800060684578626\n",
      "Stochastic Gradient Descent(47824): loss=2.0061661837272933\n",
      "Stochastic Gradient Descent(47825): loss=0.7687394421353674\n",
      "Stochastic Gradient Descent(47826): loss=4.471205071929689\n",
      "Stochastic Gradient Descent(47827): loss=2.151465164041016\n",
      "Stochastic Gradient Descent(47828): loss=0.02415823956467022\n",
      "Stochastic Gradient Descent(47829): loss=1.566221067826897\n",
      "Stochastic Gradient Descent(47830): loss=0.4592557437187504\n",
      "Stochastic Gradient Descent(47831): loss=1.0821383681451298\n",
      "Stochastic Gradient Descent(47832): loss=4.784826514824132\n",
      "Stochastic Gradient Descent(47833): loss=0.5211253108935713\n",
      "Stochastic Gradient Descent(47834): loss=11.285253728147442\n",
      "Stochastic Gradient Descent(47835): loss=0.10393407465202464\n",
      "Stochastic Gradient Descent(47836): loss=1.0829797638792684\n",
      "Stochastic Gradient Descent(47837): loss=0.01534472724628227\n",
      "Stochastic Gradient Descent(47838): loss=1.605267270536527e-06\n",
      "Stochastic Gradient Descent(47839): loss=3.056786289193817\n",
      "Stochastic Gradient Descent(47840): loss=0.2232847263174685\n",
      "Stochastic Gradient Descent(47841): loss=2.804562143269466\n",
      "Stochastic Gradient Descent(47842): loss=2.948322539214833\n",
      "Stochastic Gradient Descent(47843): loss=0.5189714823585617\n",
      "Stochastic Gradient Descent(47844): loss=6.700964051504839\n",
      "Stochastic Gradient Descent(47845): loss=4.294780708225352\n",
      "Stochastic Gradient Descent(47846): loss=7.0084022642586286\n",
      "Stochastic Gradient Descent(47847): loss=0.005577105136603395\n",
      "Stochastic Gradient Descent(47848): loss=0.35575478187460174\n",
      "Stochastic Gradient Descent(47849): loss=0.39128216348019285\n",
      "Stochastic Gradient Descent(47850): loss=0.023761600490552405\n",
      "Stochastic Gradient Descent(47851): loss=0.09740339142297129\n",
      "Stochastic Gradient Descent(47852): loss=3.8832135800653447\n",
      "Stochastic Gradient Descent(47853): loss=17.428896097521243\n",
      "Stochastic Gradient Descent(47854): loss=0.4211144107152931\n",
      "Stochastic Gradient Descent(47855): loss=7.555988276697473\n",
      "Stochastic Gradient Descent(47856): loss=9.63658129776152\n",
      "Stochastic Gradient Descent(47857): loss=8.637133880934336\n",
      "Stochastic Gradient Descent(47858): loss=7.972944869810959\n",
      "Stochastic Gradient Descent(47859): loss=6.60377025226096\n",
      "Stochastic Gradient Descent(47860): loss=0.5492014607758744\n",
      "Stochastic Gradient Descent(47861): loss=0.39526141810950505\n",
      "Stochastic Gradient Descent(47862): loss=0.20158069953525187\n",
      "Stochastic Gradient Descent(47863): loss=2.2453086097214545e-06\n",
      "Stochastic Gradient Descent(47864): loss=3.0498082693396324\n",
      "Stochastic Gradient Descent(47865): loss=2.9850349223891803\n",
      "Stochastic Gradient Descent(47866): loss=4.333897893281855\n",
      "Stochastic Gradient Descent(47867): loss=1.6964373462754916\n",
      "Stochastic Gradient Descent(47868): loss=1.86620279170097\n",
      "Stochastic Gradient Descent(47869): loss=0.48060982502925337\n",
      "Stochastic Gradient Descent(47870): loss=0.011212770628076827\n",
      "Stochastic Gradient Descent(47871): loss=0.050690191794083767\n",
      "Stochastic Gradient Descent(47872): loss=1.2036879381884946\n",
      "Stochastic Gradient Descent(47873): loss=12.773428634438696\n",
      "Stochastic Gradient Descent(47874): loss=0.04849294386310093\n",
      "Stochastic Gradient Descent(47875): loss=1.7783007908405117\n",
      "Stochastic Gradient Descent(47876): loss=10.205600545510617\n",
      "Stochastic Gradient Descent(47877): loss=5.5612044328765435\n",
      "Stochastic Gradient Descent(47878): loss=1.6438016982008221\n",
      "Stochastic Gradient Descent(47879): loss=0.0004860308663406723\n",
      "Stochastic Gradient Descent(47880): loss=26.91089150699312\n",
      "Stochastic Gradient Descent(47881): loss=0.8675289712172598\n",
      "Stochastic Gradient Descent(47882): loss=0.6081614702465022\n",
      "Stochastic Gradient Descent(47883): loss=8.064811040527408\n",
      "Stochastic Gradient Descent(47884): loss=0.021204180373286877\n",
      "Stochastic Gradient Descent(47885): loss=0.30290642363895165\n",
      "Stochastic Gradient Descent(47886): loss=4.550740292046732\n",
      "Stochastic Gradient Descent(47887): loss=1.3903571554311382\n",
      "Stochastic Gradient Descent(47888): loss=3.5609576556973117\n",
      "Stochastic Gradient Descent(47889): loss=0.8501686486916544\n",
      "Stochastic Gradient Descent(47890): loss=0.11986955901098582\n",
      "Stochastic Gradient Descent(47891): loss=8.6647450870457\n",
      "Stochastic Gradient Descent(47892): loss=5.921726048048489\n",
      "Stochastic Gradient Descent(47893): loss=0.7817428268133931\n",
      "Stochastic Gradient Descent(47894): loss=12.006084324224508\n",
      "Stochastic Gradient Descent(47895): loss=4.3869228576653745\n",
      "Stochastic Gradient Descent(47896): loss=0.010953109533683206\n",
      "Stochastic Gradient Descent(47897): loss=2.307274911336316\n",
      "Stochastic Gradient Descent(47898): loss=0.22704983616782135\n",
      "Stochastic Gradient Descent(47899): loss=0.7676222837150534\n",
      "Stochastic Gradient Descent(47900): loss=0.0004617781690374294\n",
      "Stochastic Gradient Descent(47901): loss=0.42092298266257555\n",
      "Stochastic Gradient Descent(47902): loss=0.2879689971255376\n",
      "Stochastic Gradient Descent(47903): loss=9.029651577140163\n",
      "Stochastic Gradient Descent(47904): loss=0.0025271996819754977\n",
      "Stochastic Gradient Descent(47905): loss=2.794916659666891\n",
      "Stochastic Gradient Descent(47906): loss=1.904674185234579\n",
      "Stochastic Gradient Descent(47907): loss=0.15401401305075837\n",
      "Stochastic Gradient Descent(47908): loss=0.002361734849848738\n",
      "Stochastic Gradient Descent(47909): loss=0.0007013080179969666\n",
      "Stochastic Gradient Descent(47910): loss=1.486153122735282\n",
      "Stochastic Gradient Descent(47911): loss=0.7625484016510308\n",
      "Stochastic Gradient Descent(47912): loss=1.366038508520855\n",
      "Stochastic Gradient Descent(47913): loss=2.2684717835768544\n",
      "Stochastic Gradient Descent(47914): loss=0.8658006720946608\n",
      "Stochastic Gradient Descent(47915): loss=8.029722666237927\n",
      "Stochastic Gradient Descent(47916): loss=0.09528774407662846\n",
      "Stochastic Gradient Descent(47917): loss=0.08301254001613131\n",
      "Stochastic Gradient Descent(47918): loss=0.003669299381827512\n",
      "Stochastic Gradient Descent(47919): loss=0.7264312037981627\n",
      "Stochastic Gradient Descent(47920): loss=0.04425643130323075\n",
      "Stochastic Gradient Descent(47921): loss=3.2946061755756775\n",
      "Stochastic Gradient Descent(47922): loss=0.08792440305526436\n",
      "Stochastic Gradient Descent(47923): loss=2.346688944786977\n",
      "Stochastic Gradient Descent(47924): loss=175.3749339296815\n",
      "Stochastic Gradient Descent(47925): loss=89.64315813397282\n",
      "Stochastic Gradient Descent(47926): loss=607.990893804868\n",
      "Stochastic Gradient Descent(47927): loss=0.25859681322790423\n",
      "Stochastic Gradient Descent(47928): loss=0.026591252653810557\n",
      "Stochastic Gradient Descent(47929): loss=2.091669558514646\n",
      "Stochastic Gradient Descent(47930): loss=4.264281626248335\n",
      "Stochastic Gradient Descent(47931): loss=4.381011700917471\n",
      "Stochastic Gradient Descent(47932): loss=0.3789477480733339\n",
      "Stochastic Gradient Descent(47933): loss=7.652275043078708\n",
      "Stochastic Gradient Descent(47934): loss=5.406740721135269\n",
      "Stochastic Gradient Descent(47935): loss=2.93304239656595\n",
      "Stochastic Gradient Descent(47936): loss=0.461021698408886\n",
      "Stochastic Gradient Descent(47937): loss=3.1851259709566713\n",
      "Stochastic Gradient Descent(47938): loss=1.1826354079611607\n",
      "Stochastic Gradient Descent(47939): loss=0.6327031277034961\n",
      "Stochastic Gradient Descent(47940): loss=0.14471518882880033\n",
      "Stochastic Gradient Descent(47941): loss=1.2243936768607306\n",
      "Stochastic Gradient Descent(47942): loss=1.090829768288403\n",
      "Stochastic Gradient Descent(47943): loss=0.09895753778552106\n",
      "Stochastic Gradient Descent(47944): loss=4.495518661985561\n",
      "Stochastic Gradient Descent(47945): loss=10.458322789584052\n",
      "Stochastic Gradient Descent(47946): loss=1.699517420607012\n",
      "Stochastic Gradient Descent(47947): loss=0.2665894926889442\n",
      "Stochastic Gradient Descent(47948): loss=0.4949484808873543\n",
      "Stochastic Gradient Descent(47949): loss=1.606834287443186\n",
      "Stochastic Gradient Descent(47950): loss=4.757321567354954\n",
      "Stochastic Gradient Descent(47951): loss=2.0501073148121893\n",
      "Stochastic Gradient Descent(47952): loss=4.116701029678284\n",
      "Stochastic Gradient Descent(47953): loss=0.2618667097321587\n",
      "Stochastic Gradient Descent(47954): loss=3.9801699466605323\n",
      "Stochastic Gradient Descent(47955): loss=7.259166163230275\n",
      "Stochastic Gradient Descent(47956): loss=4.259964831609712\n",
      "Stochastic Gradient Descent(47957): loss=2.129971710544072\n",
      "Stochastic Gradient Descent(47958): loss=36.6979608262765\n",
      "Stochastic Gradient Descent(47959): loss=2.5337622908438\n",
      "Stochastic Gradient Descent(47960): loss=4.374184401654625\n",
      "Stochastic Gradient Descent(47961): loss=0.6060001004110832\n",
      "Stochastic Gradient Descent(47962): loss=1.1075586568406641\n",
      "Stochastic Gradient Descent(47963): loss=0.2744372559827731\n",
      "Stochastic Gradient Descent(47964): loss=10.603958075715719\n",
      "Stochastic Gradient Descent(47965): loss=0.5638814327841788\n",
      "Stochastic Gradient Descent(47966): loss=0.14871444771486864\n",
      "Stochastic Gradient Descent(47967): loss=0.19205034904548116\n",
      "Stochastic Gradient Descent(47968): loss=2.009585065656012\n",
      "Stochastic Gradient Descent(47969): loss=3.6816592609470185\n",
      "Stochastic Gradient Descent(47970): loss=0.9307465652229153\n",
      "Stochastic Gradient Descent(47971): loss=0.42894832453083576\n",
      "Stochastic Gradient Descent(47972): loss=0.6003714790441284\n",
      "Stochastic Gradient Descent(47973): loss=7.768800697288378\n",
      "Stochastic Gradient Descent(47974): loss=3.799626682265315\n",
      "Stochastic Gradient Descent(47975): loss=0.2039607183261826\n",
      "Stochastic Gradient Descent(47976): loss=0.6324832434583213\n",
      "Stochastic Gradient Descent(47977): loss=0.2309675588209124\n",
      "Stochastic Gradient Descent(47978): loss=0.0028913501065975936\n",
      "Stochastic Gradient Descent(47979): loss=5.2164816846852045\n",
      "Stochastic Gradient Descent(47980): loss=4.479397559760034\n",
      "Stochastic Gradient Descent(47981): loss=5.021352039581899\n",
      "Stochastic Gradient Descent(47982): loss=6.065889091055488\n",
      "Stochastic Gradient Descent(47983): loss=6.72635213107735\n",
      "Stochastic Gradient Descent(47984): loss=7.831890671022013\n",
      "Stochastic Gradient Descent(47985): loss=0.022376094280928738\n",
      "Stochastic Gradient Descent(47986): loss=0.04445423702287833\n",
      "Stochastic Gradient Descent(47987): loss=0.6820375519258498\n",
      "Stochastic Gradient Descent(47988): loss=3.6379370685683545\n",
      "Stochastic Gradient Descent(47989): loss=20.949247811368213\n",
      "Stochastic Gradient Descent(47990): loss=1.864509838269376\n",
      "Stochastic Gradient Descent(47991): loss=4.657222208284529\n",
      "Stochastic Gradient Descent(47992): loss=6.72302400318527\n",
      "Stochastic Gradient Descent(47993): loss=6.222838651498661\n",
      "Stochastic Gradient Descent(47994): loss=1.9558710150030563\n",
      "Stochastic Gradient Descent(47995): loss=7.105566915528939\n",
      "Stochastic Gradient Descent(47996): loss=3.679167828718692\n",
      "Stochastic Gradient Descent(47997): loss=3.7883084402158835\n",
      "Stochastic Gradient Descent(47998): loss=0.841720395224643\n",
      "Stochastic Gradient Descent(47999): loss=5.235807468298177\n",
      "Stochastic Gradient Descent(48000): loss=0.33015849516960755\n",
      "Stochastic Gradient Descent(48001): loss=0.06654519904989084\n",
      "Stochastic Gradient Descent(48002): loss=2.939670179218328\n",
      "Stochastic Gradient Descent(48003): loss=0.708730295575793\n",
      "Stochastic Gradient Descent(48004): loss=0.0020427400102363473\n",
      "Stochastic Gradient Descent(48005): loss=3.1070641506585024\n",
      "Stochastic Gradient Descent(48006): loss=11.65321261047118\n",
      "Stochastic Gradient Descent(48007): loss=1.9955787248131331\n",
      "Stochastic Gradient Descent(48008): loss=9.562922951729856\n",
      "Stochastic Gradient Descent(48009): loss=16.355183205319804\n",
      "Stochastic Gradient Descent(48010): loss=0.02111307406533192\n",
      "Stochastic Gradient Descent(48011): loss=3.920840500710651\n",
      "Stochastic Gradient Descent(48012): loss=10.735913390709872\n",
      "Stochastic Gradient Descent(48013): loss=0.09619803362669381\n",
      "Stochastic Gradient Descent(48014): loss=0.7317700259961826\n",
      "Stochastic Gradient Descent(48015): loss=0.03624948997947571\n",
      "Stochastic Gradient Descent(48016): loss=3.366909982986458\n",
      "Stochastic Gradient Descent(48017): loss=0.4322148953146115\n",
      "Stochastic Gradient Descent(48018): loss=0.058537604991696905\n",
      "Stochastic Gradient Descent(48019): loss=0.14298434637868726\n",
      "Stochastic Gradient Descent(48020): loss=2.3827929120983993\n",
      "Stochastic Gradient Descent(48021): loss=2.8906874475635984\n",
      "Stochastic Gradient Descent(48022): loss=0.4702598039986128\n",
      "Stochastic Gradient Descent(48023): loss=0.03072239950914872\n",
      "Stochastic Gradient Descent(48024): loss=25.63857582591796\n",
      "Stochastic Gradient Descent(48025): loss=1.363444291436127\n",
      "Stochastic Gradient Descent(48026): loss=0.7800779577395325\n",
      "Stochastic Gradient Descent(48027): loss=0.5399005985386719\n",
      "Stochastic Gradient Descent(48028): loss=0.22591633338174388\n",
      "Stochastic Gradient Descent(48029): loss=1.2012078464650304\n",
      "Stochastic Gradient Descent(48030): loss=0.5610025154066166\n",
      "Stochastic Gradient Descent(48031): loss=2.7726865286692908\n",
      "Stochastic Gradient Descent(48032): loss=1.0365683520413609\n",
      "Stochastic Gradient Descent(48033): loss=7.356617829117846\n",
      "Stochastic Gradient Descent(48034): loss=3.190100630561097\n",
      "Stochastic Gradient Descent(48035): loss=7.0538102165972605\n",
      "Stochastic Gradient Descent(48036): loss=1.0251603932430315\n",
      "Stochastic Gradient Descent(48037): loss=2.824708199947081\n",
      "Stochastic Gradient Descent(48038): loss=3.0660951057803323\n",
      "Stochastic Gradient Descent(48039): loss=16.553749453497858\n",
      "Stochastic Gradient Descent(48040): loss=0.41654540352970526\n",
      "Stochastic Gradient Descent(48041): loss=0.6211039036282849\n",
      "Stochastic Gradient Descent(48042): loss=0.16891777936001914\n",
      "Stochastic Gradient Descent(48043): loss=1.5491558111442079\n",
      "Stochastic Gradient Descent(48044): loss=0.013457834766986542\n",
      "Stochastic Gradient Descent(48045): loss=0.1428468810052392\n",
      "Stochastic Gradient Descent(48046): loss=0.14331633229722987\n",
      "Stochastic Gradient Descent(48047): loss=8.874560131571068\n",
      "Stochastic Gradient Descent(48048): loss=0.030645966214269918\n",
      "Stochastic Gradient Descent(48049): loss=11.622476884480582\n",
      "Stochastic Gradient Descent(48050): loss=3.0999794810306276\n",
      "Stochastic Gradient Descent(48051): loss=3.4468720185950263\n",
      "Stochastic Gradient Descent(48052): loss=1.5803730546396972\n",
      "Stochastic Gradient Descent(48053): loss=3.805347813130872\n",
      "Stochastic Gradient Descent(48054): loss=16.69151217442541\n",
      "Stochastic Gradient Descent(48055): loss=4.84808553679069\n",
      "Stochastic Gradient Descent(48056): loss=12.737719137772574\n",
      "Stochastic Gradient Descent(48057): loss=2.8706791457852003\n",
      "Stochastic Gradient Descent(48058): loss=11.482641088432585\n",
      "Stochastic Gradient Descent(48059): loss=14.856454721793138\n",
      "Stochastic Gradient Descent(48060): loss=0.5506975629595052\n",
      "Stochastic Gradient Descent(48061): loss=5.250108088261542\n",
      "Stochastic Gradient Descent(48062): loss=1.2313151209707167\n",
      "Stochastic Gradient Descent(48063): loss=1.2937921790661617\n",
      "Stochastic Gradient Descent(48064): loss=26.412557775753264\n",
      "Stochastic Gradient Descent(48065): loss=0.04906560853135675\n",
      "Stochastic Gradient Descent(48066): loss=1.1898601710574308\n",
      "Stochastic Gradient Descent(48067): loss=5.423829305671979\n",
      "Stochastic Gradient Descent(48068): loss=4.337609479645165\n",
      "Stochastic Gradient Descent(48069): loss=6.828153976929764\n",
      "Stochastic Gradient Descent(48070): loss=5.318432878036548\n",
      "Stochastic Gradient Descent(48071): loss=0.7223255007284307\n",
      "Stochastic Gradient Descent(48072): loss=13.289273547221157\n",
      "Stochastic Gradient Descent(48073): loss=0.270281113548632\n",
      "Stochastic Gradient Descent(48074): loss=0.17744848379461184\n",
      "Stochastic Gradient Descent(48075): loss=13.730275019891215\n",
      "Stochastic Gradient Descent(48076): loss=6.387795057242634\n",
      "Stochastic Gradient Descent(48077): loss=4.330132401110788\n",
      "Stochastic Gradient Descent(48078): loss=3.7612200108063645\n",
      "Stochastic Gradient Descent(48079): loss=0.05988798412182944\n",
      "Stochastic Gradient Descent(48080): loss=3.478495239751041\n",
      "Stochastic Gradient Descent(48081): loss=0.6516836086781087\n",
      "Stochastic Gradient Descent(48082): loss=0.7112171915446672\n",
      "Stochastic Gradient Descent(48083): loss=51.563715868667856\n",
      "Stochastic Gradient Descent(48084): loss=7.928735577359882\n",
      "Stochastic Gradient Descent(48085): loss=0.24402446801999655\n",
      "Stochastic Gradient Descent(48086): loss=0.5035089547156631\n",
      "Stochastic Gradient Descent(48087): loss=5.09549445462499\n",
      "Stochastic Gradient Descent(48088): loss=14.47228421349549\n",
      "Stochastic Gradient Descent(48089): loss=8.246069868981438\n",
      "Stochastic Gradient Descent(48090): loss=0.06815683133714295\n",
      "Stochastic Gradient Descent(48091): loss=0.30987381395677954\n",
      "Stochastic Gradient Descent(48092): loss=0.0009043908834040162\n",
      "Stochastic Gradient Descent(48093): loss=1.5270721562151444\n",
      "Stochastic Gradient Descent(48094): loss=2.4435665557073887\n",
      "Stochastic Gradient Descent(48095): loss=0.017064717877488096\n",
      "Stochastic Gradient Descent(48096): loss=9.62611069994853\n",
      "Stochastic Gradient Descent(48097): loss=0.04393852995216815\n",
      "Stochastic Gradient Descent(48098): loss=4.355511797444823\n",
      "Stochastic Gradient Descent(48099): loss=1.1077721514030043\n",
      "Stochastic Gradient Descent(48100): loss=2.1149651799666245\n",
      "Stochastic Gradient Descent(48101): loss=0.33538264994316497\n",
      "Stochastic Gradient Descent(48102): loss=0.15220796342564005\n",
      "Stochastic Gradient Descent(48103): loss=4.9263706618514576\n",
      "Stochastic Gradient Descent(48104): loss=0.010638168838288612\n",
      "Stochastic Gradient Descent(48105): loss=1.3892999724441684\n",
      "Stochastic Gradient Descent(48106): loss=0.07769502564904937\n",
      "Stochastic Gradient Descent(48107): loss=0.34626712302212503\n",
      "Stochastic Gradient Descent(48108): loss=3.013403093108812\n",
      "Stochastic Gradient Descent(48109): loss=0.20180217246679905\n",
      "Stochastic Gradient Descent(48110): loss=9.723974606324902\n",
      "Stochastic Gradient Descent(48111): loss=7.12494352138069\n",
      "Stochastic Gradient Descent(48112): loss=5.621140030270774\n",
      "Stochastic Gradient Descent(48113): loss=0.9417581017347005\n",
      "Stochastic Gradient Descent(48114): loss=6.058450868651187\n",
      "Stochastic Gradient Descent(48115): loss=7.019198172608677\n",
      "Stochastic Gradient Descent(48116): loss=0.4103543426189634\n",
      "Stochastic Gradient Descent(48117): loss=7.620953584643341\n",
      "Stochastic Gradient Descent(48118): loss=0.0012664143608471698\n",
      "Stochastic Gradient Descent(48119): loss=1.9010906802046557\n",
      "Stochastic Gradient Descent(48120): loss=1.2484116354876407\n",
      "Stochastic Gradient Descent(48121): loss=15.969775508925325\n",
      "Stochastic Gradient Descent(48122): loss=0.2634977369834968\n",
      "Stochastic Gradient Descent(48123): loss=2.3548972391007004\n",
      "Stochastic Gradient Descent(48124): loss=0.11039792387071168\n",
      "Stochastic Gradient Descent(48125): loss=1.802473913605579\n",
      "Stochastic Gradient Descent(48126): loss=0.9445495473766319\n",
      "Stochastic Gradient Descent(48127): loss=19.06128411733748\n",
      "Stochastic Gradient Descent(48128): loss=3.8328849175187187\n",
      "Stochastic Gradient Descent(48129): loss=0.7857142698157598\n",
      "Stochastic Gradient Descent(48130): loss=0.25376405810527514\n",
      "Stochastic Gradient Descent(48131): loss=0.06931161162099211\n",
      "Stochastic Gradient Descent(48132): loss=2.8303058934612935e-07\n",
      "Stochastic Gradient Descent(48133): loss=3.764286307124372\n",
      "Stochastic Gradient Descent(48134): loss=2.2600307070640633\n",
      "Stochastic Gradient Descent(48135): loss=0.3269548021317949\n",
      "Stochastic Gradient Descent(48136): loss=4.508993509442469\n",
      "Stochastic Gradient Descent(48137): loss=2.065885159525351\n",
      "Stochastic Gradient Descent(48138): loss=8.904949726141703\n",
      "Stochastic Gradient Descent(48139): loss=0.04199655593104163\n",
      "Stochastic Gradient Descent(48140): loss=3.4787838607523187\n",
      "Stochastic Gradient Descent(48141): loss=1.827111209034223\n",
      "Stochastic Gradient Descent(48142): loss=0.3949393059802051\n",
      "Stochastic Gradient Descent(48143): loss=0.023362123956596335\n",
      "Stochastic Gradient Descent(48144): loss=0.0487825254563231\n",
      "Stochastic Gradient Descent(48145): loss=3.219187777655161\n",
      "Stochastic Gradient Descent(48146): loss=0.4538616130770118\n",
      "Stochastic Gradient Descent(48147): loss=0.22236955452644633\n",
      "Stochastic Gradient Descent(48148): loss=6.024722293384659\n",
      "Stochastic Gradient Descent(48149): loss=1.75223438363349e-05\n",
      "Stochastic Gradient Descent(48150): loss=4.369150214100617\n",
      "Stochastic Gradient Descent(48151): loss=1.5302158723239623\n",
      "Stochastic Gradient Descent(48152): loss=21.78918766858182\n",
      "Stochastic Gradient Descent(48153): loss=0.4351613495343591\n",
      "Stochastic Gradient Descent(48154): loss=0.443136442987129\n",
      "Stochastic Gradient Descent(48155): loss=4.863927385890944\n",
      "Stochastic Gradient Descent(48156): loss=22.6856986989772\n",
      "Stochastic Gradient Descent(48157): loss=5.42868579631767\n",
      "Stochastic Gradient Descent(48158): loss=6.728758561294204\n",
      "Stochastic Gradient Descent(48159): loss=0.04393865903274116\n",
      "Stochastic Gradient Descent(48160): loss=0.6202533203513148\n",
      "Stochastic Gradient Descent(48161): loss=5.932014782401622\n",
      "Stochastic Gradient Descent(48162): loss=5.900685189455028\n",
      "Stochastic Gradient Descent(48163): loss=0.21446881753880367\n",
      "Stochastic Gradient Descent(48164): loss=5.072946375617223\n",
      "Stochastic Gradient Descent(48165): loss=1.2335535651490064\n",
      "Stochastic Gradient Descent(48166): loss=3.162477199381181\n",
      "Stochastic Gradient Descent(48167): loss=2.1778179658621175\n",
      "Stochastic Gradient Descent(48168): loss=0.1440783670835513\n",
      "Stochastic Gradient Descent(48169): loss=7.538116143131312\n",
      "Stochastic Gradient Descent(48170): loss=0.008080346996261704\n",
      "Stochastic Gradient Descent(48171): loss=4.442430643250635\n",
      "Stochastic Gradient Descent(48172): loss=0.020603545805855508\n",
      "Stochastic Gradient Descent(48173): loss=0.08305418075141924\n",
      "Stochastic Gradient Descent(48174): loss=4.96919988266742\n",
      "Stochastic Gradient Descent(48175): loss=0.10041765245226626\n",
      "Stochastic Gradient Descent(48176): loss=0.7646898569674725\n",
      "Stochastic Gradient Descent(48177): loss=0.014186540059655313\n",
      "Stochastic Gradient Descent(48178): loss=5.017066108916514\n",
      "Stochastic Gradient Descent(48179): loss=2.5083828696286354\n",
      "Stochastic Gradient Descent(48180): loss=5.870537654542975\n",
      "Stochastic Gradient Descent(48181): loss=3.307796918286973\n",
      "Stochastic Gradient Descent(48182): loss=0.00032270184917118216\n",
      "Stochastic Gradient Descent(48183): loss=0.9288809774124365\n",
      "Stochastic Gradient Descent(48184): loss=1.8408793064212787\n",
      "Stochastic Gradient Descent(48185): loss=0.751414486586206\n",
      "Stochastic Gradient Descent(48186): loss=1.3000445064370503\n",
      "Stochastic Gradient Descent(48187): loss=1.703820293715295\n",
      "Stochastic Gradient Descent(48188): loss=27.58108751952098\n",
      "Stochastic Gradient Descent(48189): loss=1.9700762292253156\n",
      "Stochastic Gradient Descent(48190): loss=18.281468839709834\n",
      "Stochastic Gradient Descent(48191): loss=2.1336126182977417\n",
      "Stochastic Gradient Descent(48192): loss=0.15099019678830303\n",
      "Stochastic Gradient Descent(48193): loss=9.225519901581924\n",
      "Stochastic Gradient Descent(48194): loss=3.4426276744042354\n",
      "Stochastic Gradient Descent(48195): loss=9.721263339610084\n",
      "Stochastic Gradient Descent(48196): loss=0.08735255881511293\n",
      "Stochastic Gradient Descent(48197): loss=0.2597257033250036\n",
      "Stochastic Gradient Descent(48198): loss=4.704297649620513\n",
      "Stochastic Gradient Descent(48199): loss=3.9254272892250652\n",
      "Stochastic Gradient Descent(48200): loss=6.108827975336393\n",
      "Stochastic Gradient Descent(48201): loss=0.032835867481546516\n",
      "Stochastic Gradient Descent(48202): loss=0.25800453451093663\n",
      "Stochastic Gradient Descent(48203): loss=0.6491930058698681\n",
      "Stochastic Gradient Descent(48204): loss=0.16167299876280378\n",
      "Stochastic Gradient Descent(48205): loss=3.847839694352677\n",
      "Stochastic Gradient Descent(48206): loss=0.00423232408558174\n",
      "Stochastic Gradient Descent(48207): loss=0.046678857570646506\n",
      "Stochastic Gradient Descent(48208): loss=0.7805470986843978\n",
      "Stochastic Gradient Descent(48209): loss=1.9451352244163607\n",
      "Stochastic Gradient Descent(48210): loss=0.8631280217976435\n",
      "Stochastic Gradient Descent(48211): loss=0.922295404111136\n",
      "Stochastic Gradient Descent(48212): loss=3.9253448688713064\n",
      "Stochastic Gradient Descent(48213): loss=3.5245615818154796\n",
      "Stochastic Gradient Descent(48214): loss=0.2990738578239931\n",
      "Stochastic Gradient Descent(48215): loss=0.6465041517003471\n",
      "Stochastic Gradient Descent(48216): loss=0.741013605282866\n",
      "Stochastic Gradient Descent(48217): loss=2.0584245255262483\n",
      "Stochastic Gradient Descent(48218): loss=3.1391493215169537\n",
      "Stochastic Gradient Descent(48219): loss=1.7218316658647534\n",
      "Stochastic Gradient Descent(48220): loss=0.0003321958682011018\n",
      "Stochastic Gradient Descent(48221): loss=5.207713345088147\n",
      "Stochastic Gradient Descent(48222): loss=11.658126595972012\n",
      "Stochastic Gradient Descent(48223): loss=14.552581535057147\n",
      "Stochastic Gradient Descent(48224): loss=9.061074893212256\n",
      "Stochastic Gradient Descent(48225): loss=0.04736018066370079\n",
      "Stochastic Gradient Descent(48226): loss=0.2678571161183923\n",
      "Stochastic Gradient Descent(48227): loss=0.02822286267706365\n",
      "Stochastic Gradient Descent(48228): loss=0.30213787428897987\n",
      "Stochastic Gradient Descent(48229): loss=5.104667398536671\n",
      "Stochastic Gradient Descent(48230): loss=3.7328767080394725\n",
      "Stochastic Gradient Descent(48231): loss=4.204144250435943\n",
      "Stochastic Gradient Descent(48232): loss=1.0030737075885485\n",
      "Stochastic Gradient Descent(48233): loss=0.07913715978733334\n",
      "Stochastic Gradient Descent(48234): loss=4.125924408896148\n",
      "Stochastic Gradient Descent(48235): loss=7.625035955387782\n",
      "Stochastic Gradient Descent(48236): loss=1.042212225500871\n",
      "Stochastic Gradient Descent(48237): loss=0.7399819967925094\n",
      "Stochastic Gradient Descent(48238): loss=0.00573779899289211\n",
      "Stochastic Gradient Descent(48239): loss=0.28863419410713137\n",
      "Stochastic Gradient Descent(48240): loss=9.123606133474102\n",
      "Stochastic Gradient Descent(48241): loss=0.16675953347283634\n",
      "Stochastic Gradient Descent(48242): loss=0.2689584895296643\n",
      "Stochastic Gradient Descent(48243): loss=0.9271004208804406\n",
      "Stochastic Gradient Descent(48244): loss=0.3162396042648529\n",
      "Stochastic Gradient Descent(48245): loss=3.1613057633035675\n",
      "Stochastic Gradient Descent(48246): loss=15.077780295412673\n",
      "Stochastic Gradient Descent(48247): loss=3.473610638023143\n",
      "Stochastic Gradient Descent(48248): loss=0.362625826841003\n",
      "Stochastic Gradient Descent(48249): loss=0.022167051793409377\n",
      "Stochastic Gradient Descent(48250): loss=1.124949347114747\n",
      "Stochastic Gradient Descent(48251): loss=0.9248662423073504\n",
      "Stochastic Gradient Descent(48252): loss=0.24770234117138434\n",
      "Stochastic Gradient Descent(48253): loss=4.261758278937107\n",
      "Stochastic Gradient Descent(48254): loss=0.8193428929442773\n",
      "Stochastic Gradient Descent(48255): loss=1.846772223788774\n",
      "Stochastic Gradient Descent(48256): loss=7.2589142431557825\n",
      "Stochastic Gradient Descent(48257): loss=2.0811293890785234\n",
      "Stochastic Gradient Descent(48258): loss=2.251500816230559\n",
      "Stochastic Gradient Descent(48259): loss=3.3401304492841972\n",
      "Stochastic Gradient Descent(48260): loss=2.237195597160826\n",
      "Stochastic Gradient Descent(48261): loss=0.5215290355833783\n",
      "Stochastic Gradient Descent(48262): loss=0.5520427997432765\n",
      "Stochastic Gradient Descent(48263): loss=7.046034317796135\n",
      "Stochastic Gradient Descent(48264): loss=0.6001603615606921\n",
      "Stochastic Gradient Descent(48265): loss=0.7248231110725054\n",
      "Stochastic Gradient Descent(48266): loss=0.09949901800186799\n",
      "Stochastic Gradient Descent(48267): loss=11.79923237033042\n",
      "Stochastic Gradient Descent(48268): loss=2.4088616415396786\n",
      "Stochastic Gradient Descent(48269): loss=12.608285834337932\n",
      "Stochastic Gradient Descent(48270): loss=3.394856940554521\n",
      "Stochastic Gradient Descent(48271): loss=0.00043852974111448597\n",
      "Stochastic Gradient Descent(48272): loss=1.041522752349339\n",
      "Stochastic Gradient Descent(48273): loss=0.057809508045214286\n",
      "Stochastic Gradient Descent(48274): loss=0.7317019084893167\n",
      "Stochastic Gradient Descent(48275): loss=1.2759197113226135\n",
      "Stochastic Gradient Descent(48276): loss=0.6367678764407332\n",
      "Stochastic Gradient Descent(48277): loss=6.0515906669869315\n",
      "Stochastic Gradient Descent(48278): loss=0.5534956788099022\n",
      "Stochastic Gradient Descent(48279): loss=0.044882377121772704\n",
      "Stochastic Gradient Descent(48280): loss=4.87107771836718\n",
      "Stochastic Gradient Descent(48281): loss=4.488808930068721\n",
      "Stochastic Gradient Descent(48282): loss=2.941920630609197\n",
      "Stochastic Gradient Descent(48283): loss=0.7798720752109068\n",
      "Stochastic Gradient Descent(48284): loss=3.696879262448338\n",
      "Stochastic Gradient Descent(48285): loss=0.04177950809466786\n",
      "Stochastic Gradient Descent(48286): loss=0.06516434135236027\n",
      "Stochastic Gradient Descent(48287): loss=1.32226345685894\n",
      "Stochastic Gradient Descent(48288): loss=3.9334204084920783\n",
      "Stochastic Gradient Descent(48289): loss=0.0027796787509284755\n",
      "Stochastic Gradient Descent(48290): loss=0.07676814405243444\n",
      "Stochastic Gradient Descent(48291): loss=2.652214670085073\n",
      "Stochastic Gradient Descent(48292): loss=2.856239523447004\n",
      "Stochastic Gradient Descent(48293): loss=0.08058293486246262\n",
      "Stochastic Gradient Descent(48294): loss=3.068887271309579\n",
      "Stochastic Gradient Descent(48295): loss=0.6802929714017024\n",
      "Stochastic Gradient Descent(48296): loss=5.010546619869633\n",
      "Stochastic Gradient Descent(48297): loss=0.8624639360292822\n",
      "Stochastic Gradient Descent(48298): loss=3.8493247610393575\n",
      "Stochastic Gradient Descent(48299): loss=0.12143759104908182\n",
      "Stochastic Gradient Descent(48300): loss=2.312440635785477\n",
      "Stochastic Gradient Descent(48301): loss=4.308940458795059\n",
      "Stochastic Gradient Descent(48302): loss=0.5573226351697079\n",
      "Stochastic Gradient Descent(48303): loss=5.115209978316371\n",
      "Stochastic Gradient Descent(48304): loss=0.28043334723247304\n",
      "Stochastic Gradient Descent(48305): loss=0.1579731285143011\n",
      "Stochastic Gradient Descent(48306): loss=0.6531775653796255\n",
      "Stochastic Gradient Descent(48307): loss=2.7053137010856005\n",
      "Stochastic Gradient Descent(48308): loss=0.3909698812301041\n",
      "Stochastic Gradient Descent(48309): loss=0.19411379687936897\n",
      "Stochastic Gradient Descent(48310): loss=0.18323305858397526\n",
      "Stochastic Gradient Descent(48311): loss=11.105054351961828\n",
      "Stochastic Gradient Descent(48312): loss=0.31794164147220677\n",
      "Stochastic Gradient Descent(48313): loss=1.2242519098557352\n",
      "Stochastic Gradient Descent(48314): loss=6.6210059676815725\n",
      "Stochastic Gradient Descent(48315): loss=0.8500499170336555\n",
      "Stochastic Gradient Descent(48316): loss=0.09479163161395489\n",
      "Stochastic Gradient Descent(48317): loss=2.262658385487778\n",
      "Stochastic Gradient Descent(48318): loss=0.7836256013956235\n",
      "Stochastic Gradient Descent(48319): loss=63.473903430826965\n",
      "Stochastic Gradient Descent(48320): loss=0.5329689249893749\n",
      "Stochastic Gradient Descent(48321): loss=3.7165163931252794\n",
      "Stochastic Gradient Descent(48322): loss=11.013165261230602\n",
      "Stochastic Gradient Descent(48323): loss=0.23266964342462454\n",
      "Stochastic Gradient Descent(48324): loss=2.700222171811828\n",
      "Stochastic Gradient Descent(48325): loss=4.977093269164251\n",
      "Stochastic Gradient Descent(48326): loss=39.315505445894416\n",
      "Stochastic Gradient Descent(48327): loss=5.002785992933814\n",
      "Stochastic Gradient Descent(48328): loss=4.570604941342303\n",
      "Stochastic Gradient Descent(48329): loss=0.10082174302504968\n",
      "Stochastic Gradient Descent(48330): loss=0.09655950549794086\n",
      "Stochastic Gradient Descent(48331): loss=8.640262752737625\n",
      "Stochastic Gradient Descent(48332): loss=5.186837060395012\n",
      "Stochastic Gradient Descent(48333): loss=0.28279560090292416\n",
      "Stochastic Gradient Descent(48334): loss=1.161624655388984\n",
      "Stochastic Gradient Descent(48335): loss=2.4799858147426272\n",
      "Stochastic Gradient Descent(48336): loss=0.24356451637800566\n",
      "Stochastic Gradient Descent(48337): loss=4.594248441490895\n",
      "Stochastic Gradient Descent(48338): loss=0.18029364269085643\n",
      "Stochastic Gradient Descent(48339): loss=0.04860682652407873\n",
      "Stochastic Gradient Descent(48340): loss=1.3639302018514596\n",
      "Stochastic Gradient Descent(48341): loss=0.0028656350761989656\n",
      "Stochastic Gradient Descent(48342): loss=1.4690120980175259\n",
      "Stochastic Gradient Descent(48343): loss=0.004678255630860225\n",
      "Stochastic Gradient Descent(48344): loss=0.14102424838966093\n",
      "Stochastic Gradient Descent(48345): loss=1.1746065644493626\n",
      "Stochastic Gradient Descent(48346): loss=0.04624073953765491\n",
      "Stochastic Gradient Descent(48347): loss=11.939732505539006\n",
      "Stochastic Gradient Descent(48348): loss=4.0783357456647975\n",
      "Stochastic Gradient Descent(48349): loss=0.9448362097636894\n",
      "Stochastic Gradient Descent(48350): loss=5.911770575271367\n",
      "Stochastic Gradient Descent(48351): loss=0.41650822628939804\n",
      "Stochastic Gradient Descent(48352): loss=2.9972122649127804\n",
      "Stochastic Gradient Descent(48353): loss=3.7396524258901103\n",
      "Stochastic Gradient Descent(48354): loss=0.7010550689301012\n",
      "Stochastic Gradient Descent(48355): loss=24.11816685403839\n",
      "Stochastic Gradient Descent(48356): loss=0.21681989257999953\n",
      "Stochastic Gradient Descent(48357): loss=6.78238283778262\n",
      "Stochastic Gradient Descent(48358): loss=0.1646083502557103\n",
      "Stochastic Gradient Descent(48359): loss=3.3739037349591134\n",
      "Stochastic Gradient Descent(48360): loss=3.9439489508212504\n",
      "Stochastic Gradient Descent(48361): loss=10.649082369368024\n",
      "Stochastic Gradient Descent(48362): loss=0.24272371355590708\n",
      "Stochastic Gradient Descent(48363): loss=0.006887070896370676\n",
      "Stochastic Gradient Descent(48364): loss=0.05304716171819825\n",
      "Stochastic Gradient Descent(48365): loss=7.218504989250767\n",
      "Stochastic Gradient Descent(48366): loss=0.015239711222721762\n",
      "Stochastic Gradient Descent(48367): loss=1.3063877751192081\n",
      "Stochastic Gradient Descent(48368): loss=1.3775108113892898\n",
      "Stochastic Gradient Descent(48369): loss=0.02109382539244519\n",
      "Stochastic Gradient Descent(48370): loss=1.058364702746226\n",
      "Stochastic Gradient Descent(48371): loss=1.5370134745546074\n",
      "Stochastic Gradient Descent(48372): loss=8.217735767241209\n",
      "Stochastic Gradient Descent(48373): loss=1.8123027969618812\n",
      "Stochastic Gradient Descent(48374): loss=26.626673199725463\n",
      "Stochastic Gradient Descent(48375): loss=0.7753454268053442\n",
      "Stochastic Gradient Descent(48376): loss=1.6263312298638242\n",
      "Stochastic Gradient Descent(48377): loss=6.257632629212662\n",
      "Stochastic Gradient Descent(48378): loss=21.912346571845067\n",
      "Stochastic Gradient Descent(48379): loss=25.712300286835365\n",
      "Stochastic Gradient Descent(48380): loss=3.1858795992880924\n",
      "Stochastic Gradient Descent(48381): loss=4.195824371854955\n",
      "Stochastic Gradient Descent(48382): loss=0.4909829220207268\n",
      "Stochastic Gradient Descent(48383): loss=12.37393678514234\n",
      "Stochastic Gradient Descent(48384): loss=1.0677985332242064\n",
      "Stochastic Gradient Descent(48385): loss=0.22358157350771837\n",
      "Stochastic Gradient Descent(48386): loss=4.182849679100248\n",
      "Stochastic Gradient Descent(48387): loss=1.0021149096014026\n",
      "Stochastic Gradient Descent(48388): loss=0.8561662143071077\n",
      "Stochastic Gradient Descent(48389): loss=3.7229453520554627\n",
      "Stochastic Gradient Descent(48390): loss=1.0110840012034714\n",
      "Stochastic Gradient Descent(48391): loss=0.6309297193497188\n",
      "Stochastic Gradient Descent(48392): loss=0.38335933067154176\n",
      "Stochastic Gradient Descent(48393): loss=4.342246031494353\n",
      "Stochastic Gradient Descent(48394): loss=9.298256896158893\n",
      "Stochastic Gradient Descent(48395): loss=0.026872425656233764\n",
      "Stochastic Gradient Descent(48396): loss=10.728830407845217\n",
      "Stochastic Gradient Descent(48397): loss=0.32773207267507604\n",
      "Stochastic Gradient Descent(48398): loss=0.06875279096390065\n",
      "Stochastic Gradient Descent(48399): loss=0.8020902672080368\n",
      "Stochastic Gradient Descent(48400): loss=4.161016287016527\n",
      "Stochastic Gradient Descent(48401): loss=4.61827333944653\n",
      "Stochastic Gradient Descent(48402): loss=0.5714569432792543\n",
      "Stochastic Gradient Descent(48403): loss=7.5138470041083645\n",
      "Stochastic Gradient Descent(48404): loss=8.305916815240238\n",
      "Stochastic Gradient Descent(48405): loss=6.271545199526615\n",
      "Stochastic Gradient Descent(48406): loss=0.6917056867356572\n",
      "Stochastic Gradient Descent(48407): loss=1.0589391229665444\n",
      "Stochastic Gradient Descent(48408): loss=6.6256716319230655\n",
      "Stochastic Gradient Descent(48409): loss=6.9273913791279735\n",
      "Stochastic Gradient Descent(48410): loss=22.739816285669054\n",
      "Stochastic Gradient Descent(48411): loss=2.288099288135204\n",
      "Stochastic Gradient Descent(48412): loss=3.012057298496387\n",
      "Stochastic Gradient Descent(48413): loss=6.456583063613533\n",
      "Stochastic Gradient Descent(48414): loss=2.153401405936023\n",
      "Stochastic Gradient Descent(48415): loss=0.03163383527761501\n",
      "Stochastic Gradient Descent(48416): loss=2.8158331637908853\n",
      "Stochastic Gradient Descent(48417): loss=0.3585033386020849\n",
      "Stochastic Gradient Descent(48418): loss=0.7558114186230506\n",
      "Stochastic Gradient Descent(48419): loss=0.45420277507591456\n",
      "Stochastic Gradient Descent(48420): loss=4.410110271034177\n",
      "Stochastic Gradient Descent(48421): loss=19.871761263898946\n",
      "Stochastic Gradient Descent(48422): loss=1.41281252404597\n",
      "Stochastic Gradient Descent(48423): loss=15.552526144775166\n",
      "Stochastic Gradient Descent(48424): loss=0.0005852973299392495\n",
      "Stochastic Gradient Descent(48425): loss=3.6632400589834955\n",
      "Stochastic Gradient Descent(48426): loss=0.3852200724658345\n",
      "Stochastic Gradient Descent(48427): loss=16.779689733839973\n",
      "Stochastic Gradient Descent(48428): loss=6.884996355453304\n",
      "Stochastic Gradient Descent(48429): loss=0.33114771705285795\n",
      "Stochastic Gradient Descent(48430): loss=13.12331634900031\n",
      "Stochastic Gradient Descent(48431): loss=4.018574380537227\n",
      "Stochastic Gradient Descent(48432): loss=213.80924510573698\n",
      "Stochastic Gradient Descent(48433): loss=18.1161405283211\n",
      "Stochastic Gradient Descent(48434): loss=4.167525724312485\n",
      "Stochastic Gradient Descent(48435): loss=12.480599226429959\n",
      "Stochastic Gradient Descent(48436): loss=73.75162063911024\n",
      "Stochastic Gradient Descent(48437): loss=10.493502719893904\n",
      "Stochastic Gradient Descent(48438): loss=4.1269077036548305\n",
      "Stochastic Gradient Descent(48439): loss=0.20274020165030002\n",
      "Stochastic Gradient Descent(48440): loss=3.202432657450771\n",
      "Stochastic Gradient Descent(48441): loss=0.9173842456215464\n",
      "Stochastic Gradient Descent(48442): loss=0.5724099679656396\n",
      "Stochastic Gradient Descent(48443): loss=8.58815162592427\n",
      "Stochastic Gradient Descent(48444): loss=2.101822365326984\n",
      "Stochastic Gradient Descent(48445): loss=1.9472254200369563\n",
      "Stochastic Gradient Descent(48446): loss=0.2535782930662987\n",
      "Stochastic Gradient Descent(48447): loss=1.344490428474961\n",
      "Stochastic Gradient Descent(48448): loss=0.0749330602600571\n",
      "Stochastic Gradient Descent(48449): loss=5.0456043423622665\n",
      "Stochastic Gradient Descent(48450): loss=4.206862704617742\n",
      "Stochastic Gradient Descent(48451): loss=0.30588115647910336\n",
      "Stochastic Gradient Descent(48452): loss=0.05784057827501558\n",
      "Stochastic Gradient Descent(48453): loss=5.070015008407133\n",
      "Stochastic Gradient Descent(48454): loss=2.3429426343162416\n",
      "Stochastic Gradient Descent(48455): loss=4.464664064757956\n",
      "Stochastic Gradient Descent(48456): loss=0.785438348676267\n",
      "Stochastic Gradient Descent(48457): loss=0.21101054969783256\n",
      "Stochastic Gradient Descent(48458): loss=3.545624188474439\n",
      "Stochastic Gradient Descent(48459): loss=3.1088903665872003\n",
      "Stochastic Gradient Descent(48460): loss=0.0017426674944225557\n",
      "Stochastic Gradient Descent(48461): loss=0.2238990325771385\n",
      "Stochastic Gradient Descent(48462): loss=0.504610082784876\n",
      "Stochastic Gradient Descent(48463): loss=1.038571591794195\n",
      "Stochastic Gradient Descent(48464): loss=13.545434561519023\n",
      "Stochastic Gradient Descent(48465): loss=3.9120785281092996\n",
      "Stochastic Gradient Descent(48466): loss=1.364726245072524\n",
      "Stochastic Gradient Descent(48467): loss=0.9813095316753868\n",
      "Stochastic Gradient Descent(48468): loss=2.8009791421057066\n",
      "Stochastic Gradient Descent(48469): loss=2.4155495461061034\n",
      "Stochastic Gradient Descent(48470): loss=0.0492525220470142\n",
      "Stochastic Gradient Descent(48471): loss=0.01174004025795667\n",
      "Stochastic Gradient Descent(48472): loss=0.03867508745939348\n",
      "Stochastic Gradient Descent(48473): loss=2.079947778296194\n",
      "Stochastic Gradient Descent(48474): loss=1.9124750652098368\n",
      "Stochastic Gradient Descent(48475): loss=1.0514399559627665\n",
      "Stochastic Gradient Descent(48476): loss=0.2518719473211086\n",
      "Stochastic Gradient Descent(48477): loss=0.008834011503733984\n",
      "Stochastic Gradient Descent(48478): loss=5.589817225313291\n",
      "Stochastic Gradient Descent(48479): loss=1.8323077860622894\n",
      "Stochastic Gradient Descent(48480): loss=0.003144085943537356\n",
      "Stochastic Gradient Descent(48481): loss=2.4562350877804673\n",
      "Stochastic Gradient Descent(48482): loss=1.9439623126250378\n",
      "Stochastic Gradient Descent(48483): loss=3.1275652489191845\n",
      "Stochastic Gradient Descent(48484): loss=0.21040141479825003\n",
      "Stochastic Gradient Descent(48485): loss=2.8759071136919485\n",
      "Stochastic Gradient Descent(48486): loss=1.7454545766364033e-05\n",
      "Stochastic Gradient Descent(48487): loss=0.2965633801525678\n",
      "Stochastic Gradient Descent(48488): loss=1.1954281942842921\n",
      "Stochastic Gradient Descent(48489): loss=0.40163002664053776\n",
      "Stochastic Gradient Descent(48490): loss=0.1353060986712184\n",
      "Stochastic Gradient Descent(48491): loss=7.588501563974778\n",
      "Stochastic Gradient Descent(48492): loss=2.820067658690608\n",
      "Stochastic Gradient Descent(48493): loss=2.286043974159829\n",
      "Stochastic Gradient Descent(48494): loss=0.012163813800594376\n",
      "Stochastic Gradient Descent(48495): loss=13.84337458001222\n",
      "Stochastic Gradient Descent(48496): loss=0.9923486348298397\n",
      "Stochastic Gradient Descent(48497): loss=0.5163409289008043\n",
      "Stochastic Gradient Descent(48498): loss=0.14896260880980258\n",
      "Stochastic Gradient Descent(48499): loss=0.0913765076442059\n",
      "Stochastic Gradient Descent(48500): loss=0.14955137936051568\n",
      "Stochastic Gradient Descent(48501): loss=9.94644680282188\n",
      "Stochastic Gradient Descent(48502): loss=6.2975624794196055\n",
      "Stochastic Gradient Descent(48503): loss=7.081287371683291\n",
      "Stochastic Gradient Descent(48504): loss=6.230803946131599\n",
      "Stochastic Gradient Descent(48505): loss=0.0045388357631676425\n",
      "Stochastic Gradient Descent(48506): loss=0.18571993215657928\n",
      "Stochastic Gradient Descent(48507): loss=0.05074688929001341\n",
      "Stochastic Gradient Descent(48508): loss=1.193048475949343\n",
      "Stochastic Gradient Descent(48509): loss=0.05806902539160352\n",
      "Stochastic Gradient Descent(48510): loss=0.06613101747909159\n",
      "Stochastic Gradient Descent(48511): loss=1.6607346424993508\n",
      "Stochastic Gradient Descent(48512): loss=2.062852975245401\n",
      "Stochastic Gradient Descent(48513): loss=0.10335523210060815\n",
      "Stochastic Gradient Descent(48514): loss=0.012717107181881227\n",
      "Stochastic Gradient Descent(48515): loss=0.5683118639055105\n",
      "Stochastic Gradient Descent(48516): loss=0.30523088073078414\n",
      "Stochastic Gradient Descent(48517): loss=0.006276150949933623\n",
      "Stochastic Gradient Descent(48518): loss=0.006126058073362004\n",
      "Stochastic Gradient Descent(48519): loss=0.5396102123376973\n",
      "Stochastic Gradient Descent(48520): loss=0.10125477473273291\n",
      "Stochastic Gradient Descent(48521): loss=2.4699426113781184\n",
      "Stochastic Gradient Descent(48522): loss=5.292046595599653\n",
      "Stochastic Gradient Descent(48523): loss=5.798522841826229\n",
      "Stochastic Gradient Descent(48524): loss=17.912696148657623\n",
      "Stochastic Gradient Descent(48525): loss=1.5715965928008422\n",
      "Stochastic Gradient Descent(48526): loss=0.47816368013629806\n",
      "Stochastic Gradient Descent(48527): loss=0.3085123152937494\n",
      "Stochastic Gradient Descent(48528): loss=10.509262484141761\n",
      "Stochastic Gradient Descent(48529): loss=0.6687586447218785\n",
      "Stochastic Gradient Descent(48530): loss=1.9036970049630688\n",
      "Stochastic Gradient Descent(48531): loss=0.005909314856473696\n",
      "Stochastic Gradient Descent(48532): loss=4.323473914098648\n",
      "Stochastic Gradient Descent(48533): loss=0.2092500631981406\n",
      "Stochastic Gradient Descent(48534): loss=30.45269711547245\n",
      "Stochastic Gradient Descent(48535): loss=18.895949088017225\n",
      "Stochastic Gradient Descent(48536): loss=0.1052510574370056\n",
      "Stochastic Gradient Descent(48537): loss=3.839047971083585\n",
      "Stochastic Gradient Descent(48538): loss=1.668175308187732\n",
      "Stochastic Gradient Descent(48539): loss=14.533553078354268\n",
      "Stochastic Gradient Descent(48540): loss=18.544653008159838\n",
      "Stochastic Gradient Descent(48541): loss=11.00102806783692\n",
      "Stochastic Gradient Descent(48542): loss=11.815943252293701\n",
      "Stochastic Gradient Descent(48543): loss=1.6615414156311565\n",
      "Stochastic Gradient Descent(48544): loss=5.793018357714877\n",
      "Stochastic Gradient Descent(48545): loss=0.010945436184342024\n",
      "Stochastic Gradient Descent(48546): loss=49.968445870616826\n",
      "Stochastic Gradient Descent(48547): loss=5.242731387646308\n",
      "Stochastic Gradient Descent(48548): loss=14.648329988833888\n",
      "Stochastic Gradient Descent(48549): loss=19.620546405070517\n",
      "Stochastic Gradient Descent(48550): loss=5.178242999870751\n",
      "Stochastic Gradient Descent(48551): loss=1.085818175290033\n",
      "Stochastic Gradient Descent(48552): loss=6.808073827197824\n",
      "Stochastic Gradient Descent(48553): loss=1.501764629159371\n",
      "Stochastic Gradient Descent(48554): loss=6.178437237109794\n",
      "Stochastic Gradient Descent(48555): loss=8.608816938172678\n",
      "Stochastic Gradient Descent(48556): loss=0.8888216907192386\n",
      "Stochastic Gradient Descent(48557): loss=2.488442818089644\n",
      "Stochastic Gradient Descent(48558): loss=0.09997129118539143\n",
      "Stochastic Gradient Descent(48559): loss=1.5272684763915576\n",
      "Stochastic Gradient Descent(48560): loss=0.27322482393822634\n",
      "Stochastic Gradient Descent(48561): loss=1.2174840330447538\n",
      "Stochastic Gradient Descent(48562): loss=3.208321266394994\n",
      "Stochastic Gradient Descent(48563): loss=0.21485197342581352\n",
      "Stochastic Gradient Descent(48564): loss=2.5957322967456875\n",
      "Stochastic Gradient Descent(48565): loss=1.292190388050953\n",
      "Stochastic Gradient Descent(48566): loss=15.034607673192436\n",
      "Stochastic Gradient Descent(48567): loss=1.4323205470952918\n",
      "Stochastic Gradient Descent(48568): loss=1.9665468892033566\n",
      "Stochastic Gradient Descent(48569): loss=0.8871980370975535\n",
      "Stochastic Gradient Descent(48570): loss=3.2312033199532997\n",
      "Stochastic Gradient Descent(48571): loss=0.37369263493271526\n",
      "Stochastic Gradient Descent(48572): loss=13.4964014209719\n",
      "Stochastic Gradient Descent(48573): loss=5.817755734621099\n",
      "Stochastic Gradient Descent(48574): loss=13.250574491650754\n",
      "Stochastic Gradient Descent(48575): loss=6.679462609361807\n",
      "Stochastic Gradient Descent(48576): loss=0.1737279565809502\n",
      "Stochastic Gradient Descent(48577): loss=1.3691171658170127\n",
      "Stochastic Gradient Descent(48578): loss=2.3273793402952627\n",
      "Stochastic Gradient Descent(48579): loss=6.255812141173704\n",
      "Stochastic Gradient Descent(48580): loss=0.6741980790719777\n",
      "Stochastic Gradient Descent(48581): loss=18.80053012371592\n",
      "Stochastic Gradient Descent(48582): loss=5.263302608301605\n",
      "Stochastic Gradient Descent(48583): loss=3.2042523752146788\n",
      "Stochastic Gradient Descent(48584): loss=0.3182919023024166\n",
      "Stochastic Gradient Descent(48585): loss=0.49983036636379785\n",
      "Stochastic Gradient Descent(48586): loss=4.696629620702335\n",
      "Stochastic Gradient Descent(48587): loss=2.5785294266830125\n",
      "Stochastic Gradient Descent(48588): loss=4.715885593891145\n",
      "Stochastic Gradient Descent(48589): loss=8.762433004908804\n",
      "Stochastic Gradient Descent(48590): loss=0.7905782225281287\n",
      "Stochastic Gradient Descent(48591): loss=0.004427638710980523\n",
      "Stochastic Gradient Descent(48592): loss=0.8509416742187224\n",
      "Stochastic Gradient Descent(48593): loss=4.459451676792448\n",
      "Stochastic Gradient Descent(48594): loss=18.33251995416759\n",
      "Stochastic Gradient Descent(48595): loss=0.3065241109984394\n",
      "Stochastic Gradient Descent(48596): loss=2.9812413506831845\n",
      "Stochastic Gradient Descent(48597): loss=6.128356304023603\n",
      "Stochastic Gradient Descent(48598): loss=1.401513570246986\n",
      "Stochastic Gradient Descent(48599): loss=1.2033289013953583\n",
      "Stochastic Gradient Descent(48600): loss=3.7309466045239854\n",
      "Stochastic Gradient Descent(48601): loss=0.28519021971277586\n",
      "Stochastic Gradient Descent(48602): loss=5.702321609060535\n",
      "Stochastic Gradient Descent(48603): loss=1.1623456260107696\n",
      "Stochastic Gradient Descent(48604): loss=6.322518963100828\n",
      "Stochastic Gradient Descent(48605): loss=13.914284127916511\n",
      "Stochastic Gradient Descent(48606): loss=0.004391484884038591\n",
      "Stochastic Gradient Descent(48607): loss=16.055739207536462\n",
      "Stochastic Gradient Descent(48608): loss=13.90583237053396\n",
      "Stochastic Gradient Descent(48609): loss=0.7665714504863862\n",
      "Stochastic Gradient Descent(48610): loss=6.312839014355516\n",
      "Stochastic Gradient Descent(48611): loss=0.3555801471512129\n",
      "Stochastic Gradient Descent(48612): loss=17.95868373082355\n",
      "Stochastic Gradient Descent(48613): loss=2.263834351661387\n",
      "Stochastic Gradient Descent(48614): loss=3.3328110985172112\n",
      "Stochastic Gradient Descent(48615): loss=2.0458999573606937\n",
      "Stochastic Gradient Descent(48616): loss=2.99214200610169\n",
      "Stochastic Gradient Descent(48617): loss=1.241769029277996\n",
      "Stochastic Gradient Descent(48618): loss=1.6604028161915654\n",
      "Stochastic Gradient Descent(48619): loss=2.342980328061211\n",
      "Stochastic Gradient Descent(48620): loss=0.6079778704284414\n",
      "Stochastic Gradient Descent(48621): loss=1.6267294954945748\n",
      "Stochastic Gradient Descent(48622): loss=7.340776477255333\n",
      "Stochastic Gradient Descent(48623): loss=1.3758861346331452\n",
      "Stochastic Gradient Descent(48624): loss=0.029501388689310452\n",
      "Stochastic Gradient Descent(48625): loss=1.376265941968803\n",
      "Stochastic Gradient Descent(48626): loss=0.1527941590174615\n",
      "Stochastic Gradient Descent(48627): loss=5.156041466748995\n",
      "Stochastic Gradient Descent(48628): loss=16.604875237706636\n",
      "Stochastic Gradient Descent(48629): loss=0.23179293672957965\n",
      "Stochastic Gradient Descent(48630): loss=1.3526022955963894\n",
      "Stochastic Gradient Descent(48631): loss=0.18894606157796923\n",
      "Stochastic Gradient Descent(48632): loss=4.106332448057935\n",
      "Stochastic Gradient Descent(48633): loss=0.07521211704616992\n",
      "Stochastic Gradient Descent(48634): loss=0.0026872868259850725\n",
      "Stochastic Gradient Descent(48635): loss=3.6174109696568357\n",
      "Stochastic Gradient Descent(48636): loss=0.7135041630037313\n",
      "Stochastic Gradient Descent(48637): loss=1.8267867922088339\n",
      "Stochastic Gradient Descent(48638): loss=0.6535478320161453\n",
      "Stochastic Gradient Descent(48639): loss=0.1082368254956217\n",
      "Stochastic Gradient Descent(48640): loss=2.106070775121008\n",
      "Stochastic Gradient Descent(48641): loss=10.87522491976312\n",
      "Stochastic Gradient Descent(48642): loss=1.3790822134004264\n",
      "Stochastic Gradient Descent(48643): loss=0.6521845599811285\n",
      "Stochastic Gradient Descent(48644): loss=0.004893779356680437\n",
      "Stochastic Gradient Descent(48645): loss=1.406968292035115\n",
      "Stochastic Gradient Descent(48646): loss=0.06392974253564244\n",
      "Stochastic Gradient Descent(48647): loss=0.08152736641555006\n",
      "Stochastic Gradient Descent(48648): loss=17.537034583361496\n",
      "Stochastic Gradient Descent(48649): loss=4.25303838707579\n",
      "Stochastic Gradient Descent(48650): loss=1.6477819131470537\n",
      "Stochastic Gradient Descent(48651): loss=0.6614504286176014\n",
      "Stochastic Gradient Descent(48652): loss=0.17986133036577637\n",
      "Stochastic Gradient Descent(48653): loss=0.0006213811922904443\n",
      "Stochastic Gradient Descent(48654): loss=3.4396259130750986\n",
      "Stochastic Gradient Descent(48655): loss=9.999824565155688\n",
      "Stochastic Gradient Descent(48656): loss=4.010258385043273\n",
      "Stochastic Gradient Descent(48657): loss=4.956914412547758\n",
      "Stochastic Gradient Descent(48658): loss=5.931521493457094\n",
      "Stochastic Gradient Descent(48659): loss=0.48006967825253866\n",
      "Stochastic Gradient Descent(48660): loss=1.0363964168940814\n",
      "Stochastic Gradient Descent(48661): loss=5.3206927894111855\n",
      "Stochastic Gradient Descent(48662): loss=0.16921079327345848\n",
      "Stochastic Gradient Descent(48663): loss=11.461216108600022\n",
      "Stochastic Gradient Descent(48664): loss=0.41341571704122815\n",
      "Stochastic Gradient Descent(48665): loss=0.4402480678122882\n",
      "Stochastic Gradient Descent(48666): loss=1.2678986083702062\n",
      "Stochastic Gradient Descent(48667): loss=0.09402542357631063\n",
      "Stochastic Gradient Descent(48668): loss=0.972240670373662\n",
      "Stochastic Gradient Descent(48669): loss=0.04015351955721239\n",
      "Stochastic Gradient Descent(48670): loss=4.687478542891893\n",
      "Stochastic Gradient Descent(48671): loss=18.645669042544146\n",
      "Stochastic Gradient Descent(48672): loss=2.2620570201855736\n",
      "Stochastic Gradient Descent(48673): loss=3.313743984493435\n",
      "Stochastic Gradient Descent(48674): loss=0.07079626879897884\n",
      "Stochastic Gradient Descent(48675): loss=0.5051570486096996\n",
      "Stochastic Gradient Descent(48676): loss=0.1502483042534233\n",
      "Stochastic Gradient Descent(48677): loss=1.3772406717438752\n",
      "Stochastic Gradient Descent(48678): loss=1.4482961200803874\n",
      "Stochastic Gradient Descent(48679): loss=0.047024300238903395\n",
      "Stochastic Gradient Descent(48680): loss=0.44115363610459285\n",
      "Stochastic Gradient Descent(48681): loss=5.348360353689509\n",
      "Stochastic Gradient Descent(48682): loss=5.168190451016483\n",
      "Stochastic Gradient Descent(48683): loss=1.8525902270875834\n",
      "Stochastic Gradient Descent(48684): loss=0.009140374716053185\n",
      "Stochastic Gradient Descent(48685): loss=6.8067610996729675\n",
      "Stochastic Gradient Descent(48686): loss=0.7487421968414328\n",
      "Stochastic Gradient Descent(48687): loss=0.7007009254601368\n",
      "Stochastic Gradient Descent(48688): loss=1.7209423386111948\n",
      "Stochastic Gradient Descent(48689): loss=0.4507587755719542\n",
      "Stochastic Gradient Descent(48690): loss=16.23859708149006\n",
      "Stochastic Gradient Descent(48691): loss=11.617408395704576\n",
      "Stochastic Gradient Descent(48692): loss=0.6707170369550911\n",
      "Stochastic Gradient Descent(48693): loss=1.488431410926327\n",
      "Stochastic Gradient Descent(48694): loss=0.18746824852087232\n",
      "Stochastic Gradient Descent(48695): loss=0.5194270791444823\n",
      "Stochastic Gradient Descent(48696): loss=3.970928308476313\n",
      "Stochastic Gradient Descent(48697): loss=2.2952076549415765\n",
      "Stochastic Gradient Descent(48698): loss=2.4633521992483227\n",
      "Stochastic Gradient Descent(48699): loss=0.016906816384088516\n",
      "Stochastic Gradient Descent(48700): loss=0.036666584017627776\n",
      "Stochastic Gradient Descent(48701): loss=0.012680140902974667\n",
      "Stochastic Gradient Descent(48702): loss=3.1000457000849964\n",
      "Stochastic Gradient Descent(48703): loss=0.13776569314657966\n",
      "Stochastic Gradient Descent(48704): loss=0.352727554755836\n",
      "Stochastic Gradient Descent(48705): loss=0.012905209527384368\n",
      "Stochastic Gradient Descent(48706): loss=10.614558816677867\n",
      "Stochastic Gradient Descent(48707): loss=0.4699951738038972\n",
      "Stochastic Gradient Descent(48708): loss=2.0753667690119375\n",
      "Stochastic Gradient Descent(48709): loss=2.716650775718756\n",
      "Stochastic Gradient Descent(48710): loss=1.9128207453755175\n",
      "Stochastic Gradient Descent(48711): loss=1.6606730236161975\n",
      "Stochastic Gradient Descent(48712): loss=1.8401331318342586\n",
      "Stochastic Gradient Descent(48713): loss=0.00029326184357568853\n",
      "Stochastic Gradient Descent(48714): loss=0.934364861949463\n",
      "Stochastic Gradient Descent(48715): loss=0.07731286481176292\n",
      "Stochastic Gradient Descent(48716): loss=6.240894899922965\n",
      "Stochastic Gradient Descent(48717): loss=0.8625630266594533\n",
      "Stochastic Gradient Descent(48718): loss=2.397858212037427\n",
      "Stochastic Gradient Descent(48719): loss=0.8615871840640399\n",
      "Stochastic Gradient Descent(48720): loss=0.2538774900297279\n",
      "Stochastic Gradient Descent(48721): loss=3.872573075159637\n",
      "Stochastic Gradient Descent(48722): loss=7.318460027444787\n",
      "Stochastic Gradient Descent(48723): loss=3.455046525761542\n",
      "Stochastic Gradient Descent(48724): loss=0.06072034196327527\n",
      "Stochastic Gradient Descent(48725): loss=30.93386127864492\n",
      "Stochastic Gradient Descent(48726): loss=4.166466450762339\n",
      "Stochastic Gradient Descent(48727): loss=1.6172830202019943\n",
      "Stochastic Gradient Descent(48728): loss=1.7521442360724442\n",
      "Stochastic Gradient Descent(48729): loss=0.25150377787104883\n",
      "Stochastic Gradient Descent(48730): loss=0.17089218613178414\n",
      "Stochastic Gradient Descent(48731): loss=0.4037055496883793\n",
      "Stochastic Gradient Descent(48732): loss=0.18607641336789082\n",
      "Stochastic Gradient Descent(48733): loss=0.25526141981940587\n",
      "Stochastic Gradient Descent(48734): loss=13.091845950882552\n",
      "Stochastic Gradient Descent(48735): loss=0.7929292133627822\n",
      "Stochastic Gradient Descent(48736): loss=5.474728321329495\n",
      "Stochastic Gradient Descent(48737): loss=0.39543044719378234\n",
      "Stochastic Gradient Descent(48738): loss=4.155599325808784\n",
      "Stochastic Gradient Descent(48739): loss=1.2371386962946538\n",
      "Stochastic Gradient Descent(48740): loss=8.817351490305036\n",
      "Stochastic Gradient Descent(48741): loss=0.9931173824246358\n",
      "Stochastic Gradient Descent(48742): loss=0.6355080427680911\n",
      "Stochastic Gradient Descent(48743): loss=0.45691449446957055\n",
      "Stochastic Gradient Descent(48744): loss=0.3190582004799332\n",
      "Stochastic Gradient Descent(48745): loss=1.14140630860676\n",
      "Stochastic Gradient Descent(48746): loss=0.3938614675315315\n",
      "Stochastic Gradient Descent(48747): loss=4.372869496347676\n",
      "Stochastic Gradient Descent(48748): loss=1.5210620629524043\n",
      "Stochastic Gradient Descent(48749): loss=3.158768711989553\n",
      "Stochastic Gradient Descent(48750): loss=0.0027390113120762324\n",
      "Stochastic Gradient Descent(48751): loss=3.1717127362492272\n",
      "Stochastic Gradient Descent(48752): loss=0.002752711629598506\n",
      "Stochastic Gradient Descent(48753): loss=3.250299912087526\n",
      "Stochastic Gradient Descent(48754): loss=1.284277666608645\n",
      "Stochastic Gradient Descent(48755): loss=1.5457501433423348\n",
      "Stochastic Gradient Descent(48756): loss=1.403321827038003\n",
      "Stochastic Gradient Descent(48757): loss=8.219620481506256\n",
      "Stochastic Gradient Descent(48758): loss=9.020215871610845e-05\n",
      "Stochastic Gradient Descent(48759): loss=3.6493121280293055\n",
      "Stochastic Gradient Descent(48760): loss=0.2860077589290467\n",
      "Stochastic Gradient Descent(48761): loss=0.014378056568521674\n",
      "Stochastic Gradient Descent(48762): loss=2.2674462394124943\n",
      "Stochastic Gradient Descent(48763): loss=2.311684342900185\n",
      "Stochastic Gradient Descent(48764): loss=0.14770815791861266\n",
      "Stochastic Gradient Descent(48765): loss=0.18955151374149828\n",
      "Stochastic Gradient Descent(48766): loss=2.4359276598488773\n",
      "Stochastic Gradient Descent(48767): loss=3.109787478153389\n",
      "Stochastic Gradient Descent(48768): loss=1.9183795842590061\n",
      "Stochastic Gradient Descent(48769): loss=6.996142586427384\n",
      "Stochastic Gradient Descent(48770): loss=0.018427522933244983\n",
      "Stochastic Gradient Descent(48771): loss=1.7086216682819373\n",
      "Stochastic Gradient Descent(48772): loss=3.1658114586524833\n",
      "Stochastic Gradient Descent(48773): loss=19.1129595478876\n",
      "Stochastic Gradient Descent(48774): loss=0.2864104563319926\n",
      "Stochastic Gradient Descent(48775): loss=0.929203199507824\n",
      "Stochastic Gradient Descent(48776): loss=8.93432904350907\n",
      "Stochastic Gradient Descent(48777): loss=11.15931584671747\n",
      "Stochastic Gradient Descent(48778): loss=0.22811560568914802\n",
      "Stochastic Gradient Descent(48779): loss=0.7467525531885092\n",
      "Stochastic Gradient Descent(48780): loss=5.391842791418427\n",
      "Stochastic Gradient Descent(48781): loss=31.568193144522063\n",
      "Stochastic Gradient Descent(48782): loss=6.49200671387224\n",
      "Stochastic Gradient Descent(48783): loss=0.6019330012380618\n",
      "Stochastic Gradient Descent(48784): loss=2.2368893784385544\n",
      "Stochastic Gradient Descent(48785): loss=4.375119777094248\n",
      "Stochastic Gradient Descent(48786): loss=6.819853689651167\n",
      "Stochastic Gradient Descent(48787): loss=0.3756919768344396\n",
      "Stochastic Gradient Descent(48788): loss=1.3458934563710014\n",
      "Stochastic Gradient Descent(48789): loss=1.1017574996964745\n",
      "Stochastic Gradient Descent(48790): loss=2.186267901303713\n",
      "Stochastic Gradient Descent(48791): loss=4.166010201745424\n",
      "Stochastic Gradient Descent(48792): loss=3.7007747878263433\n",
      "Stochastic Gradient Descent(48793): loss=1.3149081381156809\n",
      "Stochastic Gradient Descent(48794): loss=0.5425978643588639\n",
      "Stochastic Gradient Descent(48795): loss=1.1513467505304682\n",
      "Stochastic Gradient Descent(48796): loss=9.154554343979648\n",
      "Stochastic Gradient Descent(48797): loss=1.819617856170249\n",
      "Stochastic Gradient Descent(48798): loss=1.5845303879642447\n",
      "Stochastic Gradient Descent(48799): loss=10.278098840089438\n",
      "Stochastic Gradient Descent(48800): loss=0.14849951109586457\n",
      "Stochastic Gradient Descent(48801): loss=3.711467770751252\n",
      "Stochastic Gradient Descent(48802): loss=0.013435750053758045\n",
      "Stochastic Gradient Descent(48803): loss=4.292117633446519\n",
      "Stochastic Gradient Descent(48804): loss=0.23369122648478674\n",
      "Stochastic Gradient Descent(48805): loss=1.2842336916766215\n",
      "Stochastic Gradient Descent(48806): loss=0.3556896258489368\n",
      "Stochastic Gradient Descent(48807): loss=6.345468705525567\n",
      "Stochastic Gradient Descent(48808): loss=9.776071635997438\n",
      "Stochastic Gradient Descent(48809): loss=3.045365268059348\n",
      "Stochastic Gradient Descent(48810): loss=3.762892359246607\n",
      "Stochastic Gradient Descent(48811): loss=17.011399000084147\n",
      "Stochastic Gradient Descent(48812): loss=4.145712750571389\n",
      "Stochastic Gradient Descent(48813): loss=0.32803412119792535\n",
      "Stochastic Gradient Descent(48814): loss=0.11892156910393387\n",
      "Stochastic Gradient Descent(48815): loss=7.811195721617488\n",
      "Stochastic Gradient Descent(48816): loss=8.941187238880524\n",
      "Stochastic Gradient Descent(48817): loss=0.12653184544275195\n",
      "Stochastic Gradient Descent(48818): loss=0.16875782452714946\n",
      "Stochastic Gradient Descent(48819): loss=10.445466577945597\n",
      "Stochastic Gradient Descent(48820): loss=4.581699259415242\n",
      "Stochastic Gradient Descent(48821): loss=0.6152067178875836\n",
      "Stochastic Gradient Descent(48822): loss=3.5189153233188595\n",
      "Stochastic Gradient Descent(48823): loss=2.0041803223340455\n",
      "Stochastic Gradient Descent(48824): loss=0.4122438871434953\n",
      "Stochastic Gradient Descent(48825): loss=11.441774643685477\n",
      "Stochastic Gradient Descent(48826): loss=1.9335583288373601\n",
      "Stochastic Gradient Descent(48827): loss=1.1875024000568253\n",
      "Stochastic Gradient Descent(48828): loss=0.04559953187962307\n",
      "Stochastic Gradient Descent(48829): loss=5.069645811771724\n",
      "Stochastic Gradient Descent(48830): loss=13.481178718407133\n",
      "Stochastic Gradient Descent(48831): loss=0.03553900007853336\n",
      "Stochastic Gradient Descent(48832): loss=0.890583315319054\n",
      "Stochastic Gradient Descent(48833): loss=3.438135762670868\n",
      "Stochastic Gradient Descent(48834): loss=0.7439471558070858\n",
      "Stochastic Gradient Descent(48835): loss=0.587246500631644\n",
      "Stochastic Gradient Descent(48836): loss=0.1212577764132599\n",
      "Stochastic Gradient Descent(48837): loss=1.8945716930395349\n",
      "Stochastic Gradient Descent(48838): loss=13.144415841309133\n",
      "Stochastic Gradient Descent(48839): loss=1.0031149835851967\n",
      "Stochastic Gradient Descent(48840): loss=7.505240632072707\n",
      "Stochastic Gradient Descent(48841): loss=13.350764175054712\n",
      "Stochastic Gradient Descent(48842): loss=3.9247164472134406\n",
      "Stochastic Gradient Descent(48843): loss=1.509503540460015\n",
      "Stochastic Gradient Descent(48844): loss=1.6876126566845935\n",
      "Stochastic Gradient Descent(48845): loss=72.46179792207471\n",
      "Stochastic Gradient Descent(48846): loss=1.6820975626801453\n",
      "Stochastic Gradient Descent(48847): loss=0.09167012433664272\n",
      "Stochastic Gradient Descent(48848): loss=10.903797094428771\n",
      "Stochastic Gradient Descent(48849): loss=9.25448562996374\n",
      "Stochastic Gradient Descent(48850): loss=4.094775889483607\n",
      "Stochastic Gradient Descent(48851): loss=0.7721209109673915\n",
      "Stochastic Gradient Descent(48852): loss=0.0008580853474407806\n",
      "Stochastic Gradient Descent(48853): loss=4.676300602810577\n",
      "Stochastic Gradient Descent(48854): loss=0.036407362659738284\n",
      "Stochastic Gradient Descent(48855): loss=1.2067504248901044\n",
      "Stochastic Gradient Descent(48856): loss=3.435887386103395\n",
      "Stochastic Gradient Descent(48857): loss=3.6867805703721923\n",
      "Stochastic Gradient Descent(48858): loss=0.4368178596146484\n",
      "Stochastic Gradient Descent(48859): loss=2.394850882858828\n",
      "Stochastic Gradient Descent(48860): loss=2.2313991694875477\n",
      "Stochastic Gradient Descent(48861): loss=17.256573800290344\n",
      "Stochastic Gradient Descent(48862): loss=1.5336014270852663\n",
      "Stochastic Gradient Descent(48863): loss=0.2725463276166984\n",
      "Stochastic Gradient Descent(48864): loss=0.5493756144132605\n",
      "Stochastic Gradient Descent(48865): loss=0.26459410168515723\n",
      "Stochastic Gradient Descent(48866): loss=1.7427918796777748\n",
      "Stochastic Gradient Descent(48867): loss=30.518869747027697\n",
      "Stochastic Gradient Descent(48868): loss=14.778035050987912\n",
      "Stochastic Gradient Descent(48869): loss=0.8107458009448916\n",
      "Stochastic Gradient Descent(48870): loss=7.973723715218572\n",
      "Stochastic Gradient Descent(48871): loss=1.269901746276216\n",
      "Stochastic Gradient Descent(48872): loss=1.5327105225377622\n",
      "Stochastic Gradient Descent(48873): loss=0.19735930161702694\n",
      "Stochastic Gradient Descent(48874): loss=3.63522348576478\n",
      "Stochastic Gradient Descent(48875): loss=5.100477678034105\n",
      "Stochastic Gradient Descent(48876): loss=7.6682648850633175\n",
      "Stochastic Gradient Descent(48877): loss=3.2733522088936406\n",
      "Stochastic Gradient Descent(48878): loss=2.8309894951026666\n",
      "Stochastic Gradient Descent(48879): loss=0.3750098048752118\n",
      "Stochastic Gradient Descent(48880): loss=20.860804188521513\n",
      "Stochastic Gradient Descent(48881): loss=3.1734575283199193\n",
      "Stochastic Gradient Descent(48882): loss=8.96201899710698\n",
      "Stochastic Gradient Descent(48883): loss=2.2018438118322834\n",
      "Stochastic Gradient Descent(48884): loss=10.207712660059572\n",
      "Stochastic Gradient Descent(48885): loss=0.053249680278852726\n",
      "Stochastic Gradient Descent(48886): loss=0.021484168668681684\n",
      "Stochastic Gradient Descent(48887): loss=2.549086968194313\n",
      "Stochastic Gradient Descent(48888): loss=0.9901412328777655\n",
      "Stochastic Gradient Descent(48889): loss=0.34322439571865937\n",
      "Stochastic Gradient Descent(48890): loss=3.795299695217447\n",
      "Stochastic Gradient Descent(48891): loss=0.36180058866446396\n",
      "Stochastic Gradient Descent(48892): loss=12.757613038027866\n",
      "Stochastic Gradient Descent(48893): loss=26.771962733381876\n",
      "Stochastic Gradient Descent(48894): loss=1.664625756367556\n",
      "Stochastic Gradient Descent(48895): loss=3.3594671099156637\n",
      "Stochastic Gradient Descent(48896): loss=0.018000676211675982\n",
      "Stochastic Gradient Descent(48897): loss=0.5016915140257352\n",
      "Stochastic Gradient Descent(48898): loss=0.006338985312207328\n",
      "Stochastic Gradient Descent(48899): loss=1.068399460387254\n",
      "Stochastic Gradient Descent(48900): loss=0.07788385712734297\n",
      "Stochastic Gradient Descent(48901): loss=0.1541709334999137\n",
      "Stochastic Gradient Descent(48902): loss=6.926544991677227\n",
      "Stochastic Gradient Descent(48903): loss=2.757406002192603\n",
      "Stochastic Gradient Descent(48904): loss=4.836552141120586\n",
      "Stochastic Gradient Descent(48905): loss=1.7185210305541312\n",
      "Stochastic Gradient Descent(48906): loss=0.004568146442992439\n",
      "Stochastic Gradient Descent(48907): loss=6.026503000689511\n",
      "Stochastic Gradient Descent(48908): loss=1.4720079494079683\n",
      "Stochastic Gradient Descent(48909): loss=0.18508862889065675\n",
      "Stochastic Gradient Descent(48910): loss=0.09231171348843831\n",
      "Stochastic Gradient Descent(48911): loss=0.26403821305569825\n",
      "Stochastic Gradient Descent(48912): loss=0.635263913573574\n",
      "Stochastic Gradient Descent(48913): loss=0.38037104246490555\n",
      "Stochastic Gradient Descent(48914): loss=4.211228982686007\n",
      "Stochastic Gradient Descent(48915): loss=3.6420811914155147\n",
      "Stochastic Gradient Descent(48916): loss=6.154935628318802\n",
      "Stochastic Gradient Descent(48917): loss=0.114950608903247\n",
      "Stochastic Gradient Descent(48918): loss=13.480717794902322\n",
      "Stochastic Gradient Descent(48919): loss=0.2889414593988656\n",
      "Stochastic Gradient Descent(48920): loss=4.332093325879181\n",
      "Stochastic Gradient Descent(48921): loss=0.02972311655745427\n",
      "Stochastic Gradient Descent(48922): loss=3.0955701063597485\n",
      "Stochastic Gradient Descent(48923): loss=0.0765540285406961\n",
      "Stochastic Gradient Descent(48924): loss=9.654006084530803\n",
      "Stochastic Gradient Descent(48925): loss=1.8320709434111577\n",
      "Stochastic Gradient Descent(48926): loss=0.9259138488701418\n",
      "Stochastic Gradient Descent(48927): loss=2.4817607334373815\n",
      "Stochastic Gradient Descent(48928): loss=0.6642868292565467\n",
      "Stochastic Gradient Descent(48929): loss=1.352558467523387\n",
      "Stochastic Gradient Descent(48930): loss=4.89898893411203\n",
      "Stochastic Gradient Descent(48931): loss=0.02658923825871621\n",
      "Stochastic Gradient Descent(48932): loss=2.98195903675179\n",
      "Stochastic Gradient Descent(48933): loss=1.7919366088555535\n",
      "Stochastic Gradient Descent(48934): loss=0.4427888927201777\n",
      "Stochastic Gradient Descent(48935): loss=8.649609310398883\n",
      "Stochastic Gradient Descent(48936): loss=0.15088240576386971\n",
      "Stochastic Gradient Descent(48937): loss=1.5120120050262538\n",
      "Stochastic Gradient Descent(48938): loss=8.281522658128264\n",
      "Stochastic Gradient Descent(48939): loss=0.09255421335841371\n",
      "Stochastic Gradient Descent(48940): loss=3.7762618055414783\n",
      "Stochastic Gradient Descent(48941): loss=6.30414695521029\n",
      "Stochastic Gradient Descent(48942): loss=0.00040691932423622717\n",
      "Stochastic Gradient Descent(48943): loss=0.0017871327714269686\n",
      "Stochastic Gradient Descent(48944): loss=0.04170425614849886\n",
      "Stochastic Gradient Descent(48945): loss=0.17791745768242906\n",
      "Stochastic Gradient Descent(48946): loss=0.05492823412750495\n",
      "Stochastic Gradient Descent(48947): loss=0.28342215751355526\n",
      "Stochastic Gradient Descent(48948): loss=4.5064326855278605\n",
      "Stochastic Gradient Descent(48949): loss=1.6720556925995365\n",
      "Stochastic Gradient Descent(48950): loss=6.246717624219469\n",
      "Stochastic Gradient Descent(48951): loss=4.839404718935425\n",
      "Stochastic Gradient Descent(48952): loss=0.3359465978514916\n",
      "Stochastic Gradient Descent(48953): loss=5.993120065666058\n",
      "Stochastic Gradient Descent(48954): loss=3.1083768587504534e-05\n",
      "Stochastic Gradient Descent(48955): loss=0.00012972109441004815\n",
      "Stochastic Gradient Descent(48956): loss=0.5484636077307621\n",
      "Stochastic Gradient Descent(48957): loss=0.4138721275553736\n",
      "Stochastic Gradient Descent(48958): loss=6.765163600657854\n",
      "Stochastic Gradient Descent(48959): loss=0.23235263619349655\n",
      "Stochastic Gradient Descent(48960): loss=0.06350405263925771\n",
      "Stochastic Gradient Descent(48961): loss=15.232804823711916\n",
      "Stochastic Gradient Descent(48962): loss=4.334993670295993\n",
      "Stochastic Gradient Descent(48963): loss=0.042525557872683986\n",
      "Stochastic Gradient Descent(48964): loss=0.3856786142710791\n",
      "Stochastic Gradient Descent(48965): loss=0.010453290618848475\n",
      "Stochastic Gradient Descent(48966): loss=1.6868749497657995\n",
      "Stochastic Gradient Descent(48967): loss=1.7347499092121421\n",
      "Stochastic Gradient Descent(48968): loss=10.126383931382511\n",
      "Stochastic Gradient Descent(48969): loss=7.694849894689841\n",
      "Stochastic Gradient Descent(48970): loss=3.7437509495827412\n",
      "Stochastic Gradient Descent(48971): loss=1.1519384193640738\n",
      "Stochastic Gradient Descent(48972): loss=1.709175270360788\n",
      "Stochastic Gradient Descent(48973): loss=0.197570956592206\n",
      "Stochastic Gradient Descent(48974): loss=0.3637461935450468\n",
      "Stochastic Gradient Descent(48975): loss=0.19279296255860934\n",
      "Stochastic Gradient Descent(48976): loss=0.8221951131094779\n",
      "Stochastic Gradient Descent(48977): loss=2.808052513133247\n",
      "Stochastic Gradient Descent(48978): loss=1.834390612537883\n",
      "Stochastic Gradient Descent(48979): loss=2.548817245189866\n",
      "Stochastic Gradient Descent(48980): loss=0.03857343923503145\n",
      "Stochastic Gradient Descent(48981): loss=0.046597049274898106\n",
      "Stochastic Gradient Descent(48982): loss=5.950552077643034\n",
      "Stochastic Gradient Descent(48983): loss=1.6169696861467244\n",
      "Stochastic Gradient Descent(48984): loss=0.10375939346207014\n",
      "Stochastic Gradient Descent(48985): loss=1.4841696219237464\n",
      "Stochastic Gradient Descent(48986): loss=0.5214105642486307\n",
      "Stochastic Gradient Descent(48987): loss=1.2374181644526654\n",
      "Stochastic Gradient Descent(48988): loss=2.30680372675122\n",
      "Stochastic Gradient Descent(48989): loss=2.738913920927878\n",
      "Stochastic Gradient Descent(48990): loss=0.04227505999329598\n",
      "Stochastic Gradient Descent(48991): loss=2.3107834289552844\n",
      "Stochastic Gradient Descent(48992): loss=0.3910280331053911\n",
      "Stochastic Gradient Descent(48993): loss=0.26633876229693243\n",
      "Stochastic Gradient Descent(48994): loss=2.5749464783328935\n",
      "Stochastic Gradient Descent(48995): loss=0.059456166595244726\n",
      "Stochastic Gradient Descent(48996): loss=2.5552859752500314\n",
      "Stochastic Gradient Descent(48997): loss=0.008062879816307038\n",
      "Stochastic Gradient Descent(48998): loss=0.30991518722797406\n",
      "Stochastic Gradient Descent(48999): loss=29.010585451700717\n",
      "Stochastic Gradient Descent(49000): loss=3.4640538599475788\n",
      "Stochastic Gradient Descent(49001): loss=0.27855437370505975\n",
      "Stochastic Gradient Descent(49002): loss=8.069124149173717\n",
      "Stochastic Gradient Descent(49003): loss=1.1678081766132253\n",
      "Stochastic Gradient Descent(49004): loss=29.9324073502791\n",
      "Stochastic Gradient Descent(49005): loss=0.527903588399845\n",
      "Stochastic Gradient Descent(49006): loss=3.5211119388698124\n",
      "Stochastic Gradient Descent(49007): loss=0.023896713789886878\n",
      "Stochastic Gradient Descent(49008): loss=0.021274086845295543\n",
      "Stochastic Gradient Descent(49009): loss=22.113039985810882\n",
      "Stochastic Gradient Descent(49010): loss=0.7168032090323241\n",
      "Stochastic Gradient Descent(49011): loss=0.09647584500315401\n",
      "Stochastic Gradient Descent(49012): loss=0.8871952414165147\n",
      "Stochastic Gradient Descent(49013): loss=1.9249094450578368\n",
      "Stochastic Gradient Descent(49014): loss=2.6460963023797732\n",
      "Stochastic Gradient Descent(49015): loss=0.26316408512931133\n",
      "Stochastic Gradient Descent(49016): loss=2.8118016167863216\n",
      "Stochastic Gradient Descent(49017): loss=3.534246437240107\n",
      "Stochastic Gradient Descent(49018): loss=0.4686569776757742\n",
      "Stochastic Gradient Descent(49019): loss=1.007217825124156\n",
      "Stochastic Gradient Descent(49020): loss=0.2446845322573632\n",
      "Stochastic Gradient Descent(49021): loss=0.19764369738260607\n",
      "Stochastic Gradient Descent(49022): loss=8.258461290089766\n",
      "Stochastic Gradient Descent(49023): loss=0.7747570502963166\n",
      "Stochastic Gradient Descent(49024): loss=3.1727361262201814\n",
      "Stochastic Gradient Descent(49025): loss=1.3969754395553398\n",
      "Stochastic Gradient Descent(49026): loss=3.016542663797083\n",
      "Stochastic Gradient Descent(49027): loss=1.1868125630729338\n",
      "Stochastic Gradient Descent(49028): loss=2.0266103650742417\n",
      "Stochastic Gradient Descent(49029): loss=8.252920374718066\n",
      "Stochastic Gradient Descent(49030): loss=2.8422300017232676\n",
      "Stochastic Gradient Descent(49031): loss=11.85206857280256\n",
      "Stochastic Gradient Descent(49032): loss=0.14666509018354254\n",
      "Stochastic Gradient Descent(49033): loss=0.2561872482767569\n",
      "Stochastic Gradient Descent(49034): loss=5.827966832859182\n",
      "Stochastic Gradient Descent(49035): loss=6.823584398632102\n",
      "Stochastic Gradient Descent(49036): loss=6.265815490145588\n",
      "Stochastic Gradient Descent(49037): loss=0.17445575748656916\n",
      "Stochastic Gradient Descent(49038): loss=0.0005480095748183411\n",
      "Stochastic Gradient Descent(49039): loss=3.621364668524832\n",
      "Stochastic Gradient Descent(49040): loss=1.639875217069154\n",
      "Stochastic Gradient Descent(49041): loss=2.7462560183195235\n",
      "Stochastic Gradient Descent(49042): loss=0.8473981144264668\n",
      "Stochastic Gradient Descent(49043): loss=0.11891955122497642\n",
      "Stochastic Gradient Descent(49044): loss=2.95939647165268\n",
      "Stochastic Gradient Descent(49045): loss=0.3589054221050131\n",
      "Stochastic Gradient Descent(49046): loss=0.00010269143396292092\n",
      "Stochastic Gradient Descent(49047): loss=0.8841388668832277\n",
      "Stochastic Gradient Descent(49048): loss=4.901065863592281\n",
      "Stochastic Gradient Descent(49049): loss=3.432308400066918\n",
      "Stochastic Gradient Descent(49050): loss=12.150803732076309\n",
      "Stochastic Gradient Descent(49051): loss=0.8256079655187919\n",
      "Stochastic Gradient Descent(49052): loss=1.0144890128885322\n",
      "Stochastic Gradient Descent(49053): loss=4.47665963433802\n",
      "Stochastic Gradient Descent(49054): loss=3.2102530486862273\n",
      "Stochastic Gradient Descent(49055): loss=3.772256226356321\n",
      "Stochastic Gradient Descent(49056): loss=2.91600646663868\n",
      "Stochastic Gradient Descent(49057): loss=1.7982952524596691\n",
      "Stochastic Gradient Descent(49058): loss=14.527720389396869\n",
      "Stochastic Gradient Descent(49059): loss=11.976334282531885\n",
      "Stochastic Gradient Descent(49060): loss=8.3393734474487\n",
      "Stochastic Gradient Descent(49061): loss=0.08346364533848599\n",
      "Stochastic Gradient Descent(49062): loss=0.30248626149270386\n",
      "Stochastic Gradient Descent(49063): loss=2.298158078515666\n",
      "Stochastic Gradient Descent(49064): loss=0.0629642269863582\n",
      "Stochastic Gradient Descent(49065): loss=0.11415216608432988\n",
      "Stochastic Gradient Descent(49066): loss=3.2841128429399213\n",
      "Stochastic Gradient Descent(49067): loss=1.157851490285947e-05\n",
      "Stochastic Gradient Descent(49068): loss=0.5474561082266409\n",
      "Stochastic Gradient Descent(49069): loss=13.54638970147135\n",
      "Stochastic Gradient Descent(49070): loss=4.409895126901124\n",
      "Stochastic Gradient Descent(49071): loss=1.5184975580060998\n",
      "Stochastic Gradient Descent(49072): loss=16.895321515044305\n",
      "Stochastic Gradient Descent(49073): loss=5.834937104776742\n",
      "Stochastic Gradient Descent(49074): loss=0.014715118401395477\n",
      "Stochastic Gradient Descent(49075): loss=0.046903186348849685\n",
      "Stochastic Gradient Descent(49076): loss=1.9632966721814056\n",
      "Stochastic Gradient Descent(49077): loss=1.1125945535910904\n",
      "Stochastic Gradient Descent(49078): loss=3.066534719571888\n",
      "Stochastic Gradient Descent(49079): loss=0.0697448472178498\n",
      "Stochastic Gradient Descent(49080): loss=0.035644654008614246\n",
      "Stochastic Gradient Descent(49081): loss=4.7411021192377304e-05\n",
      "Stochastic Gradient Descent(49082): loss=1.6852658094048916\n",
      "Stochastic Gradient Descent(49083): loss=1.8902206645913278\n",
      "Stochastic Gradient Descent(49084): loss=6.060273062828583\n",
      "Stochastic Gradient Descent(49085): loss=8.416263382535167\n",
      "Stochastic Gradient Descent(49086): loss=0.29894439500572484\n",
      "Stochastic Gradient Descent(49087): loss=0.6047815424271156\n",
      "Stochastic Gradient Descent(49088): loss=0.13992629156408162\n",
      "Stochastic Gradient Descent(49089): loss=1.1005248417028521\n",
      "Stochastic Gradient Descent(49090): loss=0.300198767152672\n",
      "Stochastic Gradient Descent(49091): loss=0.09122223222033442\n",
      "Stochastic Gradient Descent(49092): loss=2.239612769608506\n",
      "Stochastic Gradient Descent(49093): loss=4.613156584914332\n",
      "Stochastic Gradient Descent(49094): loss=14.036989492367727\n",
      "Stochastic Gradient Descent(49095): loss=7.045276816331696\n",
      "Stochastic Gradient Descent(49096): loss=9.830370683886583\n",
      "Stochastic Gradient Descent(49097): loss=0.12490633875771055\n",
      "Stochastic Gradient Descent(49098): loss=0.36914137450997725\n",
      "Stochastic Gradient Descent(49099): loss=2.4127645179556554\n",
      "Stochastic Gradient Descent(49100): loss=0.9591016905042279\n",
      "Stochastic Gradient Descent(49101): loss=0.25333397286587694\n",
      "Stochastic Gradient Descent(49102): loss=9.054943975780448\n",
      "Stochastic Gradient Descent(49103): loss=1.8668279035196051\n",
      "Stochastic Gradient Descent(49104): loss=21.929254563512995\n",
      "Stochastic Gradient Descent(49105): loss=12.284666057984433\n",
      "Stochastic Gradient Descent(49106): loss=0.2989704300722928\n",
      "Stochastic Gradient Descent(49107): loss=1.4202429151466063\n",
      "Stochastic Gradient Descent(49108): loss=3.5220145759338854\n",
      "Stochastic Gradient Descent(49109): loss=15.128306303360368\n",
      "Stochastic Gradient Descent(49110): loss=2.099665083792928\n",
      "Stochastic Gradient Descent(49111): loss=4.459577621154119e-05\n",
      "Stochastic Gradient Descent(49112): loss=2.6109017376416537\n",
      "Stochastic Gradient Descent(49113): loss=0.05771056236656773\n",
      "Stochastic Gradient Descent(49114): loss=4.191299279937194\n",
      "Stochastic Gradient Descent(49115): loss=15.903569734913605\n",
      "Stochastic Gradient Descent(49116): loss=1.1384158379044793\n",
      "Stochastic Gradient Descent(49117): loss=10.87467886160914\n",
      "Stochastic Gradient Descent(49118): loss=4.709607511716215\n",
      "Stochastic Gradient Descent(49119): loss=0.6241840014406425\n",
      "Stochastic Gradient Descent(49120): loss=3.696100673368587\n",
      "Stochastic Gradient Descent(49121): loss=1.7291126691134824\n",
      "Stochastic Gradient Descent(49122): loss=1.7187946362967106\n",
      "Stochastic Gradient Descent(49123): loss=5.430911458430959\n",
      "Stochastic Gradient Descent(49124): loss=5.108771120181507\n",
      "Stochastic Gradient Descent(49125): loss=1.388354618405761\n",
      "Stochastic Gradient Descent(49126): loss=4.3332443376470335\n",
      "Stochastic Gradient Descent(49127): loss=1.5890135765154558\n",
      "Stochastic Gradient Descent(49128): loss=3.0533935961032546\n",
      "Stochastic Gradient Descent(49129): loss=1.9055973381691997\n",
      "Stochastic Gradient Descent(49130): loss=0.11406816308328273\n",
      "Stochastic Gradient Descent(49131): loss=3.5566685588864972\n",
      "Stochastic Gradient Descent(49132): loss=18.723084465520902\n",
      "Stochastic Gradient Descent(49133): loss=0.09176809005744574\n",
      "Stochastic Gradient Descent(49134): loss=0.6633253353217533\n",
      "Stochastic Gradient Descent(49135): loss=2.4204482706230217\n",
      "Stochastic Gradient Descent(49136): loss=4.195628653218767\n",
      "Stochastic Gradient Descent(49137): loss=4.645193393003264\n",
      "Stochastic Gradient Descent(49138): loss=0.2865211962271468\n",
      "Stochastic Gradient Descent(49139): loss=2.891073628041536\n",
      "Stochastic Gradient Descent(49140): loss=0.0071350416486847725\n",
      "Stochastic Gradient Descent(49141): loss=2.4937350440014314\n",
      "Stochastic Gradient Descent(49142): loss=0.015107153152480523\n",
      "Stochastic Gradient Descent(49143): loss=0.3666158352120926\n",
      "Stochastic Gradient Descent(49144): loss=0.0001235244999650854\n",
      "Stochastic Gradient Descent(49145): loss=0.34043252114364825\n",
      "Stochastic Gradient Descent(49146): loss=0.6034841566436006\n",
      "Stochastic Gradient Descent(49147): loss=5.800660042342363\n",
      "Stochastic Gradient Descent(49148): loss=1.455228551833029\n",
      "Stochastic Gradient Descent(49149): loss=8.579269371255966\n",
      "Stochastic Gradient Descent(49150): loss=0.258575178607407\n",
      "Stochastic Gradient Descent(49151): loss=0.011403484598749097\n",
      "Stochastic Gradient Descent(49152): loss=3.467956087524345\n",
      "Stochastic Gradient Descent(49153): loss=28.729082921142098\n",
      "Stochastic Gradient Descent(49154): loss=0.9771056659941066\n",
      "Stochastic Gradient Descent(49155): loss=0.7901542866625583\n",
      "Stochastic Gradient Descent(49156): loss=3.183350237141874\n",
      "Stochastic Gradient Descent(49157): loss=0.4014132565336992\n",
      "Stochastic Gradient Descent(49158): loss=1.7795117939832872\n",
      "Stochastic Gradient Descent(49159): loss=0.39228967467329434\n",
      "Stochastic Gradient Descent(49160): loss=1.0803460347037757\n",
      "Stochastic Gradient Descent(49161): loss=12.679463415032002\n",
      "Stochastic Gradient Descent(49162): loss=5.11456472939383\n",
      "Stochastic Gradient Descent(49163): loss=3.839541480389221\n",
      "Stochastic Gradient Descent(49164): loss=1.749680390243193\n",
      "Stochastic Gradient Descent(49165): loss=0.2587111865784008\n",
      "Stochastic Gradient Descent(49166): loss=0.7013631557816947\n",
      "Stochastic Gradient Descent(49167): loss=0.6779953522010119\n",
      "Stochastic Gradient Descent(49168): loss=1.722883713562773\n",
      "Stochastic Gradient Descent(49169): loss=5.226621076392518\n",
      "Stochastic Gradient Descent(49170): loss=1.144820130179948\n",
      "Stochastic Gradient Descent(49171): loss=1.4077694006651102\n",
      "Stochastic Gradient Descent(49172): loss=0.3212114872585282\n",
      "Stochastic Gradient Descent(49173): loss=2.647169238214771\n",
      "Stochastic Gradient Descent(49174): loss=7.680911573635981\n",
      "Stochastic Gradient Descent(49175): loss=3.6696698915907984\n",
      "Stochastic Gradient Descent(49176): loss=4.19942191459042\n",
      "Stochastic Gradient Descent(49177): loss=0.8621699462243644\n",
      "Stochastic Gradient Descent(49178): loss=0.6195111752433313\n",
      "Stochastic Gradient Descent(49179): loss=6.700017898367373\n",
      "Stochastic Gradient Descent(49180): loss=5.866995806801492\n",
      "Stochastic Gradient Descent(49181): loss=3.8749414236747604\n",
      "Stochastic Gradient Descent(49182): loss=0.4474849042450994\n",
      "Stochastic Gradient Descent(49183): loss=1.1638537692189237\n",
      "Stochastic Gradient Descent(49184): loss=0.16797443774173348\n",
      "Stochastic Gradient Descent(49185): loss=0.03843228579367418\n",
      "Stochastic Gradient Descent(49186): loss=0.10882398631838572\n",
      "Stochastic Gradient Descent(49187): loss=0.3561791262982522\n",
      "Stochastic Gradient Descent(49188): loss=0.0014218553971551105\n",
      "Stochastic Gradient Descent(49189): loss=0.6001261898664186\n",
      "Stochastic Gradient Descent(49190): loss=2.486716782903278\n",
      "Stochastic Gradient Descent(49191): loss=0.035908070569156625\n",
      "Stochastic Gradient Descent(49192): loss=2.0006031122209125\n",
      "Stochastic Gradient Descent(49193): loss=2.726227066700376\n",
      "Stochastic Gradient Descent(49194): loss=0.4922777378500993\n",
      "Stochastic Gradient Descent(49195): loss=3.0106058020260122\n",
      "Stochastic Gradient Descent(49196): loss=0.15053094980330353\n",
      "Stochastic Gradient Descent(49197): loss=1.7156142933015226\n",
      "Stochastic Gradient Descent(49198): loss=0.2463821052988198\n",
      "Stochastic Gradient Descent(49199): loss=0.559379060392312\n",
      "Stochastic Gradient Descent(49200): loss=0.4666758169635474\n",
      "Stochastic Gradient Descent(49201): loss=7.244548992423281\n",
      "Stochastic Gradient Descent(49202): loss=3.716812924335216\n",
      "Stochastic Gradient Descent(49203): loss=14.273426086098867\n",
      "Stochastic Gradient Descent(49204): loss=0.1459428947470624\n",
      "Stochastic Gradient Descent(49205): loss=2.7951855173723414\n",
      "Stochastic Gradient Descent(49206): loss=0.29774319741643823\n",
      "Stochastic Gradient Descent(49207): loss=1.200800261754989\n",
      "Stochastic Gradient Descent(49208): loss=2.121044473826244\n",
      "Stochastic Gradient Descent(49209): loss=2.362627109069663\n",
      "Stochastic Gradient Descent(49210): loss=2.1883177722070384\n",
      "Stochastic Gradient Descent(49211): loss=2.4474726792691595\n",
      "Stochastic Gradient Descent(49212): loss=2.074970083591754\n",
      "Stochastic Gradient Descent(49213): loss=2.424956178128991\n",
      "Stochastic Gradient Descent(49214): loss=2.4863149933697204\n",
      "Stochastic Gradient Descent(49215): loss=1.1504668583619762\n",
      "Stochastic Gradient Descent(49216): loss=0.0003099135529232794\n",
      "Stochastic Gradient Descent(49217): loss=0.6657470070621826\n",
      "Stochastic Gradient Descent(49218): loss=6.80706901852731\n",
      "Stochastic Gradient Descent(49219): loss=0.364711004436482\n",
      "Stochastic Gradient Descent(49220): loss=0.18395957625421286\n",
      "Stochastic Gradient Descent(49221): loss=0.7568356117002972\n",
      "Stochastic Gradient Descent(49222): loss=0.14455178055864745\n",
      "Stochastic Gradient Descent(49223): loss=8.226316370640806\n",
      "Stochastic Gradient Descent(49224): loss=10.951895355349142\n",
      "Stochastic Gradient Descent(49225): loss=1.8869211835506374\n",
      "Stochastic Gradient Descent(49226): loss=9.662035668042984\n",
      "Stochastic Gradient Descent(49227): loss=0.00042387072111317353\n",
      "Stochastic Gradient Descent(49228): loss=0.397884273169018\n",
      "Stochastic Gradient Descent(49229): loss=0.02723834641015765\n",
      "Stochastic Gradient Descent(49230): loss=2.200452831129712\n",
      "Stochastic Gradient Descent(49231): loss=11.832332612104791\n",
      "Stochastic Gradient Descent(49232): loss=7.914122369421906\n",
      "Stochastic Gradient Descent(49233): loss=0.22944960990253924\n",
      "Stochastic Gradient Descent(49234): loss=6.618160625945086\n",
      "Stochastic Gradient Descent(49235): loss=17.21458779110972\n",
      "Stochastic Gradient Descent(49236): loss=16.324907471364444\n",
      "Stochastic Gradient Descent(49237): loss=0.09883202447841499\n",
      "Stochastic Gradient Descent(49238): loss=3.2761655576953026\n",
      "Stochastic Gradient Descent(49239): loss=0.06537125832242831\n",
      "Stochastic Gradient Descent(49240): loss=16.992073085309485\n",
      "Stochastic Gradient Descent(49241): loss=0.7696330805320821\n",
      "Stochastic Gradient Descent(49242): loss=3.8269148987755273\n",
      "Stochastic Gradient Descent(49243): loss=9.126588597413281\n",
      "Stochastic Gradient Descent(49244): loss=24.840468192939007\n",
      "Stochastic Gradient Descent(49245): loss=6.733178597765186\n",
      "Stochastic Gradient Descent(49246): loss=8.442349978271245\n",
      "Stochastic Gradient Descent(49247): loss=1.0526938274728315\n",
      "Stochastic Gradient Descent(49248): loss=1.0746351909100953\n",
      "Stochastic Gradient Descent(49249): loss=0.9628517858222551\n",
      "Stochastic Gradient Descent(49250): loss=0.5059038369129412\n",
      "Stochastic Gradient Descent(49251): loss=1.7491437637598575\n",
      "Stochastic Gradient Descent(49252): loss=5.44678528313872\n",
      "Stochastic Gradient Descent(49253): loss=1.487162626890665\n",
      "Stochastic Gradient Descent(49254): loss=0.09002908222970876\n",
      "Stochastic Gradient Descent(49255): loss=1.0045507891782444\n",
      "Stochastic Gradient Descent(49256): loss=0.00033252637527878866\n",
      "Stochastic Gradient Descent(49257): loss=7.217608037377649\n",
      "Stochastic Gradient Descent(49258): loss=0.01649085270516473\n",
      "Stochastic Gradient Descent(49259): loss=1.7257504884870545\n",
      "Stochastic Gradient Descent(49260): loss=1.5344775678195368\n",
      "Stochastic Gradient Descent(49261): loss=8.18657046933333\n",
      "Stochastic Gradient Descent(49262): loss=7.668995509126117\n",
      "Stochastic Gradient Descent(49263): loss=0.011639150745642365\n",
      "Stochastic Gradient Descent(49264): loss=7.293766832748971\n",
      "Stochastic Gradient Descent(49265): loss=9.559877926469007\n",
      "Stochastic Gradient Descent(49266): loss=0.314112048857242\n",
      "Stochastic Gradient Descent(49267): loss=12.18112600671865\n",
      "Stochastic Gradient Descent(49268): loss=4.79420900157446\n",
      "Stochastic Gradient Descent(49269): loss=0.070880273574502\n",
      "Stochastic Gradient Descent(49270): loss=10.520571128161011\n",
      "Stochastic Gradient Descent(49271): loss=9.04485265623691\n",
      "Stochastic Gradient Descent(49272): loss=0.002172252809436058\n",
      "Stochastic Gradient Descent(49273): loss=5.356163819142956\n",
      "Stochastic Gradient Descent(49274): loss=0.16449285385165152\n",
      "Stochastic Gradient Descent(49275): loss=2.2532067118714454\n",
      "Stochastic Gradient Descent(49276): loss=1.323535261895743\n",
      "Stochastic Gradient Descent(49277): loss=0.7336729633049578\n",
      "Stochastic Gradient Descent(49278): loss=3.919725875095617\n",
      "Stochastic Gradient Descent(49279): loss=13.691611481156857\n",
      "Stochastic Gradient Descent(49280): loss=0.45188602360793617\n",
      "Stochastic Gradient Descent(49281): loss=3.6878064548817386\n",
      "Stochastic Gradient Descent(49282): loss=1.9245403338061413\n",
      "Stochastic Gradient Descent(49283): loss=3.110452877831472\n",
      "Stochastic Gradient Descent(49284): loss=0.9503299616297728\n",
      "Stochastic Gradient Descent(49285): loss=0.9749521467209258\n",
      "Stochastic Gradient Descent(49286): loss=0.9283428190968139\n",
      "Stochastic Gradient Descent(49287): loss=0.6395886426097526\n",
      "Stochastic Gradient Descent(49288): loss=0.06155555701153885\n",
      "Stochastic Gradient Descent(49289): loss=7.436654665533676\n",
      "Stochastic Gradient Descent(49290): loss=5.775251919218566\n",
      "Stochastic Gradient Descent(49291): loss=29.964375858376215\n",
      "Stochastic Gradient Descent(49292): loss=9.929917297057683\n",
      "Stochastic Gradient Descent(49293): loss=16.996185642024294\n",
      "Stochastic Gradient Descent(49294): loss=2.0386445576180203\n",
      "Stochastic Gradient Descent(49295): loss=0.008647960308470266\n",
      "Stochastic Gradient Descent(49296): loss=6.738120272428793\n",
      "Stochastic Gradient Descent(49297): loss=0.7639506552883603\n",
      "Stochastic Gradient Descent(49298): loss=1.524863767795483\n",
      "Stochastic Gradient Descent(49299): loss=2.6299239180063982\n",
      "Stochastic Gradient Descent(49300): loss=3.5242177607540737\n",
      "Stochastic Gradient Descent(49301): loss=0.2666768695568006\n",
      "Stochastic Gradient Descent(49302): loss=0.16619644941927658\n",
      "Stochastic Gradient Descent(49303): loss=12.095398707068574\n",
      "Stochastic Gradient Descent(49304): loss=0.08466978828939084\n",
      "Stochastic Gradient Descent(49305): loss=9.574170743831296\n",
      "Stochastic Gradient Descent(49306): loss=5.295269784164775\n",
      "Stochastic Gradient Descent(49307): loss=3.852950314781905\n",
      "Stochastic Gradient Descent(49308): loss=18.55595219873361\n",
      "Stochastic Gradient Descent(49309): loss=9.75812113795492\n",
      "Stochastic Gradient Descent(49310): loss=2.877365654047633\n",
      "Stochastic Gradient Descent(49311): loss=0.20750588020559868\n",
      "Stochastic Gradient Descent(49312): loss=9.599919504414391\n",
      "Stochastic Gradient Descent(49313): loss=0.02396213715737119\n",
      "Stochastic Gradient Descent(49314): loss=1.412971324547495\n",
      "Stochastic Gradient Descent(49315): loss=0.26649110085608907\n",
      "Stochastic Gradient Descent(49316): loss=6.828207838343704\n",
      "Stochastic Gradient Descent(49317): loss=0.348535194217683\n",
      "Stochastic Gradient Descent(49318): loss=5.410603970796603\n",
      "Stochastic Gradient Descent(49319): loss=0.01171542221642211\n",
      "Stochastic Gradient Descent(49320): loss=0.0010409783669275167\n",
      "Stochastic Gradient Descent(49321): loss=0.04790376946730366\n",
      "Stochastic Gradient Descent(49322): loss=1.8021462292936352\n",
      "Stochastic Gradient Descent(49323): loss=14.454315237388604\n",
      "Stochastic Gradient Descent(49324): loss=2.8178482339973514\n",
      "Stochastic Gradient Descent(49325): loss=2.7836802357546535\n",
      "Stochastic Gradient Descent(49326): loss=10.098553030355816\n",
      "Stochastic Gradient Descent(49327): loss=7.752073344836975\n",
      "Stochastic Gradient Descent(49328): loss=1.7186539168550345\n",
      "Stochastic Gradient Descent(49329): loss=3.6696078102879808\n",
      "Stochastic Gradient Descent(49330): loss=1.853776548733403\n",
      "Stochastic Gradient Descent(49331): loss=1.3170256673158953\n",
      "Stochastic Gradient Descent(49332): loss=3.0490320454799567\n",
      "Stochastic Gradient Descent(49333): loss=0.23166773821349795\n",
      "Stochastic Gradient Descent(49334): loss=1.67593610386975\n",
      "Stochastic Gradient Descent(49335): loss=7.841372243216357\n",
      "Stochastic Gradient Descent(49336): loss=0.7451082165467692\n",
      "Stochastic Gradient Descent(49337): loss=0.37748525667966526\n",
      "Stochastic Gradient Descent(49338): loss=2.1985115532237764\n",
      "Stochastic Gradient Descent(49339): loss=0.03294402746843792\n",
      "Stochastic Gradient Descent(49340): loss=0.9930998840879175\n",
      "Stochastic Gradient Descent(49341): loss=9.407339650060017\n",
      "Stochastic Gradient Descent(49342): loss=0.9800211135902103\n",
      "Stochastic Gradient Descent(49343): loss=1.4118124849571103\n",
      "Stochastic Gradient Descent(49344): loss=6.570442096324304\n",
      "Stochastic Gradient Descent(49345): loss=8.754028991272078\n",
      "Stochastic Gradient Descent(49346): loss=4.055088118604343\n",
      "Stochastic Gradient Descent(49347): loss=1.0636836770053233\n",
      "Stochastic Gradient Descent(49348): loss=0.05133989332494067\n",
      "Stochastic Gradient Descent(49349): loss=0.004792943546493098\n",
      "Stochastic Gradient Descent(49350): loss=2.996986672603864\n",
      "Stochastic Gradient Descent(49351): loss=4.3077860455643835\n",
      "Stochastic Gradient Descent(49352): loss=1.4580675166497798\n",
      "Stochastic Gradient Descent(49353): loss=12.263318494618709\n",
      "Stochastic Gradient Descent(49354): loss=4.264691602495063\n",
      "Stochastic Gradient Descent(49355): loss=0.936310197285823\n",
      "Stochastic Gradient Descent(49356): loss=0.7022043665865013\n",
      "Stochastic Gradient Descent(49357): loss=6.080424881452666\n",
      "Stochastic Gradient Descent(49358): loss=1.659546169229443\n",
      "Stochastic Gradient Descent(49359): loss=11.960786540859054\n",
      "Stochastic Gradient Descent(49360): loss=0.26402670087500024\n",
      "Stochastic Gradient Descent(49361): loss=3.0119800995705077\n",
      "Stochastic Gradient Descent(49362): loss=0.5289987710178973\n",
      "Stochastic Gradient Descent(49363): loss=5.130910785440282\n",
      "Stochastic Gradient Descent(49364): loss=0.06850545704155969\n",
      "Stochastic Gradient Descent(49365): loss=0.2408939036771419\n",
      "Stochastic Gradient Descent(49366): loss=3.6417578613783\n",
      "Stochastic Gradient Descent(49367): loss=1.4207660431365774\n",
      "Stochastic Gradient Descent(49368): loss=1.6331088465130497\n",
      "Stochastic Gradient Descent(49369): loss=0.006525441134900226\n",
      "Stochastic Gradient Descent(49370): loss=4.278897043298026\n",
      "Stochastic Gradient Descent(49371): loss=16.57663011657229\n",
      "Stochastic Gradient Descent(49372): loss=20.86274072754006\n",
      "Stochastic Gradient Descent(49373): loss=3.152838570052435\n",
      "Stochastic Gradient Descent(49374): loss=4.066495464262821\n",
      "Stochastic Gradient Descent(49375): loss=5.904494557138667\n",
      "Stochastic Gradient Descent(49376): loss=0.0852621276080774\n",
      "Stochastic Gradient Descent(49377): loss=7.889618208181259\n",
      "Stochastic Gradient Descent(49378): loss=3.2121416868882107\n",
      "Stochastic Gradient Descent(49379): loss=7.767963856021217\n",
      "Stochastic Gradient Descent(49380): loss=0.39893602620528956\n",
      "Stochastic Gradient Descent(49381): loss=0.7596765129817052\n",
      "Stochastic Gradient Descent(49382): loss=3.995418770697902\n",
      "Stochastic Gradient Descent(49383): loss=0.15231173700359987\n",
      "Stochastic Gradient Descent(49384): loss=0.8522215189588214\n",
      "Stochastic Gradient Descent(49385): loss=4.452429056390447\n",
      "Stochastic Gradient Descent(49386): loss=0.14470094846572185\n",
      "Stochastic Gradient Descent(49387): loss=0.6799950834025712\n",
      "Stochastic Gradient Descent(49388): loss=0.6439362351509292\n",
      "Stochastic Gradient Descent(49389): loss=2.9692680569387946\n",
      "Stochastic Gradient Descent(49390): loss=0.005001145950516722\n",
      "Stochastic Gradient Descent(49391): loss=0.2382496348026935\n",
      "Stochastic Gradient Descent(49392): loss=0.6951215815147415\n",
      "Stochastic Gradient Descent(49393): loss=1.147881976681883\n",
      "Stochastic Gradient Descent(49394): loss=5.552781557066275\n",
      "Stochastic Gradient Descent(49395): loss=1.5386164289379345\n",
      "Stochastic Gradient Descent(49396): loss=0.02650025214242253\n",
      "Stochastic Gradient Descent(49397): loss=2.9318886511018873\n",
      "Stochastic Gradient Descent(49398): loss=1.1414291034835513\n",
      "Stochastic Gradient Descent(49399): loss=25.748766802887484\n",
      "Stochastic Gradient Descent(49400): loss=98.5228602990674\n",
      "Stochastic Gradient Descent(49401): loss=20.89351714745728\n",
      "Stochastic Gradient Descent(49402): loss=0.16097024079146058\n",
      "Stochastic Gradient Descent(49403): loss=0.7665240233546505\n",
      "Stochastic Gradient Descent(49404): loss=0.7380767835553205\n",
      "Stochastic Gradient Descent(49405): loss=0.8605832393271113\n",
      "Stochastic Gradient Descent(49406): loss=62.77489952974994\n",
      "Stochastic Gradient Descent(49407): loss=2.750816597010981\n",
      "Stochastic Gradient Descent(49408): loss=2.2210587451461974\n",
      "Stochastic Gradient Descent(49409): loss=0.15664721066657342\n",
      "Stochastic Gradient Descent(49410): loss=0.42450384446479533\n",
      "Stochastic Gradient Descent(49411): loss=0.8897425624978066\n",
      "Stochastic Gradient Descent(49412): loss=2.3700110595956545\n",
      "Stochastic Gradient Descent(49413): loss=0.13293780869926\n",
      "Stochastic Gradient Descent(49414): loss=0.7369572916586408\n",
      "Stochastic Gradient Descent(49415): loss=3.880028225751518\n",
      "Stochastic Gradient Descent(49416): loss=0.06482262943881115\n",
      "Stochastic Gradient Descent(49417): loss=24.593147215215122\n",
      "Stochastic Gradient Descent(49418): loss=4.197620856296071\n",
      "Stochastic Gradient Descent(49419): loss=3.345347364313418\n",
      "Stochastic Gradient Descent(49420): loss=0.11397564343087327\n",
      "Stochastic Gradient Descent(49421): loss=0.10891188135926357\n",
      "Stochastic Gradient Descent(49422): loss=0.2393554996883729\n",
      "Stochastic Gradient Descent(49423): loss=1.8284946620679776\n",
      "Stochastic Gradient Descent(49424): loss=0.7628248546999\n",
      "Stochastic Gradient Descent(49425): loss=0.03515827139916237\n",
      "Stochastic Gradient Descent(49426): loss=1.0315079935613578\n",
      "Stochastic Gradient Descent(49427): loss=13.4243689678596\n",
      "Stochastic Gradient Descent(49428): loss=3.9263641895845356\n",
      "Stochastic Gradient Descent(49429): loss=0.29585401509564635\n",
      "Stochastic Gradient Descent(49430): loss=0.10175289024967127\n",
      "Stochastic Gradient Descent(49431): loss=0.8151612517775102\n",
      "Stochastic Gradient Descent(49432): loss=0.0005436473560003905\n",
      "Stochastic Gradient Descent(49433): loss=2.9156200106405383\n",
      "Stochastic Gradient Descent(49434): loss=2.773871924533343\n",
      "Stochastic Gradient Descent(49435): loss=7.903765796006469\n",
      "Stochastic Gradient Descent(49436): loss=2.785266635607384\n",
      "Stochastic Gradient Descent(49437): loss=0.39104496879758294\n",
      "Stochastic Gradient Descent(49438): loss=3.7578965060122465\n",
      "Stochastic Gradient Descent(49439): loss=3.6282653746377997\n",
      "Stochastic Gradient Descent(49440): loss=13.453264658976556\n",
      "Stochastic Gradient Descent(49441): loss=1.2737499599974353\n",
      "Stochastic Gradient Descent(49442): loss=9.77678376532049\n",
      "Stochastic Gradient Descent(49443): loss=12.607143468987454\n",
      "Stochastic Gradient Descent(49444): loss=1.3049707557512937\n",
      "Stochastic Gradient Descent(49445): loss=3.3856151906378074\n",
      "Stochastic Gradient Descent(49446): loss=0.6800784828011914\n",
      "Stochastic Gradient Descent(49447): loss=3.9020007080714127\n",
      "Stochastic Gradient Descent(49448): loss=0.13417578126032356\n",
      "Stochastic Gradient Descent(49449): loss=5.888372211757908\n",
      "Stochastic Gradient Descent(49450): loss=11.63934470660488\n",
      "Stochastic Gradient Descent(49451): loss=1.4155325814946393\n",
      "Stochastic Gradient Descent(49452): loss=1.4977094470686179\n",
      "Stochastic Gradient Descent(49453): loss=0.6468007504220857\n",
      "Stochastic Gradient Descent(49454): loss=0.38858586049025245\n",
      "Stochastic Gradient Descent(49455): loss=2.1044626904346577\n",
      "Stochastic Gradient Descent(49456): loss=2.611857618674967\n",
      "Stochastic Gradient Descent(49457): loss=19.560181914434924\n",
      "Stochastic Gradient Descent(49458): loss=11.467570852010205\n",
      "Stochastic Gradient Descent(49459): loss=2.0239006933203636\n",
      "Stochastic Gradient Descent(49460): loss=0.6927442788293917\n",
      "Stochastic Gradient Descent(49461): loss=3.231984654113163\n",
      "Stochastic Gradient Descent(49462): loss=1.6189332675034813\n",
      "Stochastic Gradient Descent(49463): loss=19.975119742721635\n",
      "Stochastic Gradient Descent(49464): loss=0.3149402254910435\n",
      "Stochastic Gradient Descent(49465): loss=3.575963319802992\n",
      "Stochastic Gradient Descent(49466): loss=2.3782569750839135\n",
      "Stochastic Gradient Descent(49467): loss=2.144397076053566\n",
      "Stochastic Gradient Descent(49468): loss=10.539094659342819\n",
      "Stochastic Gradient Descent(49469): loss=3.82399614865638\n",
      "Stochastic Gradient Descent(49470): loss=0.819156334875098\n",
      "Stochastic Gradient Descent(49471): loss=1.8039495891828203\n",
      "Stochastic Gradient Descent(49472): loss=9.321063815245182\n",
      "Stochastic Gradient Descent(49473): loss=10.229294208099226\n",
      "Stochastic Gradient Descent(49474): loss=7.622076904501011\n",
      "Stochastic Gradient Descent(49475): loss=2.539391860270317\n",
      "Stochastic Gradient Descent(49476): loss=0.5600721250677981\n",
      "Stochastic Gradient Descent(49477): loss=1.3253568212499973\n",
      "Stochastic Gradient Descent(49478): loss=3.607311029910617\n",
      "Stochastic Gradient Descent(49479): loss=3.1223957029257394\n",
      "Stochastic Gradient Descent(49480): loss=8.523248256630307\n",
      "Stochastic Gradient Descent(49481): loss=1.525661541991675\n",
      "Stochastic Gradient Descent(49482): loss=0.0006454462450762309\n",
      "Stochastic Gradient Descent(49483): loss=7.784925958374913\n",
      "Stochastic Gradient Descent(49484): loss=4.492548127865559\n",
      "Stochastic Gradient Descent(49485): loss=3.2410927876853397\n",
      "Stochastic Gradient Descent(49486): loss=0.518699286700776\n",
      "Stochastic Gradient Descent(49487): loss=10.223716541158543\n",
      "Stochastic Gradient Descent(49488): loss=5.324542047580261\n",
      "Stochastic Gradient Descent(49489): loss=1.744964369481524\n",
      "Stochastic Gradient Descent(49490): loss=0.029935442931712308\n",
      "Stochastic Gradient Descent(49491): loss=0.4881257603594803\n",
      "Stochastic Gradient Descent(49492): loss=1.5563347271495382\n",
      "Stochastic Gradient Descent(49493): loss=1.074741047211687\n",
      "Stochastic Gradient Descent(49494): loss=0.516180229746356\n",
      "Stochastic Gradient Descent(49495): loss=2.733120566784006\n",
      "Stochastic Gradient Descent(49496): loss=1.7620330862285691\n",
      "Stochastic Gradient Descent(49497): loss=4.868108623354701\n",
      "Stochastic Gradient Descent(49498): loss=0.001958023866045402\n",
      "Stochastic Gradient Descent(49499): loss=5.8004048271521675\n",
      "Stochastic Gradient Descent(49500): loss=0.0005632781030500956\n",
      "Stochastic Gradient Descent(49501): loss=0.5276575514734672\n",
      "Stochastic Gradient Descent(49502): loss=6.406209076909159\n",
      "Stochastic Gradient Descent(49503): loss=0.030708534971173367\n",
      "Stochastic Gradient Descent(49504): loss=48.128497102511425\n",
      "Stochastic Gradient Descent(49505): loss=6.410072446655818\n",
      "Stochastic Gradient Descent(49506): loss=2.2856772040676074\n",
      "Stochastic Gradient Descent(49507): loss=0.675555358112391\n",
      "Stochastic Gradient Descent(49508): loss=9.112268062890356\n",
      "Stochastic Gradient Descent(49509): loss=2.865288122322499\n",
      "Stochastic Gradient Descent(49510): loss=2.1991825870147954\n",
      "Stochastic Gradient Descent(49511): loss=0.6411449430476526\n",
      "Stochastic Gradient Descent(49512): loss=0.0475713325202479\n",
      "Stochastic Gradient Descent(49513): loss=0.12419494930838795\n",
      "Stochastic Gradient Descent(49514): loss=0.04466450500930857\n",
      "Stochastic Gradient Descent(49515): loss=4.959374220731836\n",
      "Stochastic Gradient Descent(49516): loss=0.16085939361532522\n",
      "Stochastic Gradient Descent(49517): loss=2.286481934573774\n",
      "Stochastic Gradient Descent(49518): loss=0.6479521093278596\n",
      "Stochastic Gradient Descent(49519): loss=6.034084520853967\n",
      "Stochastic Gradient Descent(49520): loss=6.360525663732347\n",
      "Stochastic Gradient Descent(49521): loss=0.15731264679402812\n",
      "Stochastic Gradient Descent(49522): loss=6.464569481911284\n",
      "Stochastic Gradient Descent(49523): loss=1.8153286010514027\n",
      "Stochastic Gradient Descent(49524): loss=1.7415189106075266\n",
      "Stochastic Gradient Descent(49525): loss=1.5227658293581796\n",
      "Stochastic Gradient Descent(49526): loss=10.4900440478655\n",
      "Stochastic Gradient Descent(49527): loss=6.245294883149414\n",
      "Stochastic Gradient Descent(49528): loss=0.3008960325845078\n",
      "Stochastic Gradient Descent(49529): loss=2.9209269751247713\n",
      "Stochastic Gradient Descent(49530): loss=0.26597346347907697\n",
      "Stochastic Gradient Descent(49531): loss=0.23309923539662272\n",
      "Stochastic Gradient Descent(49532): loss=0.06331007924544725\n",
      "Stochastic Gradient Descent(49533): loss=0.8762867536419333\n",
      "Stochastic Gradient Descent(49534): loss=9.869467520599414\n",
      "Stochastic Gradient Descent(49535): loss=0.02138653701285499\n",
      "Stochastic Gradient Descent(49536): loss=8.945602794558999\n",
      "Stochastic Gradient Descent(49537): loss=3.086208168263242\n",
      "Stochastic Gradient Descent(49538): loss=1.735592714576999\n",
      "Stochastic Gradient Descent(49539): loss=2.810684400190467\n",
      "Stochastic Gradient Descent(49540): loss=0.4471935995289674\n",
      "Stochastic Gradient Descent(49541): loss=0.10958493756292019\n",
      "Stochastic Gradient Descent(49542): loss=1.366400137377596\n",
      "Stochastic Gradient Descent(49543): loss=6.649118373188626\n",
      "Stochastic Gradient Descent(49544): loss=0.04119681136504841\n",
      "Stochastic Gradient Descent(49545): loss=0.6247868711978711\n",
      "Stochastic Gradient Descent(49546): loss=0.5124740794909056\n",
      "Stochastic Gradient Descent(49547): loss=0.021381379556536642\n",
      "Stochastic Gradient Descent(49548): loss=0.7432166141478099\n",
      "Stochastic Gradient Descent(49549): loss=1.2778718506883673\n",
      "Stochastic Gradient Descent(49550): loss=0.3250321857499857\n",
      "Stochastic Gradient Descent(49551): loss=9.660565702161588\n",
      "Stochastic Gradient Descent(49552): loss=13.41143140160663\n",
      "Stochastic Gradient Descent(49553): loss=0.23990752186994602\n",
      "Stochastic Gradient Descent(49554): loss=0.03204017740357682\n",
      "Stochastic Gradient Descent(49555): loss=12.208747591567098\n",
      "Stochastic Gradient Descent(49556): loss=4.669756669898349\n",
      "Stochastic Gradient Descent(49557): loss=4.444776694830465\n",
      "Stochastic Gradient Descent(49558): loss=2.276512690530721\n",
      "Stochastic Gradient Descent(49559): loss=2.8607623885874562\n",
      "Stochastic Gradient Descent(49560): loss=1.2132410500822564\n",
      "Stochastic Gradient Descent(49561): loss=0.36677624981553886\n",
      "Stochastic Gradient Descent(49562): loss=2.186819449500844\n",
      "Stochastic Gradient Descent(49563): loss=2.775854775128885\n",
      "Stochastic Gradient Descent(49564): loss=0.3604819410571893\n",
      "Stochastic Gradient Descent(49565): loss=68.77450903625126\n",
      "Stochastic Gradient Descent(49566): loss=66.88289279057285\n",
      "Stochastic Gradient Descent(49567): loss=8.215225040968546\n",
      "Stochastic Gradient Descent(49568): loss=9.813049254894242\n",
      "Stochastic Gradient Descent(49569): loss=0.013692876136855435\n",
      "Stochastic Gradient Descent(49570): loss=0.6281516362048086\n",
      "Stochastic Gradient Descent(49571): loss=5.698456278754846\n",
      "Stochastic Gradient Descent(49572): loss=0.36973629204092756\n",
      "Stochastic Gradient Descent(49573): loss=4.666632746261345\n",
      "Stochastic Gradient Descent(49574): loss=7.206386533886464\n",
      "Stochastic Gradient Descent(49575): loss=9.171938759675557\n",
      "Stochastic Gradient Descent(49576): loss=2.348465728680081\n",
      "Stochastic Gradient Descent(49577): loss=7.87398297592741\n",
      "Stochastic Gradient Descent(49578): loss=2.2077032267563172\n",
      "Stochastic Gradient Descent(49579): loss=1.7507472871885519\n",
      "Stochastic Gradient Descent(49580): loss=17.12455623703127\n",
      "Stochastic Gradient Descent(49581): loss=3.2262884332300676\n",
      "Stochastic Gradient Descent(49582): loss=0.35414254052167593\n",
      "Stochastic Gradient Descent(49583): loss=5.764754224170402\n",
      "Stochastic Gradient Descent(49584): loss=2.3793090468787157\n",
      "Stochastic Gradient Descent(49585): loss=0.010994950821159569\n",
      "Stochastic Gradient Descent(49586): loss=0.14436168234581712\n",
      "Stochastic Gradient Descent(49587): loss=1.7350267779566744\n",
      "Stochastic Gradient Descent(49588): loss=5.40146499152929\n",
      "Stochastic Gradient Descent(49589): loss=0.18686323412577593\n",
      "Stochastic Gradient Descent(49590): loss=0.00806492881341969\n",
      "Stochastic Gradient Descent(49591): loss=3.5779097318938407\n",
      "Stochastic Gradient Descent(49592): loss=0.36963863683616593\n",
      "Stochastic Gradient Descent(49593): loss=0.06554077980221286\n",
      "Stochastic Gradient Descent(49594): loss=3.4311504127819945\n",
      "Stochastic Gradient Descent(49595): loss=0.10323842240873861\n",
      "Stochastic Gradient Descent(49596): loss=0.16813823731407165\n",
      "Stochastic Gradient Descent(49597): loss=0.6287481699918744\n",
      "Stochastic Gradient Descent(49598): loss=1.2388215981417885\n",
      "Stochastic Gradient Descent(49599): loss=0.37976818706877274\n",
      "Stochastic Gradient Descent(49600): loss=1.7356046285290716\n",
      "Stochastic Gradient Descent(49601): loss=4.996000912002349\n",
      "Stochastic Gradient Descent(49602): loss=2.7859382179100756\n",
      "Stochastic Gradient Descent(49603): loss=0.09459580063353082\n",
      "Stochastic Gradient Descent(49604): loss=1.0557032579797956\n",
      "Stochastic Gradient Descent(49605): loss=4.73115950367802\n",
      "Stochastic Gradient Descent(49606): loss=0.7420448658293415\n",
      "Stochastic Gradient Descent(49607): loss=4.066233078328364\n",
      "Stochastic Gradient Descent(49608): loss=0.5486565600587455\n",
      "Stochastic Gradient Descent(49609): loss=0.12412283945791208\n",
      "Stochastic Gradient Descent(49610): loss=3.3989673768870663\n",
      "Stochastic Gradient Descent(49611): loss=1.5256818419043814\n",
      "Stochastic Gradient Descent(49612): loss=0.4952824863142554\n",
      "Stochastic Gradient Descent(49613): loss=10.458223343408713\n",
      "Stochastic Gradient Descent(49614): loss=0.00014171169706596898\n",
      "Stochastic Gradient Descent(49615): loss=2.7284763571133075\n",
      "Stochastic Gradient Descent(49616): loss=0.8584997305874572\n",
      "Stochastic Gradient Descent(49617): loss=2.579358365269937\n",
      "Stochastic Gradient Descent(49618): loss=1.8859438291530157\n",
      "Stochastic Gradient Descent(49619): loss=2.65716017751779\n",
      "Stochastic Gradient Descent(49620): loss=1.5114282255720695\n",
      "Stochastic Gradient Descent(49621): loss=0.0007811763765028422\n",
      "Stochastic Gradient Descent(49622): loss=0.36585654845840415\n",
      "Stochastic Gradient Descent(49623): loss=12.74560098260951\n",
      "Stochastic Gradient Descent(49624): loss=0.15687624777009465\n",
      "Stochastic Gradient Descent(49625): loss=0.7617731564881659\n",
      "Stochastic Gradient Descent(49626): loss=15.245937987431224\n",
      "Stochastic Gradient Descent(49627): loss=0.6463510001035437\n",
      "Stochastic Gradient Descent(49628): loss=2.4533444550309134\n",
      "Stochastic Gradient Descent(49629): loss=1.5032256525369394\n",
      "Stochastic Gradient Descent(49630): loss=3.81674128062981\n",
      "Stochastic Gradient Descent(49631): loss=0.47560356216721095\n",
      "Stochastic Gradient Descent(49632): loss=5.081268479893922\n",
      "Stochastic Gradient Descent(49633): loss=1.1580235394686158\n",
      "Stochastic Gradient Descent(49634): loss=1.646836878889434\n",
      "Stochastic Gradient Descent(49635): loss=1.0349447330395458\n",
      "Stochastic Gradient Descent(49636): loss=2.736894763811845\n",
      "Stochastic Gradient Descent(49637): loss=1.1512853435954356\n",
      "Stochastic Gradient Descent(49638): loss=2.3076540861277026\n",
      "Stochastic Gradient Descent(49639): loss=6.547729298794886\n",
      "Stochastic Gradient Descent(49640): loss=1.2063862609347455\n",
      "Stochastic Gradient Descent(49641): loss=0.5034940851626108\n",
      "Stochastic Gradient Descent(49642): loss=1.3182597311996505\n",
      "Stochastic Gradient Descent(49643): loss=0.5277382934206325\n",
      "Stochastic Gradient Descent(49644): loss=2.1277133036208804\n",
      "Stochastic Gradient Descent(49645): loss=0.1557039062770065\n",
      "Stochastic Gradient Descent(49646): loss=2.474735586395475\n",
      "Stochastic Gradient Descent(49647): loss=0.6709712553187775\n",
      "Stochastic Gradient Descent(49648): loss=4.629404988880531\n",
      "Stochastic Gradient Descent(49649): loss=0.6215153388050926\n",
      "Stochastic Gradient Descent(49650): loss=4.983505638457616\n",
      "Stochastic Gradient Descent(49651): loss=0.4142717353264\n",
      "Stochastic Gradient Descent(49652): loss=0.0727832385922129\n",
      "Stochastic Gradient Descent(49653): loss=0.02213338077238919\n",
      "Stochastic Gradient Descent(49654): loss=4.550909860915917\n",
      "Stochastic Gradient Descent(49655): loss=0.0028699674305055715\n",
      "Stochastic Gradient Descent(49656): loss=3.775374142374242\n",
      "Stochastic Gradient Descent(49657): loss=0.15815642739213265\n",
      "Stochastic Gradient Descent(49658): loss=3.246991751009541\n",
      "Stochastic Gradient Descent(49659): loss=1.5534643722337687\n",
      "Stochastic Gradient Descent(49660): loss=3.080787723082836\n",
      "Stochastic Gradient Descent(49661): loss=0.8468612875527519\n",
      "Stochastic Gradient Descent(49662): loss=0.23551363722557245\n",
      "Stochastic Gradient Descent(49663): loss=0.039903113097861834\n",
      "Stochastic Gradient Descent(49664): loss=1.8790029302069948\n",
      "Stochastic Gradient Descent(49665): loss=0.06762257578929876\n",
      "Stochastic Gradient Descent(49666): loss=2.745371416949086\n",
      "Stochastic Gradient Descent(49667): loss=1.5853192636285451\n",
      "Stochastic Gradient Descent(49668): loss=0.32227607230911054\n",
      "Stochastic Gradient Descent(49669): loss=0.5224123983383909\n",
      "Stochastic Gradient Descent(49670): loss=0.0512387920560934\n",
      "Stochastic Gradient Descent(49671): loss=5.9420048664598815\n",
      "Stochastic Gradient Descent(49672): loss=6.047492375186272\n",
      "Stochastic Gradient Descent(49673): loss=8.73981826089574\n",
      "Stochastic Gradient Descent(49674): loss=4.331729334973931\n",
      "Stochastic Gradient Descent(49675): loss=0.7160088258083336\n",
      "Stochastic Gradient Descent(49676): loss=0.0002872469464578915\n",
      "Stochastic Gradient Descent(49677): loss=0.31735851757387057\n",
      "Stochastic Gradient Descent(49678): loss=11.732022306002403\n",
      "Stochastic Gradient Descent(49679): loss=0.4766649641744604\n",
      "Stochastic Gradient Descent(49680): loss=0.04358360744780676\n",
      "Stochastic Gradient Descent(49681): loss=0.7776127572957061\n",
      "Stochastic Gradient Descent(49682): loss=11.710198518842649\n",
      "Stochastic Gradient Descent(49683): loss=0.23356091197435622\n",
      "Stochastic Gradient Descent(49684): loss=12.560431362068618\n",
      "Stochastic Gradient Descent(49685): loss=0.17053294008466674\n",
      "Stochastic Gradient Descent(49686): loss=7.524129697695806\n",
      "Stochastic Gradient Descent(49687): loss=16.146238478489092\n",
      "Stochastic Gradient Descent(49688): loss=1.1746402520380979\n",
      "Stochastic Gradient Descent(49689): loss=15.211506438595842\n",
      "Stochastic Gradient Descent(49690): loss=0.6148743061387494\n",
      "Stochastic Gradient Descent(49691): loss=0.7095966604137438\n",
      "Stochastic Gradient Descent(49692): loss=12.67624159176825\n",
      "Stochastic Gradient Descent(49693): loss=12.249716368950336\n",
      "Stochastic Gradient Descent(49694): loss=0.9256127019121769\n",
      "Stochastic Gradient Descent(49695): loss=0.024743498167027842\n",
      "Stochastic Gradient Descent(49696): loss=1.5140253987594638\n",
      "Stochastic Gradient Descent(49697): loss=8.9462887828724\n",
      "Stochastic Gradient Descent(49698): loss=0.22968510435316475\n",
      "Stochastic Gradient Descent(49699): loss=1.481601696200456\n",
      "Stochastic Gradient Descent(49700): loss=5.72607639229548\n",
      "Stochastic Gradient Descent(49701): loss=0.7238283488168367\n",
      "Stochastic Gradient Descent(49702): loss=5.806086099821328\n",
      "Stochastic Gradient Descent(49703): loss=1.2439764803630255\n",
      "Stochastic Gradient Descent(49704): loss=21.106671821300992\n",
      "Stochastic Gradient Descent(49705): loss=0.04458836975923251\n",
      "Stochastic Gradient Descent(49706): loss=6.1883824877300455\n",
      "Stochastic Gradient Descent(49707): loss=0.8267384841897131\n",
      "Stochastic Gradient Descent(49708): loss=2.526286555800447\n",
      "Stochastic Gradient Descent(49709): loss=0.18820242496894465\n",
      "Stochastic Gradient Descent(49710): loss=0.9363370676699113\n",
      "Stochastic Gradient Descent(49711): loss=4.678836968513352\n",
      "Stochastic Gradient Descent(49712): loss=1.6864621894678211\n",
      "Stochastic Gradient Descent(49713): loss=1.7725433282609\n",
      "Stochastic Gradient Descent(49714): loss=0.011280989482452243\n",
      "Stochastic Gradient Descent(49715): loss=5.146857540016418\n",
      "Stochastic Gradient Descent(49716): loss=0.011455677871279655\n",
      "Stochastic Gradient Descent(49717): loss=0.4331027432214006\n",
      "Stochastic Gradient Descent(49718): loss=0.004136810973905912\n",
      "Stochastic Gradient Descent(49719): loss=0.7718377979291677\n",
      "Stochastic Gradient Descent(49720): loss=0.17088151062224766\n",
      "Stochastic Gradient Descent(49721): loss=10.109821931889602\n",
      "Stochastic Gradient Descent(49722): loss=0.13220795220201415\n",
      "Stochastic Gradient Descent(49723): loss=9.152062483759702\n",
      "Stochastic Gradient Descent(49724): loss=22.04210641299943\n",
      "Stochastic Gradient Descent(49725): loss=8.136497786873036\n",
      "Stochastic Gradient Descent(49726): loss=3.6340940961527948\n",
      "Stochastic Gradient Descent(49727): loss=2.208424498681241\n",
      "Stochastic Gradient Descent(49728): loss=2.8413466153244893\n",
      "Stochastic Gradient Descent(49729): loss=8.145502919810932\n",
      "Stochastic Gradient Descent(49730): loss=3.4985575949796215\n",
      "Stochastic Gradient Descent(49731): loss=1.646703370138523\n",
      "Stochastic Gradient Descent(49732): loss=4.647437334900201\n",
      "Stochastic Gradient Descent(49733): loss=0.062443427728727505\n",
      "Stochastic Gradient Descent(49734): loss=1.0158647292014635\n",
      "Stochastic Gradient Descent(49735): loss=0.16470147965076395\n",
      "Stochastic Gradient Descent(49736): loss=0.08368739320444517\n",
      "Stochastic Gradient Descent(49737): loss=5.507950840272752\n",
      "Stochastic Gradient Descent(49738): loss=11.60109174440136\n",
      "Stochastic Gradient Descent(49739): loss=0.2321062956061617\n",
      "Stochastic Gradient Descent(49740): loss=0.10499734905547377\n",
      "Stochastic Gradient Descent(49741): loss=1.895546161215304\n",
      "Stochastic Gradient Descent(49742): loss=0.16122612682495327\n",
      "Stochastic Gradient Descent(49743): loss=5.186782264099687\n",
      "Stochastic Gradient Descent(49744): loss=7.60579248748127\n",
      "Stochastic Gradient Descent(49745): loss=0.011341190668172083\n",
      "Stochastic Gradient Descent(49746): loss=0.06007602799292279\n",
      "Stochastic Gradient Descent(49747): loss=10.956632925257257\n",
      "Stochastic Gradient Descent(49748): loss=1.4404596663467073\n",
      "Stochastic Gradient Descent(49749): loss=22.215597463211502\n",
      "Stochastic Gradient Descent(49750): loss=18.267938894649998\n",
      "Stochastic Gradient Descent(49751): loss=2.0981311207683793\n",
      "Stochastic Gradient Descent(49752): loss=15.964828203090027\n",
      "Stochastic Gradient Descent(49753): loss=0.04800608098688836\n",
      "Stochastic Gradient Descent(49754): loss=1.2190254224475507\n",
      "Stochastic Gradient Descent(49755): loss=3.59027554943877\n",
      "Stochastic Gradient Descent(49756): loss=0.0017323051937168565\n",
      "Stochastic Gradient Descent(49757): loss=52.29859832554475\n",
      "Stochastic Gradient Descent(49758): loss=0.3496787795710221\n",
      "Stochastic Gradient Descent(49759): loss=0.28955415452534866\n",
      "Stochastic Gradient Descent(49760): loss=7.6554119737102715\n",
      "Stochastic Gradient Descent(49761): loss=1.630705455312518\n",
      "Stochastic Gradient Descent(49762): loss=2.4928487700531092\n",
      "Stochastic Gradient Descent(49763): loss=25.02882852734836\n",
      "Stochastic Gradient Descent(49764): loss=1.745599537840979\n",
      "Stochastic Gradient Descent(49765): loss=0.02172338576282953\n",
      "Stochastic Gradient Descent(49766): loss=8.715020969669407\n",
      "Stochastic Gradient Descent(49767): loss=0.033336086039998554\n",
      "Stochastic Gradient Descent(49768): loss=0.481958685041549\n",
      "Stochastic Gradient Descent(49769): loss=1.2604663153760216\n",
      "Stochastic Gradient Descent(49770): loss=0.003211932570979701\n",
      "Stochastic Gradient Descent(49771): loss=1.8373474128586722\n",
      "Stochastic Gradient Descent(49772): loss=0.6058486153087765\n",
      "Stochastic Gradient Descent(49773): loss=1.65575750141584\n",
      "Stochastic Gradient Descent(49774): loss=5.075669282974861\n",
      "Stochastic Gradient Descent(49775): loss=0.3596521036428829\n",
      "Stochastic Gradient Descent(49776): loss=0.18630135495846206\n",
      "Stochastic Gradient Descent(49777): loss=0.0009717423027401484\n",
      "Stochastic Gradient Descent(49778): loss=0.4011754557711282\n",
      "Stochastic Gradient Descent(49779): loss=0.010139269877417494\n",
      "Stochastic Gradient Descent(49780): loss=1.4184174113760257\n",
      "Stochastic Gradient Descent(49781): loss=0.5842593307403299\n",
      "Stochastic Gradient Descent(49782): loss=2.263649222690622\n",
      "Stochastic Gradient Descent(49783): loss=0.017640392900377\n",
      "Stochastic Gradient Descent(49784): loss=3.48672329929617\n",
      "Stochastic Gradient Descent(49785): loss=0.0010076091968103406\n",
      "Stochastic Gradient Descent(49786): loss=0.02430229663091269\n",
      "Stochastic Gradient Descent(49787): loss=0.4540120401517044\n",
      "Stochastic Gradient Descent(49788): loss=4.783341668770924\n",
      "Stochastic Gradient Descent(49789): loss=3.442349623828659\n",
      "Stochastic Gradient Descent(49790): loss=0.5118317944343126\n",
      "Stochastic Gradient Descent(49791): loss=3.485028214053717\n",
      "Stochastic Gradient Descent(49792): loss=1.32198133538963\n",
      "Stochastic Gradient Descent(49793): loss=2.5800889090314256\n",
      "Stochastic Gradient Descent(49794): loss=2.2900286814544586\n",
      "Stochastic Gradient Descent(49795): loss=1.2838050508443293\n",
      "Stochastic Gradient Descent(49796): loss=1.5239233114792103\n",
      "Stochastic Gradient Descent(49797): loss=0.4727013698881223\n",
      "Stochastic Gradient Descent(49798): loss=11.548878591057111\n",
      "Stochastic Gradient Descent(49799): loss=0.12155581307701202\n",
      "Stochastic Gradient Descent(49800): loss=0.9642509871808315\n",
      "Stochastic Gradient Descent(49801): loss=0.09358188405295681\n",
      "Stochastic Gradient Descent(49802): loss=2.499671686750118\n",
      "Stochastic Gradient Descent(49803): loss=1.7084539705621098\n",
      "Stochastic Gradient Descent(49804): loss=0.15854599958090848\n",
      "Stochastic Gradient Descent(49805): loss=9.270601364061108\n",
      "Stochastic Gradient Descent(49806): loss=4.050193300571492\n",
      "Stochastic Gradient Descent(49807): loss=7.354045860636783\n",
      "Stochastic Gradient Descent(49808): loss=0.5277978342949907\n",
      "Stochastic Gradient Descent(49809): loss=2.0210526646471227\n",
      "Stochastic Gradient Descent(49810): loss=0.8978388945320159\n",
      "Stochastic Gradient Descent(49811): loss=1.8129963157454567\n",
      "Stochastic Gradient Descent(49812): loss=1.6274959199359482\n",
      "Stochastic Gradient Descent(49813): loss=10.40695850191027\n",
      "Stochastic Gradient Descent(49814): loss=6.178517726997111\n",
      "Stochastic Gradient Descent(49815): loss=0.01736708340836246\n",
      "Stochastic Gradient Descent(49816): loss=3.659453737347498\n",
      "Stochastic Gradient Descent(49817): loss=36.73990890687752\n",
      "Stochastic Gradient Descent(49818): loss=14.953629615434691\n",
      "Stochastic Gradient Descent(49819): loss=3.3489068795864187\n",
      "Stochastic Gradient Descent(49820): loss=29.81516226025583\n",
      "Stochastic Gradient Descent(49821): loss=0.24628209581232627\n",
      "Stochastic Gradient Descent(49822): loss=0.508063016442435\n",
      "Stochastic Gradient Descent(49823): loss=5.721920479814627\n",
      "Stochastic Gradient Descent(49824): loss=8.575258761671932\n",
      "Stochastic Gradient Descent(49825): loss=0.2718345221720229\n",
      "Stochastic Gradient Descent(49826): loss=0.9553115350803367\n",
      "Stochastic Gradient Descent(49827): loss=1.3670680082465771\n",
      "Stochastic Gradient Descent(49828): loss=1.3722350829302394\n",
      "Stochastic Gradient Descent(49829): loss=0.5666058813712154\n",
      "Stochastic Gradient Descent(49830): loss=0.21542343102680178\n",
      "Stochastic Gradient Descent(49831): loss=0.1308075947361377\n",
      "Stochastic Gradient Descent(49832): loss=5.926266182695484\n",
      "Stochastic Gradient Descent(49833): loss=2.9106352712381436\n",
      "Stochastic Gradient Descent(49834): loss=0.7381535524928216\n",
      "Stochastic Gradient Descent(49835): loss=7.7517622256420315\n",
      "Stochastic Gradient Descent(49836): loss=1.9491673809277805\n",
      "Stochastic Gradient Descent(49837): loss=1.5406587660172408\n",
      "Stochastic Gradient Descent(49838): loss=0.23471853599421513\n",
      "Stochastic Gradient Descent(49839): loss=22.06151088307117\n",
      "Stochastic Gradient Descent(49840): loss=10.109418337427522\n",
      "Stochastic Gradient Descent(49841): loss=1.9410368263632203\n",
      "Stochastic Gradient Descent(49842): loss=0.39143810120782635\n",
      "Stochastic Gradient Descent(49843): loss=3.7568388277427287\n",
      "Stochastic Gradient Descent(49844): loss=16.501209092850765\n",
      "Stochastic Gradient Descent(49845): loss=1.715015868784283\n",
      "Stochastic Gradient Descent(49846): loss=0.06037393032193367\n",
      "Stochastic Gradient Descent(49847): loss=0.07298886149501589\n",
      "Stochastic Gradient Descent(49848): loss=2.176116311601662\n",
      "Stochastic Gradient Descent(49849): loss=3.570205218857165\n",
      "Stochastic Gradient Descent(49850): loss=0.8038509599715035\n",
      "Stochastic Gradient Descent(49851): loss=1.9734003312285646\n",
      "Stochastic Gradient Descent(49852): loss=7.7038670830148535\n",
      "Stochastic Gradient Descent(49853): loss=1.5562462173279776\n",
      "Stochastic Gradient Descent(49854): loss=0.00937810326353543\n",
      "Stochastic Gradient Descent(49855): loss=0.057968524212397024\n",
      "Stochastic Gradient Descent(49856): loss=3.3703651990578107\n",
      "Stochastic Gradient Descent(49857): loss=5.0172969856281195\n",
      "Stochastic Gradient Descent(49858): loss=0.03604699388773689\n",
      "Stochastic Gradient Descent(49859): loss=3.742611220768897\n",
      "Stochastic Gradient Descent(49860): loss=5.599207262402923\n",
      "Stochastic Gradient Descent(49861): loss=0.5088775651087056\n",
      "Stochastic Gradient Descent(49862): loss=0.08129027498987362\n",
      "Stochastic Gradient Descent(49863): loss=2.271574166355881\n",
      "Stochastic Gradient Descent(49864): loss=0.21248843736050727\n",
      "Stochastic Gradient Descent(49865): loss=0.24335992823081085\n",
      "Stochastic Gradient Descent(49866): loss=6.920984092933726e-06\n",
      "Stochastic Gradient Descent(49867): loss=13.994892614172183\n",
      "Stochastic Gradient Descent(49868): loss=1.8548372563997115\n",
      "Stochastic Gradient Descent(49869): loss=0.021519413627192934\n",
      "Stochastic Gradient Descent(49870): loss=4.918821663317973\n",
      "Stochastic Gradient Descent(49871): loss=46.01264612410321\n",
      "Stochastic Gradient Descent(49872): loss=2.4750379741395547\n",
      "Stochastic Gradient Descent(49873): loss=2.0580466572638665\n",
      "Stochastic Gradient Descent(49874): loss=5.140517509568566\n",
      "Stochastic Gradient Descent(49875): loss=0.34386525524785255\n",
      "Stochastic Gradient Descent(49876): loss=1.6896978717089455\n",
      "Stochastic Gradient Descent(49877): loss=60.80551187863127\n",
      "Stochastic Gradient Descent(49878): loss=0.8951727260463223\n",
      "Stochastic Gradient Descent(49879): loss=4.118786056318674\n",
      "Stochastic Gradient Descent(49880): loss=7.921604227074837\n",
      "Stochastic Gradient Descent(49881): loss=0.4485202305535213\n",
      "Stochastic Gradient Descent(49882): loss=7.895973451388536\n",
      "Stochastic Gradient Descent(49883): loss=2.7862743093406417\n",
      "Stochastic Gradient Descent(49884): loss=2.259066069640286\n",
      "Stochastic Gradient Descent(49885): loss=0.9212229400809039\n",
      "Stochastic Gradient Descent(49886): loss=4.426768998245645\n",
      "Stochastic Gradient Descent(49887): loss=1.83958318993901\n",
      "Stochastic Gradient Descent(49888): loss=0.30925302149375405\n",
      "Stochastic Gradient Descent(49889): loss=0.5872388259926747\n",
      "Stochastic Gradient Descent(49890): loss=7.438324970408856\n",
      "Stochastic Gradient Descent(49891): loss=18.942766474440592\n",
      "Stochastic Gradient Descent(49892): loss=14.25397424526696\n",
      "Stochastic Gradient Descent(49893): loss=0.7500766559926769\n",
      "Stochastic Gradient Descent(49894): loss=5.940770266983126\n",
      "Stochastic Gradient Descent(49895): loss=13.254272867435303\n",
      "Stochastic Gradient Descent(49896): loss=16.82091407268048\n",
      "Stochastic Gradient Descent(49897): loss=1.5296911579054069\n",
      "Stochastic Gradient Descent(49898): loss=7.68895684683402\n",
      "Stochastic Gradient Descent(49899): loss=3.6079194550977824\n",
      "Stochastic Gradient Descent(49900): loss=0.08363655891279818\n",
      "Stochastic Gradient Descent(49901): loss=0.046393203881106265\n",
      "Stochastic Gradient Descent(49902): loss=5.687837644886708\n",
      "Stochastic Gradient Descent(49903): loss=2.7986402830481847\n",
      "Stochastic Gradient Descent(49904): loss=0.15572593920969208\n",
      "Stochastic Gradient Descent(49905): loss=4.801665511326471\n",
      "Stochastic Gradient Descent(49906): loss=0.6113891633522306\n",
      "Stochastic Gradient Descent(49907): loss=7.0796310215850875\n",
      "Stochastic Gradient Descent(49908): loss=0.030236466973325306\n",
      "Stochastic Gradient Descent(49909): loss=5.471360851632887\n",
      "Stochastic Gradient Descent(49910): loss=2.17737494132582\n",
      "Stochastic Gradient Descent(49911): loss=1.6114177168312287\n",
      "Stochastic Gradient Descent(49912): loss=0.19948015703449729\n",
      "Stochastic Gradient Descent(49913): loss=4.9829467828102585\n",
      "Stochastic Gradient Descent(49914): loss=10.440646855704287\n",
      "Stochastic Gradient Descent(49915): loss=10.74670980331379\n",
      "Stochastic Gradient Descent(49916): loss=1.439796149652384\n",
      "Stochastic Gradient Descent(49917): loss=4.409657368291006\n",
      "Stochastic Gradient Descent(49918): loss=0.19281053333353723\n",
      "Stochastic Gradient Descent(49919): loss=8.109916216116996\n",
      "Stochastic Gradient Descent(49920): loss=5.287005002015386\n",
      "Stochastic Gradient Descent(49921): loss=0.0823121589576417\n",
      "Stochastic Gradient Descent(49922): loss=2.542854666266855\n",
      "Stochastic Gradient Descent(49923): loss=1.302136612442341\n",
      "Stochastic Gradient Descent(49924): loss=4.052382739413434\n",
      "Stochastic Gradient Descent(49925): loss=0.4527691831502385\n",
      "Stochastic Gradient Descent(49926): loss=1.4313151078878437\n",
      "Stochastic Gradient Descent(49927): loss=0.9502122684875705\n",
      "Stochastic Gradient Descent(49928): loss=1.405362530537897\n",
      "Stochastic Gradient Descent(49929): loss=2.519774946604436\n",
      "Stochastic Gradient Descent(49930): loss=1.698758870893052\n",
      "Stochastic Gradient Descent(49931): loss=0.2458617259084846\n",
      "Stochastic Gradient Descent(49932): loss=0.5486884210221028\n",
      "Stochastic Gradient Descent(49933): loss=1.8513766530557578\n",
      "Stochastic Gradient Descent(49934): loss=3.2257423212093967\n",
      "Stochastic Gradient Descent(49935): loss=8.958092196356173\n",
      "Stochastic Gradient Descent(49936): loss=5.441558605829615\n",
      "Stochastic Gradient Descent(49937): loss=5.335997453254373\n",
      "Stochastic Gradient Descent(49938): loss=0.47387787967351275\n",
      "Stochastic Gradient Descent(49939): loss=0.6143719402378727\n",
      "Stochastic Gradient Descent(49940): loss=4.797300449928063\n",
      "Stochastic Gradient Descent(49941): loss=0.129595248358401\n",
      "Stochastic Gradient Descent(49942): loss=1.47178393033047\n",
      "Stochastic Gradient Descent(49943): loss=2.117204673946556\n",
      "Stochastic Gradient Descent(49944): loss=14.287583693877522\n",
      "Stochastic Gradient Descent(49945): loss=2.5149494375779873\n",
      "Stochastic Gradient Descent(49946): loss=0.9652411994503443\n",
      "Stochastic Gradient Descent(49947): loss=22.467910093352053\n",
      "Stochastic Gradient Descent(49948): loss=0.51858830568429\n",
      "Stochastic Gradient Descent(49949): loss=4.253146968552402\n",
      "Stochastic Gradient Descent(49950): loss=10.757883607533778\n",
      "Stochastic Gradient Descent(49951): loss=2.754571849095217\n",
      "Stochastic Gradient Descent(49952): loss=10.703713372266913\n",
      "Stochastic Gradient Descent(49953): loss=6.437047561486998\n",
      "Stochastic Gradient Descent(49954): loss=0.9373434619965165\n",
      "Stochastic Gradient Descent(49955): loss=1.9370792912123507\n",
      "Stochastic Gradient Descent(49956): loss=5.22076659636362\n",
      "Stochastic Gradient Descent(49957): loss=12.099676396204531\n",
      "Stochastic Gradient Descent(49958): loss=1.4077238932125407\n",
      "Stochastic Gradient Descent(49959): loss=0.03618995453918298\n",
      "Stochastic Gradient Descent(49960): loss=63.446516420397074\n",
      "Stochastic Gradient Descent(49961): loss=17.954043282488602\n",
      "Stochastic Gradient Descent(49962): loss=6.522674805697727\n",
      "Stochastic Gradient Descent(49963): loss=11.054795365544852\n",
      "Stochastic Gradient Descent(49964): loss=2.294490718458683\n",
      "Stochastic Gradient Descent(49965): loss=3.608972533883965\n",
      "Stochastic Gradient Descent(49966): loss=1.8773095790062326\n",
      "Stochastic Gradient Descent(49967): loss=18.216983996720906\n",
      "Stochastic Gradient Descent(49968): loss=2.218433390142287\n",
      "Stochastic Gradient Descent(49969): loss=14.783904040644849\n",
      "Stochastic Gradient Descent(49970): loss=0.0016596110069268626\n",
      "Stochastic Gradient Descent(49971): loss=2.370862378069057\n",
      "Stochastic Gradient Descent(49972): loss=0.12953983584426187\n",
      "Stochastic Gradient Descent(49973): loss=6.605916720684589\n",
      "Stochastic Gradient Descent(49974): loss=1.3812886698611697\n",
      "Stochastic Gradient Descent(49975): loss=0.04789594560979693\n",
      "Stochastic Gradient Descent(49976): loss=1.2497451371507065\n",
      "Stochastic Gradient Descent(49977): loss=13.710329455324404\n",
      "Stochastic Gradient Descent(49978): loss=8.100350813254217\n",
      "Stochastic Gradient Descent(49979): loss=2.667155661590862\n",
      "Stochastic Gradient Descent(49980): loss=4.6268842091969615\n",
      "Stochastic Gradient Descent(49981): loss=3.899026356698011\n",
      "Stochastic Gradient Descent(49982): loss=1.3605785481580115\n",
      "Stochastic Gradient Descent(49983): loss=0.867687272045083\n",
      "Stochastic Gradient Descent(49984): loss=0.00792376000524618\n",
      "Stochastic Gradient Descent(49985): loss=3.6262498349131236\n",
      "Stochastic Gradient Descent(49986): loss=41.67478452168776\n",
      "Stochastic Gradient Descent(49987): loss=0.019290393401292755\n",
      "Stochastic Gradient Descent(49988): loss=4.55233812226689\n",
      "Stochastic Gradient Descent(49989): loss=17.20875907514194\n",
      "Stochastic Gradient Descent(49990): loss=0.7062608682941238\n",
      "Stochastic Gradient Descent(49991): loss=0.4383809275841054\n",
      "Stochastic Gradient Descent(49992): loss=5.39057040495371\n",
      "Stochastic Gradient Descent(49993): loss=5.356081132123527\n",
      "Stochastic Gradient Descent(49994): loss=0.12948160731851935\n",
      "Stochastic Gradient Descent(49995): loss=3.149352292247383\n",
      "Stochastic Gradient Descent(49996): loss=3.412027851680587\n",
      "Stochastic Gradient Descent(49997): loss=2.6884232822218177\n",
      "Stochastic Gradient Descent(49998): loss=2.818808480851266\n",
      "Stochastic Gradient Descent(49999): loss=0.2580733384159859\n",
      "Stochastic Gradient Descent(50000): loss=4.506218251535359\n",
      "SGD: execution time=1.306 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_epochs = 2\n",
    "gamma = 0.000002\n",
    "\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(\n",
    "    y, tX, gamma, max_epochs)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25645"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(gradient_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub=gradient_ws[25645]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Least squares regression using normal equations: least_squares (y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y,tx):\n",
    "    weights=np.dot( np.linalg.inv(np.dot(tX.T,tX)), np.dot(tX.T,y) )\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=104277.74827748955, w0=0.5331945156671677, w1=-1.892826192761536\n",
      "Gradient Descent(1/999): loss=59693.89344263162, w0=0.5561225327707616, w1=-1.8445377386462873\n",
      "Gradient Descent(2/999): loss=34962.69866796885, w0=0.5724560372222726, w1=-1.8082358119000954\n",
      "Gradient Descent(3/999): loss=21238.2742435524, w0=0.5838865404996956, w1=-1.780852099878007\n",
      "Gradient Descent(4/999): loss=13616.331494764829, w0=0.5916734254132643, w1=-1.7601021000638324\n",
      "Gradient Descent(5/999): loss=9377.860473887842, w0=0.5967543176350527, w1=-1.7442849249245123\n",
      "Gradient Descent(6/999): loss=7015.400359953048, w0=0.5998272671915122, w1=-1.7321342386790617\n",
      "Gradient Descent(7/999): loss=5693.190177128206, w0=0.6014119399425585, w1=-1.722707266368582\n",
      "Gradient Descent(8/999): loss=4947.868816526901, w0=0.6018951801112692, w1=-1.7153021511724786\n",
      "Gradient Descent(9/999): loss=4522.538948754126, w0=0.6015649356577523, w1=-1.7093964195389204\n",
      "Gradient Descent(10/999): loss=4274.769738633382, w0=0.6006355187459049, w1=-1.7046011629778388\n",
      "Gradient Descent(11/999): loss=4125.592263749883, w0=0.5992664144091104, w1=-1.7006269223199588\n",
      "Gradient Descent(12/999): loss=4031.2218601152426, w0=0.5975762852710316, w1=-1.6972582855121336\n",
      "Gradient Descent(13/999): loss=3967.380593455663, w0=0.5956533992983709, w1=-1.694334973422435\n",
      "Gradient Descent(14/999): loss=3920.6054160527083, w0=0.5935633941800392, w1=-1.6917377565506582\n",
      "Gradient Descent(15/999): loss=3883.4292852105036, w0=0.5913550585858235, w1=-1.689377968781274\n",
      "Gradient Descent(16/999): loss=3851.709505764987, w0=0.5890646368140563, w1=-1.6871896994570714\n",
      "Gradient Descent(17/999): loss=3823.14652363389, w0=0.5867190339700841, w1=-1.6851239797026498\n",
      "Gradient Descent(18/999): loss=3796.4627221020305, w0=0.5843382024914646, w1=-1.6831444536456093\n",
      "Gradient Descent(19/999): loss=3770.947132897016, w0=0.5819369191125622, w1=-1.6812241552770293\n",
      "Gradient Descent(20/999): loss=3746.203015592945, w0=0.5795261079568028, w1=-1.6793431085593045\n",
      "Gradient Descent(21/999): loss=3722.007910106264, w0=0.5771138256804778, w1=-1.677486540515191\n",
      "Gradient Descent(22/999): loss=3698.2360459017523, w0=0.574705994983833, w1=-1.6756435507360474\n",
      "Gradient Descent(23/999): loss=3674.815322685431, w0=0.5723069507592595, w1=-1.6738061207347923\n",
      "Gradient Descent(24/999): loss=3651.7034580672216, w0=0.5699198467312271, w1=-1.6719683763434203\n",
      "Gradient Descent(25/999): loss=3628.8747617124627, w0=0.5675469582200383, w1=-1.670126038524572\n",
      "Gradient Descent(26/999): loss=3606.3128010189494, w0=0.565189907560678, w1=-1.6682760144739426\n",
      "Gradient Descent(27/999): loss=3584.0063331882166, w0=0.5628498319316714, w1=-1.666416093181488\n",
      "Gradient Descent(28/999): loss=3561.9470482801808, w0=0.5605275083032556, w1=-1.664544718771254\n",
      "Gradient Descent(29/999): loss=3540.128316349649, w0=0.5582234464572573, w1=-1.6626608217540528\n",
      "Gradient Descent(30/999): loss=3518.5444913057813, w0=0.5559379582337144, w1=-1.6607636934011345\n",
      "Gradient Descent(31/999): loss=3497.1905234712294, w0=0.5536712090763953, w1=-1.6588528922249985\n",
      "Gradient Descent(32/999): loss=3476.061743331997, w0=0.5514232563984773, w1=-1.6569281743665452\n",
      "Gradient Descent(33/999): loss=3455.1537402399517, w0=0.5491940781348645, w1=-1.6549894417823372\n",
      "Gradient Descent(34/999): loss=3434.4622937992253, w0=0.5469835939877904, w1=-1.6530367036853437\n",
      "Gradient Descent(35/999): loss=3413.983334500998, w0=0.5447916812321233, w1=-1.6510700478537963\n",
      "Gradient Descent(36/999): loss=3393.7129206124514, w0=0.5426181864700887, w1=-1.649089619287447\n",
      "Gradient Descent(37/999): loss=3373.6472241146803, w0=0.5404629343701708, w1=-1.647095604334343\n",
      "Gradient Descent(38/999): loss=3353.7825216937267, w0=0.5383257341606646, w1=-1.6450882188906077\n",
      "Gradient Descent(39/999): loss=3334.1151885683707, w0=0.5362063844515621, w1=-1.6430676996326625\n",
      "Gradient Descent(40/999): loss=3314.6416939248465, w0=0.5341046768119245, w1=-1.6410342975070984\n",
      "Gradient Descent(41/999): loss=3295.358597275668, w0=0.5320203984207957, w1=-1.6389882729013021\n",
      "Gradient Descent(42/999): loss=3276.262545363, w0=0.5299533340284708, w1=-1.636929892065285\n",
      "Gradient Descent(43/999): loss=3257.350269395229, w0=0.5279032674044483, w1=-1.6348594244648869\n",
      "Gradient Descent(44/999): loss=3238.618582498636, w0=0.5258699824033545, w1=-1.6327771408282088\n",
      "Gradient Descent(45/999): loss=3220.064377317784, w0=0.5238532637465949, w1=-1.6306833117079664\n",
      "Gradient Descent(46/999): loss=3201.6846237270115, w0=0.521852897592515, w1=-1.6285782064277392\n",
      "Gradient Descent(47/999): loss=3183.4763666312665, w0=0.519868671949263, w1=-1.626462092313818\n",
      "Gradient Descent(48/999): loss=3165.436723843495, w0=0.5179003769707027, w1=-1.6243352341394626\n",
      "Gradient Descent(49/999): loss=3147.562884030595, w0=0.5159478051654148, w1=-1.622197893727078\n",
      "Gradient Descent(50/999): loss=3129.8521047228746, w0=0.5140107515411544, w1=-1.6200503296677384\n",
      "Gradient Descent(51/999): loss=3112.3017103833245, w0=0.5120890137014114, w1=-1.6178927971278538\n",
      "Gradient Descent(52/999): loss=3094.9090905340613, w0=0.510182391906473, w1=-1.6157255477204935\n",
      "Gradient Descent(53/999): loss=3077.671697937717, w0=0.5082906891082118, w1=-1.613548829424626\n",
      "Gradient Descent(54/999): loss=3060.5870468318117, w0=0.5064137109654702, w1=-1.6113628865398135\n",
      "Gradient Descent(55/999): loss=3043.652711214519, w0=0.5045512658451494, w1=-1.609167959667087\n",
      "Gradient Descent(56/999): loss=3026.8663231801147, w0=0.5027031648128112, w1=-1.6069642857090973\n",
      "Gradient Descent(57/999): loss=3010.225571302671, w0=0.500869221615617, w1=-1.6047520978844094\n",
      "Gradient Descent(58/999): loss=2993.7281990665433, w0=0.4990492526597139, w1=-1.602531625752112\n",
      "Gradient Descent(59/999): loss=2977.372003342264, w0=0.49724307698362974, w1=-1.6003030952439061\n",
      "Gradient Descent(60/999): loss=2961.1548329065217, w0=0.4954505162288418, w1=-1.598066728701556\n",
      "Gradient Descent(61/999): loss=2945.0745870049104, w0=0.49367139460838244, w1=-1.595822744918133\n",
      "Gradient Descent(62/999): loss=2929.129213956226, w0=0.49190553887412336, w1=-1.593571359181885\n",
      "Gradient Descent(63/999): loss=2913.316709797076, w0=0.4901527782832137, w1=-1.5913127833218663\n",
      "Gradient Descent(64/999): loss=2897.635116965642, w0=0.4884129445640237, w1=-1.5890472257546873\n",
      "Gradient Descent(65/999): loss=2882.0825230234113, w0=0.4866858718818541, w1=-1.5867748915319064\n",
      "Gradient Descent(66/999): loss=2866.65705941384, w0=0.4849713968046034, w1=-1.5844959823877172\n",
      "Gradient Descent(67/999): loss=2851.356900256794, w0=0.4832693582685327, w1=-1.5822106967866676\n",
      "Gradient Descent(68/999): loss=2836.1802611777866, w0=0.4815795975442319, w1=-1.579919229971226\n",
      "Gradient Descent(69/999): loss=2821.1253981709433, w0=0.4799019582028619, w1=-1.5776217740090517\n",
      "Gradient Descent(70/999): loss=2806.1906064947552, w0=0.4782362860827258, w1=-1.5753185178398734\n",
      "Gradient Descent(71/999): loss=2791.3742195996424, w0=0.4765824292562083, w1=-1.5730096473218982\n",
      "Gradient Descent(72/999): loss=2776.674608086391, w0=0.4749402379971096, w1=-1.5706953452777046\n",
      "Gradient Descent(73/999): loss=2762.090178694595, w0=0.47330956474839136, w1=-1.5683757915395813\n",
      "Gradient Descent(74/999): loss=2747.6193733201803, w0=0.47169026409034714, w1=-1.5660511629942877\n",
      "Gradient Descent(75/999): loss=2733.260668061218, w0=0.470082192709204, w1=-1.5637216336272224\n",
      "Gradient Descent(76/999): loss=2719.0125722911616, w0=0.4684852093661584, w1=-1.561387374565988\n",
      "Gradient Descent(77/999): loss=2704.8736277587172, w0=0.4668991748668483, w1=-1.559048554123351\n",
      "Gradient Descent(78/999): loss=2690.8424077135774, w0=0.4653239520312592, w1=-1.5567053378395912\n",
      "Gradient Descent(79/999): loss=2676.917516057271, w0=0.463759405664063, w1=-1.5543578885242482\n",
      "Gradient Descent(80/999): loss=2663.097586518375, w0=0.46220540252538544, w1=-1.5520063662972632\n",
      "Gradient Descent(81/999): loss=2649.381281851394, w0=0.46066181130199857, w1=-1.5496509286295232\n",
      "Gradient Descent(82/999): loss=2635.767293058632, w0=0.4591285025789334, w1=-1.5472917303828142\n",
      "Gradient Descent(83/999): loss=2622.2543386343305, w0=0.45760534881150783, w1=-1.544928923849187\n",
      "Gradient Descent(84/999): loss=2608.8411638305006, w0=0.4560922242977649, w1=-1.5425626587897463\n",
      "Gradient Descent(85/999): loss=2595.5265399437667, w0=0.4545890051513157, w1=-1.5401930824728671\n",
      "Gradient Descent(86/999): loss=2582.3092636226197, w0=0.4530955692745812, w1=-1.5378203397118482\n",
      "Gradient Descent(87/999): loss=2569.1881561944774, w0=0.4516117963324289, w1=-1.5354445729020083\n",
      "Gradient Descent(88/999): loss=2556.1620630120233, w0=0.45013756772619656, w1=-1.533065922057235\n",
      "Gradient Descent(89/999): loss=2543.229852818178, w0=0.4486727665680993, w1=-1.5306845248459917\n",
      "Gradient Descent(90/999): loss=2530.390417129238, w0=0.44721727765601416, w1=-1.5283005166267924\n",
      "Gradient Descent(91/999): loss=2517.642669635621, w0=0.4457709874486361, w1=-1.5259140304831507\n",
      "Gradient Descent(92/999): loss=2504.985545619684, w0=0.44433378404100043, w1=-1.52352519725801\n",
      "Gradient Descent(93/999): loss=2492.418001390185, w0=0.44290555714036683, w1=-1.521134145587665\n",
      "Gradient Descent(94/999): loss=2479.9390137328046, w0=0.4414861980424584, w1=-1.51874100193518\n",
      "Gradient Descent(95/999): loss=2467.547579376364, w0=0.44007559960805204, w1=-1.5163458906233096\n",
      "Gradient Descent(96/999): loss=2455.242714474208, w0=0.43867365623991367, w1=-1.5139489338669345\n",
      "Gradient Descent(97/999): loss=2443.023454100332, w0=0.43728026386007446, w1=-1.5115502518050137\n",
      "Gradient Descent(98/999): loss=2430.88885175986, w0=0.43589531988744185, w1=-1.509149962532063\n",
      "Gradient Descent(99/999): loss=2418.8379789133924, w0=0.4345187232157413, w1=-1.5067481821291684\n",
      "Gradient Descent(100/999): loss=2406.8699245148837, w0=0.43315037419178354, w1=-1.5043450246945365\n",
      "Gradient Descent(101/999): loss=2394.983794562607, w0=0.4317901745940526, w1=-1.5019406023735937\n",
      "Gradient Descent(102/999): loss=2383.17871166285, w0=0.4304380276116098, w1=-1.499535025388638\n",
      "Gradient Descent(103/999): loss=2371.453814605987, w0=0.4290938378233093, w1=-1.4971284020680506\n",
      "Gradient Descent(104/999): loss=2359.8082579545107, w0=0.4277575111773202, w1=-1.4947208388750743\n",
      "Gradient Descent(105/999): loss=2348.241211642757, w0=0.42642895497095107, w1=-1.4923124404361643\n",
      "Gradient Descent(106/999): loss=2336.751860587909, w0=0.4251080778307723, w1=-1.4899033095689178\n",
      "Gradient Descent(107/999): loss=2325.339404311996, w0=0.4237947896930318, w1=-1.4874935473095907\n",
      "Gradient Descent(108/999): loss=2314.0030565745633, w0=0.42248900178435955, w1=-1.4850832529402034\n",
      "Gradient Descent(109/999): loss=2302.742045015687, w0=0.4211906266027575, w1=-1.4826725240152456\n",
      "Gradient Descent(110/999): loss=2291.555610809033, w0=0.4198995778988697, w1=-1.4802614563879843\n",
      "Gradient Descent(111/999): loss=2280.443008324714, w0=0.41861577065752886, w1=-1.477850144236381\n",
      "Gradient Descent(112/999): loss=2269.4035048015803, w0=0.4173391210795756, w1=-1.4754386800886232\n",
      "Gradient Descent(113/999): loss=2258.4363800287533, w0=0.41606954656394585, w1=-1.4730271548482787\n",
      "Gradient Descent(114/999): loss=2247.540926036068, w0=0.41480696569002273, w1=-1.4706156578190734\n",
      "Gradient Descent(115/999): loss=2236.7164467932166, w0=0.413551298200249, w1=-1.4682042767293038\n",
      "Gradient Descent(116/999): loss=2225.9622579173024, w0=0.4123024649829961, w1=-1.4657930977558848\n",
      "Gradient Descent(117/999): loss=2215.2776863885847, w0=0.4110603880556861, w1=-1.4633822055480412\n",
      "Gradient Descent(118/999): loss=2204.662070274165, w0=0.4098249905481629, w1=-1.4609716832506467\n",
      "Gradient Descent(119/999): loss=2194.1147584593978, w0=0.4085961966863087, w1=-1.4585616125272163\n",
      "Gradient Descent(120/999): loss=2183.6351103867796, w0=0.40737393177590253, w1=-1.4561520735825568\n",
      "Gradient Descent(121/999): loss=2173.222495802128, w0=0.40615812218671704, w1=-1.4537431451850809\n",
      "Gradient Descent(122/999): loss=2162.8762945078106, w0=0.40494869533684985, w1=-1.451334904688789\n",
      "Gradient Descent(123/999): loss=2152.5958961228484, w0=0.4037455796772866, w1=-1.4489274280549251\n",
      "Gradient Descent(124/999): loss=2142.380699849675, w0=0.40254870467669135, w1=-1.4465207898733101\n",
      "Gradient Descent(125/999): loss=2132.2301142473593, w0=0.4013580008064222, w1=-1.4441150633833575\n",
      "Gradient Descent(126/999): loss=2122.1435570111225, w0=0.40017339952576747, w1=-1.4417103204947779\n",
      "Gradient Descent(127/999): loss=2112.12045475795, w0=0.39899483326740015, w1=-1.439306631807974\n",
      "Gradient Descent(128/999): loss=2102.160242818128, w0=0.39782223542304723, w1=-1.4369040666341335\n",
      "Gradient Descent(129/999): loss=2092.2623650325286, w0=0.3966555403293702, w1=-1.4345026930150229\n",
      "Gradient Descent(130/999): loss=2082.4262735554926, w0=0.3954946832540544, w1=-1.4321025777424856\n",
      "Gradient Descent(131/999): loss=2072.6514286631327, w0=0.3943396003821034, w1=-1.429703786377652\n",
      "Gradient Descent(132/999): loss=2062.937298566908, w0=0.39319022880233606, w1=-1.4273063832698618\n",
      "Gradient Descent(133/999): loss=2053.283359232312, w0=0.3920465064940826, w1=-1.4249104315753056\n",
      "Gradient Descent(134/999): loss=2043.6890942025361, w0=0.3909083723140775, w1=-1.4225159932753897\n",
      "Gradient Descent(135/999): loss=2034.1539944269584, w0=0.3897757659835452, w1=-1.4201231291948264\n",
      "Gradient Descent(136/999): loss=2024.6775580943206, w0=0.3886486280754775, w1=-1.4177318990194552\n",
      "Gradient Descent(137/999): loss=2015.2592904704527, w0=0.38752690000209783, w1=-1.4153423613137992\n",
      "Gradient Descent(138/999): loss=2005.8987037404265, w0=0.38641052400251114, w1=-1.4129545735383602\n",
      "Gradient Descent(139/999): loss=1996.5953168550027, w0=0.385299443130536, w1=-1.4105685920666555\n",
      "Gradient Descent(140/999): loss=1987.3486553812427, w0=0.3841936012427163, w1=-1.4081844722020014\n",
      "Gradient Descent(141/999): loss=1978.1582513571782, w0=0.38309294298651, w1=-1.4058022681940465\n",
      "Gradient Descent(142/999): loss=1969.023643150411, w0=0.38199741378865226, w1=-1.40342203325506\n",
      "Gradient Descent(143/999): loss=1959.9443753205408, w0=0.38090695984369044, w1=-1.4010438195759747\n",
      "Gradient Descent(144/999): loss=1950.9199984852926, w0=0.3798215281026882, w1=-1.3986676783421939\n",
      "Gradient Descent(145/999): loss=1941.9500691902626, w0=0.3787410662620967, w1=-1.3962936597491604\n",
      "Gradient Descent(146/999): loss=1933.0341497821594, w0=0.37766552275278964, w1=-1.3939218130176956\n",
      "Gradient Descent(147/999): loss=1924.1718082854443, w0=0.37659484672926086, w1=-1.3915521864091074\n",
      "Gradient Descent(148/999): loss=1915.3626182822802, w0=0.3755289880589808, w1=-1.3891848272400755\n",
      "Gradient Descent(149/999): loss=1906.6061587956913, w0=0.3744678973119104, w1=-1.3868197818973134\n",
      "Gradient Descent(150/999): loss=1897.9020141758283, w0=0.3734115257501699, w1=-1.3844570958520115\n",
      "Gradient Descent(151/999): loss=1889.2497739892765, w0=0.3723598253178601, w1=-1.3820968136740661\n",
      "Gradient Descent(152/999): loss=1880.6490329112996, w0=0.3713127486310339, w1=-1.3797389790460945\n",
      "Gradient Descent(153/999): loss=1872.0993906209367, w0=0.37027024896781596, w1=-1.377383634777243\n",
      "Gradient Descent(154/999): loss=1863.6004516988748, w0=0.36923228025866817, w1=-1.375030822816787\n",
      "Gradient Descent(155/999): loss=1855.1518255280318, w0=0.3681987970767989, w1=-1.372680584267529\n",
      "Gradient Descent(156/999): loss=1846.7531261967354, w0=0.36716975462871415, w1=-1.3703329593989977\n",
      "Gradient Descent(157/999): loss=1838.4039724044715, w0=0.3661451087449076, w1=-1.3679879876604484\n",
      "Gradient Descent(158/999): loss=1830.1039873700815, w0=0.36512481587068873, w1=-1.3656457076936699\n",
      "Gradient Descent(159/999): loss=1821.852798742382, w0=0.3641088330571462, w1=-1.3633061573456011\n",
      "Gradient Descent(160/999): loss=1813.6500385131008, w0=0.36309711795224436, w1=-1.360969373680758\n",
      "Gradient Descent(161/999): loss=1805.4953429320892, w0=0.36208962879205187, w1=-1.3586353929934758\n",
      "Gradient Descent(162/999): loss=1797.3883524247383, w0=0.36108632439209914, w1=-1.3563042508199679\n",
      "Gradient Descent(163/999): loss=1789.328711511528, w0=0.3600871641388639, w1=-1.3539759819502053\n",
      "Gradient Descent(164/999): loss=1781.3160687296538, w0=0.3590921079813823, w1=-1.3516506204396177\n",
      "Gradient Descent(165/999): loss=1773.3500765566878, w0=0.358101116422984, w1=-1.3493281996206203\n",
      "Gradient Descent(166/999): loss=1765.4303913361825, w0=0.35711415051314904, w1=-1.3470087521139686\n",
      "Gradient Descent(167/999): loss=1757.556673205202, w0=0.3561311718394854, w1=-1.3446923098399426\n",
      "Gradient Descent(168/999): loss=1749.7285860236843, w0=0.3551521425198247, w1=-1.3423789040293646\n",
      "Gradient Descent(169/999): loss=1741.9457973056278, w0=0.3541770251944346, w1=-1.3400685652344517\n",
      "Gradient Descent(170/999): loss=1734.2079781520038, w0=0.3532057830183466, w1=-1.3377613233395074\n",
      "Gradient Descent(171/999): loss=1726.5148031853878, w0=0.35223837965379656, w1=-1.3354572075714506\n",
      "Gradient Descent(172/999): loss=1718.8659504862374, w0=0.35127477926277745, w1=-1.3331562465101903\n",
      "Gradient Descent(173/999): loss=1711.261101530765, w0=0.35031494649970163, w1=-1.3308584680988427\n",
      "Gradient Descent(174/999): loss=1703.6999411303952, w0=0.3493588465041718, w1=-1.328563899653796\n",
      "Gradient Descent(175/999): loss=1696.1821573727068, w0=0.3484064448938585, w1=-1.3262725678746257\n",
      "Gradient Descent(176/999): loss=1688.7074415638751, w0=0.3474577077574829, w1=-1.3239844988538598\n",
      "Gradient Descent(177/999): loss=1681.2754881725361, w0=0.34651260164790326, w1=-1.3216997180865995\n",
      "Gradient Descent(178/999): loss=1673.8859947750373, w0=0.3455710935753033, w1=-1.3194182504799936\n",
      "Gradient Descent(179/999): loss=1666.5386620020568, w0=0.34463315100048153, w1=-1.3171401203625739\n",
      "Gradient Descent(180/999): loss=1659.2331934865144, w0=0.3436987418282394, w1=-1.3148653514934483\n",
      "Gradient Descent(181/999): loss=1651.9692958127875, w0=0.34276783440086706, w1=-1.3125939670713573\n",
      "Gradient Descent(182/999): loss=1644.7466784671462, w0=0.34184039749172557, w1=-1.310325989743595\n",
      "Gradient Descent(183/999): loss=1637.5650537894167, w0=0.34091640029892384, w1=-1.3080614416147953\n",
      "Gradient Descent(184/999): loss=1630.424136925791, w0=0.3399958124390888, w1=-1.3058003442555874\n",
      "Gradient Descent(185/999): loss=1623.3236457828102, w0=0.3390786039412276, w1=-1.303542718711122\n",
      "Gradient Descent(186/999): loss=1616.2633009824365, w0=0.33816474524068074, w1=-1.3012885855094671\n",
      "Gradient Descent(187/999): loss=1609.2428258182067, w0=0.33725420717316423, w1=-1.299037964669882\n",
      "Gradient Descent(188/999): loss=1602.2619462124371, w0=0.3363469609688998, w1=-1.2967908757109625\n",
      "Gradient Descent(189/999): loss=1595.3203906744504, w0=0.3354429782468319, w1=-1.294547337658668\n",
      "Gradient Descent(190/999): loss=1588.4178902597803, w0=0.33454223100893, w1=-1.2923073690542255\n",
      "Gradient Descent(191/999): loss=1581.5541785303653, w0=0.33364469163457516, w1=-1.2900709879619157\n",
      "Gradient Descent(192/999): loss=1574.7289915156487, w0=0.33275033287502953, w1=-1.2878382119767426\n",
      "Gradient Descent(193/999): loss=1567.9420676746138, w0=0.3318591278479873, w1=-1.285609058231988\n",
      "Gradient Descent(194/999): loss=1561.1931478586878, w0=0.3309710500322065, w1=-1.2833835434066512\n",
      "Gradient Descent(195/999): loss=1554.4819752755286, w0=0.33008607326221967, w1=-1.2811616837327786\n",
      "Gradient Descent(196/999): loss=1547.8082954536203, w0=0.32920417172312316, w1=-1.2789434950026826\n",
      "Gradient Descent(197/999): loss=1541.1718562077092, w0=0.328325319945443, w1=-1.276728992576051\n",
      "Gradient Descent(198/999): loss=1534.5724076050205, w0=0.3274494928000767, w1=-1.2745181913869512\n",
      "Gradient Descent(199/999): loss=1528.0097019322443, w0=0.32657666549330977, w1=-1.2723111059507282\n",
      "Gradient Descent(200/999): loss=1521.483493663275, w0=0.3257068135619061, w1=-1.2701077503707978\n",
      "Gradient Descent(201/999): loss=1514.9935394276818, w0=0.3248399128682705, w1=-1.2679081383453406\n",
      "Gradient Descent(202/999): loss=1508.5395979798802, w0=0.3239759395956829, w1=-1.2657122831738916\n",
      "Gradient Descent(203/999): loss=1502.121430169002, w0=0.3231148702436027, w1=-1.2635201977638335\n",
      "Gradient Descent(204/999): loss=1495.7387989094318, w0=0.32225668162304305, w1=-1.2613318946367906\n",
      "Gradient Descent(205/999): loss=1489.3914691519965, w0=0.32140135085201277, w1=-1.2591473859349276\n",
      "Gradient Descent(206/999): loss=1483.0792078557886, w0=0.3205488553510262, w1=-1.2569666834271518\n",
      "Gradient Descent(207/999): loss=1476.8017839606157, w0=0.31969917283867877, w1=-1.254789798515224\n",
      "Gradient Descent(208/999): loss=1470.558968360035, w0=0.3188522813272887, w1=-1.2526167422397763\n",
      "Gradient Descent(209/999): loss=1464.3505338749924, w0=0.31800815911860236, w1=-1.250447525286238\n",
      "Gradient Descent(210/999): loss=1458.176255228015, w0=0.31716678479956334, w1=-1.2482821579906747\n",
      "Gradient Descent(211/999): loss=1452.0359090179638, w0=0.31632813723814424, w1=-1.2461206503455373\n",
      "Gradient Descent(212/999): loss=1445.9292736953262, w0=0.3154921955792397, w1=-1.243963012005325\n",
      "Gradient Descent(213/999): loss=1439.8561295380302, w0=0.31465893924062033, w1=-1.2418092522921629\n",
      "Gradient Descent(214/999): loss=1433.8162586277751, w0=0.3138283479089463, w1=-1.239659380201295\n",
      "Gradient Descent(215/999): loss=1427.8094448268473, w0=0.3130004015358399, w1=-1.2375134044064948\n",
      "Gradient Descent(216/999): loss=1421.8354737554344, w0=0.31217508033401603, w1=-1.2353713332653946\n",
      "Gradient Descent(217/999): loss=1415.8941327693954, w0=0.31135236477346984, w1=-1.2332331748247327\n",
      "Gradient Descent(218/999): loss=1409.9852109385085, w0=0.31053223557772086, w1=-1.2310989368255227\n",
      "Gradient Descent(219/999): loss=1404.1084990251397, w0=0.30971467372011247, w1=-1.2289686267081443\n",
      "Gradient Descent(220/999): loss=1398.2637894633679, w0=0.30889966042016614, w1=-1.226842251617357\n",
      "Gradient Descent(221/999): loss=1392.450876338521, w0=0.30808717713998945, w1=-1.2247198184072383\n",
      "Gradient Descent(222/999): loss=1386.6695553671293, w0=0.3072772055807371, w1=-1.2226013336460468\n",
      "Gradient Descent(223/999): loss=1380.919623877277, w0=0.30646972767912445, w1=-1.220486803621012\n",
      "Gradient Descent(224/999): loss=1375.200880789347, w0=0.3056647256039924, w1=-1.218376234343051\n",
      "Gradient Descent(225/999): loss=1369.5131265971354, w0=0.3048621817529229, w1=-1.2162696315514157\n",
      "Gradient Descent(226/999): loss=1363.8561633493591, w0=0.30406207874890495, w1=-1.214167000718266\n",
      "Gradient Descent(227/999): loss=1358.229794631501, w0=0.3032643994370492, w1=-1.212068347053178\n",
      "Gradient Descent(228/999): loss=1352.6338255480136, w0=0.3024691268813516, w1=-1.20997367550758\n",
      "Gradient Descent(229/999): loss=1347.0680627048782, w0=0.3016762443615045, w1=-1.2078829907791249\n",
      "Gradient Descent(230/999): loss=1341.5323141924728, w0=0.30088573536975527, w1=-1.2057962973159924\n",
      "Gradient Descent(231/999): loss=1336.0263895687906, w0=0.30009758360781097, w1=-1.2037135993211296\n",
      "Gradient Descent(232/999): loss=1330.5500998429566, w0=0.29931177298378897, w1=-1.201634900756425\n",
      "Gradient Descent(233/999): loss=1325.1032574590563, w0=0.29852828760921246, w1=-1.1995602053468197\n",
      "Gradient Descent(234/999): loss=1319.6856762802795, w0=0.2977471117960505, w1=-1.1974895165843558\n",
      "Gradient Descent(235/999): loss=1314.2971715733343, w0=0.29696823005380174, w1=-1.195422837732164\n",
      "Gradient Descent(236/999): loss=1308.9375599931611, w0=0.2961916270866213, w1=-1.1933601718283915\n",
      "Gradient Descent(237/999): loss=1303.6066595679267, w0=0.29541728779048976, w1=-1.1913015216900666\n",
      "Gradient Descent(238/999): loss=1298.304289684275, w0=0.29464519725042476, w1=-1.189246889916909\n",
      "Gradient Descent(239/999): loss=1293.0302710728597, w0=0.29387534073773275, w1=-1.1871962788950792\n",
      "Gradient Descent(240/999): loss=1287.7844257941274, w0=0.29310770370730227, w1=-1.1851496908008714\n",
      "Gradient Descent(241/999): loss=1282.5665772243435, w0=0.2923422717949367, w1=-1.1831071276043514\n",
      "Gradient Descent(242/999): loss=1277.3765500418808, w0=0.2915790308147266, w1=-1.1810685910729375\n",
      "Gradient Descent(243/999): loss=1272.2141702137326, w0=0.2908179667564607, w1=-1.1790340827749275\n",
      "Gradient Descent(244/999): loss=1267.079264982264, w0=0.29005906578307566, w1=-1.177003604082972\n",
      "Gradient Descent(245/999): loss=1261.9716628521937, w0=0.2893023142281428, w1=-1.1749771561774955\n",
      "Gradient Descent(246/999): loss=1256.8911935777912, w0=0.2885476985933923, w1=-1.1729547400500628\n",
      "Gradient Descent(247/999): loss=1251.837688150297, w0=0.28779520554627425, w1=-1.1709363565066977\n",
      "Gradient Descent(248/999): loss=1246.8109787855483, w0=0.2870448219175553, w1=-1.1689220061711476\n",
      "Gradient Descent(249/999): loss=1241.8108989118132, w0=0.2862965346989511, w1=-1.1669116894881006\n",
      "Gradient Descent(250/999): loss=1236.8372831578283, w0=0.2855503310407934, w1=-1.164905406726352\n",
      "Gradient Descent(251/999): loss=1231.8899673410242, w0=0.28480619824973225, w1=-1.1629031579819233\n",
      "Gradient Descent(252/999): loss=1226.9687884559567, w0=0.2840641237864713, w1=-1.1609049431811334\n",
      "Gradient Descent(253/999): loss=1222.0735846629066, w0=0.28332409526353736, w1=-1.1589107620836216\n",
      "Gradient Descent(254/999): loss=1217.2041952766765, w0=0.2825861004430824, w1=-1.1569206142853268\n",
      "Gradient Descent(255/999): loss=1212.360460755563, w0=0.28185012723471836, w1=-1.1549344992214179\n",
      "Gradient Descent(256/999): loss=1207.542222690495, w0=0.2811161636933836, w1=-1.1529524161691826\n",
      "Gradient Descent(257/999): loss=1202.7493237943493, w0=0.28038419801724124, w1=-1.1509743642508696\n",
      "Gradient Descent(258/999): loss=1197.9816078914293, w0=0.27965421854560824, w1=-1.1490003424364879\n",
      "Gradient Descent(259/999): loss=1193.2389199071097, w0=0.27892621375691534, w1=-1.1470303495465632\n",
      "Gradient Descent(260/999): loss=1188.5211058576265, w0=0.2782001722666971, w1=-1.1450643842548525\n",
      "Gradient Descent(261/999): loss=1183.8280128400384, w0=0.27747608282561137, w1=-1.1431024450910159\n",
      "Gradient Descent(262/999): loss=1179.15948902233, w0=0.2767539343174885, w1=-1.1411445304432486\n",
      "Gradient Descent(263/999): loss=1174.5153836336617, w0=0.2760337157574089, w1=-1.1391906385608712\n",
      "Gradient Descent(264/999): loss=1169.8955469547661, w0=0.2753154162898094, w1=-1.1372407675568816\n",
      "Gradient Descent(265/999): loss=1165.2998303084842, w0=0.27459902518661705, w1=-1.1352949154104661\n",
      "Gradient Descent(266/999): loss=1160.7280860504384, w0=0.27388453184541134, w1=-1.133353079969473\n",
      "Gradient Descent(267/999): loss=1156.1801675598465, w0=0.27317192578761285, w1=-1.1314152589528477\n",
      "Gradient Descent(268/999): loss=1151.6559292304557, w0=0.2724611966566989, w1=-1.1294814499530301\n",
      "Gradient Descent(269/999): loss=1147.1552264616128, w0=0.27175233421644557, w1=-1.1275516504383154\n",
      "Gradient Descent(270/999): loss=1142.6779156494608, w0=0.2710453283491958, w1=-1.1256258577551783\n",
      "Gradient Descent(271/999): loss=1138.223854178254, w0=0.27034016905415303, w1=-1.1237040691305615\n",
      "Gradient Descent(272/999): loss=1133.792900411798, w0=0.26963684644569974, w1=-1.1217862816741282\n",
      "Gradient Descent(273/999): loss=1129.3849136850004, w0=0.26893535075174146, w1=-1.1198724923804806\n",
      "Gradient Descent(274/999): loss=1124.999754295544, w0=0.2682356723120746, w1=-1.117962698131344\n",
      "Gradient Descent(275/999): loss=1120.6372834956683, w0=0.267537801576779, w1=-1.1160568956977166\n",
      "Gradient Descent(276/999): loss=1116.297363484065, w0=0.2668417291046336, w1=-1.1141550817419867\n",
      "Gradient Descent(277/999): loss=1111.9798573978724, w0=0.2661474455615562, w1=-1.1122572528200165\n",
      "Gradient Descent(278/999): loss=1107.684629304786, w0=0.26545494171906553, w1=-1.1103634053831941\n",
      "Gradient Descent(279/999): loss=1103.411544195271, w0=0.2647642084527669, w1=-1.108473535780453\n",
      "Gradient Descent(280/999): loss=1099.16046797486, w0=0.2640752367408593, w1=-1.1065876402602612\n",
      "Gradient Descent(281/999): loss=1094.9312674565774, w0=0.26338801766266556, w1=-1.104705714972579\n",
      "Gradient Descent(282/999): loss=1090.7238103534335, w0=0.26270254239718316, w1=-1.1028277559707862\n",
      "Gradient Descent(283/999): loss=1086.537965271032, w0=0.2620188022216574, w1=-1.1009537592135792\n",
      "Gradient Descent(284/999): loss=1082.3736017002625, w0=0.261336788510175, w1=-1.0990837205668396\n",
      "Gradient Descent(285/999): loss=1078.2305900100837, w0=0.2606564927322789, w1=-1.0972176358054724\n",
      "Gradient Descent(286/999): loss=1074.1088014403988, w0=0.2599779064516033, w1=-1.0953555006152162\n",
      "Gradient Descent(287/999): loss=1070.0081080950158, w0=0.25930102132452876, w1=-1.0934973105944261\n",
      "Gradient Descent(288/999): loss=1065.9283829346978, w0=0.2586258290988576, w1=-1.0916430612558266\n",
      "Gradient Descent(289/999): loss=1061.8694997702885, w0=0.2579523216125086, w1=-1.0897927480282392\n",
      "Gradient Descent(290/999): loss=1057.8313332559308, w0=0.25728049079223064, w1=-1.0879463662582824\n",
      "Gradient Descent(291/999): loss=1053.813758882358, w0=0.25661032865233624, w1=-1.0861039112120443\n",
      "Gradient Descent(292/999): loss=1049.8166529702655, w0=0.25594182729345283, w1=-1.0842653780767304\n",
      "Gradient Descent(293/999): loss=1045.8398926637653, w0=0.25527497890129314, w1=-1.0824307619622848\n",
      "Gradient Descent(294/999): loss=1041.8833559239094, w0=0.2546097757454435, w1=-1.080600057902986\n",
      "Gradient Descent(295/999): loss=1037.9469215222907, w0=0.25394621017817015, w1=-1.0787732608590181\n",
      "Gradient Descent(296/999): loss=1034.0304690347182, w0=0.2532842746332428, w1=-1.076950365718017\n",
      "Gradient Descent(297/999): loss=1030.133878834965, w0=0.2526239616247764, w1=-1.0751313672965925\n",
      "Gradient Descent(298/999): loss=1026.257032088579, w0=0.251965263746089, w1=-1.0733162603418267\n",
      "Gradient Descent(299/999): loss=1022.3998107467759, w0=0.2513081736685774, w1=-1.0715050395327483\n",
      "Gradient Descent(300/999): loss=1018.5620975403871, w0=0.25065268414060876, w1=-1.0696976994817842\n",
      "Gradient Descent(301/999): loss=1014.7437759738814, w0=0.24999878798642894, w1=-1.0678942347361877\n",
      "Gradient Descent(302/999): loss=1010.9447303194489, w0=0.2493464781050868, w1=-1.0660946397794457\n",
      "Gradient Descent(303/999): loss=1007.164845611155, w0=0.24869574746937456, w1=-1.0642989090326613\n",
      "Gradient Descent(304/999): loss=1003.404007639147, w0=0.24804658912478356, w1=-1.0625070368559164\n",
      "Gradient Descent(305/999): loss=999.6621029439323, w0=0.24739899618847572, w1=-1.0607190175496124\n",
      "Gradient Descent(306/999): loss=995.9390188107109, w0=0.24675296184827, w1=-1.0589348453557896\n",
      "Gradient Descent(307/999): loss=992.2346432637756, w0=0.24610847936164396, w1=-1.0571545144594254\n",
      "Gradient Descent(308/999): loss=988.5488650609597, w0=0.24546554205474994, w1=-1.0553780189897126\n",
      "Gradient Descent(309/999): loss=984.8815736881481, w0=0.2448241433214459, w1=-1.0536053530213165\n",
      "Gradient Descent(310/999): loss=981.2326593538513, w0=0.24418427662234052, w1=-1.0518365105756136\n",
      "Gradient Descent(311/999): loss=977.6020129838239, w0=0.24354593548385242, w1=-1.0500714856219084\n",
      "Gradient Descent(312/999): loss=973.9895262157447, w0=0.24290911349728322, w1=-1.0483102720786337\n",
      "Gradient Descent(313/999): loss=970.3950913939483, w0=0.24227380431790438, w1=-1.046552863814529\n",
      "Gradient Descent(314/999): loss=966.81860156421, w0=0.24164000166405747, w1=-1.0447992546498017\n",
      "Gradient Descent(315/999): loss=963.2599504685849, w0=0.24100769931626775, w1=-1.0430494383572702\n",
      "Gradient Descent(316/999): loss=959.7190325402939, w0=0.24037689111637087, w1=-1.041303408663487\n",
      "Gradient Descent(317/999): loss=956.195742898663, w0=0.23974757096665233, w1=-1.0395611592498457\n",
      "Gradient Descent(318/999): loss=952.6899773441132, w0=0.23911973282899995, w1=-1.0378226837536695\n",
      "Gradient Descent(319/999): loss=949.2016323531961, w0=0.23849337072406848, w1=-1.036087975769282\n",
      "Gradient Descent(320/999): loss=945.7306050736785, w0=0.23786847873045683, w1=-1.0343570288490613\n",
      "Gradient Descent(321/999): loss=942.2767933196744, w0=0.2372450509838973, w1=-1.0326298365044773\n",
      "Gradient Descent(322/999): loss=938.8400955668261, w0=0.236623081676457, w1=-1.0309063922071118\n",
      "Gradient Descent(323/999): loss=935.4204109475266, w0=0.2360025650557506, w1=-1.029186689389663\n",
      "Gradient Descent(324/999): loss=932.0176392461893, w0=0.2353834954241653, w1=-1.0274707214469332\n",
      "Gradient Descent(325/999): loss=928.631680894561, w0=0.23476586713809688, w1=-1.0257584817368008\n",
      "Gradient Descent(326/999): loss=925.2624369670806, w0=0.23414967460719696, w1=-1.0240499635811766\n",
      "Gradient Descent(327/999): loss=921.9098091762793, w0=0.23353491229363166, w1=-1.0223451602669449\n",
      "Gradient Descent(328/999): loss=918.5736998682227, w0=0.23292157471135108, w1=-1.0206440650468893\n",
      "Gradient Descent(329/999): loss=915.2540120179968, w0=0.23230965642536947, w1=-1.0189466711406032\n",
      "Gradient Descent(330/999): loss=911.9506492252331, w0=0.23169915205105615, w1=-1.0172529717353858\n",
      "Gradient Descent(331/999): loss=908.6635157096717, w0=0.23109005625343687, w1=-1.0155629599871239\n",
      "Gradient Descent(332/999): loss=905.3925163067726, w0=0.23048236374650544, w1=-1.0138766290211592\n",
      "Gradient Descent(333/999): loss=902.1375564633588, w0=0.22987606929254562, w1=-1.0121939719331408\n",
      "Gradient Descent(334/999): loss=898.8985422332978, w0=0.22927116770146289, w1=-1.010514981789865\n",
      "Gradient Descent(335/999): loss=895.6753802732311, w0=0.2286676538301262, w1=-1.008839651630101\n",
      "Gradient Descent(336/999): loss=892.467977838326, w0=0.22806552258171936, w1=-1.0071679744654023\n",
      "Gradient Descent(337/999): loss=889.2762427780783, w0=0.2274647689051021, w1=-1.0054999432809062\n",
      "Gradient Descent(338/999): loss=886.1000835321469, w0=0.2268653877941804, w1=-1.0038355510361194\n",
      "Gradient Descent(339/999): loss=882.9394091262199, w0=0.22626737428728624, w1=-1.0021747906656908\n",
      "Gradient Descent(340/999): loss=879.7941291679243, w0=0.22567072346656655, w1=-1.000517655080172\n",
      "Gradient Descent(341/999): loss=876.6641538427658, w0=0.225075430457381, w1=-0.9988641371667649\n",
      "Gradient Descent(342/999): loss=873.5493939101032, w0=0.2244814904277088, w1=-0.9972142297900577\n",
      "Gradient Descent(343/999): loss=870.4497606991623, w0=0.2238888985875643, w1=-0.9955679257927474\n",
      "Gradient Descent(344/999): loss=867.3651661050749, w0=0.22329765018842113, w1=-0.9939252179963524\n",
      "Gradient Descent(345/999): loss=864.295522584959, w0=0.22270774052264475, w1=-0.9922860992019111\n",
      "Gradient Descent(346/999): loss=861.24074315403, w0=0.2221191649229336, w1=-0.9906505621906712\n",
      "Gradient Descent(347/999): loss=858.2007413817423, w0=0.22153191876176834, w1=-0.9890185997247655\n",
      "Gradient Descent(348/999): loss=855.1754313879619, w0=0.2209459974508692, w1=-0.9873902045478783\n",
      "Gradient Descent(349/999): loss=852.1647278391756, w0=0.2203613964406615, w1=-0.9857653693858993\n",
      "Gradient Descent(350/999): loss=849.1685459447293, w0=0.2197781112197488, w1=-0.9841440869475678\n",
      "Gradient Descent(351/999): loss=846.1868014530943, w0=0.21919613731439402, w1=-0.9825263499251053\n",
      "Gradient Descent(352/999): loss=843.2194106481702, w0=0.21861547028800823, w1=-0.980912150994838\n",
      "Gradient Descent(353/999): loss=840.2662903456085, w0=0.21803610574064677, w1=-0.9793014828178086\n",
      "Gradient Descent(354/999): loss=837.3273578891785, w0=0.217458039308513, w1=-0.9776943380403784\n",
      "Gradient Descent(355/999): loss=834.4025311471516, w0=0.21688126666346938, w1=-0.9760907092948188\n",
      "Gradient Descent(356/999): loss=831.4917285087203, w0=0.21630578351255556, w1=-0.974490589199893\n",
      "Gradient Descent(357/999): loss=828.5948688804449, w0=0.21573158559751374, w1=-0.972893970361428\n",
      "Gradient Descent(358/999): loss=825.7118716827275, w0=0.21515866869432101, w1=-0.9713008453728772\n",
      "Gradient Descent(359/999): loss=822.8426568463162, w0=0.21458702861272863, w1=-0.9697112068158728\n",
      "Gradient Descent(360/999): loss=819.9871448088352, w0=0.2140166611958079, w1=-0.9681250472607696\n",
      "Gradient Descent(361/999): loss=817.1452565113423, w0=0.21344756231950293, w1=-0.9665423592671796\n",
      "Gradient Descent(362/999): loss=814.3169133949143, w0=0.21287972789218998, w1=-0.9649631353844963\n",
      "Gradient Descent(363/999): loss=811.5020373972598, w0=0.21231315385424318, w1=-0.9633873681524122\n",
      "Gradient Descent(364/999): loss=808.7005509493567, w0=0.21174783617760673, w1=-0.9618150501014258\n",
      "Gradient Descent(365/999): loss=805.9123769721175, w0=0.21118377086537343, w1=-0.9602461737533401\n",
      "Gradient Descent(366/999): loss=803.137438873078, w0=0.2106209539513694, w1=-0.9586807316217539\n",
      "Gradient Descent(367/999): loss=800.3756605431176, w0=0.21005938149974482, w1=-0.9571187162125432\n",
      "Gradient Descent(368/999): loss=797.6269663531973, w0=0.20949904960457086, w1=-0.9555601200243348\n",
      "Gradient Descent(369/999): loss=794.8912811511258, w0=0.20893995438944246, w1=-0.9540049355489721\n",
      "Gradient Descent(370/999): loss=792.1685302583537, w0=0.208382092007087, w1=-0.9524531552719732\n",
      "Gradient Descent(371/999): loss=789.458639466788, w0=0.2078254586389786, w1=-0.9509047716729793\n",
      "Gradient Descent(372/999): loss=786.7615350356326, w0=0.2072700504949584, w1=-0.9493597772261979\n",
      "Gradient Descent(373/999): loss=784.077143688252, w0=0.20671586381286003, w1=-0.9478181644008363\n",
      "Gradient Descent(374/999): loss=781.405392609059, w0=0.20616289485814096, w1=-0.9462799256615286\n",
      "Gradient Descent(375/999): loss=778.7462094404282, w0=0.2056111399235191, w1=-0.9447450534687549\n",
      "Gradient Descent(376/999): loss=776.0995222796273, w0=0.2050605953286147, w1=-0.9432135402792532\n",
      "Gradient Descent(377/999): loss=773.465259675779, w0=0.20451125741959775, w1=-0.9416853785464244\n",
      "Gradient Descent(378/999): loss=770.843350626837, w0=0.20396312256884022, w1=-0.9401605607207298\n",
      "Gradient Descent(379/999): loss=768.2337245765949, w0=0.2034161871745738, w1=-0.938639079250082\n",
      "Gradient Descent(380/999): loss=765.6363114117056, w0=0.20287044766055237, w1=-0.9371209265802285\n",
      "Gradient Descent(381/999): loss=763.0510414587335, w0=0.20232590047571966, w1=-0.9356060951551289\n",
      "Gradient Descent(382/999): loss=760.4778454812223, w0=0.2017825420938816, w1=-0.9340945774173254\n",
      "Gradient Descent(383/999): loss=757.9166546767842, w0=0.20124036901338369, w1=-0.9325863658083065\n",
      "Gradient Descent(384/999): loss=755.3674006742184, w0=0.20069937775679295, w1=-0.9310814527688644\n",
      "Gradient Descent(385/999): loss=752.8300155306371, w0=0.20015956487058467, w1=-0.9295798307394464\n",
      "Gradient Descent(386/999): loss=750.3044317286272, w0=0.19962092692483366, w1=-0.9280814921604988\n",
      "Gradient Descent(387/999): loss=747.7905821734232, w0=0.19908346051291015, w1=-0.9265864294728063\n",
      "Gradient Descent(388/999): loss=745.2884001901035, w0=0.19854716225118005, w1=-0.9250946351178242\n",
      "Gradient Descent(389/999): loss=742.7978195208082, w0=0.19801202877870971, w1=-0.9236061015380048\n",
      "Gradient Descent(390/999): loss=740.3187743219773, w0=0.197478056756975, w1=-0.9221208211771184\n",
      "Gradient Descent(391/999): loss=737.8511991616048, w0=0.19694524286957457, w1=-0.9206387864805675\n",
      "Gradient Descent(392/999): loss=735.3950290165172, w0=0.19641358382194748, w1=-0.9191599898956967\n",
      "Gradient Descent(393/999): loss=732.9501992696752, w0=0.1958830763410949, w1=-0.9176844238720957\n",
      "Gradient Descent(394/999): loss=730.5166457074813, w0=0.19535371717530584, w1=-0.9162120808618973\n",
      "Gradient Descent(395/999): loss=728.0943045171231, w0=0.194825503093887, w1=-0.9147429533200702\n",
      "Gradient Descent(396/999): loss=725.6831122839241, w0=0.19429843088689658, w1=-0.9132770337047056\n",
      "Gradient Descent(397/999): loss=723.2830059887178, w0=0.19377249736488197, w1=-0.9118143144772997\n",
      "Gradient Descent(398/999): loss=720.893923005243, w0=0.19324769935862127, w1=-0.9103547881030298\n",
      "Gradient Descent(399/999): loss=718.5158010975528, w0=0.19272403371886862, w1=-0.9088984470510263\n",
      "Gradient Descent(400/999): loss=716.1485784174463, w0=0.19220149731610334, w1=-0.9074452837946388\n",
      "Gradient Descent(401/999): loss=713.7921935019131, w0=0.19168008704028264, w1=-0.9059952908116979\n",
      "Gradient Descent(402/999): loss=711.4465852706046, w0=0.19115979980059805, w1=-0.9045484605847716\n",
      "Gradient Descent(403/999): loss=709.1116930233136, w0=0.19064063252523536, w1=-0.9031047856014172\n",
      "Gradient Descent(404/999): loss=706.7874564374806, w0=0.19012258216113812, w1=-0.9016642583544283\n",
      "Gradient Descent(405/999): loss=704.473815565711, w0=0.18960564567377453, w1=-0.9002268713420776\n",
      "Gradient Descent(406/999): loss=702.1707108333122, w0=0.18908982004690794, w1=-0.8987926170683541\n",
      "Gradient Descent(407/999): loss=699.8780830358517, w0=0.18857510228237045, w1=-0.8973614880431968\n",
      "Gradient Descent(408/999): loss=697.5958733367241, w0=0.18806148939984008, w1=-0.8959334767827234\n",
      "Gradient Descent(409/999): loss=695.3240232647447, w0=0.18754897843662102, w1=-0.8945085758094546\n",
      "Gradient Descent(410/999): loss=693.06247471175, w0=0.1870375664474272, w1=-0.8930867776525346\n",
      "Gradient Descent(411/999): loss=690.8111699302286, w0=0.18652725050416896, w1=-0.8916680748479467\n",
      "Gradient Descent(412/999): loss=688.5700515309543, w0=0.18601802769574297, w1=-0.890252459938725\n",
      "Gradient Descent(413/999): loss=686.3390624806422, w0=0.1855098951278251, w1=-0.8888399254751621\n",
      "Gradient Descent(414/999): loss=684.118146099623, w0=0.1850028499226663, w1=-0.8874304640150124\n",
      "Gradient Descent(415/999): loss=681.9072460595338, w0=0.18449688921889165, w1=-0.8860240681236919\n",
      "Gradient Descent(416/999): loss=679.7063063810162, w0=0.18399201017130218, w1=-0.8846207303744739\n",
      "Gradient Descent(417/999): loss=677.5152714314437, w0=0.18348820995067963, w1=-0.8832204433486801\n",
      "Gradient Descent(418/999): loss=675.3340859226532, w0=0.18298548574359408, w1=-0.8818231996358692\n",
      "Gradient Descent(419/999): loss=673.1626949087006, w0=0.18248383475221444, w1=-0.8804289918340208\n",
      "Gradient Descent(420/999): loss=671.0010437836268, w0=0.18198325419412165, w1=-0.8790378125497158\n",
      "Gradient Descent(421/999): loss=668.8490782792418, w0=0.18148374130212455, w1=-0.8776496543983133\n",
      "Gradient Descent(422/999): loss=666.7067444629217, w0=0.18098529332407862, w1=-0.8762645100041242\n",
      "Gradient Descent(423/999): loss=664.5739887354229, w0=0.18048790752270724, w1=-0.8748823720005803\n",
      "Gradient Descent(424/999): loss=662.4507578287139, w0=0.17999158117542555, w1=-0.8735032330304011\n",
      "Gradient Descent(425/999): loss=660.3369988038139, w0=0.17949631157416693, w1=-0.8721270857457567\n",
      "Gradient Descent(426/999): loss=658.2326590486576, w0=0.17900209602521203, w1=-0.870753922808427\n",
      "Gradient Descent(427/999): loss=656.1376862759606, w0=0.17850893184902025, w1=-0.8693837368899581\n",
      "Gradient Descent(428/999): loss=654.0520285211165, w0=0.1780168163800637, w1=-0.8680165206718152\n",
      "Gradient Descent(429/999): loss=651.9756341400921, w0=0.17752574696666354, w1=-0.8666522668455325\n",
      "Gradient Descent(430/999): loss=649.9084518073475, w0=0.17703572097082884, w1=-0.8652909681128597\n",
      "Gradient Descent(431/999): loss=647.8504305137668, w0=0.17654673576809762, w1=-0.8639326171859053\n",
      "Gradient Descent(432/999): loss=645.8015195646036, w0=0.17605878874738032, w1=-0.8625772067872777\n",
      "Gradient Descent(433/999): loss=643.7616685774391, w0=0.17557187731080545, w1=-0.8612247296502217\n",
      "Gradient Descent(434/999): loss=641.730827480157, w0=0.17508599887356766, w1=-0.859875178518754\n",
      "Gradient Descent(435/999): loss=639.7089465089299, w0=0.1746011508637778, w1=-0.8585285461477938\n",
      "Gradient Descent(436/999): loss=637.6959762062179, w0=0.17411733072231542, w1=-0.8571848253032927\n",
      "Gradient Descent(437/999): loss=635.6918674187887, w0=0.1736345359026831, w1=-0.8558440087623591\n",
      "Gradient Descent(438/999): loss=633.6965712957382, w0=0.17315276387086323, w1=-0.8545060893133827\n",
      "Gradient Descent(439/999): loss=631.710039286538, w0=0.17267201210517663, w1=-0.853171059756154\n",
      "Gradient Descent(440/999): loss=629.7322231390867, w0=0.17219227809614338, w1=-0.8518389129019828\n",
      "Gradient Descent(441/999): loss=627.7630748977796, w0=0.1717135593463455, w1=-0.850509641573812\n",
      "Gradient Descent(442/999): loss=625.8025469015857, w0=0.17123585337029187, w1=-0.8491832386063316\n",
      "Gradient Descent(443/999): loss=623.8505917821493, w0=0.17075915769428487, w1=-0.8478596968460873\n",
      "Gradient Descent(444/999): loss=621.9071624618886, w0=0.17028346985628912, w1=-0.8465390091515888\n",
      "Gradient Descent(445/999): loss=619.9722121521202, w0=0.16980878740580202, w1=-0.8452211683934138\n",
      "Gradient Descent(446/999): loss=618.0456943511905, w0=0.16933510790372627, w1=-0.8439061674543112\n",
      "Gradient Descent(447/999): loss=616.1275628426204, w0=0.16886242892224415, w1=-0.8425939992293011\n",
      "Gradient Descent(448/999): loss=614.217771693263, w0=0.16839074804469367, w1=-0.8412846566257719\n",
      "Gradient Descent(449/999): loss=612.3162752514723, w0=0.1679200628654465, w1=-0.8399781325635761\n",
      "Gradient Descent(450/999): loss=610.42302814529, w0=0.16745037098978763, w1=-0.8386744199751233\n",
      "Gradient Descent(451/999): loss=608.5379852806326, w0=0.1669816700337969, w1=-0.8373735118054709\n",
      "Gradient Descent(452/999): loss=606.6611018395056, w0=0.1665139576242321, w1=-0.8360754010124131\n",
      "Gradient Descent(453/999): loss=604.7923332782177, w0=0.16604723139841385, w1=-0.8347800805665666\n",
      "Gradient Descent(454/999): loss=602.9316353256133, w0=0.16558148900411207, w1=-0.8334875434514555\n",
      "Gradient Descent(455/999): loss=601.0789639813144, w0=0.16511672809943423, w1=-0.8321977826635931\n",
      "Gradient Descent(456/999): loss=599.2342755139744, w0=0.164652946352715, w1=-0.8309107912125622\n",
      "Gradient Descent(457/999): loss=597.3975264595487, w0=0.16419014144240773, w1=-0.829626562121093\n",
      "Gradient Descent(458/999): loss=595.568673619565, w0=0.16372831105697733, w1=-0.8283450884251391\n",
      "Gradient Descent(459/999): loss=593.7476740594195, w0=0.16326745289479475, w1=-0.8270663631739517\n",
      "Gradient Descent(460/999): loss=591.9344851066735, w0=0.16280756466403298, w1=-0.8257903794301515\n",
      "Gradient Descent(461/999): loss=590.1290643493682, w0=0.1623486440825646, w1=-0.8245171302697992\n",
      "Gradient Descent(462/999): loss=588.3313696343469, w0=0.16189068887786073, w1=-0.8232466087824635\n",
      "Gradient Descent(463/999): loss=586.5413590655887, w0=0.16143369678689143, w1=-0.8219788080712874\n",
      "Gradient Descent(464/999): loss=584.7589910025571, w0=0.1609776655560276, w1=-0.8207137212530534\n",
      "Gradient Descent(465/999): loss=582.9842240585552, w0=0.16052259294094434, w1=-0.8194513414582458\n",
      "Gradient Descent(466/999): loss=581.2170170990925, w0=0.16006847670652546, w1=-0.8181916618311121\n",
      "Gradient Descent(467/999): loss=579.457329240267, w0=0.15961531462676967, w1=-0.8169346755297221\n",
      "Gradient Descent(468/999): loss=577.7051198471495, w0=0.15916310448469792, w1=-0.8156803757260255\n",
      "Gradient Descent(469/999): loss=575.9603485321907, w0=0.15871184407226208, w1=-0.8144287556059081\n",
      "Gradient Descent(470/999): loss=574.222975153627, w0=0.15826153119025504, w1=-0.813179808369246\n",
      "Gradient Descent(471/999): loss=572.4929598139037, w0=0.15781216364822206, w1=-0.811933527229958\n",
      "Gradient Descent(472/999): loss=570.7702628581075, w0=0.15736373926437325, w1=-0.8106899054160571\n",
      "Gradient Descent(473/999): loss=569.0548448724087, w0=0.15691625586549754, w1=-0.8094489361696996\n",
      "Gradient Descent(474/999): loss=567.3466666825141, w0=0.1564697112868777, w1=-0.8082106127472333\n",
      "Gradient Descent(475/999): loss=565.6456893521296, w0=0.15602410337220665, w1=-0.8069749284192437\n",
      "Gradient Descent(476/999): loss=563.9518741814355, w0=0.15557942997350496, w1=-0.8057418764705994\n",
      "Gradient Descent(477/999): loss=562.2651827055681, w0=0.15513568895103952, w1=-0.8045114502004949\n",
      "Gradient Descent(478/999): loss=560.5855766931137, w0=0.15469287817324343, w1=-0.8032836429224931\n",
      "Gradient Descent(479/999): loss=558.9130181446128, w0=0.1542509955166369, w1=-0.8020584479645658\n",
      "Gradient Descent(480/999): loss=557.2474692910737, w0=0.15381003886574943, w1=-0.8008358586691328\n",
      "Gradient Descent(481/999): loss=555.5888925924957, w0=0.15337000611304297, w1=-0.7996158683930995\n",
      "Gradient Descent(482/999): loss=553.9372507364016, w0=0.1529308951588363, w1=-0.798398470507894\n",
      "Gradient Descent(483/999): loss=552.2925066363814, w0=0.15249270391123035, w1=-0.7971836583995018\n",
      "Gradient Descent(484/999): loss=550.6546234306464, w0=0.15205543028603477, w1=-0.7959714254684994\n",
      "Gradient Descent(485/999): loss=549.0235644805879, w0=0.15161907220669532, w1=-0.7947617651300873\n",
      "Gradient Descent(486/999): loss=547.3992933693518, w0=0.15118362760422244, w1=-0.7935546708141211\n",
      "Gradient Descent(487/999): loss=545.781773900418, w0=0.1507490944171208, w1=-0.7923501359651415\n",
      "Gradient Descent(488/999): loss=544.1709700961935, w0=0.1503154705913199, w1=-0.791148154042403\n",
      "Gradient Descent(489/999): loss=542.5668461966083, w0=0.14988275408010546, w1=-0.7899487185199019\n",
      "Gradient Descent(490/999): loss=540.9693666577282, w0=0.1494509428440521, w1=-0.7887518228864021\n",
      "Gradient Descent(491/999): loss=539.3784961503719, w0=0.14902003485095675, w1=-0.7875574606454611\n",
      "Gradient Descent(492/999): loss=537.794199558737, w0=0.14859002807577287, w1=-0.7863656253154534\n",
      "Gradient Descent(493/999): loss=536.2164419790398, w0=0.14816092050054605, w1=-0.7851763104295941\n",
      "Gradient Descent(494/999): loss=534.6451887181569, w0=0.14773271011435007, w1=-0.7839895095359604\n",
      "Gradient Descent(495/999): loss=533.0804052922823, w0=0.14730539491322414, w1=-0.7828052161975124\n",
      "Gradient Descent(496/999): loss=531.5220574255912, w0=0.14687897290011084, w1=-0.7816234239921133\n",
      "Gradient Descent(497/999): loss=529.9701110489098, w0=0.14645344208479513, w1=-0.7804441265125475\n",
      "Gradient Descent(498/999): loss=528.4245322983976, w0=0.146028800483844, w1=-0.7792673173665386\n",
      "Gradient Descent(499/999): loss=526.885287514238, w0=0.14560504612054717, w1=-0.7780929901767661\n",
      "Gradient Descent(500/999): loss=525.3523432393346, w0=0.14518217702485847, w1=-0.7769211385808809\n",
      "Gradient Descent(501/999): loss=523.825666218019, w0=0.14476019123333816, w1=-0.7757517562315197\n",
      "Gradient Descent(502/999): loss=522.3052233947675, w0=0.144339086789096, w1=-0.7745848367963194\n",
      "Gradient Descent(503/999): loss=520.7909819129221, w0=0.14391886174173507, w1=-0.7734203739579294\n",
      "Gradient Descent(504/999): loss=519.2829091134245, w0=0.1434995141472965, w1=-0.7722583614140235\n",
      "Gradient Descent(505/999): loss=517.7809725335571, w0=0.1430810420682049, w1=-0.7710987928773109\n",
      "Gradient Descent(506/999): loss=516.2851399056914, w0=0.14266344357321453, w1=-0.7699416620755462\n",
      "Gradient Descent(507/999): loss=514.7953791560435, w0=0.14224671673735623, w1=-0.7687869627515385\n",
      "Gradient Descent(508/999): loss=513.3116584034423, w0=0.1418308596418852, w1=-0.7676346886631598\n",
      "Gradient Descent(509/999): loss=511.8339459581015, w0=0.14141587037422929, w1=-0.7664848335833523\n",
      "Gradient Descent(510/999): loss=510.3622103203994, w0=0.1410017470279383, w1=-0.7653373913001352\n",
      "Gradient Descent(511/999): loss=508.89642017967134, w0=0.1405884877026337, w1=-0.7641923556166104\n",
      "Gradient Descent(512/999): loss=507.4365444130051, w0=0.14017609050395924, w1=-0.7630497203509671\n",
      "Gradient Descent(513/999): loss=505.98255208404754, w0=0.13976455354353212, w1=-0.7619094793364867\n",
      "Gradient Descent(514/999): loss=504.53441244181596, w0=0.13935387493889498, w1=-0.7607716264215455\n",
      "Gradient Descent(515/999): loss=503.09209491952174, w0=0.13894405281346836, w1=-0.7596361554696179\n",
      "Gradient Descent(516/999): loss=501.65556913339594, w0=0.13853508529650407, w1=-0.7585030603592781\n",
      "Gradient Descent(517/999): loss=500.2248048815285, w0=0.13812697052303896, w1=-0.7573723349842008\n",
      "Gradient Descent(518/999): loss=498.79977214270986, w0=0.13771970663384947, w1=-0.7562439732531624\n",
      "Gradient Descent(519/999): loss=497.3804410752846, w0=0.13731329177540677, w1=-0.7551179690900403\n",
      "Gradient Descent(520/999): loss=495.9667820160097, w0=0.1369077240998325, w1=-0.7539943164338117\n",
      "Gradient Descent(521/999): loss=494.55876547892046, w0=0.13650300176485522, w1=-0.7528730092385524\n",
      "Gradient Descent(522/999): loss=493.15636215420596, w0=0.1360991229337672, w1=-0.7517540414734344\n",
      "Gradient Descent(523/999): loss=491.75954290708995, w0=0.13569608577538214, w1=-0.7506374071227224\n",
      "Gradient Descent(524/999): loss=490.36827877671834, w0=0.13529388846399326, w1=-0.7495231001857708\n",
      "Gradient Descent(525/999): loss=488.9825409750571, w0=0.13489252917933203, w1=-0.748411114677019\n",
      "Gradient Descent(526/999): loss=487.60230088579544, w0=0.1344920061065274, w1=-0.7473014446259866\n",
      "Gradient Descent(527/999): loss=486.22753006325394, w0=0.13409231743606562, w1=-0.7461940840772677\n",
      "Gradient Descent(528/999): loss=484.8582002313052, w0=0.13369346136375074, w1=-0.7450890270905254\n",
      "Gradient Descent(529/999): loss=483.494283282297, w0=0.13329543609066538, w1=-0.743986267740484\n",
      "Gradient Descent(530/999): loss=482.13575127598295, w0=0.1328982398231323, w1=-0.742885800116923\n",
      "Gradient Descent(531/999): loss=480.78257643846337, w0=0.13250187077267633, w1=-0.7417876183246679\n",
      "Gradient Descent(532/999): loss=479.4347311611296, w0=0.1321063271559868, w1=-0.7406917164835828\n",
      "Gradient Descent(533/999): loss=478.0921879996173, w0=0.13171160719488065, w1=-0.7395980887285609\n",
      "Gradient Descent(534/999): loss=476.75491967276406, w0=0.13131770911626586, w1=-0.7385067292095152\n",
      "Gradient Descent(535/999): loss=475.4228990615784, w0=0.1309246311521055, w1=-0.7374176320913683\n",
      "Gradient Descent(536/999): loss=474.0960992082098, w0=0.13053237153938216, w1=-0.736330791554042\n",
      "Gradient Descent(537/999): loss=472.7744933149315, w0=0.13014092852006287, w1=-0.7352462017924458\n",
      "Gradient Descent(538/999): loss=471.4580547431233, w0=0.1297503003410646, w1=-0.734163857016466\n",
      "Gradient Descent(539/999): loss=470.1467570122667, w0=0.12936048525422017, w1=-0.7330837514509532\n",
      "Gradient Descent(540/999): loss=468.8405737989455, w0=0.12897148151624452, w1=-0.7320058793357096\n",
      "Gradient Descent(541/999): loss=467.53947893584785, w0=0.12858328738870156, w1=-0.7309302349254766\n",
      "Gradient Descent(542/999): loss=466.2434464107832, w0=0.1281959011379714, w1=-0.7298568124899206\n",
      "Gradient Descent(543/999): loss=464.9524503656989, w0=0.12780932103521808, w1=-0.7287856063136197\n",
      "Gradient Descent(544/999): loss=463.66646509570546, w0=0.12742354535635755, w1=-0.7277166106960486\n",
      "Gradient Descent(545/999): loss=462.38546504811023, w0=0.12703857238202634, w1=-0.7266498199515649\n",
      "Gradient Descent(546/999): loss=461.1094248214525, w0=0.1266544003975505, w1=-0.7255852284093925\n",
      "Gradient Descent(547/999): loss=459.8383191645505, w0=0.12627102769291482, w1=-0.7245228304136072\n",
      "Gradient Descent(548/999): loss=458.57212297555077, w0=0.12588845256273282, w1=-0.7234626203231197\n",
      "Gradient Descent(549/999): loss=457.3108113009857, w0=0.12550667330621676, w1=-0.7224045925116596\n",
      "Gradient Descent(550/999): loss=456.0543593348342, w0=0.12512568822714823, w1=-0.7213487413677584\n",
      "Gradient Descent(551/999): loss=454.80274241759463, w0=0.12474549563384917, w1=-0.7202950612947324\n",
      "Gradient Descent(552/999): loss=453.5559360353559, w0=0.12436609383915308, w1=-0.719243546710665\n",
      "Gradient Descent(553/999): loss=452.31391581888136, w0=0.12398748116037685, w1=-0.7181941920483886\n",
      "Gradient Descent(554/999): loss=451.0766575426941, w0=0.12360965591929277, w1=-0.7171469917554661\n",
      "Gradient Descent(555/999): loss=449.8441371241718, w0=0.12323261644210096, w1=-0.7161019402941727\n",
      "Gradient Descent(556/999): loss=448.61633062264383, w0=0.12285636105940222, w1=-0.7150590321414765\n",
      "Gradient Descent(557/999): loss=447.3932142384972, w0=0.12248088810617115, w1=-0.7140182617890187\n",
      "Gradient Descent(558/999): loss=446.1747643122864, w0=0.12210619592172967, w1=-0.7129796237430949\n",
      "Gradient Descent(559/999): loss=444.9609573238519, w0=0.12173228284972083, w1=-0.7119431125246338\n",
      "Gradient Descent(560/999): loss=443.7517698914398, w0=0.12135914723808307, w1=-0.7109087226691779\n",
      "Gradient Descent(561/999): loss=442.5471787708303, w0=0.12098678743902463, w1=-0.7098764487268621\n",
      "Gradient Descent(562/999): loss=441.34716085447343, w0=0.12061520180899853, w1=-0.7088462852623928\n",
      "Gradient Descent(563/999): loss=440.15169317062447, w0=0.1202443887086776, w1=-0.7078182268550268\n",
      "Gradient Descent(564/999): loss=438.960752882492, w0=0.11987434650293007, w1=-0.7067922680985497\n",
      "Gradient Descent(565/999): loss=437.77431728738867, w0=0.11950507356079532, w1=-0.7057684036012536\n",
      "Gradient Descent(566/999): loss=436.5923638158817, w0=0.11913656825546, w1=-0.7047466279859155\n",
      "Gradient Descent(567/999): loss=435.4148700309628, w0=0.11876882896423445, w1=-0.7037269358897743\n",
      "Gradient Descent(568/999): loss=434.241813627207, w0=0.11840185406852942, w1=-0.7027093219645085\n",
      "Gradient Descent(569/999): loss=433.07317242995003, w0=0.11803564195383306, w1=-0.701693780876213\n",
      "Gradient Descent(570/999): loss=431.90892439446327, w0=0.11767019100968826, w1=-0.700680307305376\n",
      "Gradient Descent(571/999): loss=430.7490476051387, w0=0.11730549962967014, w1=-0.6996688959468552\n",
      "Gradient Descent(572/999): loss=429.59352027467537, w0=0.11694156621136406, w1=-0.6986595415098544\n",
      "Gradient Descent(573/999): loss=428.4423207432741, w0=0.11657838915634364, w1=-0.6976522387178995\n",
      "Gradient Descent(574/999): loss=427.29542747783574, w0=0.11621596687014925, w1=-0.696646982308814\n",
      "Gradient Descent(575/999): loss=426.1528190711667, w0=0.11585429776226669, w1=-0.6956437670346947\n",
      "Gradient Descent(576/999): loss=425.0144742411845, w0=0.11549338024610611, w1=-0.6946425876618871\n",
      "Gradient Descent(577/999): loss=423.88037183013614, w0=0.11513321273898128, w1=-0.6936434389709606\n",
      "Gradient Descent(578/999): loss=422.75049080381524, w0=0.11477379366208902, w1=-0.692646315756683\n",
      "Gradient Descent(579/999): loss=421.62481025078716, w0=0.11441512144048896, w1=-0.6916512128279957\n",
      "Gradient Descent(580/999): loss=420.50330938161824, w0=0.11405719450308349, w1=-0.6906581250079876\n",
      "Gradient Descent(581/999): loss=419.3859675281106, w0=0.11370001128259802, w1=-0.68966704713387\n",
      "Gradient Descent(582/999): loss=418.2727641425409, w0=0.11334357021556142, w1=-0.68867797405695\n",
      "Gradient Descent(583/999): loss=417.1636787969061, w0=0.11298786974228674, w1=-0.6876909006426051\n",
      "Gradient Descent(584/999): loss=416.05869118217083, w0=0.11263290830685212, w1=-0.6867058217702564\n",
      "Gradient Descent(585/999): loss=414.95778110752474, w0=0.11227868435708203, w1=-0.6857227323333424\n",
      "Gradient Descent(586/999): loss=413.8609284996364, w0=0.11192519634452862, w1=-0.6847416272392921\n",
      "Gradient Descent(587/999): loss=412.7681134019227, w0=0.11157244272445335, w1=-0.6837625014094986\n",
      "Gradient Descent(588/999): loss=411.6793159738141, w0=0.11122042195580886, w1=-0.6827853497792916\n",
      "Gradient Descent(589/999): loss=410.5945164900283, w0=0.11086913250122105, w1=-0.6818101672979107\n",
      "Gradient Descent(590/999): loss=409.5136953398491, w0=0.11051857282697128, w1=-0.6808369489284776\n",
      "Gradient Descent(591/999): loss=408.43683302640784, w0=0.11016874140297898, w1=-0.6798656896479692\n",
      "Gradient Descent(592/999): loss=407.3639101659734, w0=0.10981963670278429, w1=-0.6788963844471897\n",
      "Gradient Descent(593/999): loss=406.29490748724015, w0=0.10947125720353093, w1=-0.6779290283307426\n",
      "Gradient Descent(594/999): loss=405.2298058306287, w0=0.10912360138594943, w1=-0.6769636163170033\n",
      "Gradient Descent(595/999): loss=404.16858614758365, w0=0.10877666773434029, w1=-0.6760001434380907\n",
      "Gradient Descent(596/999): loss=403.11122949988055, w0=0.10843045473655763, w1=-0.675038604739839\n",
      "Gradient Descent(597/999): loss=402.0577170589372, w0=0.10808496088399279, w1=-0.6740789952817697\n",
      "Gradient Descent(598/999): loss=401.00803010512595, w0=0.10774018467155824, w1=-0.673121310137063\n",
      "Gradient Descent(599/999): loss=399.9621500270946, w0=0.10739612459767171, w1=-0.672165544392529\n",
      "Gradient Descent(600/999): loss=398.9200583210884, w0=0.1070527791642404, w1=-0.6712116931485793\n",
      "Gradient Descent(601/999): loss=397.8817365902792, w0=0.10671014687664543, w1=-0.6702597515191984\n",
      "Gradient Descent(602/999): loss=396.84716654409635, w0=0.1063682262437265, w1=-0.6693097146319141\n",
      "Gradient Descent(603/999): loss=395.81632999756494, w0=0.10602701577776663, w1=-0.6683615776277694\n",
      "Gradient Descent(604/999): loss=394.7892088706435, w0=0.10568651399447723, w1=-0.6674153356612923\n",
      "Gradient Descent(605/999): loss=393.7657851875727, w0=0.10534671941298317, w1=-0.6664709839004677\n",
      "Gradient Descent(606/999): loss=392.7460410762223, w0=0.10500763055580808, w1=-0.665528517526707\n",
      "Gradient Descent(607/999): loss=391.72995876744386, w0=0.10466924594885993, w1=-0.6645879317348197\n",
      "Gradient Descent(608/999): loss=390.71752059443185, w0=0.10433156412141659, w1=-0.6636492217329829\n",
      "Gradient Descent(609/999): loss=389.70870899208154, w0=0.10399458360611165, w1=-0.6627123827427125\n",
      "Gradient Descent(610/999): loss=388.70350649635776, w0=0.10365830293892044, w1=-0.661777409998833\n",
      "Gradient Descent(611/999): loss=387.70189574366333, w0=0.10332272065914608, w1=-0.660844298749448\n",
      "Gradient Descent(612/999): loss=386.7038594702136, w0=0.10298783530940583, w1=-0.6599130442559099\n",
      "Gradient Descent(613/999): loss=385.70938051141485, w0=0.10265364543561747, w1=-0.6589836417927906\n",
      "Gradient Descent(614/999): loss=384.71844180124566, w0=0.10232014958698589, w1=-0.658056086647851\n",
      "Gradient Descent(615/999): loss=383.7310263716445, w0=0.10198734631598982, w1=-0.657130374122011\n",
      "Gradient Descent(616/999): loss=382.7471173518978, w0=0.10165523417836873, w1=-0.6562064995293196\n",
      "Gradient Descent(617/999): loss=381.76669796803606, w0=0.1013238117331098, w1=-0.6552844581969243\n",
      "Gradient Descent(618/999): loss=380.7897515422303, w0=0.10099307754243508, w1=-0.6543642454650409\n",
      "Gradient Descent(619/999): loss=379.81626149219454, w0=0.10066303017178886, w1=-0.6534458566869233\n",
      "Gradient Descent(620/999): loss=378.84621133059125, w0=0.100333668189825, w1=-0.6525292872288331\n",
      "Gradient Descent(621/999): loss=377.8795846644412, w0=0.10000499016839455, w1=-0.6516145324700087\n",
      "Gradient Descent(622/999): loss=376.9163651945363, w0=0.09967699468253347, w1=-0.6507015878026353\n",
      "Gradient Descent(623/999): loss=375.95653671485746, w0=0.0993496803104504, w1=-0.6497904486318141\n",
      "Gradient Descent(624/999): loss=375.00008311199423, w0=0.09902304563351468, w1=-0.6488811103755316\n",
      "Gradient Descent(625/999): loss=374.046988364571, w0=0.09869708923624439, w1=-0.6479735684646292\n",
      "Gradient Descent(626/999): loss=373.09723654267384, w0=0.0983718097062946, w1=-0.6470678183427718\n",
      "Gradient Descent(627/999): loss=372.1508118072836, w0=0.09804720563444569, w1=-0.6461638554664181\n",
      "Gradient Descent(628/999): loss=371.2076984097113, w0=0.09772327561459179, w1=-0.6452616753047886\n",
      "Gradient Descent(629/999): loss=370.26788069103645, w0=0.09740001824372937, w1=-0.6443612733398357\n",
      "Gradient Descent(630/999): loss=369.33134308155155, w0=0.09707743212194594, w1=-0.6434626450662125\n",
      "Gradient Descent(631/999): loss=368.3980701002071, w0=0.09675551585240885, w1=-0.6425657859912416\n",
      "Gradient Descent(632/999): loss=367.4680463540638, w0=0.0964342680413542, w1=-0.6416706916348844\n",
      "Gradient Descent(633/999): loss=366.54125653774315, w0=0.09611368729807591, w1=-0.6407773575297104\n",
      "Gradient Descent(634/999): loss=365.61768543288684, w0=0.09579377223491485, w1=-0.6398857792208656\n",
      "Gradient Descent(635/999): loss=364.6973179076163, w0=0.09547452146724807, w1=-0.638995952266042\n",
      "Gradient Descent(636/999): loss=363.7801389159986, w0=0.0951559336134782, w1=-0.6381078722354461\n",
      "Gradient Descent(637/999): loss=362.86613349751116, w0=0.0948380072950229, w1=-0.637221534711768\n",
      "Gradient Descent(638/999): loss=361.95528677651413, w0=0.09452074113630445, w1=-0.6363369352901507\n",
      "Gradient Descent(639/999): loss=361.04758396172616, w0=0.0942041337647394, w1=-0.6354540695781582\n",
      "Gradient Descent(640/999): loss=360.14301034569945, w0=0.0938881838107284, w1=-0.6345729331957449\n",
      "Gradient Descent(641/999): loss=359.2415513043029, w0=0.09357288990764599, w1=-0.6336935217752244\n",
      "Gradient Descent(642/999): loss=358.343192296206, w0=0.09325825069183066, w1=-0.6328158309612382\n",
      "Gradient Descent(643/999): loss=357.44791886236607, w0=0.0929442648025749, w1=-0.6319398564107245\n",
      "Gradient Descent(644/999): loss=356.55571662551984, w0=0.09263093088211533, w1=-0.6310655937928873\n",
      "Gradient Descent(645/999): loss=355.6665712896783, w0=0.09231824757562301, w1=-0.6301930387891648\n",
      "Gradient Descent(646/999): loss=354.78046863962254, w0=0.09200621353119379, w1=-0.6293221870931984\n",
      "Gradient Descent(647/999): loss=353.8973945404075, w0=0.09169482739983871, w1=-0.6284530344108017\n",
      "Gradient Descent(648/999): loss=353.0173349368621, w0=0.0913840878354746, w1=-0.6275855764599289\n",
      "Gradient Descent(649/999): loss=352.140275853101, w0=0.09107399349491463, w1=-0.6267198089706438\n",
      "Gradient Descent(650/999): loss=351.2662033920302, w0=0.09076454303785912, w1=-0.6258557276850887\n",
      "Gradient Descent(651/999): loss=350.39510373486434, w0=0.09045573512688626, w1=-0.6249933283574528\n",
      "Gradient Descent(652/999): loss=349.526963140642, w0=0.09014756842744301, w1=-0.6241326067539416\n",
      "Gradient Descent(653/999): loss=348.6617679457452, w0=0.08984004160783607, w1=-0.6232735586527449\n",
      "Gradient Descent(654/999): loss=347.79950456342175, w0=0.08953315333922293, w1=-0.6224161798440065\n",
      "Gradient Descent(655/999): loss=346.94015948331315, w0=0.089226902295603, w1=-0.6215604661297924\n",
      "Gradient Descent(656/999): loss=346.08371927098176, w0=0.08892128715380881, w1=-0.6207064133240597\n",
      "Gradient Descent(657/999): loss=345.23017056744266, w0=0.08861630659349734, w1=-0.6198540172526256\n",
      "Gradient Descent(658/999): loss=344.37950008870047, w0=0.0883119592971413, w1=-0.6190032737531359\n",
      "Gradient Descent(659/999): loss=343.5316946252849, w0=0.08800824395002063, w1=-0.6181541786750345\n",
      "Gradient Descent(660/999): loss=342.6867410417934, w0=0.08770515924021403, w1=-0.6173067278795316\n",
      "Gradient Descent(661/999): loss=341.8446262764348, w0=0.08740270385859052, w1=-0.6164609172395729\n",
      "Gradient Descent(662/999): loss=341.0053373405756, w0=0.08710087649880106, w1=-0.6156167426398084\n",
      "Gradient Descent(663/999): loss=340.1688613182896, w0=0.0867996758572704, w1=-0.6147741999765615\n",
      "Gradient Descent(664/999): loss=339.33518536591123, w0=0.08649910063318877, w1=-0.6139332851577977\n",
      "Gradient Descent(665/999): loss=338.50429671159065, w0=0.08619914952850384, w1=-0.6130939941030935\n",
      "Gradient Descent(666/999): loss=337.6761826548514, w0=0.0858998212479126, w1=-0.6122563227436058\n",
      "Gradient Descent(667/999): loss=336.8508305661525, w0=0.08560111449885341, w1=-0.6114202670220402\n",
      "Gradient Descent(668/999): loss=336.0282278864523, w0=0.08530302799149807, w1=-0.6105858228926209\n",
      "Gradient Descent(669/999): loss=335.2083621267748, w0=0.08500556043874397, w1=-0.6097529863210587\n",
      "Gradient Descent(670/999): loss=334.39122086777894, w0=0.08470871055620627, w1=-0.608921753284521\n",
      "Gradient Descent(671/999): loss=333.5767917593318, w0=0.08441247706221022, w1=-0.6080921197716006\n",
      "Gradient Descent(672/999): loss=332.7650625200834, w0=0.08411685867778347, w1=-0.6072640817822843\n",
      "Gradient Descent(673/999): loss=331.95602093704343, w0=0.08382185412664844, w1=-0.6064376353279228\n",
      "Gradient Descent(674/999): loss=331.14965486516286, w0=0.08352746213521484, w1=-0.6056127764311995\n",
      "Gradient Descent(675/999): loss=330.34595222691667, w0=0.08323368143257212, w1=-0.6047895011260996\n",
      "Gradient Descent(676/999): loss=329.54490101189003, w0=0.08294051075048212, w1=-0.6039678054578798\n",
      "Gradient Descent(677/999): loss=328.74648927636537, w0=0.08264794882337165, w1=-0.6031476854830367\n",
      "Gradient Descent(678/999): loss=327.95070514291547, w0=0.08235599438832522, w1=-0.6023291372692772\n",
      "Gradient Descent(679/999): loss=327.15753679999574, w0=0.08206464618507776, w1=-0.601512156895487\n",
      "Gradient Descent(680/999): loss=326.3669725015412, w0=0.08177390295600746, w1=-0.6006967404517003\n",
      "Gradient Descent(681/999): loss=325.5790005665651, w0=0.08148376344612861, w1=-0.5998828840390691\n",
      "Gradient Descent(682/999): loss=324.7936093787601, w0=0.0811942264030845, w1=-0.5990705837698328\n",
      "Gradient Descent(683/999): loss=324.0107873861034, w0=0.08090529057714045, w1=-0.5982598357672875\n",
      "Gradient Descent(684/999): loss=323.2305231004616, w0=0.08061695472117675, w1=-0.5974506361657556\n",
      "Gradient Descent(685/999): loss=322.4528050972015, w0=0.0803292175906818, w1=-0.5966429811105552\n",
      "Gradient Descent(686/999): loss=321.6776220148003, w0=0.08004207794374522, w1=-0.5958368667579701\n",
      "Gradient Descent(687/999): loss=320.9049625544601, w0=0.07975553454105101, w1=-0.5950322892752188\n",
      "Gradient Descent(688/999): loss=320.13481547972447, w0=0.07946958614587078, w1=-0.5942292448404246\n",
      "Gradient Descent(689/999): loss=319.36716961609767, w0=0.07918423152405704, w1=-0.5934277296425854\n",
      "Gradient Descent(690/999): loss=318.6020138506657, w0=0.07889946944403649, w1=-0.5926277398815432\n",
      "Gradient Descent(691/999): loss=317.83933713171996, w0=0.07861529867680343, w1=-0.5918292717679537\n",
      "Gradient Descent(692/999): loss=317.0791284683838, w0=0.07833171799591317, w1=-0.591032321523257\n",
      "Gradient Descent(693/999): loss=316.3213769302411, w0=0.07804872617747545, w1=-0.5902368853796464\n",
      "Gradient Descent(694/999): loss=315.5660716469668, w0=0.07776632200014802, w1=-0.5894429595800388\n",
      "Gradient Descent(695/999): loss=314.813201807961, w0=0.07748450424513012, w1=-0.588650540378045\n",
      "Gradient Descent(696/999): loss=314.0627566619836, w0=0.07720327169615614, w1=-0.5878596240379392\n",
      "Gradient Descent(697/999): loss=313.31472551679366, w0=0.07692262313948921, w1=-0.5870702068346295\n",
      "Gradient Descent(698/999): loss=312.56909773878846, w0=0.07664255736391494, w1=-0.5862822850536276\n",
      "Gradient Descent(699/999): loss=311.8258627526463, w0=0.07636307316073511, w1=-0.5854958549910191\n",
      "Gradient Descent(700/999): loss=311.08501004097235, w0=0.07608416932376144, w1=-0.5847109129534339\n",
      "Gradient Descent(701/999): loss=310.3465291439444, w0=0.07580584464930941, w1=-0.5839274552580164\n",
      "Gradient Descent(702/999): loss=309.6104096589633, w0=0.0755280979361921, w1=-0.5831454782323955\n",
      "Gradient Descent(703/999): loss=308.87664124030425, w0=0.0752509279857141, w1=-0.5823649782146553\n",
      "Gradient Descent(704/999): loss=308.14521359877057, w0=0.07497433360166543, w1=-0.5815859515533053\n",
      "Gradient Descent(705/999): loss=307.4161165013499, w0=0.0746983135903155, w1=-0.5808083946072511\n",
      "Gradient Descent(706/999): loss=306.68933977087187, w0=0.0744228667604071, w1=-0.5800323037457646\n",
      "Gradient Descent(707/999): loss=305.9648732856699, w0=0.07414799192315051, w1=-0.5792576753484548\n",
      "Gradient Descent(708/999): loss=305.24270697924254, w0=0.07387368789221752, w1=-0.5784845058052385\n",
      "Gradient Descent(709/999): loss=304.522830839918, w0=0.07359995348373558, w1=-0.5777127915163103\n",
      "Gradient Descent(710/999): loss=303.80523491052276, w0=0.07332678751628192, w1=-0.5769425288921142\n",
      "Gradient Descent(711/999): loss=303.0899092880487, w0=0.0730541888108778, w1=-0.5761737143533139\n",
      "Gradient Descent(712/999): loss=302.3768441233253, w0=0.07278215619098269, w1=-0.5754063443307638\n",
      "Gradient Descent(713/999): loss=301.66602962069226, w0=0.07251068848248853, w1=-0.5746404152654795\n",
      "Gradient Descent(714/999): loss=300.9574560376748, w0=0.07223978451371403, w1=-0.5738759236086094\n",
      "Gradient Descent(715/999): loss=300.25111368466185, w0=0.07196944311539903, w1=-0.5731128658214051\n",
      "Gradient Descent(716/999): loss=299.54699292458474, w0=0.07169966312069882, w1=-0.5723512383751925\n",
      "Gradient Descent(717/999): loss=298.84508417259786, w0=0.07143044336517856, w1=-0.5715910377513435\n",
      "Gradient Descent(718/999): loss=298.14537789576434, w0=0.07116178268680769, w1=-0.5708322604412462\n",
      "Gradient Descent(719/999): loss=297.44786461273907, w0=0.07089367992595438, w1=-0.5700749029462773\n",
      "Gradient Descent(720/999): loss=296.7525348934582, w0=0.07062613392538009, w1=-0.569318961777772\n",
      "Gradient Descent(721/999): loss=296.059379358827, w0=0.07035914353023397, w1=-0.5685644334569965\n",
      "Gradient Descent(722/999): loss=295.368388680413, w0=0.07009270758804752, w1=-0.5678113145151188\n",
      "Gradient Descent(723/999): loss=294.679553580137, w0=0.0698268249487291, w1=-0.5670596014931802\n",
      "Gradient Descent(724/999): loss=293.9928648299717, w0=0.0695614944645586, w1=-0.5663092909420671\n",
      "Gradient Descent(725/999): loss=293.30831325163507, w0=0.06929671499018203, w1=-0.5655603794224819\n",
      "Gradient Descent(726/999): loss=292.62588971629367, w0=0.0690324853826062, w1=-0.5648128635049157\n",
      "Gradient Descent(727/999): loss=291.94558514426086, w0=0.06876880450119346, w1=-0.5640667397696187\n",
      "Gradient Descent(728/999): loss=291.2673905047004, w0=0.06850567120765637, w1=-0.5633220048065732\n",
      "Gradient Descent(729/999): loss=290.5912968153324, w0=0.06824308436605248, w1=-0.5625786552154644\n",
      "Gradient Descent(730/999): loss=289.9172951421391, w0=0.06798104284277909, w1=-0.5618366876056525\n",
      "Gradient Descent(731/999): loss=289.2453765990739, w0=0.06771954550656811, w1=-0.5610960985961452\n",
      "Gradient Descent(732/999): loss=288.57553234777185, w0=0.06745859122848083, w1=-0.5603568848155689\n",
      "Gradient Descent(733/999): loss=287.9077535972618, w0=0.06719817888190283, w1=-0.5596190429021414\n",
      "Gradient Descent(734/999): loss=287.2420316036808, w0=0.06693830734253882, w1=-0.5588825695036433\n",
      "Gradient Descent(735/999): loss=286.57835766999017, w0=0.06667897548840761, w1=-0.558147461277391\n",
      "Gradient Descent(736/999): loss=285.9167231456926, w0=0.06642018219983699, w1=-0.5574137148902082\n",
      "Gradient Descent(737/999): loss=285.25711942655323, w0=0.06616192635945871, w1=-0.5566813270183986\n",
      "Gradient Descent(738/999): loss=284.5995379543184, w0=0.06590420685220351, w1=-0.5559502943477179\n",
      "Gradient Descent(739/999): loss=283.9439702164421, w0=0.06564702256529605, w1=-0.5552206135733468\n",
      "Gradient Descent(740/999): loss=283.2904077458075, w0=0.06539037238825, w1=-0.554492281399863\n",
      "Gradient Descent(741/999): loss=282.63884212045656, w0=0.06513425521286312, w1=-0.5537652945412135\n",
      "Gradient Descent(742/999): loss=281.98926496331615, w0=0.06487866993321224, w1=-0.553039649720688\n",
      "Gradient Descent(743/999): loss=281.34166794192913, w0=0.0646236154456485, w1=-0.5523153436708909\n",
      "Gradient Descent(744/999): loss=280.6960427681862, w0=0.06436909064879234, w1=-0.5515923731337141\n",
      "Gradient Descent(745/999): loss=280.05238119805847, w0=0.06411509444352877, w1=-0.5508707348603104\n",
      "Gradient Descent(746/999): loss=279.4106750313343, w0=0.06386162573300244, w1=-0.5501504256110652\n",
      "Gradient Descent(747/999): loss=278.7709161113542, w0=0.06360868342261292, w1=-0.5494314421555707\n",
      "Gradient Descent(748/999): loss=278.13309632475085, w0=0.06335626642000984, w1=-0.548713781272598\n",
      "Gradient Descent(749/999): loss=277.4972076011889, w0=0.06310437363508817, w1=-0.5479974397500703\n",
      "Gradient Descent(750/999): loss=276.86324191310587, w0=0.06285300397998345, w1=-0.5472824143850363\n",
      "Gradient Descent(751/999): loss=276.2311912754568, w0=0.06260215636906707, w1=-0.5465687019836433\n",
      "Gradient Descent(752/999): loss=275.601047745459, w0=0.06235182971894162, w1=-0.5458562993611099\n",
      "Gradient Descent(753/999): loss=274.9728034223379, w0=0.06210202294843611, w1=-0.5451452033417002\n",
      "Gradient Descent(754/999): loss=274.3464504470769, w0=0.06185273497860137, w1=-0.5444354107586965\n",
      "Gradient Descent(755/999): loss=273.7219810021659, w0=0.06160396473270542, w1=-0.5437269184543728\n",
      "Gradient Descent(756/999): loss=273.09938731135304, w0=0.06135571113622879, w1=-0.5430197232799686\n",
      "Gradient Descent(757/999): loss=272.4786616393982, w0=0.061107973116859955, w1=-0.5423138220956621\n",
      "Gradient Descent(758/999): loss=271.8597962918273, w0=0.06086074960449073, w1=-0.5416092117705441\n",
      "Gradient Descent(759/999): loss=271.24278361468873, w0=0.06061403953121171, w1=-0.5409058891825915\n",
      "Gradient Descent(760/999): loss=270.62761599431, w0=0.060367841831307716, w1=-0.5402038512186413\n",
      "Gradient Descent(761/999): loss=270.0142858570586, w0=0.06012215544125326, w1=-0.5395030947743641\n",
      "Gradient Descent(762/999): loss=269.4027856691014, w0=0.05987697929970805, w1=-0.538803616754238\n",
      "Gradient Descent(763/999): loss=268.79310793616725, w0=0.059632312347512456, w1=-0.538105414071523\n",
      "Gradient Descent(764/999): loss=268.1852452033113, w0=0.05938815352768307, w1=-0.5374084836482343\n",
      "Gradient Descent(765/999): loss=267.5791900546794, w0=0.05914450178540821, w1=-0.5367128224151172\n",
      "Gradient Descent(766/999): loss=266.9749351132754, w0=0.05890135606804352, w1=-0.5360184273116204\n",
      "Gradient Descent(767/999): loss=266.37247304072923, w0=0.058658715325107476, w1=-0.5353252952858709\n",
      "Gradient Descent(768/999): loss=265.77179653706656, w0=0.05841657850827704, w1=-0.5346334232946479\n",
      "Gradient Descent(769/999): loss=265.1728983404794, w0=0.05817494457138323, w1=-0.5339428083033572\n",
      "Gradient Descent(770/999): loss=264.57577122709995, w0=0.057933812470406736, w1=-0.5332534472860057\n",
      "Gradient Descent(771/999): loss=263.9804080107731, w0=0.05769318116347357, w1=-0.5325653372251756\n",
      "Gradient Descent(772/999): loss=263.38680154283253, w0=0.05745304961085072, w1=-0.5318784751119996\n",
      "Gradient Descent(773/999): loss=262.7949447118777, w0=0.05721341677494183, w1=-0.5311928579461346\n",
      "Gradient Descent(774/999): loss=262.20483044355257, w0=0.05697428162028283, w1=-0.5305084827357369\n",
      "Gradient Descent(775/999): loss=261.61645170032347, w0=0.056735643113537695, w1=-0.529825346497437\n",
      "Gradient Descent(776/999): loss=261.02980148126176, w0=0.05649750022349414, w1=-0.529143446256314\n",
      "Gradient Descent(777/999): loss=260.4448728218261, w0=0.056259851921059326, w1=-0.5284627790458711\n",
      "Gradient Descent(778/999): loss=259.86165879364535, w0=0.05602269717925563, w1=-0.5277833419080099\n",
      "Gradient Descent(779/999): loss=259.2801525043046, w0=0.05578603497321639, w1=-0.5271051318930055\n",
      "Gradient Descent(780/999): loss=258.7003470971314, w0=0.055549864280181674, w1=-0.5264281460594818\n",
      "Gradient Descent(781/999): loss=258.1222357509825, w0=0.05531418407949408, w1=-0.5257523814743867\n",
      "Gradient Descent(782/999): loss=257.54581168003557, w0=0.05507899335259453, w1=-0.5250778352129669\n",
      "Gradient Descent(783/999): loss=256.9710681335772, w0=0.05484429108301806, w1=-0.5244045043587435\n",
      "Gradient Descent(784/999): loss=256.39799839579587, w0=0.054610076256389695, w1=-0.5237323860034871\n",
      "Gradient Descent(785/999): loss=255.82659578557525, w0=0.05437634786042024, w1=-0.5230614772471934\n",
      "Gradient Descent(786/999): loss=255.2568536562876, w0=0.05414310488490217, w1=-0.5223917751980586\n",
      "Gradient Descent(787/999): loss=254.6887653955912, w0=0.05391034632170547, w1=-0.5217232769724545\n",
      "Gradient Descent(788/999): loss=254.12232442522526, w0=0.05367807116477354, w1=-0.5210559796949048\n",
      "Gradient Descent(789/999): loss=253.55752420081006, w0=0.05344627841011908, w1=-0.5203898804980605\n",
      "Gradient Descent(790/999): loss=252.99435821164553, w0=0.053214967055819976, w1=-0.5197249765226751\n",
      "Gradient Descent(791/999): loss=252.4328199805123, w0=0.05298413610201525, w1=-0.5190612649175813\n",
      "Gradient Descent(792/999): loss=251.8729030634739, w0=0.052753784550900976, w1=-0.518398742839666\n",
      "Gradient Descent(793/999): loss=251.31460104967968, w0=0.052523911406726215, w1=-0.5177374074538467\n",
      "Gradient Descent(794/999): loss=250.75790756116993, w0=0.05229451567578899, w1=-0.5170772559330473\n",
      "Gradient Descent(795/999): loss=250.20281625268095, w0=0.052065596366432224, w1=-0.5164182854581744\n",
      "Gradient Descent(796/999): loss=249.6493208114526, w0=0.05183715248903975, w1=-0.5157604932180929\n",
      "Gradient Descent(797/999): loss=249.0974149570371, w0=0.05160918305603229, w1=-0.5151038764096028\n",
      "Gradient Descent(798/999): loss=248.54709244110626, w0=0.051381687081863456, w1=-0.5144484322374149\n",
      "Gradient Descent(799/999): loss=247.9983470472646, w0=0.051154663583015764, w1=-0.5137941579141274\n",
      "Gradient Descent(800/999): loss=247.45117259086, w0=0.050928111577996656, w1=-0.5131410506602023\n",
      "Gradient Descent(801/999): loss=246.90556291879622, w0=0.05070203008733455, w1=-0.5124891077039417\n",
      "Gradient Descent(802/999): loss=246.3615119093481, w0=0.05047641813357488, w1=-0.5118383262814643\n",
      "Gradient Descent(803/999): loss=245.81901347197626, w0=0.05025127474127615, w1=-0.511188703636682\n",
      "Gradient Descent(804/999): loss=245.27806154714355, w0=0.050026598937006006, w1=-0.5105402370212766\n",
      "Gradient Descent(805/999): loss=244.7386501061326, w0=0.04980238974933733, w1=-0.5098929236946764\n",
      "Gradient Descent(806/999): loss=244.2007731508654, w0=0.04957864620884432, w1=-0.5092467609240332\n",
      "Gradient Descent(807/999): loss=243.66442471372216, w0=0.04935536734809859, w1=-0.5086017459841986\n",
      "Gradient Descent(808/999): loss=243.1295988573629, w0=0.04913255220166528, w1=-0.5079578761577016\n",
      "Gradient Descent(809/999): loss=242.5962896745497, w0=0.04891019980609919, w1=-0.5073151487347248\n",
      "Gradient Descent(810/999): loss=242.0644912879696, w0=0.04868830919994092, w1=-0.5066735610130821\n",
      "Gradient Descent(811/999): loss=241.53419785005897, w0=0.04846687942371298, w1=-0.5060331102981954\n",
      "Gradient Descent(812/999): loss=241.00540354282936, w0=0.04824590951991599, w1=-0.5053937939030718\n",
      "Gradient Descent(813/999): loss=240.4781025776941, w0=0.048025398533024795, w1=-0.5047556091482809\n",
      "Gradient Descent(814/999): loss=239.9522891952955, w0=0.04780534550948468, w1=-0.504118553361932\n",
      "Gradient Descent(815/999): loss=239.42795766533368, w0=0.04758574949770754, w1=-0.5034826238796515\n",
      "Gradient Descent(816/999): loss=238.90510228639675, w0=0.04736660954806804, w1=-0.5028478180445601\n",
      "Gradient Descent(817/999): loss=238.3837173857915, w0=0.04714792471289987, w1=-0.5022141332072508\n",
      "Gradient Descent(818/999): loss=237.8637973193752, w0=0.04692969404649192, w1=-0.5015815667257658\n",
      "Gradient Descent(819/999): loss=237.3453364713883, w0=0.04671191660508453, w1=-0.5009501159655745\n",
      "Gradient Descent(820/999): loss=236.82832925428946, w0=0.04649459144686568, w1=-0.500319778299551\n",
      "Gradient Descent(821/999): loss=236.31277010858935, w0=0.046277717631967266, w1=-0.49969055110795185\n",
      "Gradient Descent(822/999): loss=235.7986535026881, w0=0.04606129422246134, w1=-0.499062431778394\n",
      "Gradient Descent(823/999): loss=235.28597393271096, w0=0.04584532028235635, w1=-0.49843541770583233\n",
      "Gradient Descent(824/999): loss=234.77472592234724, w0=0.045629794877593444, w1=-0.4978095062925378\n",
      "Gradient Descent(825/999): loss=234.2649040226898, w0=0.04541471707604271, w1=-0.49718469494807543\n",
      "Gradient Descent(826/999): loss=233.75650281207442, w0=0.04520008594749946, w1=-0.49656098108928226\n",
      "Gradient Descent(827/999): loss=233.24951689592112, w0=0.04498590056368057, w1=-0.49593836214024545\n",
      "Gradient Descent(828/999): loss=232.7439409065774, w0=0.04477215999822072, w1=-0.49531683553228056\n",
      "Gradient Descent(829/999): loss=232.23976950315927, w0=0.044558863326668745, w1=-0.49469639870390963\n",
      "Gradient Descent(830/999): loss=231.73699737139717, w0=0.04434600962648394, w1=-0.49407704910083955\n",
      "Gradient Descent(831/999): loss=231.23561922348014, w0=0.04413359797703239, w1=-0.49345878417594047\n",
      "Gradient Descent(832/999): loss=230.73562979790256, w0=0.043921627459583314, w1=-0.4928416013892241\n",
      "Gradient Descent(833/999): loss=230.23702385930997, w0=0.04371009715730539, w1=-0.4922254982078223\n",
      "Gradient Descent(834/999): loss=229.73979619834864, w0=0.043499006155263115, w1=-0.4916104721059656\n",
      "Gradient Descent(835/999): loss=229.24394163151305, w0=0.04328835354041319, w1=-0.4909965205649617\n",
      "Gradient Descent(836/999): loss=228.7494550009964, w0=0.04307813840160084, w1=-0.4903836410731743\n",
      "Gradient Descent(837/999): loss=228.25633117454123, w0=0.04286835982955624, w1=-0.48977183112600176\n",
      "Gradient Descent(838/999): loss=227.76456504529173, w0=0.04265901691689086, w1=-0.48916108822585586\n",
      "Gradient Descent(839/999): loss=227.27415153164512, w0=0.042450108758093894, w1=-0.48855140988214063\n",
      "Gradient Descent(840/999): loss=226.78508557710614, w0=0.042241634449528616, w1=-0.48794279361123144\n",
      "Gradient Descent(841/999): loss=226.29736215014157, w0=0.042033593089428826, w1=-0.48733523693645375\n",
      "Gradient Descent(842/999): loss=225.81097624403534, w0=0.04182598377789525, w1=-0.4867287373880623\n",
      "Gradient Descent(843/999): loss=225.32592287674427, w0=0.04161880561689198, w1=-0.48612329250322023\n",
      "Gradient Descent(844/999): loss=224.84219709075617, w0=0.04141205771024286, w1=-0.485518899825978\n",
      "Gradient Descent(845/999): loss=224.35979395294768, w0=0.04120573916362798, w1=-0.484915556907253\n",
      "Gradient Descent(846/999): loss=223.8787085544425, w0=0.04099984908458008, w1=-0.4843132613048085\n",
      "Gradient Descent(847/999): loss=223.3989360104724, w0=0.040794386582481024, w1=-0.48371201058323315\n",
      "Gradient Descent(848/999): loss=222.9204714602366, w0=0.04058935076855826, w1=-0.48311180231392037\n",
      "Gradient Descent(849/999): loss=222.44331006676418, w0=0.040384740755881283, w1=-0.48251263407504774\n",
      "Gradient Descent(850/999): loss=221.9674470167762, w0=0.0401805556593581, w1=-0.4819145034515567\n",
      "Gradient Descent(851/999): loss=221.49287752054946, w0=0.03997679459573174, w1=-0.48131740803513184\n",
      "Gradient Descent(852/999): loss=221.01959681177962, w0=0.03977345668357671, w1=-0.48072134542418093\n",
      "Gradient Descent(853/999): loss=220.54760014744699, w0=0.03957054104329551, w1=-0.4801263132238144\n",
      "Gradient Descent(854/999): loss=220.07688280768218, w0=0.039368046797115135, w1=-0.479532309045825\n",
      "Gradient Descent(855/999): loss=219.60744009563254, w0=0.039165973069083586, w1=-0.47893933050866794\n",
      "Gradient Descent(856/999): loss=219.1392673373296, w0=0.03896431898506638, w1=-0.4783473752374405\n",
      "Gradient Descent(857/999): loss=218.67235988155787, w0=0.03876308367274309, w1=-0.477756440863862\n",
      "Gradient Descent(858/999): loss=218.20671309972298, w0=0.03856226626160385, w1=-0.4771665250262541\n",
      "Gradient Descent(859/999): loss=217.74232238572253, w0=0.03836186588294592, w1=-0.47657762536952053\n",
      "Gradient Descent(860/999): loss=217.27918315581636, w0=0.03816188166987021, w1=-0.4759897395451273\n",
      "Gradient Descent(861/999): loss=216.81729084849786, w0=0.03796231275727784, w1=-0.475402865211083\n",
      "Gradient Descent(862/999): loss=216.35664092436747, w0=0.037763158281866716, w1=-0.4748170000319189\n",
      "Gradient Descent(863/999): loss=215.89722886600387, w0=0.03756441738212804, w1=-0.4742321416786694\n",
      "Gradient Descent(864/999): loss=215.43905017783922, w0=0.03736608919834295, w1=-0.47364828782885227\n",
      "Gradient Descent(865/999): loss=214.982100386034, w0=0.03716817287257905, w1=-0.4730654361664491\n",
      "Gradient Descent(866/999): loss=214.5263750383514, w0=0.03697066754868701, w1=-0.4724835843818857\n",
      "Gradient Descent(867/999): loss=214.07186970403413, w0=0.036773572372297156, w1=-0.4719027301720129\n",
      "Gradient Descent(868/999): loss=213.61857997368207, w0=0.03657688649081606, w1=-0.47132287124008687\n",
      "Gradient Descent(869/999): loss=213.16650145912848, w0=0.036380609053423156, w1=-0.4707440052957499\n",
      "Gradient Descent(870/999): loss=212.71562979332035, w0=0.03618473921106735, w1=-0.470166130055011\n",
      "Gradient Descent(871/999): loss=212.26596063019574, w0=0.03598927611646362, w1=-0.4695892432402269\n",
      "Gradient Descent(872/999): loss=211.8174896445655, w0=0.035794218924089634, w1=-0.46901334258008254\n",
      "Gradient Descent(873/999): loss=211.3702125319929, w0=0.03559956679018241, w1=-0.46843842580957235\n",
      "Gradient Descent(874/999): loss=210.9241250086754, w0=0.03540531887273493, w1=-0.4678644906699808\n",
      "Gradient Descent(875/999): loss=210.4792228113274, w0=0.03521147433149278, w1=-0.4672915349088637\n",
      "Gradient Descent(876/999): loss=210.03550169706253, w0=0.03501803232795079, w1=-0.4667195562800291\n",
      "Gradient Descent(877/999): loss=209.59295744327747, w0=0.03482499202534971, w1=-0.4661485525435184\n",
      "Gradient Descent(878/999): loss=209.15158584753627, w0=0.034632352588672845, w1=-0.46557852146558765\n",
      "Gradient Descent(879/999): loss=208.71138272745603, w0=0.03444011318464274, w1=-0.4650094608186886\n",
      "Gradient Descent(880/999): loss=208.27234392059202, w0=0.03424827298171783, w1=-0.4644413683814501\n",
      "Gradient Descent(881/999): loss=207.8344652843244, w0=0.034056831150089124, w1=-0.4638742419386595\n",
      "Gradient Descent(882/999): loss=207.3977426957455, w0=0.03386578686167691, w1=-0.4633080792812438\n",
      "Gradient Descent(883/999): loss=206.96217205154827, w0=0.033675139290127415, w1=-0.4627428782062513\n",
      "Gradient Descent(884/999): loss=206.52774926791344, w0=0.033484887610809505, w1=-0.4621786365168332\n",
      "Gradient Descent(885/999): loss=206.09447028040006, w0=0.033295031000811395, w1=-0.4616153520222249\n",
      "Gradient Descent(886/999): loss=205.66233104383576, w0=0.03310556863893735, w1=-0.4610530225377278\n",
      "Gradient Descent(887/999): loss=205.2313275322061, w0=0.0329164997057044, w1=-0.4604916458846908\n",
      "Gradient Descent(888/999): loss=204.8014557385468, w0=0.032727823383339044, w1=-0.45993121989049246\n",
      "Gradient Descent(889/999): loss=204.37271167483544, w0=0.032539538855774, w1=-0.4593717423885222\n",
      "Gradient Descent(890/999): loss=203.94509137188422, w0=0.03235164530864491, w1=-0.4588132112181625\n",
      "Gradient Descent(891/999): loss=203.51859087923353, w0=0.03216414192928708, w1=-0.4582556242247708\n",
      "Gradient Descent(892/999): loss=203.0932062650455, w0=0.03197702790673223, w1=-0.45769897925966135\n",
      "Gradient Descent(893/999): loss=202.66893361599853, w0=0.03179030243170523, w1=-0.4571432741800872\n",
      "Gradient Descent(894/999): loss=202.24576903718324, w0=0.03160396469662085, w1=-0.4565885068492224\n",
      "Gradient Descent(895/999): loss=201.82370865199786, w0=0.03141801389558053, w1=-0.45603467513614393\n",
      "Gradient Descent(896/999): loss=201.40274860204514, w0=0.03123244922436913, w1=-0.45548177691581393\n",
      "Gradient Descent(897/999): loss=200.98288504702853, w0=0.03104726988045171, w1=-0.4549298100690621\n",
      "Gradient Descent(898/999): loss=200.56411416465252, w0=0.03086247506297029, w1=-0.45437877248256775\n",
      "Gradient Descent(899/999): loss=200.14643215051757, w0=0.030678063972740633, w1=-0.4538286620488422\n",
      "Gradient Descent(900/999): loss=199.7298352180223, w0=0.030494035812249038, w1=-0.45327947666621127\n",
      "Gradient Descent(901/999): loss=199.31431959826148, w0=0.030310389785649126, w1=-0.4527312142387976\n",
      "Gradient Descent(902/999): loss=198.89988153992675, w0=0.030127125098758624, w1=-0.4521838726765031\n",
      "Gradient Descent(903/999): loss=198.4865173092081, w0=0.02994424095905619, w1=-0.4516374498949917\n",
      "Gradient Descent(904/999): loss=198.07422318969427, w0=0.02976173657567819, w1=-0.45109194381567186\n",
      "Gradient Descent(905/999): loss=197.6629954822765, w0=0.02957961115941553, w1=-0.450547352365679\n",
      "Gradient Descent(906/999): loss=197.25283050504916, w0=0.029397863922710474, w1=-0.45000367347785863\n",
      "Gradient Descent(907/999): loss=196.84372459321565, w0=0.029216494079653446, w1=-0.44946090509074876\n",
      "Gradient Descent(908/999): loss=196.43567409898992, w0=0.02903550084597988, w1=-0.44891904514856285\n",
      "Gradient Descent(909/999): loss=196.02867539150262, w0=0.028854883439067034, w1=-0.4483780916011727\n",
      "Gradient Descent(910/999): loss=195.62272485670528, w0=0.028674641077930847, w1=-0.44783804240409125\n",
      "Gradient Descent(911/999): loss=195.21781889727694, w0=0.02849477298322276, w1=-0.44729889551845575\n",
      "Gradient Descent(912/999): loss=194.81395393252922, w0=0.02831527837722659, w1=-0.44676064891101064\n",
      "Gradient Descent(913/999): loss=194.4111263983144, w0=0.028136156483855358, w1=-0.44622330055409054\n",
      "Gradient Descent(914/999): loss=194.00933274693205, w0=0.02795740652864816, w1=-0.4456868484256036\n",
      "Gradient Descent(915/999): loss=193.60856944703704, w0=0.02777902773876704, w1=-0.4451512905090145\n",
      "Gradient Descent(916/999): loss=193.20883298354846, w0=0.027601019342993832, w1=-0.44461662479332775\n",
      "Gradient Descent(917/999): loss=192.81011985755862, w0=0.02742338057172707, w1=-0.44408284927307107\n",
      "Gradient Descent(918/999): loss=192.41242658624236, w0=0.02724611065697883, w1=-0.44354996194827845\n",
      "Gradient Descent(919/999): loss=192.01574970276818, w0=0.02706920883237164, w1=-0.4430179608244738\n",
      "Gradient Descent(920/999): loss=191.6200857562074, w0=0.02689267433313536, w1=-0.4424868439126543\n",
      "Gradient Descent(921/999): loss=191.22543131144738, w0=0.02671650639610408, w1=-0.4419566092292738\n",
      "Gradient Descent(922/999): loss=190.83178294910178, w0=0.026540704259713013, w1=-0.4414272547962266\n",
      "Gradient Descent(923/999): loss=190.43913726542397, w0=0.026365267163995402, w1=-0.44089877864083055\n",
      "Gradient Descent(924/999): loss=190.04749087221956, w0=0.026190194350579434, w1=-0.44037117879581134\n",
      "Gradient Descent(925/999): loss=189.6568403967601, w0=0.026015485062685147, w1=-0.4398444532992856\n",
      "Gradient Descent(926/999): loss=189.2671824816962, w0=0.025841138545121357, w1=-0.4393186001947451\n",
      "Gradient Descent(927/999): loss=188.87851378497336, w0=0.025667154044282575, w1=-0.43879361753104024\n",
      "Gradient Descent(928/999): loss=188.49083097974633, w0=0.025493530808145936, w1=-0.4382695033623639\n",
      "Gradient Descent(929/999): loss=188.10413075429452, w0=0.025320268086268142, w1=-0.43774625574823556\n",
      "Gradient Descent(930/999): loss=187.71840981193833, w0=0.02514736512978239, w1=-0.4372238727534849\n",
      "Gradient Descent(931/999): loss=187.33366487095535, w0=0.024974821191395328, w1=-0.4367023524482362\n",
      "Gradient Descent(932/999): loss=186.9498926644986, w0=0.02480263552538399, w1=-0.43618169290789194\n",
      "Gradient Descent(933/999): loss=186.5670899405122, w0=0.02463080738759276, w1=-0.43566189221311724\n",
      "Gradient Descent(934/999): loss=186.18525346165134, w0=0.024459336035430327, w1=-0.4351429484498238\n",
      "Gradient Descent(935/999): loss=185.80438000519996, w0=0.024288220727866648, w1=-0.43462485970915415\n",
      "Gradient Descent(936/999): loss=185.42446636298936, w0=0.024117460725429918, w1=-0.43410762408746584\n",
      "Gradient Descent(937/999): loss=185.0455093413193, w0=0.02394705529020354, w1=-0.4335912396863158\n",
      "Gradient Descent(938/999): loss=184.6675057608767, w0=0.023777003685823105, w1=-0.4330757046124446\n",
      "Gradient Descent(939/999): loss=184.2904524566568, w0=0.023607305177473375, w1=-0.43256101697776084\n",
      "Gradient Descent(940/999): loss=183.914346277884, w0=0.02343795903188527, w1=-0.4320471748993256\n",
      "Gradient Descent(941/999): loss=183.53918408793345, w0=0.02326896451733286, w1=-0.4315341764993369\n",
      "Gradient Descent(942/999): loss=183.1649627642532, w0=0.023100320903630362, w1=-0.43102201990511424\n",
      "Gradient Descent(943/999): loss=182.79167919828646, w0=0.022932027462129145, w1=-0.43051070324908314\n",
      "Gradient Descent(944/999): loss=182.41933029539464, w0=0.022764083465714725, w1=-0.4300002246687598\n",
      "Gradient Descent(945/999): loss=182.04791297478093, w0=0.022596488188803793, w1=-0.42949058230673576\n",
      "Gradient Descent(946/999): loss=181.67742416941394, w0=0.022429240907341223, w1=-0.42898177431066253\n",
      "Gradient Descent(947/999): loss=181.30786082595168, w0=0.022262340898797084, w1=-0.42847379883323655\n",
      "Gradient Descent(948/999): loss=180.93921990466774, w0=0.02209578744216368, w1=-0.42796665403218376\n",
      "Gradient Descent(949/999): loss=180.57149837937482, w0=0.021929579817952575, w1=-0.4274603380702445\n",
      "Gradient Descent(950/999): loss=180.20469323735142, w0=0.021763717308191627, w1=-0.42695484911515874\n",
      "Gradient Descent(951/999): loss=179.8388014792681, w0=0.021598199196422023, w1=-0.4264501853396505\n",
      "Gradient Descent(952/999): loss=179.4738201191137, w0=0.02143302476769533, w1=-0.4259463449214133\n",
      "Gradient Descent(953/999): loss=179.10974618412232, w0=0.021268193308570542, w1=-0.42544332604309487\n",
      "Gradient Descent(954/999): loss=178.74657671470132, w0=0.021103704107111133, w1=-0.4249411268922825\n",
      "Gradient Descent(955/999): loss=178.38430876435862, w0=0.020939556452882112, w1=-0.42443974566148795\n",
      "Gradient Descent(956/999): loss=178.02293939963204, w0=0.02077574963694709, w1=-0.4239391805481329\n",
      "Gradient Descent(957/999): loss=177.66246570001647, w0=0.020612282951865346, w1=-0.42343942975453386\n",
      "Gradient Descent(958/999): loss=177.30288475789513, w0=0.0204491556916889, w1=-0.42294049148788776\n",
      "Gradient Descent(959/999): loss=176.94419367846763, w0=0.020286367151959587, w1=-0.422442363960257\n",
      "Gradient Descent(960/999): loss=176.5863895796809, w0=0.02012391662970614, w1=-0.4219450453885549\n",
      "Gradient Descent(961/999): loss=176.22946959215957, w0=0.01996180342344128, w1=-0.42144853399453125\n",
      "Gradient Descent(962/999): loss=175.87343085913665, w0=0.01980002683315879, w1=-0.4209528280047576\n",
      "Gradient Descent(963/999): loss=175.51827053638482, w0=0.019638586160330634, w1=-0.4204579256506128\n",
      "Gradient Descent(964/999): loss=175.16398579214908, w0=0.01947748070790404, w1=-0.41996382516826863\n",
      "Gradient Descent(965/999): loss=174.81057380707796, w0=0.019316709780298605, w1=-0.41947052479867525\n",
      "Gradient Descent(966/999): loss=174.45803177415584, w0=0.019156272683403416, w1=-0.41897802278754687\n",
      "Gradient Descent(967/999): loss=174.10635689863773, w0=0.01899616872457414, w1=-0.4184863173853476\n",
      "Gradient Descent(968/999): loss=173.75554639798045, w0=0.01883639721263017, w1=-0.4179954068472769\n",
      "Gradient Descent(969/999): loss=173.40559750177806, w0=0.018676957457851726, w1=-0.4175052894332556\n",
      "Gradient Descent(970/999): loss=173.05650745169564, w0=0.01851784877197699, w1=-0.4170159634079115\n",
      "Gradient Descent(971/999): loss=172.70827350140283, w0=0.01835907046819923, w1=-0.4165274270405655\n",
      "Gradient Descent(972/999): loss=172.3608929165106, w0=0.018200621861163956, w1=-0.4160396786052171\n",
      "Gradient Descent(973/999): loss=172.01436297450564, w0=0.01804250226696603, w1=-0.41555271638053076\n",
      "Gradient Descent(974/999): loss=171.6686809646861, w0=0.01788471100314684, w1=-0.4150665386498216\n",
      "Gradient Descent(975/999): loss=171.32384418809798, w0=0.01772724738869143, w1=-0.41458114370104165\n",
      "Gradient Descent(976/999): loss=170.97984995747169, w0=0.01757011074402566, w1=-0.41409652982676565\n",
      "Gradient Descent(977/999): loss=170.63669559715848, w0=0.017413300391013364, w1=-0.4136126953241775\n",
      "Gradient Descent(978/999): loss=170.29437844306912, w0=0.017256815652953512, w1=-0.41312963849505613\n",
      "Gradient Descent(979/999): loss=169.95289584261005, w0=0.017100655854577384, w1=-0.41264735764576194\n",
      "Gradient Descent(980/999): loss=169.6122451546218, w0=0.01694482032204573, w1=-0.41216585108722287\n",
      "Gradient Descent(981/999): loss=169.2724237493185, w0=0.016789308382945952, w1=-0.4116851171349208\n",
      "Gradient Descent(982/999): loss=168.93342900822503, w0=0.016634119366289286, w1=-0.4112051541088778\n",
      "Gradient Descent(983/999): loss=168.59525832411782, w0=0.016479252602507984, w1=-0.4107259603336426\n",
      "Gradient Descent(984/999): loss=168.25790910096288, w0=0.016324707423452503, w1=-0.41024753413827697\n",
      "Gradient Descent(985/999): loss=167.92137875385683, w0=0.016170483162388706, w1=-0.4097698738563421\n",
      "Gradient Descent(986/999): loss=167.58566470896685, w0=0.016016579153995052, w1=-0.4092929778258853\n",
      "Gradient Descent(987/999): loss=167.2507644034711, w0=0.015862994734359797, w1=-0.4088168443894263\n",
      "Gradient Descent(988/999): loss=166.91667528550002, w0=0.01570972924097821, w1=-0.40834147189394393\n",
      "Gradient Descent(989/999): loss=166.58339481407683, w0=0.015556782012749775, w1=-0.407866858690863\n",
      "Gradient Descent(990/999): loss=166.25092045906055, w0=0.01540415238997542, w1=-0.40739300313604054\n",
      "Gradient Descent(991/999): loss=165.91924970108718, w0=0.015251839714354724, w1=-0.40691990358975283\n",
      "Gradient Descent(992/999): loss=165.588380031512, w0=0.015099843328983144, w1=-0.406447558416682\n",
      "Gradient Descent(993/999): loss=165.25830895235305, w0=0.014948162578349253, w1=-0.4059759659859029\n",
      "Gradient Descent(994/999): loss=164.9290339762329, w0=0.014796796808331961, w1=-0.4055051246708699\n",
      "Gradient Descent(995/999): loss=164.6005526263237, w0=0.014645745366197763, w1=-0.4050350328494038\n",
      "Gradient Descent(996/999): loss=164.27286243628953, w0=0.014495007600597975, w1=-0.40456568890367867\n",
      "Gradient Descent(997/999): loss=163.9459609502313, w0=0.014344582861565983, w1=-0.404097091220209\n",
      "Gradient Descent(998/999): loss=163.61984572263108, w0=0.014194470500514495, w1=-0.40362923818983637\n",
      "Gradient Descent(999/999): loss=163.2945143182962, w0=0.014044669870232793, w1=-0.40316212820771685\n"
     ]
    }
   ],
   "source": [
    "gamma=0.000002\n",
    "max_iters=1000\n",
    "final_w, ws, losses=least_squares_GD(y,tX,gamma,max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights=sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.31184728e-03,  -1.76989211e-02,   8.25828393e-03,\n",
       "         1.00478093e+00,  -7.21505116e-03,  -3.64544091e-01,\n",
       "        -1.91313618e+00,  -1.23041263e-01,   3.41408530e-01,\n",
       "         3.15214012e-01,   7.57535507e-01,   4.08582140e-01,\n",
       "         7.33652432e-01,   6.43396186e-01,  -1.85351865e-03,\n",
       "         8.22319557e-01,   1.07755902e-02,  -2.99934094e-01,\n",
       "         3.56013136e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test = np.delete(tX_test, del_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = least_squares(y,tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/op0.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
