{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, tX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "6.320208\n"
     ]
    }
   ],
   "source": [
    "# For entries with missing data, the value -999 is filled, therefore we try to figure out ...\n",
    "# ... how much of the data is missing\n",
    "\n",
    "count_miss_instances=np.zeros((len(y),1))\n",
    "for id in ids:\n",
    "    count_miss_instances[id-100000]=sum(tX[id-100000] == -999.0)\n",
    "print(np.median(count_miss_instances))\n",
    "print(np.mean(count_miss_instances))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above, for every instance on an average about 6 field/attribute values are missing, we further perform a feature-wise check for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  38114.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [ 177457.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [      0.]\n",
      " [  99913.]\n",
      " [  99913.]\n",
      " [  99913.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [ 177457.]\n",
      " [      0.]]\n"
     ]
    }
   ],
   "source": [
    "count_miss_features=np.zeros((tX.shape[1],1))\n",
    "for d in range(tX.shape[1]):\n",
    "    count_miss_features[d]=sum(tX[:,d] == -999.0)\n",
    "print(count_miss_features)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we realize that only some (although not few) of the features have missing values. Since the number of instances where these features have missing values is quite a large fraction of the data, we decide to drop these features from our data for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n",
      "[  0.   4.   5.   6.  12.  23.  24.  25.  26.  27.  28.]\n",
      "(250000, 19)\n"
     ]
    }
   ],
   "source": [
    "#Counting the number of features\n",
    "print(sum(count_miss_features > 0))\n",
    "\n",
    "count_miss_features=np.zeros((tX.shape[1],1))\n",
    "del_features=[]\n",
    "\n",
    "# We create an array del_features (since we plan to drop these features) ...\n",
    "# ... to store the index of the attributes with missing values \n",
    "for d in range(tX.shape[1]):\n",
    "    count_miss_features[d]=sum(tX[:,d] == -999.0)\n",
    "    if count_miss_features[d]>0:\n",
    "            del_features=np.r_[del_features,d]\n",
    "print(del_features)   \n",
    "    \n",
    "# The features having indices in del_features computed above are now dropped from the data ... \n",
    "# ... thus reducing the tX matrix to 19 columns (deleting 11)\n",
    "\n",
    "tX_with_missing = tX # Let's keep a copy of the old data, before cleaning it\n",
    "tX = np.delete(tX, del_features, axis=1)\n",
    "print(tX.shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying to find outliers\n",
    "#plt.scatter(tX1[:,4],tX1[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Linear regression using gradient descent: least_squares_GD (y, tx, gamma, max_iters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "\n",
    "def least_squares_GD(y,tx,gamma,max_iters):\n",
    "    \n",
    "    initial_w = np.random.randn(tx.shape[1])\n",
    "    losses, ws = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    final_w = ws[-1][:]\n",
    "    \n",
    "    return final_w, ws, losses\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    e=y-np.dot(tx,w)\n",
    "    L= ( 1/(2*len(y)) )*np.dot(e.T,e) # Least squares error - assuming the (1/2N)*(e.T*e) form\n",
    "    return L\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    e=y-np.dot(tx,w)\n",
    "    grad_L = (-1/len(y))*np.dot(tx.T,e) #Using the expression gradient of Loss = (-1/N)*(X.T*e)\n",
    "    return grad_L\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # Compute Loss and Gradient\n",
    "        L = compute_loss(y, tx, w)\n",
    "        grad_L = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma*grad_L\n",
    "        \n",
    "        loss = L\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=104277.74827748955, w0=0.5331945156671677, w1=-1.892826192761536\n",
      "Gradient Descent(1/999): loss=59693.89344263162, w0=0.5561225327707616, w1=-1.8445377386462873\n",
      "Gradient Descent(2/999): loss=34962.69866796885, w0=0.5724560372222726, w1=-1.8082358119000954\n",
      "Gradient Descent(3/999): loss=21238.2742435524, w0=0.5838865404996956, w1=-1.780852099878007\n",
      "Gradient Descent(4/999): loss=13616.331494764829, w0=0.5916734254132643, w1=-1.7601021000638324\n",
      "Gradient Descent(5/999): loss=9377.860473887842, w0=0.5967543176350527, w1=-1.7442849249245123\n",
      "Gradient Descent(6/999): loss=7015.400359953048, w0=0.5998272671915122, w1=-1.7321342386790617\n",
      "Gradient Descent(7/999): loss=5693.190177128206, w0=0.6014119399425585, w1=-1.722707266368582\n",
      "Gradient Descent(8/999): loss=4947.868816526901, w0=0.6018951801112692, w1=-1.7153021511724786\n",
      "Gradient Descent(9/999): loss=4522.538948754126, w0=0.6015649356577523, w1=-1.7093964195389204\n",
      "Gradient Descent(10/999): loss=4274.769738633382, w0=0.6006355187459049, w1=-1.7046011629778388\n",
      "Gradient Descent(11/999): loss=4125.592263749883, w0=0.5992664144091104, w1=-1.7006269223199588\n",
      "Gradient Descent(12/999): loss=4031.2218601152426, w0=0.5975762852710316, w1=-1.6972582855121336\n",
      "Gradient Descent(13/999): loss=3967.380593455663, w0=0.5956533992983709, w1=-1.694334973422435\n",
      "Gradient Descent(14/999): loss=3920.6054160527083, w0=0.5935633941800392, w1=-1.6917377565506582\n",
      "Gradient Descent(15/999): loss=3883.4292852105036, w0=0.5913550585858235, w1=-1.689377968781274\n",
      "Gradient Descent(16/999): loss=3851.709505764987, w0=0.5890646368140563, w1=-1.6871896994570714\n",
      "Gradient Descent(17/999): loss=3823.14652363389, w0=0.5867190339700841, w1=-1.6851239797026498\n",
      "Gradient Descent(18/999): loss=3796.4627221020305, w0=0.5843382024914646, w1=-1.6831444536456093\n",
      "Gradient Descent(19/999): loss=3770.947132897016, w0=0.5819369191125622, w1=-1.6812241552770293\n",
      "Gradient Descent(20/999): loss=3746.203015592945, w0=0.5795261079568028, w1=-1.6793431085593045\n",
      "Gradient Descent(21/999): loss=3722.007910106264, w0=0.5771138256804778, w1=-1.677486540515191\n",
      "Gradient Descent(22/999): loss=3698.2360459017523, w0=0.574705994983833, w1=-1.6756435507360474\n",
      "Gradient Descent(23/999): loss=3674.815322685431, w0=0.5723069507592595, w1=-1.6738061207347923\n",
      "Gradient Descent(24/999): loss=3651.7034580672216, w0=0.5699198467312271, w1=-1.6719683763434203\n",
      "Gradient Descent(25/999): loss=3628.8747617124627, w0=0.5675469582200383, w1=-1.670126038524572\n",
      "Gradient Descent(26/999): loss=3606.3128010189494, w0=0.565189907560678, w1=-1.6682760144739426\n",
      "Gradient Descent(27/999): loss=3584.0063331882166, w0=0.5628498319316714, w1=-1.666416093181488\n",
      "Gradient Descent(28/999): loss=3561.9470482801808, w0=0.5605275083032556, w1=-1.664544718771254\n",
      "Gradient Descent(29/999): loss=3540.128316349649, w0=0.5582234464572573, w1=-1.6626608217540528\n",
      "Gradient Descent(30/999): loss=3518.5444913057813, w0=0.5559379582337144, w1=-1.6607636934011345\n",
      "Gradient Descent(31/999): loss=3497.1905234712294, w0=0.5536712090763953, w1=-1.6588528922249985\n",
      "Gradient Descent(32/999): loss=3476.061743331997, w0=0.5514232563984773, w1=-1.6569281743665452\n",
      "Gradient Descent(33/999): loss=3455.1537402399517, w0=0.5491940781348645, w1=-1.6549894417823372\n",
      "Gradient Descent(34/999): loss=3434.4622937992253, w0=0.5469835939877904, w1=-1.6530367036853437\n",
      "Gradient Descent(35/999): loss=3413.983334500998, w0=0.5447916812321233, w1=-1.6510700478537963\n",
      "Gradient Descent(36/999): loss=3393.7129206124514, w0=0.5426181864700887, w1=-1.649089619287447\n",
      "Gradient Descent(37/999): loss=3373.6472241146803, w0=0.5404629343701708, w1=-1.647095604334343\n",
      "Gradient Descent(38/999): loss=3353.7825216937267, w0=0.5383257341606646, w1=-1.6450882188906077\n",
      "Gradient Descent(39/999): loss=3334.1151885683707, w0=0.5362063844515621, w1=-1.6430676996326625\n",
      "Gradient Descent(40/999): loss=3314.6416939248465, w0=0.5341046768119245, w1=-1.6410342975070984\n",
      "Gradient Descent(41/999): loss=3295.358597275668, w0=0.5320203984207957, w1=-1.6389882729013021\n",
      "Gradient Descent(42/999): loss=3276.262545363, w0=0.5299533340284708, w1=-1.636929892065285\n",
      "Gradient Descent(43/999): loss=3257.350269395229, w0=0.5279032674044483, w1=-1.6348594244648869\n",
      "Gradient Descent(44/999): loss=3238.618582498636, w0=0.5258699824033545, w1=-1.6327771408282088\n",
      "Gradient Descent(45/999): loss=3220.064377317784, w0=0.5238532637465949, w1=-1.6306833117079664\n",
      "Gradient Descent(46/999): loss=3201.6846237270115, w0=0.521852897592515, w1=-1.6285782064277392\n",
      "Gradient Descent(47/999): loss=3183.4763666312665, w0=0.519868671949263, w1=-1.626462092313818\n",
      "Gradient Descent(48/999): loss=3165.436723843495, w0=0.5179003769707027, w1=-1.6243352341394626\n",
      "Gradient Descent(49/999): loss=3147.562884030595, w0=0.5159478051654148, w1=-1.622197893727078\n",
      "Gradient Descent(50/999): loss=3129.8521047228746, w0=0.5140107515411544, w1=-1.6200503296677384\n",
      "Gradient Descent(51/999): loss=3112.3017103833245, w0=0.5120890137014114, w1=-1.6178927971278538\n",
      "Gradient Descent(52/999): loss=3094.9090905340613, w0=0.510182391906473, w1=-1.6157255477204935\n",
      "Gradient Descent(53/999): loss=3077.671697937717, w0=0.5082906891082118, w1=-1.613548829424626\n",
      "Gradient Descent(54/999): loss=3060.5870468318117, w0=0.5064137109654702, w1=-1.6113628865398135\n",
      "Gradient Descent(55/999): loss=3043.652711214519, w0=0.5045512658451494, w1=-1.609167959667087\n",
      "Gradient Descent(56/999): loss=3026.8663231801147, w0=0.5027031648128112, w1=-1.6069642857090973\n",
      "Gradient Descent(57/999): loss=3010.225571302671, w0=0.500869221615617, w1=-1.6047520978844094\n",
      "Gradient Descent(58/999): loss=2993.7281990665433, w0=0.4990492526597139, w1=-1.602531625752112\n",
      "Gradient Descent(59/999): loss=2977.372003342264, w0=0.49724307698362974, w1=-1.6003030952439061\n",
      "Gradient Descent(60/999): loss=2961.1548329065217, w0=0.4954505162288418, w1=-1.598066728701556\n",
      "Gradient Descent(61/999): loss=2945.0745870049104, w0=0.49367139460838244, w1=-1.595822744918133\n",
      "Gradient Descent(62/999): loss=2929.129213956226, w0=0.49190553887412336, w1=-1.593571359181885\n",
      "Gradient Descent(63/999): loss=2913.316709797076, w0=0.4901527782832137, w1=-1.5913127833218663\n",
      "Gradient Descent(64/999): loss=2897.635116965642, w0=0.4884129445640237, w1=-1.5890472257546873\n",
      "Gradient Descent(65/999): loss=2882.0825230234113, w0=0.4866858718818541, w1=-1.5867748915319064\n",
      "Gradient Descent(66/999): loss=2866.65705941384, w0=0.4849713968046034, w1=-1.5844959823877172\n",
      "Gradient Descent(67/999): loss=2851.356900256794, w0=0.4832693582685327, w1=-1.5822106967866676\n",
      "Gradient Descent(68/999): loss=2836.1802611777866, w0=0.4815795975442319, w1=-1.579919229971226\n",
      "Gradient Descent(69/999): loss=2821.1253981709433, w0=0.4799019582028619, w1=-1.5776217740090517\n",
      "Gradient Descent(70/999): loss=2806.1906064947552, w0=0.4782362860827258, w1=-1.5753185178398734\n",
      "Gradient Descent(71/999): loss=2791.3742195996424, w0=0.4765824292562083, w1=-1.5730096473218982\n",
      "Gradient Descent(72/999): loss=2776.674608086391, w0=0.4749402379971096, w1=-1.5706953452777046\n",
      "Gradient Descent(73/999): loss=2762.090178694595, w0=0.47330956474839136, w1=-1.5683757915395813\n",
      "Gradient Descent(74/999): loss=2747.6193733201803, w0=0.47169026409034714, w1=-1.5660511629942877\n",
      "Gradient Descent(75/999): loss=2733.260668061218, w0=0.470082192709204, w1=-1.5637216336272224\n",
      "Gradient Descent(76/999): loss=2719.0125722911616, w0=0.4684852093661584, w1=-1.561387374565988\n",
      "Gradient Descent(77/999): loss=2704.8736277587172, w0=0.4668991748668483, w1=-1.559048554123351\n",
      "Gradient Descent(78/999): loss=2690.8424077135774, w0=0.4653239520312592, w1=-1.5567053378395912\n",
      "Gradient Descent(79/999): loss=2676.917516057271, w0=0.463759405664063, w1=-1.5543578885242482\n",
      "Gradient Descent(80/999): loss=2663.097586518375, w0=0.46220540252538544, w1=-1.5520063662972632\n",
      "Gradient Descent(81/999): loss=2649.381281851394, w0=0.46066181130199857, w1=-1.5496509286295232\n",
      "Gradient Descent(82/999): loss=2635.767293058632, w0=0.4591285025789334, w1=-1.5472917303828142\n",
      "Gradient Descent(83/999): loss=2622.2543386343305, w0=0.45760534881150783, w1=-1.544928923849187\n",
      "Gradient Descent(84/999): loss=2608.8411638305006, w0=0.4560922242977649, w1=-1.5425626587897463\n",
      "Gradient Descent(85/999): loss=2595.5265399437667, w0=0.4545890051513157, w1=-1.5401930824728671\n",
      "Gradient Descent(86/999): loss=2582.3092636226197, w0=0.4530955692745812, w1=-1.5378203397118482\n",
      "Gradient Descent(87/999): loss=2569.1881561944774, w0=0.4516117963324289, w1=-1.5354445729020083\n",
      "Gradient Descent(88/999): loss=2556.1620630120233, w0=0.45013756772619656, w1=-1.533065922057235\n",
      "Gradient Descent(89/999): loss=2543.229852818178, w0=0.4486727665680993, w1=-1.5306845248459917\n",
      "Gradient Descent(90/999): loss=2530.390417129238, w0=0.44721727765601416, w1=-1.5283005166267924\n",
      "Gradient Descent(91/999): loss=2517.642669635621, w0=0.4457709874486361, w1=-1.5259140304831507\n",
      "Gradient Descent(92/999): loss=2504.985545619684, w0=0.44433378404100043, w1=-1.52352519725801\n",
      "Gradient Descent(93/999): loss=2492.418001390185, w0=0.44290555714036683, w1=-1.521134145587665\n",
      "Gradient Descent(94/999): loss=2479.9390137328046, w0=0.4414861980424584, w1=-1.51874100193518\n",
      "Gradient Descent(95/999): loss=2467.547579376364, w0=0.44007559960805204, w1=-1.5163458906233096\n",
      "Gradient Descent(96/999): loss=2455.242714474208, w0=0.43867365623991367, w1=-1.5139489338669345\n",
      "Gradient Descent(97/999): loss=2443.023454100332, w0=0.43728026386007446, w1=-1.5115502518050137\n",
      "Gradient Descent(98/999): loss=2430.88885175986, w0=0.43589531988744185, w1=-1.509149962532063\n",
      "Gradient Descent(99/999): loss=2418.8379789133924, w0=0.4345187232157413, w1=-1.5067481821291684\n",
      "Gradient Descent(100/999): loss=2406.8699245148837, w0=0.43315037419178354, w1=-1.5043450246945365\n",
      "Gradient Descent(101/999): loss=2394.983794562607, w0=0.4317901745940526, w1=-1.5019406023735937\n",
      "Gradient Descent(102/999): loss=2383.17871166285, w0=0.4304380276116098, w1=-1.499535025388638\n",
      "Gradient Descent(103/999): loss=2371.453814605987, w0=0.4290938378233093, w1=-1.4971284020680506\n",
      "Gradient Descent(104/999): loss=2359.8082579545107, w0=0.4277575111773202, w1=-1.4947208388750743\n",
      "Gradient Descent(105/999): loss=2348.241211642757, w0=0.42642895497095107, w1=-1.4923124404361643\n",
      "Gradient Descent(106/999): loss=2336.751860587909, w0=0.4251080778307723, w1=-1.4899033095689178\n",
      "Gradient Descent(107/999): loss=2325.339404311996, w0=0.4237947896930318, w1=-1.4874935473095907\n",
      "Gradient Descent(108/999): loss=2314.0030565745633, w0=0.42248900178435955, w1=-1.4850832529402034\n",
      "Gradient Descent(109/999): loss=2302.742045015687, w0=0.4211906266027575, w1=-1.4826725240152456\n",
      "Gradient Descent(110/999): loss=2291.555610809033, w0=0.4198995778988697, w1=-1.4802614563879843\n",
      "Gradient Descent(111/999): loss=2280.443008324714, w0=0.41861577065752886, w1=-1.477850144236381\n",
      "Gradient Descent(112/999): loss=2269.4035048015803, w0=0.4173391210795756, w1=-1.4754386800886232\n",
      "Gradient Descent(113/999): loss=2258.4363800287533, w0=0.41606954656394585, w1=-1.4730271548482787\n",
      "Gradient Descent(114/999): loss=2247.540926036068, w0=0.41480696569002273, w1=-1.4706156578190734\n",
      "Gradient Descent(115/999): loss=2236.7164467932166, w0=0.413551298200249, w1=-1.4682042767293038\n",
      "Gradient Descent(116/999): loss=2225.9622579173024, w0=0.4123024649829961, w1=-1.4657930977558848\n",
      "Gradient Descent(117/999): loss=2215.2776863885847, w0=0.4110603880556861, w1=-1.4633822055480412\n",
      "Gradient Descent(118/999): loss=2204.662070274165, w0=0.4098249905481629, w1=-1.4609716832506467\n",
      "Gradient Descent(119/999): loss=2194.1147584593978, w0=0.4085961966863087, w1=-1.4585616125272163\n",
      "Gradient Descent(120/999): loss=2183.6351103867796, w0=0.40737393177590253, w1=-1.4561520735825568\n",
      "Gradient Descent(121/999): loss=2173.222495802128, w0=0.40615812218671704, w1=-1.4537431451850809\n",
      "Gradient Descent(122/999): loss=2162.8762945078106, w0=0.40494869533684985, w1=-1.451334904688789\n",
      "Gradient Descent(123/999): loss=2152.5958961228484, w0=0.4037455796772866, w1=-1.4489274280549251\n",
      "Gradient Descent(124/999): loss=2142.380699849675, w0=0.40254870467669135, w1=-1.4465207898733101\n",
      "Gradient Descent(125/999): loss=2132.2301142473593, w0=0.4013580008064222, w1=-1.4441150633833575\n",
      "Gradient Descent(126/999): loss=2122.1435570111225, w0=0.40017339952576747, w1=-1.4417103204947779\n",
      "Gradient Descent(127/999): loss=2112.12045475795, w0=0.39899483326740015, w1=-1.439306631807974\n",
      "Gradient Descent(128/999): loss=2102.160242818128, w0=0.39782223542304723, w1=-1.4369040666341335\n",
      "Gradient Descent(129/999): loss=2092.2623650325286, w0=0.3966555403293702, w1=-1.4345026930150229\n",
      "Gradient Descent(130/999): loss=2082.4262735554926, w0=0.3954946832540544, w1=-1.4321025777424856\n",
      "Gradient Descent(131/999): loss=2072.6514286631327, w0=0.3943396003821034, w1=-1.429703786377652\n",
      "Gradient Descent(132/999): loss=2062.937298566908, w0=0.39319022880233606, w1=-1.4273063832698618\n",
      "Gradient Descent(133/999): loss=2053.283359232312, w0=0.3920465064940826, w1=-1.4249104315753056\n",
      "Gradient Descent(134/999): loss=2043.6890942025361, w0=0.3909083723140775, w1=-1.4225159932753897\n",
      "Gradient Descent(135/999): loss=2034.1539944269584, w0=0.3897757659835452, w1=-1.4201231291948264\n",
      "Gradient Descent(136/999): loss=2024.6775580943206, w0=0.3886486280754775, w1=-1.4177318990194552\n",
      "Gradient Descent(137/999): loss=2015.2592904704527, w0=0.38752690000209783, w1=-1.4153423613137992\n",
      "Gradient Descent(138/999): loss=2005.8987037404265, w0=0.38641052400251114, w1=-1.4129545735383602\n",
      "Gradient Descent(139/999): loss=1996.5953168550027, w0=0.385299443130536, w1=-1.4105685920666555\n",
      "Gradient Descent(140/999): loss=1987.3486553812427, w0=0.3841936012427163, w1=-1.4081844722020014\n",
      "Gradient Descent(141/999): loss=1978.1582513571782, w0=0.38309294298651, w1=-1.4058022681940465\n",
      "Gradient Descent(142/999): loss=1969.023643150411, w0=0.38199741378865226, w1=-1.40342203325506\n",
      "Gradient Descent(143/999): loss=1959.9443753205408, w0=0.38090695984369044, w1=-1.4010438195759747\n",
      "Gradient Descent(144/999): loss=1950.9199984852926, w0=0.3798215281026882, w1=-1.3986676783421939\n",
      "Gradient Descent(145/999): loss=1941.9500691902626, w0=0.3787410662620967, w1=-1.3962936597491604\n",
      "Gradient Descent(146/999): loss=1933.0341497821594, w0=0.37766552275278964, w1=-1.3939218130176956\n",
      "Gradient Descent(147/999): loss=1924.1718082854443, w0=0.37659484672926086, w1=-1.3915521864091074\n",
      "Gradient Descent(148/999): loss=1915.3626182822802, w0=0.3755289880589808, w1=-1.3891848272400755\n",
      "Gradient Descent(149/999): loss=1906.6061587956913, w0=0.3744678973119104, w1=-1.3868197818973134\n",
      "Gradient Descent(150/999): loss=1897.9020141758283, w0=0.3734115257501699, w1=-1.3844570958520115\n",
      "Gradient Descent(151/999): loss=1889.2497739892765, w0=0.3723598253178601, w1=-1.3820968136740661\n",
      "Gradient Descent(152/999): loss=1880.6490329112996, w0=0.3713127486310339, w1=-1.3797389790460945\n",
      "Gradient Descent(153/999): loss=1872.0993906209367, w0=0.37027024896781596, w1=-1.377383634777243\n",
      "Gradient Descent(154/999): loss=1863.6004516988748, w0=0.36923228025866817, w1=-1.375030822816787\n",
      "Gradient Descent(155/999): loss=1855.1518255280318, w0=0.3681987970767989, w1=-1.372680584267529\n",
      "Gradient Descent(156/999): loss=1846.7531261967354, w0=0.36716975462871415, w1=-1.3703329593989977\n",
      "Gradient Descent(157/999): loss=1838.4039724044715, w0=0.3661451087449076, w1=-1.3679879876604484\n",
      "Gradient Descent(158/999): loss=1830.1039873700815, w0=0.36512481587068873, w1=-1.3656457076936699\n",
      "Gradient Descent(159/999): loss=1821.852798742382, w0=0.3641088330571462, w1=-1.3633061573456011\n",
      "Gradient Descent(160/999): loss=1813.6500385131008, w0=0.36309711795224436, w1=-1.360969373680758\n",
      "Gradient Descent(161/999): loss=1805.4953429320892, w0=0.36208962879205187, w1=-1.3586353929934758\n",
      "Gradient Descent(162/999): loss=1797.3883524247383, w0=0.36108632439209914, w1=-1.3563042508199679\n",
      "Gradient Descent(163/999): loss=1789.328711511528, w0=0.3600871641388639, w1=-1.3539759819502053\n",
      "Gradient Descent(164/999): loss=1781.3160687296538, w0=0.3590921079813823, w1=-1.3516506204396177\n",
      "Gradient Descent(165/999): loss=1773.3500765566878, w0=0.358101116422984, w1=-1.3493281996206203\n",
      "Gradient Descent(166/999): loss=1765.4303913361825, w0=0.35711415051314904, w1=-1.3470087521139686\n",
      "Gradient Descent(167/999): loss=1757.556673205202, w0=0.3561311718394854, w1=-1.3446923098399426\n",
      "Gradient Descent(168/999): loss=1749.7285860236843, w0=0.3551521425198247, w1=-1.3423789040293646\n",
      "Gradient Descent(169/999): loss=1741.9457973056278, w0=0.3541770251944346, w1=-1.3400685652344517\n",
      "Gradient Descent(170/999): loss=1734.2079781520038, w0=0.3532057830183466, w1=-1.3377613233395074\n",
      "Gradient Descent(171/999): loss=1726.5148031853878, w0=0.35223837965379656, w1=-1.3354572075714506\n",
      "Gradient Descent(172/999): loss=1718.8659504862374, w0=0.35127477926277745, w1=-1.3331562465101903\n",
      "Gradient Descent(173/999): loss=1711.261101530765, w0=0.35031494649970163, w1=-1.3308584680988427\n",
      "Gradient Descent(174/999): loss=1703.6999411303952, w0=0.3493588465041718, w1=-1.328563899653796\n",
      "Gradient Descent(175/999): loss=1696.1821573727068, w0=0.3484064448938585, w1=-1.3262725678746257\n",
      "Gradient Descent(176/999): loss=1688.7074415638751, w0=0.3474577077574829, w1=-1.3239844988538598\n",
      "Gradient Descent(177/999): loss=1681.2754881725361, w0=0.34651260164790326, w1=-1.3216997180865995\n",
      "Gradient Descent(178/999): loss=1673.8859947750373, w0=0.3455710935753033, w1=-1.3194182504799936\n",
      "Gradient Descent(179/999): loss=1666.5386620020568, w0=0.34463315100048153, w1=-1.3171401203625739\n",
      "Gradient Descent(180/999): loss=1659.2331934865144, w0=0.3436987418282394, w1=-1.3148653514934483\n",
      "Gradient Descent(181/999): loss=1651.9692958127875, w0=0.34276783440086706, w1=-1.3125939670713573\n",
      "Gradient Descent(182/999): loss=1644.7466784671462, w0=0.34184039749172557, w1=-1.310325989743595\n",
      "Gradient Descent(183/999): loss=1637.5650537894167, w0=0.34091640029892384, w1=-1.3080614416147953\n",
      "Gradient Descent(184/999): loss=1630.424136925791, w0=0.3399958124390888, w1=-1.3058003442555874\n",
      "Gradient Descent(185/999): loss=1623.3236457828102, w0=0.3390786039412276, w1=-1.303542718711122\n",
      "Gradient Descent(186/999): loss=1616.2633009824365, w0=0.33816474524068074, w1=-1.3012885855094671\n",
      "Gradient Descent(187/999): loss=1609.2428258182067, w0=0.33725420717316423, w1=-1.299037964669882\n",
      "Gradient Descent(188/999): loss=1602.2619462124371, w0=0.3363469609688998, w1=-1.2967908757109625\n",
      "Gradient Descent(189/999): loss=1595.3203906744504, w0=0.3354429782468319, w1=-1.294547337658668\n",
      "Gradient Descent(190/999): loss=1588.4178902597803, w0=0.33454223100893, w1=-1.2923073690542255\n",
      "Gradient Descent(191/999): loss=1581.5541785303653, w0=0.33364469163457516, w1=-1.2900709879619157\n",
      "Gradient Descent(192/999): loss=1574.7289915156487, w0=0.33275033287502953, w1=-1.2878382119767426\n",
      "Gradient Descent(193/999): loss=1567.9420676746138, w0=0.3318591278479873, w1=-1.285609058231988\n",
      "Gradient Descent(194/999): loss=1561.1931478586878, w0=0.3309710500322065, w1=-1.2833835434066512\n",
      "Gradient Descent(195/999): loss=1554.4819752755286, w0=0.33008607326221967, w1=-1.2811616837327786\n",
      "Gradient Descent(196/999): loss=1547.8082954536203, w0=0.32920417172312316, w1=-1.2789434950026826\n",
      "Gradient Descent(197/999): loss=1541.1718562077092, w0=0.328325319945443, w1=-1.276728992576051\n",
      "Gradient Descent(198/999): loss=1534.5724076050205, w0=0.3274494928000767, w1=-1.2745181913869512\n",
      "Gradient Descent(199/999): loss=1528.0097019322443, w0=0.32657666549330977, w1=-1.2723111059507282\n",
      "Gradient Descent(200/999): loss=1521.483493663275, w0=0.3257068135619061, w1=-1.2701077503707978\n",
      "Gradient Descent(201/999): loss=1514.9935394276818, w0=0.3248399128682705, w1=-1.2679081383453406\n",
      "Gradient Descent(202/999): loss=1508.5395979798802, w0=0.3239759395956829, w1=-1.2657122831738916\n",
      "Gradient Descent(203/999): loss=1502.121430169002, w0=0.3231148702436027, w1=-1.2635201977638335\n",
      "Gradient Descent(204/999): loss=1495.7387989094318, w0=0.32225668162304305, w1=-1.2613318946367906\n",
      "Gradient Descent(205/999): loss=1489.3914691519965, w0=0.32140135085201277, w1=-1.2591473859349276\n",
      "Gradient Descent(206/999): loss=1483.0792078557886, w0=0.3205488553510262, w1=-1.2569666834271518\n",
      "Gradient Descent(207/999): loss=1476.8017839606157, w0=0.31969917283867877, w1=-1.254789798515224\n",
      "Gradient Descent(208/999): loss=1470.558968360035, w0=0.3188522813272887, w1=-1.2526167422397763\n",
      "Gradient Descent(209/999): loss=1464.3505338749924, w0=0.31800815911860236, w1=-1.250447525286238\n",
      "Gradient Descent(210/999): loss=1458.176255228015, w0=0.31716678479956334, w1=-1.2482821579906747\n",
      "Gradient Descent(211/999): loss=1452.0359090179638, w0=0.31632813723814424, w1=-1.2461206503455373\n",
      "Gradient Descent(212/999): loss=1445.9292736953262, w0=0.3154921955792397, w1=-1.243963012005325\n",
      "Gradient Descent(213/999): loss=1439.8561295380302, w0=0.31465893924062033, w1=-1.2418092522921629\n",
      "Gradient Descent(214/999): loss=1433.8162586277751, w0=0.3138283479089463, w1=-1.239659380201295\n",
      "Gradient Descent(215/999): loss=1427.8094448268473, w0=0.3130004015358399, w1=-1.2375134044064948\n",
      "Gradient Descent(216/999): loss=1421.8354737554344, w0=0.31217508033401603, w1=-1.2353713332653946\n",
      "Gradient Descent(217/999): loss=1415.8941327693954, w0=0.31135236477346984, w1=-1.2332331748247327\n",
      "Gradient Descent(218/999): loss=1409.9852109385085, w0=0.31053223557772086, w1=-1.2310989368255227\n",
      "Gradient Descent(219/999): loss=1404.1084990251397, w0=0.30971467372011247, w1=-1.2289686267081443\n",
      "Gradient Descent(220/999): loss=1398.2637894633679, w0=0.30889966042016614, w1=-1.226842251617357\n",
      "Gradient Descent(221/999): loss=1392.450876338521, w0=0.30808717713998945, w1=-1.2247198184072383\n",
      "Gradient Descent(222/999): loss=1386.6695553671293, w0=0.3072772055807371, w1=-1.2226013336460468\n",
      "Gradient Descent(223/999): loss=1380.919623877277, w0=0.30646972767912445, w1=-1.220486803621012\n",
      "Gradient Descent(224/999): loss=1375.200880789347, w0=0.3056647256039924, w1=-1.218376234343051\n",
      "Gradient Descent(225/999): loss=1369.5131265971354, w0=0.3048621817529229, w1=-1.2162696315514157\n",
      "Gradient Descent(226/999): loss=1363.8561633493591, w0=0.30406207874890495, w1=-1.214167000718266\n",
      "Gradient Descent(227/999): loss=1358.229794631501, w0=0.3032643994370492, w1=-1.212068347053178\n",
      "Gradient Descent(228/999): loss=1352.6338255480136, w0=0.3024691268813516, w1=-1.20997367550758\n",
      "Gradient Descent(229/999): loss=1347.0680627048782, w0=0.3016762443615045, w1=-1.2078829907791249\n",
      "Gradient Descent(230/999): loss=1341.5323141924728, w0=0.30088573536975527, w1=-1.2057962973159924\n",
      "Gradient Descent(231/999): loss=1336.0263895687906, w0=0.30009758360781097, w1=-1.2037135993211296\n",
      "Gradient Descent(232/999): loss=1330.5500998429566, w0=0.29931177298378897, w1=-1.201634900756425\n",
      "Gradient Descent(233/999): loss=1325.1032574590563, w0=0.29852828760921246, w1=-1.1995602053468197\n",
      "Gradient Descent(234/999): loss=1319.6856762802795, w0=0.2977471117960505, w1=-1.1974895165843558\n",
      "Gradient Descent(235/999): loss=1314.2971715733343, w0=0.29696823005380174, w1=-1.195422837732164\n",
      "Gradient Descent(236/999): loss=1308.9375599931611, w0=0.2961916270866213, w1=-1.1933601718283915\n",
      "Gradient Descent(237/999): loss=1303.6066595679267, w0=0.29541728779048976, w1=-1.1913015216900666\n",
      "Gradient Descent(238/999): loss=1298.304289684275, w0=0.29464519725042476, w1=-1.189246889916909\n",
      "Gradient Descent(239/999): loss=1293.0302710728597, w0=0.29387534073773275, w1=-1.1871962788950792\n",
      "Gradient Descent(240/999): loss=1287.7844257941274, w0=0.29310770370730227, w1=-1.1851496908008714\n",
      "Gradient Descent(241/999): loss=1282.5665772243435, w0=0.2923422717949367, w1=-1.1831071276043514\n",
      "Gradient Descent(242/999): loss=1277.3765500418808, w0=0.2915790308147266, w1=-1.1810685910729375\n",
      "Gradient Descent(243/999): loss=1272.2141702137326, w0=0.2908179667564607, w1=-1.1790340827749275\n",
      "Gradient Descent(244/999): loss=1267.079264982264, w0=0.29005906578307566, w1=-1.177003604082972\n",
      "Gradient Descent(245/999): loss=1261.9716628521937, w0=0.2893023142281428, w1=-1.1749771561774955\n",
      "Gradient Descent(246/999): loss=1256.8911935777912, w0=0.2885476985933923, w1=-1.1729547400500628\n",
      "Gradient Descent(247/999): loss=1251.837688150297, w0=0.28779520554627425, w1=-1.1709363565066977\n",
      "Gradient Descent(248/999): loss=1246.8109787855483, w0=0.2870448219175553, w1=-1.1689220061711476\n",
      "Gradient Descent(249/999): loss=1241.8108989118132, w0=0.2862965346989511, w1=-1.1669116894881006\n",
      "Gradient Descent(250/999): loss=1236.8372831578283, w0=0.2855503310407934, w1=-1.164905406726352\n",
      "Gradient Descent(251/999): loss=1231.8899673410242, w0=0.28480619824973225, w1=-1.1629031579819233\n",
      "Gradient Descent(252/999): loss=1226.9687884559567, w0=0.2840641237864713, w1=-1.1609049431811334\n",
      "Gradient Descent(253/999): loss=1222.0735846629066, w0=0.28332409526353736, w1=-1.1589107620836216\n",
      "Gradient Descent(254/999): loss=1217.2041952766765, w0=0.2825861004430824, w1=-1.1569206142853268\n",
      "Gradient Descent(255/999): loss=1212.360460755563, w0=0.28185012723471836, w1=-1.1549344992214179\n",
      "Gradient Descent(256/999): loss=1207.542222690495, w0=0.2811161636933836, w1=-1.1529524161691826\n",
      "Gradient Descent(257/999): loss=1202.7493237943493, w0=0.28038419801724124, w1=-1.1509743642508696\n",
      "Gradient Descent(258/999): loss=1197.9816078914293, w0=0.27965421854560824, w1=-1.1490003424364879\n",
      "Gradient Descent(259/999): loss=1193.2389199071097, w0=0.27892621375691534, w1=-1.1470303495465632\n",
      "Gradient Descent(260/999): loss=1188.5211058576265, w0=0.2782001722666971, w1=-1.1450643842548525\n",
      "Gradient Descent(261/999): loss=1183.8280128400384, w0=0.27747608282561137, w1=-1.1431024450910159\n",
      "Gradient Descent(262/999): loss=1179.15948902233, w0=0.2767539343174885, w1=-1.1411445304432486\n",
      "Gradient Descent(263/999): loss=1174.5153836336617, w0=0.2760337157574089, w1=-1.1391906385608712\n",
      "Gradient Descent(264/999): loss=1169.8955469547661, w0=0.2753154162898094, w1=-1.1372407675568816\n",
      "Gradient Descent(265/999): loss=1165.2998303084842, w0=0.27459902518661705, w1=-1.1352949154104661\n",
      "Gradient Descent(266/999): loss=1160.7280860504384, w0=0.27388453184541134, w1=-1.133353079969473\n",
      "Gradient Descent(267/999): loss=1156.1801675598465, w0=0.27317192578761285, w1=-1.1314152589528477\n",
      "Gradient Descent(268/999): loss=1151.6559292304557, w0=0.2724611966566989, w1=-1.1294814499530301\n",
      "Gradient Descent(269/999): loss=1147.1552264616128, w0=0.27175233421644557, w1=-1.1275516504383154\n",
      "Gradient Descent(270/999): loss=1142.6779156494608, w0=0.2710453283491958, w1=-1.1256258577551783\n",
      "Gradient Descent(271/999): loss=1138.223854178254, w0=0.27034016905415303, w1=-1.1237040691305615\n",
      "Gradient Descent(272/999): loss=1133.792900411798, w0=0.26963684644569974, w1=-1.1217862816741282\n",
      "Gradient Descent(273/999): loss=1129.3849136850004, w0=0.26893535075174146, w1=-1.1198724923804806\n",
      "Gradient Descent(274/999): loss=1124.999754295544, w0=0.2682356723120746, w1=-1.117962698131344\n",
      "Gradient Descent(275/999): loss=1120.6372834956683, w0=0.267537801576779, w1=-1.1160568956977166\n",
      "Gradient Descent(276/999): loss=1116.297363484065, w0=0.2668417291046336, w1=-1.1141550817419867\n",
      "Gradient Descent(277/999): loss=1111.9798573978724, w0=0.2661474455615562, w1=-1.1122572528200165\n",
      "Gradient Descent(278/999): loss=1107.684629304786, w0=0.26545494171906553, w1=-1.1103634053831941\n",
      "Gradient Descent(279/999): loss=1103.411544195271, w0=0.2647642084527669, w1=-1.108473535780453\n",
      "Gradient Descent(280/999): loss=1099.16046797486, w0=0.2640752367408593, w1=-1.1065876402602612\n",
      "Gradient Descent(281/999): loss=1094.9312674565774, w0=0.26338801766266556, w1=-1.104705714972579\n",
      "Gradient Descent(282/999): loss=1090.7238103534335, w0=0.26270254239718316, w1=-1.1028277559707862\n",
      "Gradient Descent(283/999): loss=1086.537965271032, w0=0.2620188022216574, w1=-1.1009537592135792\n",
      "Gradient Descent(284/999): loss=1082.3736017002625, w0=0.261336788510175, w1=-1.0990837205668396\n",
      "Gradient Descent(285/999): loss=1078.2305900100837, w0=0.2606564927322789, w1=-1.0972176358054724\n",
      "Gradient Descent(286/999): loss=1074.1088014403988, w0=0.2599779064516033, w1=-1.0953555006152162\n",
      "Gradient Descent(287/999): loss=1070.0081080950158, w0=0.25930102132452876, w1=-1.0934973105944261\n",
      "Gradient Descent(288/999): loss=1065.9283829346978, w0=0.2586258290988576, w1=-1.0916430612558266\n",
      "Gradient Descent(289/999): loss=1061.8694997702885, w0=0.2579523216125086, w1=-1.0897927480282392\n",
      "Gradient Descent(290/999): loss=1057.8313332559308, w0=0.25728049079223064, w1=-1.0879463662582824\n",
      "Gradient Descent(291/999): loss=1053.813758882358, w0=0.25661032865233624, w1=-1.0861039112120443\n",
      "Gradient Descent(292/999): loss=1049.8166529702655, w0=0.25594182729345283, w1=-1.0842653780767304\n",
      "Gradient Descent(293/999): loss=1045.8398926637653, w0=0.25527497890129314, w1=-1.0824307619622848\n",
      "Gradient Descent(294/999): loss=1041.8833559239094, w0=0.2546097757454435, w1=-1.080600057902986\n",
      "Gradient Descent(295/999): loss=1037.9469215222907, w0=0.25394621017817015, w1=-1.0787732608590181\n",
      "Gradient Descent(296/999): loss=1034.0304690347182, w0=0.2532842746332428, w1=-1.076950365718017\n",
      "Gradient Descent(297/999): loss=1030.133878834965, w0=0.2526239616247764, w1=-1.0751313672965925\n",
      "Gradient Descent(298/999): loss=1026.257032088579, w0=0.251965263746089, w1=-1.0733162603418267\n",
      "Gradient Descent(299/999): loss=1022.3998107467759, w0=0.2513081736685774, w1=-1.0715050395327483\n",
      "Gradient Descent(300/999): loss=1018.5620975403871, w0=0.25065268414060876, w1=-1.0696976994817842\n",
      "Gradient Descent(301/999): loss=1014.7437759738814, w0=0.24999878798642894, w1=-1.0678942347361877\n",
      "Gradient Descent(302/999): loss=1010.9447303194489, w0=0.2493464781050868, w1=-1.0660946397794457\n",
      "Gradient Descent(303/999): loss=1007.164845611155, w0=0.24869574746937456, w1=-1.0642989090326613\n",
      "Gradient Descent(304/999): loss=1003.404007639147, w0=0.24804658912478356, w1=-1.0625070368559164\n",
      "Gradient Descent(305/999): loss=999.6621029439323, w0=0.24739899618847572, w1=-1.0607190175496124\n",
      "Gradient Descent(306/999): loss=995.9390188107109, w0=0.24675296184827, w1=-1.0589348453557896\n",
      "Gradient Descent(307/999): loss=992.2346432637756, w0=0.24610847936164396, w1=-1.0571545144594254\n",
      "Gradient Descent(308/999): loss=988.5488650609597, w0=0.24546554205474994, w1=-1.0553780189897126\n",
      "Gradient Descent(309/999): loss=984.8815736881481, w0=0.2448241433214459, w1=-1.0536053530213165\n",
      "Gradient Descent(310/999): loss=981.2326593538513, w0=0.24418427662234052, w1=-1.0518365105756136\n",
      "Gradient Descent(311/999): loss=977.6020129838239, w0=0.24354593548385242, w1=-1.0500714856219084\n",
      "Gradient Descent(312/999): loss=973.9895262157447, w0=0.24290911349728322, w1=-1.0483102720786337\n",
      "Gradient Descent(313/999): loss=970.3950913939483, w0=0.24227380431790438, w1=-1.046552863814529\n",
      "Gradient Descent(314/999): loss=966.81860156421, w0=0.24164000166405747, w1=-1.0447992546498017\n",
      "Gradient Descent(315/999): loss=963.2599504685849, w0=0.24100769931626775, w1=-1.0430494383572702\n",
      "Gradient Descent(316/999): loss=959.7190325402939, w0=0.24037689111637087, w1=-1.041303408663487\n",
      "Gradient Descent(317/999): loss=956.195742898663, w0=0.23974757096665233, w1=-1.0395611592498457\n",
      "Gradient Descent(318/999): loss=952.6899773441132, w0=0.23911973282899995, w1=-1.0378226837536695\n",
      "Gradient Descent(319/999): loss=949.2016323531961, w0=0.23849337072406848, w1=-1.036087975769282\n",
      "Gradient Descent(320/999): loss=945.7306050736785, w0=0.23786847873045683, w1=-1.0343570288490613\n",
      "Gradient Descent(321/999): loss=942.2767933196744, w0=0.2372450509838973, w1=-1.0326298365044773\n",
      "Gradient Descent(322/999): loss=938.8400955668261, w0=0.236623081676457, w1=-1.0309063922071118\n",
      "Gradient Descent(323/999): loss=935.4204109475266, w0=0.2360025650557506, w1=-1.029186689389663\n",
      "Gradient Descent(324/999): loss=932.0176392461893, w0=0.2353834954241653, w1=-1.0274707214469332\n",
      "Gradient Descent(325/999): loss=928.631680894561, w0=0.23476586713809688, w1=-1.0257584817368008\n",
      "Gradient Descent(326/999): loss=925.2624369670806, w0=0.23414967460719696, w1=-1.0240499635811766\n",
      "Gradient Descent(327/999): loss=921.9098091762793, w0=0.23353491229363166, w1=-1.0223451602669449\n",
      "Gradient Descent(328/999): loss=918.5736998682227, w0=0.23292157471135108, w1=-1.0206440650468893\n",
      "Gradient Descent(329/999): loss=915.2540120179968, w0=0.23230965642536947, w1=-1.0189466711406032\n",
      "Gradient Descent(330/999): loss=911.9506492252331, w0=0.23169915205105615, w1=-1.0172529717353858\n",
      "Gradient Descent(331/999): loss=908.6635157096717, w0=0.23109005625343687, w1=-1.0155629599871239\n",
      "Gradient Descent(332/999): loss=905.3925163067726, w0=0.23048236374650544, w1=-1.0138766290211592\n",
      "Gradient Descent(333/999): loss=902.1375564633588, w0=0.22987606929254562, w1=-1.0121939719331408\n",
      "Gradient Descent(334/999): loss=898.8985422332978, w0=0.22927116770146289, w1=-1.010514981789865\n",
      "Gradient Descent(335/999): loss=895.6753802732311, w0=0.2286676538301262, w1=-1.008839651630101\n",
      "Gradient Descent(336/999): loss=892.467977838326, w0=0.22806552258171936, w1=-1.0071679744654023\n",
      "Gradient Descent(337/999): loss=889.2762427780783, w0=0.2274647689051021, w1=-1.0054999432809062\n",
      "Gradient Descent(338/999): loss=886.1000835321469, w0=0.2268653877941804, w1=-1.0038355510361194\n",
      "Gradient Descent(339/999): loss=882.9394091262199, w0=0.22626737428728624, w1=-1.0021747906656908\n",
      "Gradient Descent(340/999): loss=879.7941291679243, w0=0.22567072346656655, w1=-1.000517655080172\n",
      "Gradient Descent(341/999): loss=876.6641538427658, w0=0.225075430457381, w1=-0.9988641371667649\n",
      "Gradient Descent(342/999): loss=873.5493939101032, w0=0.2244814904277088, w1=-0.9972142297900577\n",
      "Gradient Descent(343/999): loss=870.4497606991623, w0=0.2238888985875643, w1=-0.9955679257927474\n",
      "Gradient Descent(344/999): loss=867.3651661050749, w0=0.22329765018842113, w1=-0.9939252179963524\n",
      "Gradient Descent(345/999): loss=864.295522584959, w0=0.22270774052264475, w1=-0.9922860992019111\n",
      "Gradient Descent(346/999): loss=861.24074315403, w0=0.2221191649229336, w1=-0.9906505621906712\n",
      "Gradient Descent(347/999): loss=858.2007413817423, w0=0.22153191876176834, w1=-0.9890185997247655\n",
      "Gradient Descent(348/999): loss=855.1754313879619, w0=0.2209459974508692, w1=-0.9873902045478783\n",
      "Gradient Descent(349/999): loss=852.1647278391756, w0=0.2203613964406615, w1=-0.9857653693858993\n",
      "Gradient Descent(350/999): loss=849.1685459447293, w0=0.2197781112197488, w1=-0.9841440869475678\n",
      "Gradient Descent(351/999): loss=846.1868014530943, w0=0.21919613731439402, w1=-0.9825263499251053\n",
      "Gradient Descent(352/999): loss=843.2194106481702, w0=0.21861547028800823, w1=-0.980912150994838\n",
      "Gradient Descent(353/999): loss=840.2662903456085, w0=0.21803610574064677, w1=-0.9793014828178086\n",
      "Gradient Descent(354/999): loss=837.3273578891785, w0=0.217458039308513, w1=-0.9776943380403784\n",
      "Gradient Descent(355/999): loss=834.4025311471516, w0=0.21688126666346938, w1=-0.9760907092948188\n",
      "Gradient Descent(356/999): loss=831.4917285087203, w0=0.21630578351255556, w1=-0.974490589199893\n",
      "Gradient Descent(357/999): loss=828.5948688804449, w0=0.21573158559751374, w1=-0.972893970361428\n",
      "Gradient Descent(358/999): loss=825.7118716827275, w0=0.21515866869432101, w1=-0.9713008453728772\n",
      "Gradient Descent(359/999): loss=822.8426568463162, w0=0.21458702861272863, w1=-0.9697112068158728\n",
      "Gradient Descent(360/999): loss=819.9871448088352, w0=0.2140166611958079, w1=-0.9681250472607696\n",
      "Gradient Descent(361/999): loss=817.1452565113423, w0=0.21344756231950293, w1=-0.9665423592671796\n",
      "Gradient Descent(362/999): loss=814.3169133949143, w0=0.21287972789218998, w1=-0.9649631353844963\n",
      "Gradient Descent(363/999): loss=811.5020373972598, w0=0.21231315385424318, w1=-0.9633873681524122\n",
      "Gradient Descent(364/999): loss=808.7005509493567, w0=0.21174783617760673, w1=-0.9618150501014258\n",
      "Gradient Descent(365/999): loss=805.9123769721175, w0=0.21118377086537343, w1=-0.9602461737533401\n",
      "Gradient Descent(366/999): loss=803.137438873078, w0=0.2106209539513694, w1=-0.9586807316217539\n",
      "Gradient Descent(367/999): loss=800.3756605431176, w0=0.21005938149974482, w1=-0.9571187162125432\n",
      "Gradient Descent(368/999): loss=797.6269663531973, w0=0.20949904960457086, w1=-0.9555601200243348\n",
      "Gradient Descent(369/999): loss=794.8912811511258, w0=0.20893995438944246, w1=-0.9540049355489721\n",
      "Gradient Descent(370/999): loss=792.1685302583537, w0=0.208382092007087, w1=-0.9524531552719732\n",
      "Gradient Descent(371/999): loss=789.458639466788, w0=0.2078254586389786, w1=-0.9509047716729793\n",
      "Gradient Descent(372/999): loss=786.7615350356326, w0=0.2072700504949584, w1=-0.9493597772261979\n",
      "Gradient Descent(373/999): loss=784.077143688252, w0=0.20671586381286003, w1=-0.9478181644008363\n",
      "Gradient Descent(374/999): loss=781.405392609059, w0=0.20616289485814096, w1=-0.9462799256615286\n",
      "Gradient Descent(375/999): loss=778.7462094404282, w0=0.2056111399235191, w1=-0.9447450534687549\n",
      "Gradient Descent(376/999): loss=776.0995222796273, w0=0.2050605953286147, w1=-0.9432135402792532\n",
      "Gradient Descent(377/999): loss=773.465259675779, w0=0.20451125741959775, w1=-0.9416853785464244\n",
      "Gradient Descent(378/999): loss=770.843350626837, w0=0.20396312256884022, w1=-0.9401605607207298\n",
      "Gradient Descent(379/999): loss=768.2337245765949, w0=0.2034161871745738, w1=-0.938639079250082\n",
      "Gradient Descent(380/999): loss=765.6363114117056, w0=0.20287044766055237, w1=-0.9371209265802285\n",
      "Gradient Descent(381/999): loss=763.0510414587335, w0=0.20232590047571966, w1=-0.9356060951551289\n",
      "Gradient Descent(382/999): loss=760.4778454812223, w0=0.2017825420938816, w1=-0.9340945774173254\n",
      "Gradient Descent(383/999): loss=757.9166546767842, w0=0.20124036901338369, w1=-0.9325863658083065\n",
      "Gradient Descent(384/999): loss=755.3674006742184, w0=0.20069937775679295, w1=-0.9310814527688644\n",
      "Gradient Descent(385/999): loss=752.8300155306371, w0=0.20015956487058467, w1=-0.9295798307394464\n",
      "Gradient Descent(386/999): loss=750.3044317286272, w0=0.19962092692483366, w1=-0.9280814921604988\n",
      "Gradient Descent(387/999): loss=747.7905821734232, w0=0.19908346051291015, w1=-0.9265864294728063\n",
      "Gradient Descent(388/999): loss=745.2884001901035, w0=0.19854716225118005, w1=-0.9250946351178242\n",
      "Gradient Descent(389/999): loss=742.7978195208082, w0=0.19801202877870971, w1=-0.9236061015380048\n",
      "Gradient Descent(390/999): loss=740.3187743219773, w0=0.197478056756975, w1=-0.9221208211771184\n",
      "Gradient Descent(391/999): loss=737.8511991616048, w0=0.19694524286957457, w1=-0.9206387864805675\n",
      "Gradient Descent(392/999): loss=735.3950290165172, w0=0.19641358382194748, w1=-0.9191599898956967\n",
      "Gradient Descent(393/999): loss=732.9501992696752, w0=0.1958830763410949, w1=-0.9176844238720957\n",
      "Gradient Descent(394/999): loss=730.5166457074813, w0=0.19535371717530584, w1=-0.9162120808618973\n",
      "Gradient Descent(395/999): loss=728.0943045171231, w0=0.194825503093887, w1=-0.9147429533200702\n",
      "Gradient Descent(396/999): loss=725.6831122839241, w0=0.19429843088689658, w1=-0.9132770337047056\n",
      "Gradient Descent(397/999): loss=723.2830059887178, w0=0.19377249736488197, w1=-0.9118143144772997\n",
      "Gradient Descent(398/999): loss=720.893923005243, w0=0.19324769935862127, w1=-0.9103547881030298\n",
      "Gradient Descent(399/999): loss=718.5158010975528, w0=0.19272403371886862, w1=-0.9088984470510263\n",
      "Gradient Descent(400/999): loss=716.1485784174463, w0=0.19220149731610334, w1=-0.9074452837946388\n",
      "Gradient Descent(401/999): loss=713.7921935019131, w0=0.19168008704028264, w1=-0.9059952908116979\n",
      "Gradient Descent(402/999): loss=711.4465852706046, w0=0.19115979980059805, w1=-0.9045484605847716\n",
      "Gradient Descent(403/999): loss=709.1116930233136, w0=0.19064063252523536, w1=-0.9031047856014172\n",
      "Gradient Descent(404/999): loss=706.7874564374806, w0=0.19012258216113812, w1=-0.9016642583544283\n",
      "Gradient Descent(405/999): loss=704.473815565711, w0=0.18960564567377453, w1=-0.9002268713420776\n",
      "Gradient Descent(406/999): loss=702.1707108333122, w0=0.18908982004690794, w1=-0.8987926170683541\n",
      "Gradient Descent(407/999): loss=699.8780830358517, w0=0.18857510228237045, w1=-0.8973614880431968\n",
      "Gradient Descent(408/999): loss=697.5958733367241, w0=0.18806148939984008, w1=-0.8959334767827234\n",
      "Gradient Descent(409/999): loss=695.3240232647447, w0=0.18754897843662102, w1=-0.8945085758094546\n",
      "Gradient Descent(410/999): loss=693.06247471175, w0=0.1870375664474272, w1=-0.8930867776525346\n",
      "Gradient Descent(411/999): loss=690.8111699302286, w0=0.18652725050416896, w1=-0.8916680748479467\n",
      "Gradient Descent(412/999): loss=688.5700515309543, w0=0.18601802769574297, w1=-0.890252459938725\n",
      "Gradient Descent(413/999): loss=686.3390624806422, w0=0.1855098951278251, w1=-0.8888399254751621\n",
      "Gradient Descent(414/999): loss=684.118146099623, w0=0.1850028499226663, w1=-0.8874304640150124\n",
      "Gradient Descent(415/999): loss=681.9072460595338, w0=0.18449688921889165, w1=-0.8860240681236919\n",
      "Gradient Descent(416/999): loss=679.7063063810162, w0=0.18399201017130218, w1=-0.8846207303744739\n",
      "Gradient Descent(417/999): loss=677.5152714314437, w0=0.18348820995067963, w1=-0.8832204433486801\n",
      "Gradient Descent(418/999): loss=675.3340859226532, w0=0.18298548574359408, w1=-0.8818231996358692\n",
      "Gradient Descent(419/999): loss=673.1626949087006, w0=0.18248383475221444, w1=-0.8804289918340208\n",
      "Gradient Descent(420/999): loss=671.0010437836268, w0=0.18198325419412165, w1=-0.8790378125497158\n",
      "Gradient Descent(421/999): loss=668.8490782792418, w0=0.18148374130212455, w1=-0.8776496543983133\n",
      "Gradient Descent(422/999): loss=666.7067444629217, w0=0.18098529332407862, w1=-0.8762645100041242\n",
      "Gradient Descent(423/999): loss=664.5739887354229, w0=0.18048790752270724, w1=-0.8748823720005803\n",
      "Gradient Descent(424/999): loss=662.4507578287139, w0=0.17999158117542555, w1=-0.8735032330304011\n",
      "Gradient Descent(425/999): loss=660.3369988038139, w0=0.17949631157416693, w1=-0.8721270857457567\n",
      "Gradient Descent(426/999): loss=658.2326590486576, w0=0.17900209602521203, w1=-0.870753922808427\n",
      "Gradient Descent(427/999): loss=656.1376862759606, w0=0.17850893184902025, w1=-0.8693837368899581\n",
      "Gradient Descent(428/999): loss=654.0520285211165, w0=0.1780168163800637, w1=-0.8680165206718152\n",
      "Gradient Descent(429/999): loss=651.9756341400921, w0=0.17752574696666354, w1=-0.8666522668455325\n",
      "Gradient Descent(430/999): loss=649.9084518073475, w0=0.17703572097082884, w1=-0.8652909681128597\n",
      "Gradient Descent(431/999): loss=647.8504305137668, w0=0.17654673576809762, w1=-0.8639326171859053\n",
      "Gradient Descent(432/999): loss=645.8015195646036, w0=0.17605878874738032, w1=-0.8625772067872777\n",
      "Gradient Descent(433/999): loss=643.7616685774391, w0=0.17557187731080545, w1=-0.8612247296502217\n",
      "Gradient Descent(434/999): loss=641.730827480157, w0=0.17508599887356766, w1=-0.859875178518754\n",
      "Gradient Descent(435/999): loss=639.7089465089299, w0=0.1746011508637778, w1=-0.8585285461477938\n",
      "Gradient Descent(436/999): loss=637.6959762062179, w0=0.17411733072231542, w1=-0.8571848253032927\n",
      "Gradient Descent(437/999): loss=635.6918674187887, w0=0.1736345359026831, w1=-0.8558440087623591\n",
      "Gradient Descent(438/999): loss=633.6965712957382, w0=0.17315276387086323, w1=-0.8545060893133827\n",
      "Gradient Descent(439/999): loss=631.710039286538, w0=0.17267201210517663, w1=-0.853171059756154\n",
      "Gradient Descent(440/999): loss=629.7322231390867, w0=0.17219227809614338, w1=-0.8518389129019828\n",
      "Gradient Descent(441/999): loss=627.7630748977796, w0=0.1717135593463455, w1=-0.850509641573812\n",
      "Gradient Descent(442/999): loss=625.8025469015857, w0=0.17123585337029187, w1=-0.8491832386063316\n",
      "Gradient Descent(443/999): loss=623.8505917821493, w0=0.17075915769428487, w1=-0.8478596968460873\n",
      "Gradient Descent(444/999): loss=621.9071624618886, w0=0.17028346985628912, w1=-0.8465390091515888\n",
      "Gradient Descent(445/999): loss=619.9722121521202, w0=0.16980878740580202, w1=-0.8452211683934138\n",
      "Gradient Descent(446/999): loss=618.0456943511905, w0=0.16933510790372627, w1=-0.8439061674543112\n",
      "Gradient Descent(447/999): loss=616.1275628426204, w0=0.16886242892224415, w1=-0.8425939992293011\n",
      "Gradient Descent(448/999): loss=614.217771693263, w0=0.16839074804469367, w1=-0.8412846566257719\n",
      "Gradient Descent(449/999): loss=612.3162752514723, w0=0.1679200628654465, w1=-0.8399781325635761\n",
      "Gradient Descent(450/999): loss=610.42302814529, w0=0.16745037098978763, w1=-0.8386744199751233\n",
      "Gradient Descent(451/999): loss=608.5379852806326, w0=0.1669816700337969, w1=-0.8373735118054709\n",
      "Gradient Descent(452/999): loss=606.6611018395056, w0=0.1665139576242321, w1=-0.8360754010124131\n",
      "Gradient Descent(453/999): loss=604.7923332782177, w0=0.16604723139841385, w1=-0.8347800805665666\n",
      "Gradient Descent(454/999): loss=602.9316353256133, w0=0.16558148900411207, w1=-0.8334875434514555\n",
      "Gradient Descent(455/999): loss=601.0789639813144, w0=0.16511672809943423, w1=-0.8321977826635931\n",
      "Gradient Descent(456/999): loss=599.2342755139744, w0=0.164652946352715, w1=-0.8309107912125622\n",
      "Gradient Descent(457/999): loss=597.3975264595487, w0=0.16419014144240773, w1=-0.829626562121093\n",
      "Gradient Descent(458/999): loss=595.568673619565, w0=0.16372831105697733, w1=-0.8283450884251391\n",
      "Gradient Descent(459/999): loss=593.7476740594195, w0=0.16326745289479475, w1=-0.8270663631739517\n",
      "Gradient Descent(460/999): loss=591.9344851066735, w0=0.16280756466403298, w1=-0.8257903794301515\n",
      "Gradient Descent(461/999): loss=590.1290643493682, w0=0.1623486440825646, w1=-0.8245171302697992\n",
      "Gradient Descent(462/999): loss=588.3313696343469, w0=0.16189068887786073, w1=-0.8232466087824635\n",
      "Gradient Descent(463/999): loss=586.5413590655887, w0=0.16143369678689143, w1=-0.8219788080712874\n",
      "Gradient Descent(464/999): loss=584.7589910025571, w0=0.1609776655560276, w1=-0.8207137212530534\n",
      "Gradient Descent(465/999): loss=582.9842240585552, w0=0.16052259294094434, w1=-0.8194513414582458\n",
      "Gradient Descent(466/999): loss=581.2170170990925, w0=0.16006847670652546, w1=-0.8181916618311121\n",
      "Gradient Descent(467/999): loss=579.457329240267, w0=0.15961531462676967, w1=-0.8169346755297221\n",
      "Gradient Descent(468/999): loss=577.7051198471495, w0=0.15916310448469792, w1=-0.8156803757260255\n",
      "Gradient Descent(469/999): loss=575.9603485321907, w0=0.15871184407226208, w1=-0.8144287556059081\n",
      "Gradient Descent(470/999): loss=574.222975153627, w0=0.15826153119025504, w1=-0.813179808369246\n",
      "Gradient Descent(471/999): loss=572.4929598139037, w0=0.15781216364822206, w1=-0.811933527229958\n",
      "Gradient Descent(472/999): loss=570.7702628581075, w0=0.15736373926437325, w1=-0.8106899054160571\n",
      "Gradient Descent(473/999): loss=569.0548448724087, w0=0.15691625586549754, w1=-0.8094489361696996\n",
      "Gradient Descent(474/999): loss=567.3466666825141, w0=0.1564697112868777, w1=-0.8082106127472333\n",
      "Gradient Descent(475/999): loss=565.6456893521296, w0=0.15602410337220665, w1=-0.8069749284192437\n",
      "Gradient Descent(476/999): loss=563.9518741814355, w0=0.15557942997350496, w1=-0.8057418764705994\n",
      "Gradient Descent(477/999): loss=562.2651827055681, w0=0.15513568895103952, w1=-0.8045114502004949\n",
      "Gradient Descent(478/999): loss=560.5855766931137, w0=0.15469287817324343, w1=-0.8032836429224931\n",
      "Gradient Descent(479/999): loss=558.9130181446128, w0=0.1542509955166369, w1=-0.8020584479645658\n",
      "Gradient Descent(480/999): loss=557.2474692910737, w0=0.15381003886574943, w1=-0.8008358586691328\n",
      "Gradient Descent(481/999): loss=555.5888925924957, w0=0.15337000611304297, w1=-0.7996158683930995\n",
      "Gradient Descent(482/999): loss=553.9372507364016, w0=0.1529308951588363, w1=-0.798398470507894\n",
      "Gradient Descent(483/999): loss=552.2925066363814, w0=0.15249270391123035, w1=-0.7971836583995018\n",
      "Gradient Descent(484/999): loss=550.6546234306464, w0=0.15205543028603477, w1=-0.7959714254684994\n",
      "Gradient Descent(485/999): loss=549.0235644805879, w0=0.15161907220669532, w1=-0.7947617651300873\n",
      "Gradient Descent(486/999): loss=547.3992933693518, w0=0.15118362760422244, w1=-0.7935546708141211\n",
      "Gradient Descent(487/999): loss=545.781773900418, w0=0.1507490944171208, w1=-0.7923501359651415\n",
      "Gradient Descent(488/999): loss=544.1709700961935, w0=0.1503154705913199, w1=-0.791148154042403\n",
      "Gradient Descent(489/999): loss=542.5668461966083, w0=0.14988275408010546, w1=-0.7899487185199019\n",
      "Gradient Descent(490/999): loss=540.9693666577282, w0=0.1494509428440521, w1=-0.7887518228864021\n",
      "Gradient Descent(491/999): loss=539.3784961503719, w0=0.14902003485095675, w1=-0.7875574606454611\n",
      "Gradient Descent(492/999): loss=537.794199558737, w0=0.14859002807577287, w1=-0.7863656253154534\n",
      "Gradient Descent(493/999): loss=536.2164419790398, w0=0.14816092050054605, w1=-0.7851763104295941\n",
      "Gradient Descent(494/999): loss=534.6451887181569, w0=0.14773271011435007, w1=-0.7839895095359604\n",
      "Gradient Descent(495/999): loss=533.0804052922823, w0=0.14730539491322414, w1=-0.7828052161975124\n",
      "Gradient Descent(496/999): loss=531.5220574255912, w0=0.14687897290011084, w1=-0.7816234239921133\n",
      "Gradient Descent(497/999): loss=529.9701110489098, w0=0.14645344208479513, w1=-0.7804441265125475\n",
      "Gradient Descent(498/999): loss=528.4245322983976, w0=0.146028800483844, w1=-0.7792673173665386\n",
      "Gradient Descent(499/999): loss=526.885287514238, w0=0.14560504612054717, w1=-0.7780929901767661\n",
      "Gradient Descent(500/999): loss=525.3523432393346, w0=0.14518217702485847, w1=-0.7769211385808809\n",
      "Gradient Descent(501/999): loss=523.825666218019, w0=0.14476019123333816, w1=-0.7757517562315197\n",
      "Gradient Descent(502/999): loss=522.3052233947675, w0=0.144339086789096, w1=-0.7745848367963194\n",
      "Gradient Descent(503/999): loss=520.7909819129221, w0=0.14391886174173507, w1=-0.7734203739579294\n",
      "Gradient Descent(504/999): loss=519.2829091134245, w0=0.1434995141472965, w1=-0.7722583614140235\n",
      "Gradient Descent(505/999): loss=517.7809725335571, w0=0.1430810420682049, w1=-0.7710987928773109\n",
      "Gradient Descent(506/999): loss=516.2851399056914, w0=0.14266344357321453, w1=-0.7699416620755462\n",
      "Gradient Descent(507/999): loss=514.7953791560435, w0=0.14224671673735623, w1=-0.7687869627515385\n",
      "Gradient Descent(508/999): loss=513.3116584034423, w0=0.1418308596418852, w1=-0.7676346886631598\n",
      "Gradient Descent(509/999): loss=511.8339459581015, w0=0.14141587037422929, w1=-0.7664848335833523\n",
      "Gradient Descent(510/999): loss=510.3622103203994, w0=0.1410017470279383, w1=-0.7653373913001352\n",
      "Gradient Descent(511/999): loss=508.89642017967134, w0=0.1405884877026337, w1=-0.7641923556166104\n",
      "Gradient Descent(512/999): loss=507.4365444130051, w0=0.14017609050395924, w1=-0.7630497203509671\n",
      "Gradient Descent(513/999): loss=505.98255208404754, w0=0.13976455354353212, w1=-0.7619094793364867\n",
      "Gradient Descent(514/999): loss=504.53441244181596, w0=0.13935387493889498, w1=-0.7607716264215455\n",
      "Gradient Descent(515/999): loss=503.09209491952174, w0=0.13894405281346836, w1=-0.7596361554696179\n",
      "Gradient Descent(516/999): loss=501.65556913339594, w0=0.13853508529650407, w1=-0.7585030603592781\n",
      "Gradient Descent(517/999): loss=500.2248048815285, w0=0.13812697052303896, w1=-0.7573723349842008\n",
      "Gradient Descent(518/999): loss=498.79977214270986, w0=0.13771970663384947, w1=-0.7562439732531624\n",
      "Gradient Descent(519/999): loss=497.3804410752846, w0=0.13731329177540677, w1=-0.7551179690900403\n",
      "Gradient Descent(520/999): loss=495.9667820160097, w0=0.1369077240998325, w1=-0.7539943164338117\n",
      "Gradient Descent(521/999): loss=494.55876547892046, w0=0.13650300176485522, w1=-0.7528730092385524\n",
      "Gradient Descent(522/999): loss=493.15636215420596, w0=0.1360991229337672, w1=-0.7517540414734344\n",
      "Gradient Descent(523/999): loss=491.75954290708995, w0=0.13569608577538214, w1=-0.7506374071227224\n",
      "Gradient Descent(524/999): loss=490.36827877671834, w0=0.13529388846399326, w1=-0.7495231001857708\n",
      "Gradient Descent(525/999): loss=488.9825409750571, w0=0.13489252917933203, w1=-0.748411114677019\n",
      "Gradient Descent(526/999): loss=487.60230088579544, w0=0.1344920061065274, w1=-0.7473014446259866\n",
      "Gradient Descent(527/999): loss=486.22753006325394, w0=0.13409231743606562, w1=-0.7461940840772677\n",
      "Gradient Descent(528/999): loss=484.8582002313052, w0=0.13369346136375074, w1=-0.7450890270905254\n",
      "Gradient Descent(529/999): loss=483.494283282297, w0=0.13329543609066538, w1=-0.743986267740484\n",
      "Gradient Descent(530/999): loss=482.13575127598295, w0=0.1328982398231323, w1=-0.742885800116923\n",
      "Gradient Descent(531/999): loss=480.78257643846337, w0=0.13250187077267633, w1=-0.7417876183246679\n",
      "Gradient Descent(532/999): loss=479.4347311611296, w0=0.1321063271559868, w1=-0.7406917164835828\n",
      "Gradient Descent(533/999): loss=478.0921879996173, w0=0.13171160719488065, w1=-0.7395980887285609\n",
      "Gradient Descent(534/999): loss=476.75491967276406, w0=0.13131770911626586, w1=-0.7385067292095152\n",
      "Gradient Descent(535/999): loss=475.4228990615784, w0=0.1309246311521055, w1=-0.7374176320913683\n",
      "Gradient Descent(536/999): loss=474.0960992082098, w0=0.13053237153938216, w1=-0.736330791554042\n",
      "Gradient Descent(537/999): loss=472.7744933149315, w0=0.13014092852006287, w1=-0.7352462017924458\n",
      "Gradient Descent(538/999): loss=471.4580547431233, w0=0.1297503003410646, w1=-0.734163857016466\n",
      "Gradient Descent(539/999): loss=470.1467570122667, w0=0.12936048525422017, w1=-0.7330837514509532\n",
      "Gradient Descent(540/999): loss=468.8405737989455, w0=0.12897148151624452, w1=-0.7320058793357096\n",
      "Gradient Descent(541/999): loss=467.53947893584785, w0=0.12858328738870156, w1=-0.7309302349254766\n",
      "Gradient Descent(542/999): loss=466.2434464107832, w0=0.1281959011379714, w1=-0.7298568124899206\n",
      "Gradient Descent(543/999): loss=464.9524503656989, w0=0.12780932103521808, w1=-0.7287856063136197\n",
      "Gradient Descent(544/999): loss=463.66646509570546, w0=0.12742354535635755, w1=-0.7277166106960486\n",
      "Gradient Descent(545/999): loss=462.38546504811023, w0=0.12703857238202634, w1=-0.7266498199515649\n",
      "Gradient Descent(546/999): loss=461.1094248214525, w0=0.1266544003975505, w1=-0.7255852284093925\n",
      "Gradient Descent(547/999): loss=459.8383191645505, w0=0.12627102769291482, w1=-0.7245228304136072\n",
      "Gradient Descent(548/999): loss=458.57212297555077, w0=0.12588845256273282, w1=-0.7234626203231197\n",
      "Gradient Descent(549/999): loss=457.3108113009857, w0=0.12550667330621676, w1=-0.7224045925116596\n",
      "Gradient Descent(550/999): loss=456.0543593348342, w0=0.12512568822714823, w1=-0.7213487413677584\n",
      "Gradient Descent(551/999): loss=454.80274241759463, w0=0.12474549563384917, w1=-0.7202950612947324\n",
      "Gradient Descent(552/999): loss=453.5559360353559, w0=0.12436609383915308, w1=-0.719243546710665\n",
      "Gradient Descent(553/999): loss=452.31391581888136, w0=0.12398748116037685, w1=-0.7181941920483886\n",
      "Gradient Descent(554/999): loss=451.0766575426941, w0=0.12360965591929277, w1=-0.7171469917554661\n",
      "Gradient Descent(555/999): loss=449.8441371241718, w0=0.12323261644210096, w1=-0.7161019402941727\n",
      "Gradient Descent(556/999): loss=448.61633062264383, w0=0.12285636105940222, w1=-0.7150590321414765\n",
      "Gradient Descent(557/999): loss=447.3932142384972, w0=0.12248088810617115, w1=-0.7140182617890187\n",
      "Gradient Descent(558/999): loss=446.1747643122864, w0=0.12210619592172967, w1=-0.7129796237430949\n",
      "Gradient Descent(559/999): loss=444.9609573238519, w0=0.12173228284972083, w1=-0.7119431125246338\n",
      "Gradient Descent(560/999): loss=443.7517698914398, w0=0.12135914723808307, w1=-0.7109087226691779\n",
      "Gradient Descent(561/999): loss=442.5471787708303, w0=0.12098678743902463, w1=-0.7098764487268621\n",
      "Gradient Descent(562/999): loss=441.34716085447343, w0=0.12061520180899853, w1=-0.7088462852623928\n",
      "Gradient Descent(563/999): loss=440.15169317062447, w0=0.1202443887086776, w1=-0.7078182268550268\n",
      "Gradient Descent(564/999): loss=438.960752882492, w0=0.11987434650293007, w1=-0.7067922680985497\n",
      "Gradient Descent(565/999): loss=437.77431728738867, w0=0.11950507356079532, w1=-0.7057684036012536\n",
      "Gradient Descent(566/999): loss=436.5923638158817, w0=0.11913656825546, w1=-0.7047466279859155\n",
      "Gradient Descent(567/999): loss=435.4148700309628, w0=0.11876882896423445, w1=-0.7037269358897743\n",
      "Gradient Descent(568/999): loss=434.241813627207, w0=0.11840185406852942, w1=-0.7027093219645085\n",
      "Gradient Descent(569/999): loss=433.07317242995003, w0=0.11803564195383306, w1=-0.701693780876213\n",
      "Gradient Descent(570/999): loss=431.90892439446327, w0=0.11767019100968826, w1=-0.700680307305376\n",
      "Gradient Descent(571/999): loss=430.7490476051387, w0=0.11730549962967014, w1=-0.6996688959468552\n",
      "Gradient Descent(572/999): loss=429.59352027467537, w0=0.11694156621136406, w1=-0.6986595415098544\n",
      "Gradient Descent(573/999): loss=428.4423207432741, w0=0.11657838915634364, w1=-0.6976522387178995\n",
      "Gradient Descent(574/999): loss=427.29542747783574, w0=0.11621596687014925, w1=-0.696646982308814\n",
      "Gradient Descent(575/999): loss=426.1528190711667, w0=0.11585429776226669, w1=-0.6956437670346947\n",
      "Gradient Descent(576/999): loss=425.0144742411845, w0=0.11549338024610611, w1=-0.6946425876618871\n",
      "Gradient Descent(577/999): loss=423.88037183013614, w0=0.11513321273898128, w1=-0.6936434389709606\n",
      "Gradient Descent(578/999): loss=422.75049080381524, w0=0.11477379366208902, w1=-0.692646315756683\n",
      "Gradient Descent(579/999): loss=421.62481025078716, w0=0.11441512144048896, w1=-0.6916512128279957\n",
      "Gradient Descent(580/999): loss=420.50330938161824, w0=0.11405719450308349, w1=-0.6906581250079876\n",
      "Gradient Descent(581/999): loss=419.3859675281106, w0=0.11370001128259802, w1=-0.68966704713387\n",
      "Gradient Descent(582/999): loss=418.2727641425409, w0=0.11334357021556142, w1=-0.68867797405695\n",
      "Gradient Descent(583/999): loss=417.1636787969061, w0=0.11298786974228674, w1=-0.6876909006426051\n",
      "Gradient Descent(584/999): loss=416.05869118217083, w0=0.11263290830685212, w1=-0.6867058217702564\n",
      "Gradient Descent(585/999): loss=414.95778110752474, w0=0.11227868435708203, w1=-0.6857227323333424\n",
      "Gradient Descent(586/999): loss=413.8609284996364, w0=0.11192519634452862, w1=-0.6847416272392921\n",
      "Gradient Descent(587/999): loss=412.7681134019227, w0=0.11157244272445335, w1=-0.6837625014094986\n",
      "Gradient Descent(588/999): loss=411.6793159738141, w0=0.11122042195580886, w1=-0.6827853497792916\n",
      "Gradient Descent(589/999): loss=410.5945164900283, w0=0.11086913250122105, w1=-0.6818101672979107\n",
      "Gradient Descent(590/999): loss=409.5136953398491, w0=0.11051857282697128, w1=-0.6808369489284776\n",
      "Gradient Descent(591/999): loss=408.43683302640784, w0=0.11016874140297898, w1=-0.6798656896479692\n",
      "Gradient Descent(592/999): loss=407.3639101659734, w0=0.10981963670278429, w1=-0.6788963844471897\n",
      "Gradient Descent(593/999): loss=406.29490748724015, w0=0.10947125720353093, w1=-0.6779290283307426\n",
      "Gradient Descent(594/999): loss=405.2298058306287, w0=0.10912360138594943, w1=-0.6769636163170033\n",
      "Gradient Descent(595/999): loss=404.16858614758365, w0=0.10877666773434029, w1=-0.6760001434380907\n",
      "Gradient Descent(596/999): loss=403.11122949988055, w0=0.10843045473655763, w1=-0.675038604739839\n",
      "Gradient Descent(597/999): loss=402.0577170589372, w0=0.10808496088399279, w1=-0.6740789952817697\n",
      "Gradient Descent(598/999): loss=401.00803010512595, w0=0.10774018467155824, w1=-0.673121310137063\n",
      "Gradient Descent(599/999): loss=399.9621500270946, w0=0.10739612459767171, w1=-0.672165544392529\n",
      "Gradient Descent(600/999): loss=398.9200583210884, w0=0.1070527791642404, w1=-0.6712116931485793\n",
      "Gradient Descent(601/999): loss=397.8817365902792, w0=0.10671014687664543, w1=-0.6702597515191984\n",
      "Gradient Descent(602/999): loss=396.84716654409635, w0=0.1063682262437265, w1=-0.6693097146319141\n",
      "Gradient Descent(603/999): loss=395.81632999756494, w0=0.10602701577776663, w1=-0.6683615776277694\n",
      "Gradient Descent(604/999): loss=394.7892088706435, w0=0.10568651399447723, w1=-0.6674153356612923\n",
      "Gradient Descent(605/999): loss=393.7657851875727, w0=0.10534671941298317, w1=-0.6664709839004677\n",
      "Gradient Descent(606/999): loss=392.7460410762223, w0=0.10500763055580808, w1=-0.665528517526707\n",
      "Gradient Descent(607/999): loss=391.72995876744386, w0=0.10466924594885993, w1=-0.6645879317348197\n",
      "Gradient Descent(608/999): loss=390.71752059443185, w0=0.10433156412141659, w1=-0.6636492217329829\n",
      "Gradient Descent(609/999): loss=389.70870899208154, w0=0.10399458360611165, w1=-0.6627123827427125\n",
      "Gradient Descent(610/999): loss=388.70350649635776, w0=0.10365830293892044, w1=-0.661777409998833\n",
      "Gradient Descent(611/999): loss=387.70189574366333, w0=0.10332272065914608, w1=-0.660844298749448\n",
      "Gradient Descent(612/999): loss=386.7038594702136, w0=0.10298783530940583, w1=-0.6599130442559099\n",
      "Gradient Descent(613/999): loss=385.70938051141485, w0=0.10265364543561747, w1=-0.6589836417927906\n",
      "Gradient Descent(614/999): loss=384.71844180124566, w0=0.10232014958698589, w1=-0.658056086647851\n",
      "Gradient Descent(615/999): loss=383.7310263716445, w0=0.10198734631598982, w1=-0.657130374122011\n",
      "Gradient Descent(616/999): loss=382.7471173518978, w0=0.10165523417836873, w1=-0.6562064995293196\n",
      "Gradient Descent(617/999): loss=381.76669796803606, w0=0.1013238117331098, w1=-0.6552844581969243\n",
      "Gradient Descent(618/999): loss=380.7897515422303, w0=0.10099307754243508, w1=-0.6543642454650409\n",
      "Gradient Descent(619/999): loss=379.81626149219454, w0=0.10066303017178886, w1=-0.6534458566869233\n",
      "Gradient Descent(620/999): loss=378.84621133059125, w0=0.100333668189825, w1=-0.6525292872288331\n",
      "Gradient Descent(621/999): loss=377.8795846644412, w0=0.10000499016839455, w1=-0.6516145324700087\n",
      "Gradient Descent(622/999): loss=376.9163651945363, w0=0.09967699468253347, w1=-0.6507015878026353\n",
      "Gradient Descent(623/999): loss=375.95653671485746, w0=0.0993496803104504, w1=-0.6497904486318141\n",
      "Gradient Descent(624/999): loss=375.00008311199423, w0=0.09902304563351468, w1=-0.6488811103755316\n",
      "Gradient Descent(625/999): loss=374.046988364571, w0=0.09869708923624439, w1=-0.6479735684646292\n",
      "Gradient Descent(626/999): loss=373.09723654267384, w0=0.0983718097062946, w1=-0.6470678183427718\n",
      "Gradient Descent(627/999): loss=372.1508118072836, w0=0.09804720563444569, w1=-0.6461638554664181\n",
      "Gradient Descent(628/999): loss=371.2076984097113, w0=0.09772327561459179, w1=-0.6452616753047886\n",
      "Gradient Descent(629/999): loss=370.26788069103645, w0=0.09740001824372937, w1=-0.6443612733398357\n",
      "Gradient Descent(630/999): loss=369.33134308155155, w0=0.09707743212194594, w1=-0.6434626450662125\n",
      "Gradient Descent(631/999): loss=368.3980701002071, w0=0.09675551585240885, w1=-0.6425657859912416\n",
      "Gradient Descent(632/999): loss=367.4680463540638, w0=0.0964342680413542, w1=-0.6416706916348844\n",
      "Gradient Descent(633/999): loss=366.54125653774315, w0=0.09611368729807591, w1=-0.6407773575297104\n",
      "Gradient Descent(634/999): loss=365.61768543288684, w0=0.09579377223491485, w1=-0.6398857792208656\n",
      "Gradient Descent(635/999): loss=364.6973179076163, w0=0.09547452146724807, w1=-0.638995952266042\n",
      "Gradient Descent(636/999): loss=363.7801389159986, w0=0.0951559336134782, w1=-0.6381078722354461\n",
      "Gradient Descent(637/999): loss=362.86613349751116, w0=0.0948380072950229, w1=-0.637221534711768\n",
      "Gradient Descent(638/999): loss=361.95528677651413, w0=0.09452074113630445, w1=-0.6363369352901507\n",
      "Gradient Descent(639/999): loss=361.04758396172616, w0=0.0942041337647394, w1=-0.6354540695781582\n",
      "Gradient Descent(640/999): loss=360.14301034569945, w0=0.0938881838107284, w1=-0.6345729331957449\n",
      "Gradient Descent(641/999): loss=359.2415513043029, w0=0.09357288990764599, w1=-0.6336935217752244\n",
      "Gradient Descent(642/999): loss=358.343192296206, w0=0.09325825069183066, w1=-0.6328158309612382\n",
      "Gradient Descent(643/999): loss=357.44791886236607, w0=0.0929442648025749, w1=-0.6319398564107245\n",
      "Gradient Descent(644/999): loss=356.55571662551984, w0=0.09263093088211533, w1=-0.6310655937928873\n",
      "Gradient Descent(645/999): loss=355.6665712896783, w0=0.09231824757562301, w1=-0.6301930387891648\n",
      "Gradient Descent(646/999): loss=354.78046863962254, w0=0.09200621353119379, w1=-0.6293221870931984\n",
      "Gradient Descent(647/999): loss=353.8973945404075, w0=0.09169482739983871, w1=-0.6284530344108017\n",
      "Gradient Descent(648/999): loss=353.0173349368621, w0=0.0913840878354746, w1=-0.6275855764599289\n",
      "Gradient Descent(649/999): loss=352.140275853101, w0=0.09107399349491463, w1=-0.6267198089706438\n",
      "Gradient Descent(650/999): loss=351.2662033920302, w0=0.09076454303785912, w1=-0.6258557276850887\n",
      "Gradient Descent(651/999): loss=350.39510373486434, w0=0.09045573512688626, w1=-0.6249933283574528\n",
      "Gradient Descent(652/999): loss=349.526963140642, w0=0.09014756842744301, w1=-0.6241326067539416\n",
      "Gradient Descent(653/999): loss=348.6617679457452, w0=0.08984004160783607, w1=-0.6232735586527449\n",
      "Gradient Descent(654/999): loss=347.79950456342175, w0=0.08953315333922293, w1=-0.6224161798440065\n",
      "Gradient Descent(655/999): loss=346.94015948331315, w0=0.089226902295603, w1=-0.6215604661297924\n",
      "Gradient Descent(656/999): loss=346.08371927098176, w0=0.08892128715380881, w1=-0.6207064133240597\n",
      "Gradient Descent(657/999): loss=345.23017056744266, w0=0.08861630659349734, w1=-0.6198540172526256\n",
      "Gradient Descent(658/999): loss=344.37950008870047, w0=0.0883119592971413, w1=-0.6190032737531359\n",
      "Gradient Descent(659/999): loss=343.5316946252849, w0=0.08800824395002063, w1=-0.6181541786750345\n",
      "Gradient Descent(660/999): loss=342.6867410417934, w0=0.08770515924021403, w1=-0.6173067278795316\n",
      "Gradient Descent(661/999): loss=341.8446262764348, w0=0.08740270385859052, w1=-0.6164609172395729\n",
      "Gradient Descent(662/999): loss=341.0053373405756, w0=0.08710087649880106, w1=-0.6156167426398084\n",
      "Gradient Descent(663/999): loss=340.1688613182896, w0=0.0867996758572704, w1=-0.6147741999765615\n",
      "Gradient Descent(664/999): loss=339.33518536591123, w0=0.08649910063318877, w1=-0.6139332851577977\n",
      "Gradient Descent(665/999): loss=338.50429671159065, w0=0.08619914952850384, w1=-0.6130939941030935\n",
      "Gradient Descent(666/999): loss=337.6761826548514, w0=0.0858998212479126, w1=-0.6122563227436058\n",
      "Gradient Descent(667/999): loss=336.8508305661525, w0=0.08560111449885341, w1=-0.6114202670220402\n",
      "Gradient Descent(668/999): loss=336.0282278864523, w0=0.08530302799149807, w1=-0.6105858228926209\n",
      "Gradient Descent(669/999): loss=335.2083621267748, w0=0.08500556043874397, w1=-0.6097529863210587\n",
      "Gradient Descent(670/999): loss=334.39122086777894, w0=0.08470871055620627, w1=-0.608921753284521\n",
      "Gradient Descent(671/999): loss=333.5767917593318, w0=0.08441247706221022, w1=-0.6080921197716006\n",
      "Gradient Descent(672/999): loss=332.7650625200834, w0=0.08411685867778347, w1=-0.6072640817822843\n",
      "Gradient Descent(673/999): loss=331.95602093704343, w0=0.08382185412664844, w1=-0.6064376353279228\n",
      "Gradient Descent(674/999): loss=331.14965486516286, w0=0.08352746213521484, w1=-0.6056127764311995\n",
      "Gradient Descent(675/999): loss=330.34595222691667, w0=0.08323368143257212, w1=-0.6047895011260996\n",
      "Gradient Descent(676/999): loss=329.54490101189003, w0=0.08294051075048212, w1=-0.6039678054578798\n",
      "Gradient Descent(677/999): loss=328.74648927636537, w0=0.08264794882337165, w1=-0.6031476854830367\n",
      "Gradient Descent(678/999): loss=327.95070514291547, w0=0.08235599438832522, w1=-0.6023291372692772\n",
      "Gradient Descent(679/999): loss=327.15753679999574, w0=0.08206464618507776, w1=-0.601512156895487\n",
      "Gradient Descent(680/999): loss=326.3669725015412, w0=0.08177390295600746, w1=-0.6006967404517003\n",
      "Gradient Descent(681/999): loss=325.5790005665651, w0=0.08148376344612861, w1=-0.5998828840390691\n",
      "Gradient Descent(682/999): loss=324.7936093787601, w0=0.0811942264030845, w1=-0.5990705837698328\n",
      "Gradient Descent(683/999): loss=324.0107873861034, w0=0.08090529057714045, w1=-0.5982598357672875\n",
      "Gradient Descent(684/999): loss=323.2305231004616, w0=0.08061695472117675, w1=-0.5974506361657556\n",
      "Gradient Descent(685/999): loss=322.4528050972015, w0=0.0803292175906818, w1=-0.5966429811105552\n",
      "Gradient Descent(686/999): loss=321.6776220148003, w0=0.08004207794374522, w1=-0.5958368667579701\n",
      "Gradient Descent(687/999): loss=320.9049625544601, w0=0.07975553454105101, w1=-0.5950322892752188\n",
      "Gradient Descent(688/999): loss=320.13481547972447, w0=0.07946958614587078, w1=-0.5942292448404246\n",
      "Gradient Descent(689/999): loss=319.36716961609767, w0=0.07918423152405704, w1=-0.5934277296425854\n",
      "Gradient Descent(690/999): loss=318.6020138506657, w0=0.07889946944403649, w1=-0.5926277398815432\n",
      "Gradient Descent(691/999): loss=317.83933713171996, w0=0.07861529867680343, w1=-0.5918292717679537\n",
      "Gradient Descent(692/999): loss=317.0791284683838, w0=0.07833171799591317, w1=-0.591032321523257\n",
      "Gradient Descent(693/999): loss=316.3213769302411, w0=0.07804872617747545, w1=-0.5902368853796464\n",
      "Gradient Descent(694/999): loss=315.5660716469668, w0=0.07776632200014802, w1=-0.5894429595800388\n",
      "Gradient Descent(695/999): loss=314.813201807961, w0=0.07748450424513012, w1=-0.588650540378045\n",
      "Gradient Descent(696/999): loss=314.0627566619836, w0=0.07720327169615614, w1=-0.5878596240379392\n",
      "Gradient Descent(697/999): loss=313.31472551679366, w0=0.07692262313948921, w1=-0.5870702068346295\n",
      "Gradient Descent(698/999): loss=312.56909773878846, w0=0.07664255736391494, w1=-0.5862822850536276\n",
      "Gradient Descent(699/999): loss=311.8258627526463, w0=0.07636307316073511, w1=-0.5854958549910191\n",
      "Gradient Descent(700/999): loss=311.08501004097235, w0=0.07608416932376144, w1=-0.5847109129534339\n",
      "Gradient Descent(701/999): loss=310.3465291439444, w0=0.07580584464930941, w1=-0.5839274552580164\n",
      "Gradient Descent(702/999): loss=309.6104096589633, w0=0.0755280979361921, w1=-0.5831454782323955\n",
      "Gradient Descent(703/999): loss=308.87664124030425, w0=0.0752509279857141, w1=-0.5823649782146553\n",
      "Gradient Descent(704/999): loss=308.14521359877057, w0=0.07497433360166543, w1=-0.5815859515533053\n",
      "Gradient Descent(705/999): loss=307.4161165013499, w0=0.0746983135903155, w1=-0.5808083946072511\n",
      "Gradient Descent(706/999): loss=306.68933977087187, w0=0.0744228667604071, w1=-0.5800323037457646\n",
      "Gradient Descent(707/999): loss=305.9648732856699, w0=0.07414799192315051, w1=-0.5792576753484548\n",
      "Gradient Descent(708/999): loss=305.24270697924254, w0=0.07387368789221752, w1=-0.5784845058052385\n",
      "Gradient Descent(709/999): loss=304.522830839918, w0=0.07359995348373558, w1=-0.5777127915163103\n",
      "Gradient Descent(710/999): loss=303.80523491052276, w0=0.07332678751628192, w1=-0.5769425288921142\n",
      "Gradient Descent(711/999): loss=303.0899092880487, w0=0.0730541888108778, w1=-0.5761737143533139\n",
      "Gradient Descent(712/999): loss=302.3768441233253, w0=0.07278215619098269, w1=-0.5754063443307638\n",
      "Gradient Descent(713/999): loss=301.66602962069226, w0=0.07251068848248853, w1=-0.5746404152654795\n",
      "Gradient Descent(714/999): loss=300.9574560376748, w0=0.07223978451371403, w1=-0.5738759236086094\n",
      "Gradient Descent(715/999): loss=300.25111368466185, w0=0.07196944311539903, w1=-0.5731128658214051\n",
      "Gradient Descent(716/999): loss=299.54699292458474, w0=0.07169966312069882, w1=-0.5723512383751925\n",
      "Gradient Descent(717/999): loss=298.84508417259786, w0=0.07143044336517856, w1=-0.5715910377513435\n",
      "Gradient Descent(718/999): loss=298.14537789576434, w0=0.07116178268680769, w1=-0.5708322604412462\n",
      "Gradient Descent(719/999): loss=297.44786461273907, w0=0.07089367992595438, w1=-0.5700749029462773\n",
      "Gradient Descent(720/999): loss=296.7525348934582, w0=0.07062613392538009, w1=-0.569318961777772\n",
      "Gradient Descent(721/999): loss=296.059379358827, w0=0.07035914353023397, w1=-0.5685644334569965\n",
      "Gradient Descent(722/999): loss=295.368388680413, w0=0.07009270758804752, w1=-0.5678113145151188\n",
      "Gradient Descent(723/999): loss=294.679553580137, w0=0.0698268249487291, w1=-0.5670596014931802\n",
      "Gradient Descent(724/999): loss=293.9928648299717, w0=0.0695614944645586, w1=-0.5663092909420671\n",
      "Gradient Descent(725/999): loss=293.30831325163507, w0=0.06929671499018203, w1=-0.5655603794224819\n",
      "Gradient Descent(726/999): loss=292.62588971629367, w0=0.0690324853826062, w1=-0.5648128635049157\n",
      "Gradient Descent(727/999): loss=291.94558514426086, w0=0.06876880450119346, w1=-0.5640667397696187\n",
      "Gradient Descent(728/999): loss=291.2673905047004, w0=0.06850567120765637, w1=-0.5633220048065732\n",
      "Gradient Descent(729/999): loss=290.5912968153324, w0=0.06824308436605248, w1=-0.5625786552154644\n",
      "Gradient Descent(730/999): loss=289.9172951421391, w0=0.06798104284277909, w1=-0.5618366876056525\n",
      "Gradient Descent(731/999): loss=289.2453765990739, w0=0.06771954550656811, w1=-0.5610960985961452\n",
      "Gradient Descent(732/999): loss=288.57553234777185, w0=0.06745859122848083, w1=-0.5603568848155689\n",
      "Gradient Descent(733/999): loss=287.9077535972618, w0=0.06719817888190283, w1=-0.5596190429021414\n",
      "Gradient Descent(734/999): loss=287.2420316036808, w0=0.06693830734253882, w1=-0.5588825695036433\n",
      "Gradient Descent(735/999): loss=286.57835766999017, w0=0.06667897548840761, w1=-0.558147461277391\n",
      "Gradient Descent(736/999): loss=285.9167231456926, w0=0.06642018219983699, w1=-0.5574137148902082\n",
      "Gradient Descent(737/999): loss=285.25711942655323, w0=0.06616192635945871, w1=-0.5566813270183986\n",
      "Gradient Descent(738/999): loss=284.5995379543184, w0=0.06590420685220351, w1=-0.5559502943477179\n",
      "Gradient Descent(739/999): loss=283.9439702164421, w0=0.06564702256529605, w1=-0.5552206135733468\n",
      "Gradient Descent(740/999): loss=283.2904077458075, w0=0.06539037238825, w1=-0.554492281399863\n",
      "Gradient Descent(741/999): loss=282.63884212045656, w0=0.06513425521286312, w1=-0.5537652945412135\n",
      "Gradient Descent(742/999): loss=281.98926496331615, w0=0.06487866993321224, w1=-0.553039649720688\n",
      "Gradient Descent(743/999): loss=281.34166794192913, w0=0.0646236154456485, w1=-0.5523153436708909\n",
      "Gradient Descent(744/999): loss=280.6960427681862, w0=0.06436909064879234, w1=-0.5515923731337141\n",
      "Gradient Descent(745/999): loss=280.05238119805847, w0=0.06411509444352877, w1=-0.5508707348603104\n",
      "Gradient Descent(746/999): loss=279.4106750313343, w0=0.06386162573300244, w1=-0.5501504256110652\n",
      "Gradient Descent(747/999): loss=278.7709161113542, w0=0.06360868342261292, w1=-0.5494314421555707\n",
      "Gradient Descent(748/999): loss=278.13309632475085, w0=0.06335626642000984, w1=-0.548713781272598\n",
      "Gradient Descent(749/999): loss=277.4972076011889, w0=0.06310437363508817, w1=-0.5479974397500703\n",
      "Gradient Descent(750/999): loss=276.86324191310587, w0=0.06285300397998345, w1=-0.5472824143850363\n",
      "Gradient Descent(751/999): loss=276.2311912754568, w0=0.06260215636906707, w1=-0.5465687019836433\n",
      "Gradient Descent(752/999): loss=275.601047745459, w0=0.06235182971894162, w1=-0.5458562993611099\n",
      "Gradient Descent(753/999): loss=274.9728034223379, w0=0.06210202294843611, w1=-0.5451452033417002\n",
      "Gradient Descent(754/999): loss=274.3464504470769, w0=0.06185273497860137, w1=-0.5444354107586965\n",
      "Gradient Descent(755/999): loss=273.7219810021659, w0=0.06160396473270542, w1=-0.5437269184543728\n",
      "Gradient Descent(756/999): loss=273.09938731135304, w0=0.06135571113622879, w1=-0.5430197232799686\n",
      "Gradient Descent(757/999): loss=272.4786616393982, w0=0.061107973116859955, w1=-0.5423138220956621\n",
      "Gradient Descent(758/999): loss=271.8597962918273, w0=0.06086074960449073, w1=-0.5416092117705441\n",
      "Gradient Descent(759/999): loss=271.24278361468873, w0=0.06061403953121171, w1=-0.5409058891825915\n",
      "Gradient Descent(760/999): loss=270.62761599431, w0=0.060367841831307716, w1=-0.5402038512186413\n",
      "Gradient Descent(761/999): loss=270.0142858570586, w0=0.06012215544125326, w1=-0.5395030947743641\n",
      "Gradient Descent(762/999): loss=269.4027856691014, w0=0.05987697929970805, w1=-0.538803616754238\n",
      "Gradient Descent(763/999): loss=268.79310793616725, w0=0.059632312347512456, w1=-0.538105414071523\n",
      "Gradient Descent(764/999): loss=268.1852452033113, w0=0.05938815352768307, w1=-0.5374084836482343\n",
      "Gradient Descent(765/999): loss=267.5791900546794, w0=0.05914450178540821, w1=-0.5367128224151172\n",
      "Gradient Descent(766/999): loss=266.9749351132754, w0=0.05890135606804352, w1=-0.5360184273116204\n",
      "Gradient Descent(767/999): loss=266.37247304072923, w0=0.058658715325107476, w1=-0.5353252952858709\n",
      "Gradient Descent(768/999): loss=265.77179653706656, w0=0.05841657850827704, w1=-0.5346334232946479\n",
      "Gradient Descent(769/999): loss=265.1728983404794, w0=0.05817494457138323, w1=-0.5339428083033572\n",
      "Gradient Descent(770/999): loss=264.57577122709995, w0=0.057933812470406736, w1=-0.5332534472860057\n",
      "Gradient Descent(771/999): loss=263.9804080107731, w0=0.05769318116347357, w1=-0.5325653372251756\n",
      "Gradient Descent(772/999): loss=263.38680154283253, w0=0.05745304961085072, w1=-0.5318784751119996\n",
      "Gradient Descent(773/999): loss=262.7949447118777, w0=0.05721341677494183, w1=-0.5311928579461346\n",
      "Gradient Descent(774/999): loss=262.20483044355257, w0=0.05697428162028283, w1=-0.5305084827357369\n",
      "Gradient Descent(775/999): loss=261.61645170032347, w0=0.056735643113537695, w1=-0.529825346497437\n",
      "Gradient Descent(776/999): loss=261.02980148126176, w0=0.05649750022349414, w1=-0.529143446256314\n",
      "Gradient Descent(777/999): loss=260.4448728218261, w0=0.056259851921059326, w1=-0.5284627790458711\n",
      "Gradient Descent(778/999): loss=259.86165879364535, w0=0.05602269717925563, w1=-0.5277833419080099\n",
      "Gradient Descent(779/999): loss=259.2801525043046, w0=0.05578603497321639, w1=-0.5271051318930055\n",
      "Gradient Descent(780/999): loss=258.7003470971314, w0=0.055549864280181674, w1=-0.5264281460594818\n",
      "Gradient Descent(781/999): loss=258.1222357509825, w0=0.05531418407949408, w1=-0.5257523814743867\n",
      "Gradient Descent(782/999): loss=257.54581168003557, w0=0.05507899335259453, w1=-0.5250778352129669\n",
      "Gradient Descent(783/999): loss=256.9710681335772, w0=0.05484429108301806, w1=-0.5244045043587435\n",
      "Gradient Descent(784/999): loss=256.39799839579587, w0=0.054610076256389695, w1=-0.5237323860034871\n",
      "Gradient Descent(785/999): loss=255.82659578557525, w0=0.05437634786042024, w1=-0.5230614772471934\n",
      "Gradient Descent(786/999): loss=255.2568536562876, w0=0.05414310488490217, w1=-0.5223917751980586\n",
      "Gradient Descent(787/999): loss=254.6887653955912, w0=0.05391034632170547, w1=-0.5217232769724545\n",
      "Gradient Descent(788/999): loss=254.12232442522526, w0=0.05367807116477354, w1=-0.5210559796949048\n",
      "Gradient Descent(789/999): loss=253.55752420081006, w0=0.05344627841011908, w1=-0.5203898804980605\n",
      "Gradient Descent(790/999): loss=252.99435821164553, w0=0.053214967055819976, w1=-0.5197249765226751\n",
      "Gradient Descent(791/999): loss=252.4328199805123, w0=0.05298413610201525, w1=-0.5190612649175813\n",
      "Gradient Descent(792/999): loss=251.8729030634739, w0=0.052753784550900976, w1=-0.518398742839666\n",
      "Gradient Descent(793/999): loss=251.31460104967968, w0=0.052523911406726215, w1=-0.5177374074538467\n",
      "Gradient Descent(794/999): loss=250.75790756116993, w0=0.05229451567578899, w1=-0.5170772559330473\n",
      "Gradient Descent(795/999): loss=250.20281625268095, w0=0.052065596366432224, w1=-0.5164182854581744\n",
      "Gradient Descent(796/999): loss=249.6493208114526, w0=0.05183715248903975, w1=-0.5157604932180929\n",
      "Gradient Descent(797/999): loss=249.0974149570371, w0=0.05160918305603229, w1=-0.5151038764096028\n",
      "Gradient Descent(798/999): loss=248.54709244110626, w0=0.051381687081863456, w1=-0.5144484322374149\n",
      "Gradient Descent(799/999): loss=247.9983470472646, w0=0.051154663583015764, w1=-0.5137941579141274\n",
      "Gradient Descent(800/999): loss=247.45117259086, w0=0.050928111577996656, w1=-0.5131410506602023\n",
      "Gradient Descent(801/999): loss=246.90556291879622, w0=0.05070203008733455, w1=-0.5124891077039417\n",
      "Gradient Descent(802/999): loss=246.3615119093481, w0=0.05047641813357488, w1=-0.5118383262814643\n",
      "Gradient Descent(803/999): loss=245.81901347197626, w0=0.05025127474127615, w1=-0.511188703636682\n",
      "Gradient Descent(804/999): loss=245.27806154714355, w0=0.050026598937006006, w1=-0.5105402370212766\n",
      "Gradient Descent(805/999): loss=244.7386501061326, w0=0.04980238974933733, w1=-0.5098929236946764\n",
      "Gradient Descent(806/999): loss=244.2007731508654, w0=0.04957864620884432, w1=-0.5092467609240332\n",
      "Gradient Descent(807/999): loss=243.66442471372216, w0=0.04935536734809859, w1=-0.5086017459841986\n",
      "Gradient Descent(808/999): loss=243.1295988573629, w0=0.04913255220166528, w1=-0.5079578761577016\n",
      "Gradient Descent(809/999): loss=242.5962896745497, w0=0.04891019980609919, w1=-0.5073151487347248\n",
      "Gradient Descent(810/999): loss=242.0644912879696, w0=0.04868830919994092, w1=-0.5066735610130821\n",
      "Gradient Descent(811/999): loss=241.53419785005897, w0=0.04846687942371298, w1=-0.5060331102981954\n",
      "Gradient Descent(812/999): loss=241.00540354282936, w0=0.04824590951991599, w1=-0.5053937939030718\n",
      "Gradient Descent(813/999): loss=240.4781025776941, w0=0.048025398533024795, w1=-0.5047556091482809\n",
      "Gradient Descent(814/999): loss=239.9522891952955, w0=0.04780534550948468, w1=-0.504118553361932\n",
      "Gradient Descent(815/999): loss=239.42795766533368, w0=0.04758574949770754, w1=-0.5034826238796515\n",
      "Gradient Descent(816/999): loss=238.90510228639675, w0=0.04736660954806804, w1=-0.5028478180445601\n",
      "Gradient Descent(817/999): loss=238.3837173857915, w0=0.04714792471289987, w1=-0.5022141332072508\n",
      "Gradient Descent(818/999): loss=237.8637973193752, w0=0.04692969404649192, w1=-0.5015815667257658\n",
      "Gradient Descent(819/999): loss=237.3453364713883, w0=0.04671191660508453, w1=-0.5009501159655745\n",
      "Gradient Descent(820/999): loss=236.82832925428946, w0=0.04649459144686568, w1=-0.500319778299551\n",
      "Gradient Descent(821/999): loss=236.31277010858935, w0=0.046277717631967266, w1=-0.49969055110795185\n",
      "Gradient Descent(822/999): loss=235.7986535026881, w0=0.04606129422246134, w1=-0.499062431778394\n",
      "Gradient Descent(823/999): loss=235.28597393271096, w0=0.04584532028235635, w1=-0.49843541770583233\n",
      "Gradient Descent(824/999): loss=234.77472592234724, w0=0.045629794877593444, w1=-0.4978095062925378\n",
      "Gradient Descent(825/999): loss=234.2649040226898, w0=0.04541471707604271, w1=-0.49718469494807543\n",
      "Gradient Descent(826/999): loss=233.75650281207442, w0=0.04520008594749946, w1=-0.49656098108928226\n",
      "Gradient Descent(827/999): loss=233.24951689592112, w0=0.04498590056368057, w1=-0.49593836214024545\n",
      "Gradient Descent(828/999): loss=232.7439409065774, w0=0.04477215999822072, w1=-0.49531683553228056\n",
      "Gradient Descent(829/999): loss=232.23976950315927, w0=0.044558863326668745, w1=-0.49469639870390963\n",
      "Gradient Descent(830/999): loss=231.73699737139717, w0=0.04434600962648394, w1=-0.49407704910083955\n",
      "Gradient Descent(831/999): loss=231.23561922348014, w0=0.04413359797703239, w1=-0.49345878417594047\n",
      "Gradient Descent(832/999): loss=230.73562979790256, w0=0.043921627459583314, w1=-0.4928416013892241\n",
      "Gradient Descent(833/999): loss=230.23702385930997, w0=0.04371009715730539, w1=-0.4922254982078223\n",
      "Gradient Descent(834/999): loss=229.73979619834864, w0=0.043499006155263115, w1=-0.4916104721059656\n",
      "Gradient Descent(835/999): loss=229.24394163151305, w0=0.04328835354041319, w1=-0.4909965205649617\n",
      "Gradient Descent(836/999): loss=228.7494550009964, w0=0.04307813840160084, w1=-0.4903836410731743\n",
      "Gradient Descent(837/999): loss=228.25633117454123, w0=0.04286835982955624, w1=-0.48977183112600176\n",
      "Gradient Descent(838/999): loss=227.76456504529173, w0=0.04265901691689086, w1=-0.48916108822585586\n",
      "Gradient Descent(839/999): loss=227.27415153164512, w0=0.042450108758093894, w1=-0.48855140988214063\n",
      "Gradient Descent(840/999): loss=226.78508557710614, w0=0.042241634449528616, w1=-0.48794279361123144\n",
      "Gradient Descent(841/999): loss=226.29736215014157, w0=0.042033593089428826, w1=-0.48733523693645375\n",
      "Gradient Descent(842/999): loss=225.81097624403534, w0=0.04182598377789525, w1=-0.4867287373880623\n",
      "Gradient Descent(843/999): loss=225.32592287674427, w0=0.04161880561689198, w1=-0.48612329250322023\n",
      "Gradient Descent(844/999): loss=224.84219709075617, w0=0.04141205771024286, w1=-0.485518899825978\n",
      "Gradient Descent(845/999): loss=224.35979395294768, w0=0.04120573916362798, w1=-0.484915556907253\n",
      "Gradient Descent(846/999): loss=223.8787085544425, w0=0.04099984908458008, w1=-0.4843132613048085\n",
      "Gradient Descent(847/999): loss=223.3989360104724, w0=0.040794386582481024, w1=-0.48371201058323315\n",
      "Gradient Descent(848/999): loss=222.9204714602366, w0=0.04058935076855826, w1=-0.48311180231392037\n",
      "Gradient Descent(849/999): loss=222.44331006676418, w0=0.040384740755881283, w1=-0.48251263407504774\n",
      "Gradient Descent(850/999): loss=221.9674470167762, w0=0.0401805556593581, w1=-0.4819145034515567\n",
      "Gradient Descent(851/999): loss=221.49287752054946, w0=0.03997679459573174, w1=-0.48131740803513184\n",
      "Gradient Descent(852/999): loss=221.01959681177962, w0=0.03977345668357671, w1=-0.48072134542418093\n",
      "Gradient Descent(853/999): loss=220.54760014744699, w0=0.03957054104329551, w1=-0.4801263132238144\n",
      "Gradient Descent(854/999): loss=220.07688280768218, w0=0.039368046797115135, w1=-0.479532309045825\n",
      "Gradient Descent(855/999): loss=219.60744009563254, w0=0.039165973069083586, w1=-0.47893933050866794\n",
      "Gradient Descent(856/999): loss=219.1392673373296, w0=0.03896431898506638, w1=-0.4783473752374405\n",
      "Gradient Descent(857/999): loss=218.67235988155787, w0=0.03876308367274309, w1=-0.477756440863862\n",
      "Gradient Descent(858/999): loss=218.20671309972298, w0=0.03856226626160385, w1=-0.4771665250262541\n",
      "Gradient Descent(859/999): loss=217.74232238572253, w0=0.03836186588294592, w1=-0.47657762536952053\n",
      "Gradient Descent(860/999): loss=217.27918315581636, w0=0.03816188166987021, w1=-0.4759897395451273\n",
      "Gradient Descent(861/999): loss=216.81729084849786, w0=0.03796231275727784, w1=-0.475402865211083\n",
      "Gradient Descent(862/999): loss=216.35664092436747, w0=0.037763158281866716, w1=-0.4748170000319189\n",
      "Gradient Descent(863/999): loss=215.89722886600387, w0=0.03756441738212804, w1=-0.4742321416786694\n",
      "Gradient Descent(864/999): loss=215.43905017783922, w0=0.03736608919834295, w1=-0.47364828782885227\n",
      "Gradient Descent(865/999): loss=214.982100386034, w0=0.03716817287257905, w1=-0.4730654361664491\n",
      "Gradient Descent(866/999): loss=214.5263750383514, w0=0.03697066754868701, w1=-0.4724835843818857\n",
      "Gradient Descent(867/999): loss=214.07186970403413, w0=0.036773572372297156, w1=-0.4719027301720129\n",
      "Gradient Descent(868/999): loss=213.61857997368207, w0=0.03657688649081606, w1=-0.47132287124008687\n",
      "Gradient Descent(869/999): loss=213.16650145912848, w0=0.036380609053423156, w1=-0.4707440052957499\n",
      "Gradient Descent(870/999): loss=212.71562979332035, w0=0.03618473921106735, w1=-0.470166130055011\n",
      "Gradient Descent(871/999): loss=212.26596063019574, w0=0.03598927611646362, w1=-0.4695892432402269\n",
      "Gradient Descent(872/999): loss=211.8174896445655, w0=0.035794218924089634, w1=-0.46901334258008254\n",
      "Gradient Descent(873/999): loss=211.3702125319929, w0=0.03559956679018241, w1=-0.46843842580957235\n",
      "Gradient Descent(874/999): loss=210.9241250086754, w0=0.03540531887273493, w1=-0.4678644906699808\n",
      "Gradient Descent(875/999): loss=210.4792228113274, w0=0.03521147433149278, w1=-0.4672915349088637\n",
      "Gradient Descent(876/999): loss=210.03550169706253, w0=0.03501803232795079, w1=-0.4667195562800291\n",
      "Gradient Descent(877/999): loss=209.59295744327747, w0=0.03482499202534971, w1=-0.4661485525435184\n",
      "Gradient Descent(878/999): loss=209.15158584753627, w0=0.034632352588672845, w1=-0.46557852146558765\n",
      "Gradient Descent(879/999): loss=208.71138272745603, w0=0.03444011318464274, w1=-0.4650094608186886\n",
      "Gradient Descent(880/999): loss=208.27234392059202, w0=0.03424827298171783, w1=-0.4644413683814501\n",
      "Gradient Descent(881/999): loss=207.8344652843244, w0=0.034056831150089124, w1=-0.4638742419386595\n",
      "Gradient Descent(882/999): loss=207.3977426957455, w0=0.03386578686167691, w1=-0.4633080792812438\n",
      "Gradient Descent(883/999): loss=206.96217205154827, w0=0.033675139290127415, w1=-0.4627428782062513\n",
      "Gradient Descent(884/999): loss=206.52774926791344, w0=0.033484887610809505, w1=-0.4621786365168332\n",
      "Gradient Descent(885/999): loss=206.09447028040006, w0=0.033295031000811395, w1=-0.4616153520222249\n",
      "Gradient Descent(886/999): loss=205.66233104383576, w0=0.03310556863893735, w1=-0.4610530225377278\n",
      "Gradient Descent(887/999): loss=205.2313275322061, w0=0.0329164997057044, w1=-0.4604916458846908\n",
      "Gradient Descent(888/999): loss=204.8014557385468, w0=0.032727823383339044, w1=-0.45993121989049246\n",
      "Gradient Descent(889/999): loss=204.37271167483544, w0=0.032539538855774, w1=-0.4593717423885222\n",
      "Gradient Descent(890/999): loss=203.94509137188422, w0=0.03235164530864491, w1=-0.4588132112181625\n",
      "Gradient Descent(891/999): loss=203.51859087923353, w0=0.03216414192928708, w1=-0.4582556242247708\n",
      "Gradient Descent(892/999): loss=203.0932062650455, w0=0.03197702790673223, w1=-0.45769897925966135\n",
      "Gradient Descent(893/999): loss=202.66893361599853, w0=0.03179030243170523, w1=-0.4571432741800872\n",
      "Gradient Descent(894/999): loss=202.24576903718324, w0=0.03160396469662085, w1=-0.4565885068492224\n",
      "Gradient Descent(895/999): loss=201.82370865199786, w0=0.03141801389558053, w1=-0.45603467513614393\n",
      "Gradient Descent(896/999): loss=201.40274860204514, w0=0.03123244922436913, w1=-0.45548177691581393\n",
      "Gradient Descent(897/999): loss=200.98288504702853, w0=0.03104726988045171, w1=-0.4549298100690621\n",
      "Gradient Descent(898/999): loss=200.56411416465252, w0=0.03086247506297029, w1=-0.45437877248256775\n",
      "Gradient Descent(899/999): loss=200.14643215051757, w0=0.030678063972740633, w1=-0.4538286620488422\n",
      "Gradient Descent(900/999): loss=199.7298352180223, w0=0.030494035812249038, w1=-0.45327947666621127\n",
      "Gradient Descent(901/999): loss=199.31431959826148, w0=0.030310389785649126, w1=-0.4527312142387976\n",
      "Gradient Descent(902/999): loss=198.89988153992675, w0=0.030127125098758624, w1=-0.4521838726765031\n",
      "Gradient Descent(903/999): loss=198.4865173092081, w0=0.02994424095905619, w1=-0.4516374498949917\n",
      "Gradient Descent(904/999): loss=198.07422318969427, w0=0.02976173657567819, w1=-0.45109194381567186\n",
      "Gradient Descent(905/999): loss=197.6629954822765, w0=0.02957961115941553, w1=-0.450547352365679\n",
      "Gradient Descent(906/999): loss=197.25283050504916, w0=0.029397863922710474, w1=-0.45000367347785863\n",
      "Gradient Descent(907/999): loss=196.84372459321565, w0=0.029216494079653446, w1=-0.44946090509074876\n",
      "Gradient Descent(908/999): loss=196.43567409898992, w0=0.02903550084597988, w1=-0.44891904514856285\n",
      "Gradient Descent(909/999): loss=196.02867539150262, w0=0.028854883439067034, w1=-0.4483780916011727\n",
      "Gradient Descent(910/999): loss=195.62272485670528, w0=0.028674641077930847, w1=-0.44783804240409125\n",
      "Gradient Descent(911/999): loss=195.21781889727694, w0=0.02849477298322276, w1=-0.44729889551845575\n",
      "Gradient Descent(912/999): loss=194.81395393252922, w0=0.02831527837722659, w1=-0.44676064891101064\n",
      "Gradient Descent(913/999): loss=194.4111263983144, w0=0.028136156483855358, w1=-0.44622330055409054\n",
      "Gradient Descent(914/999): loss=194.00933274693205, w0=0.02795740652864816, w1=-0.4456868484256036\n",
      "Gradient Descent(915/999): loss=193.60856944703704, w0=0.02777902773876704, w1=-0.4451512905090145\n",
      "Gradient Descent(916/999): loss=193.20883298354846, w0=0.027601019342993832, w1=-0.44461662479332775\n",
      "Gradient Descent(917/999): loss=192.81011985755862, w0=0.02742338057172707, w1=-0.44408284927307107\n",
      "Gradient Descent(918/999): loss=192.41242658624236, w0=0.02724611065697883, w1=-0.44354996194827845\n",
      "Gradient Descent(919/999): loss=192.01574970276818, w0=0.02706920883237164, w1=-0.4430179608244738\n",
      "Gradient Descent(920/999): loss=191.6200857562074, w0=0.02689267433313536, w1=-0.4424868439126543\n",
      "Gradient Descent(921/999): loss=191.22543131144738, w0=0.02671650639610408, w1=-0.4419566092292738\n",
      "Gradient Descent(922/999): loss=190.83178294910178, w0=0.026540704259713013, w1=-0.4414272547962266\n",
      "Gradient Descent(923/999): loss=190.43913726542397, w0=0.026365267163995402, w1=-0.44089877864083055\n",
      "Gradient Descent(924/999): loss=190.04749087221956, w0=0.026190194350579434, w1=-0.44037117879581134\n",
      "Gradient Descent(925/999): loss=189.6568403967601, w0=0.026015485062685147, w1=-0.4398444532992856\n",
      "Gradient Descent(926/999): loss=189.2671824816962, w0=0.025841138545121357, w1=-0.4393186001947451\n",
      "Gradient Descent(927/999): loss=188.87851378497336, w0=0.025667154044282575, w1=-0.43879361753104024\n",
      "Gradient Descent(928/999): loss=188.49083097974633, w0=0.025493530808145936, w1=-0.4382695033623639\n",
      "Gradient Descent(929/999): loss=188.10413075429452, w0=0.025320268086268142, w1=-0.43774625574823556\n",
      "Gradient Descent(930/999): loss=187.71840981193833, w0=0.02514736512978239, w1=-0.4372238727534849\n",
      "Gradient Descent(931/999): loss=187.33366487095535, w0=0.024974821191395328, w1=-0.4367023524482362\n",
      "Gradient Descent(932/999): loss=186.9498926644986, w0=0.02480263552538399, w1=-0.43618169290789194\n",
      "Gradient Descent(933/999): loss=186.5670899405122, w0=0.02463080738759276, w1=-0.43566189221311724\n",
      "Gradient Descent(934/999): loss=186.18525346165134, w0=0.024459336035430327, w1=-0.4351429484498238\n",
      "Gradient Descent(935/999): loss=185.80438000519996, w0=0.024288220727866648, w1=-0.43462485970915415\n",
      "Gradient Descent(936/999): loss=185.42446636298936, w0=0.024117460725429918, w1=-0.43410762408746584\n",
      "Gradient Descent(937/999): loss=185.0455093413193, w0=0.02394705529020354, w1=-0.4335912396863158\n",
      "Gradient Descent(938/999): loss=184.6675057608767, w0=0.023777003685823105, w1=-0.4330757046124446\n",
      "Gradient Descent(939/999): loss=184.2904524566568, w0=0.023607305177473375, w1=-0.43256101697776084\n",
      "Gradient Descent(940/999): loss=183.914346277884, w0=0.02343795903188527, w1=-0.4320471748993256\n",
      "Gradient Descent(941/999): loss=183.53918408793345, w0=0.02326896451733286, w1=-0.4315341764993369\n",
      "Gradient Descent(942/999): loss=183.1649627642532, w0=0.023100320903630362, w1=-0.43102201990511424\n",
      "Gradient Descent(943/999): loss=182.79167919828646, w0=0.022932027462129145, w1=-0.43051070324908314\n",
      "Gradient Descent(944/999): loss=182.41933029539464, w0=0.022764083465714725, w1=-0.4300002246687598\n",
      "Gradient Descent(945/999): loss=182.04791297478093, w0=0.022596488188803793, w1=-0.42949058230673576\n",
      "Gradient Descent(946/999): loss=181.67742416941394, w0=0.022429240907341223, w1=-0.42898177431066253\n",
      "Gradient Descent(947/999): loss=181.30786082595168, w0=0.022262340898797084, w1=-0.42847379883323655\n",
      "Gradient Descent(948/999): loss=180.93921990466774, w0=0.02209578744216368, w1=-0.42796665403218376\n",
      "Gradient Descent(949/999): loss=180.57149837937482, w0=0.021929579817952575, w1=-0.4274603380702445\n",
      "Gradient Descent(950/999): loss=180.20469323735142, w0=0.021763717308191627, w1=-0.42695484911515874\n",
      "Gradient Descent(951/999): loss=179.8388014792681, w0=0.021598199196422023, w1=-0.4264501853396505\n",
      "Gradient Descent(952/999): loss=179.4738201191137, w0=0.02143302476769533, w1=-0.4259463449214133\n",
      "Gradient Descent(953/999): loss=179.10974618412232, w0=0.021268193308570542, w1=-0.42544332604309487\n",
      "Gradient Descent(954/999): loss=178.74657671470132, w0=0.021103704107111133, w1=-0.4249411268922825\n",
      "Gradient Descent(955/999): loss=178.38430876435862, w0=0.020939556452882112, w1=-0.42443974566148795\n",
      "Gradient Descent(956/999): loss=178.02293939963204, w0=0.02077574963694709, w1=-0.4239391805481329\n",
      "Gradient Descent(957/999): loss=177.66246570001647, w0=0.020612282951865346, w1=-0.42343942975453386\n",
      "Gradient Descent(958/999): loss=177.30288475789513, w0=0.0204491556916889, w1=-0.42294049148788776\n",
      "Gradient Descent(959/999): loss=176.94419367846763, w0=0.020286367151959587, w1=-0.422442363960257\n",
      "Gradient Descent(960/999): loss=176.5863895796809, w0=0.02012391662970614, w1=-0.4219450453885549\n",
      "Gradient Descent(961/999): loss=176.22946959215957, w0=0.01996180342344128, w1=-0.42144853399453125\n",
      "Gradient Descent(962/999): loss=175.87343085913665, w0=0.01980002683315879, w1=-0.4209528280047576\n",
      "Gradient Descent(963/999): loss=175.51827053638482, w0=0.019638586160330634, w1=-0.4204579256506128\n",
      "Gradient Descent(964/999): loss=175.16398579214908, w0=0.01947748070790404, w1=-0.41996382516826863\n",
      "Gradient Descent(965/999): loss=174.81057380707796, w0=0.019316709780298605, w1=-0.41947052479867525\n",
      "Gradient Descent(966/999): loss=174.45803177415584, w0=0.019156272683403416, w1=-0.41897802278754687\n",
      "Gradient Descent(967/999): loss=174.10635689863773, w0=0.01899616872457414, w1=-0.4184863173853476\n",
      "Gradient Descent(968/999): loss=173.75554639798045, w0=0.01883639721263017, w1=-0.4179954068472769\n",
      "Gradient Descent(969/999): loss=173.40559750177806, w0=0.018676957457851726, w1=-0.4175052894332556\n",
      "Gradient Descent(970/999): loss=173.05650745169564, w0=0.01851784877197699, w1=-0.4170159634079115\n",
      "Gradient Descent(971/999): loss=172.70827350140283, w0=0.01835907046819923, w1=-0.4165274270405655\n",
      "Gradient Descent(972/999): loss=172.3608929165106, w0=0.018200621861163956, w1=-0.4160396786052171\n",
      "Gradient Descent(973/999): loss=172.01436297450564, w0=0.01804250226696603, w1=-0.41555271638053076\n",
      "Gradient Descent(974/999): loss=171.6686809646861, w0=0.01788471100314684, w1=-0.4150665386498216\n",
      "Gradient Descent(975/999): loss=171.32384418809798, w0=0.01772724738869143, w1=-0.41458114370104165\n",
      "Gradient Descent(976/999): loss=170.97984995747169, w0=0.01757011074402566, w1=-0.41409652982676565\n",
      "Gradient Descent(977/999): loss=170.63669559715848, w0=0.017413300391013364, w1=-0.4136126953241775\n",
      "Gradient Descent(978/999): loss=170.29437844306912, w0=0.017256815652953512, w1=-0.41312963849505613\n",
      "Gradient Descent(979/999): loss=169.95289584261005, w0=0.017100655854577384, w1=-0.41264735764576194\n",
      "Gradient Descent(980/999): loss=169.6122451546218, w0=0.01694482032204573, w1=-0.41216585108722287\n",
      "Gradient Descent(981/999): loss=169.2724237493185, w0=0.016789308382945952, w1=-0.4116851171349208\n",
      "Gradient Descent(982/999): loss=168.93342900822503, w0=0.016634119366289286, w1=-0.4112051541088778\n",
      "Gradient Descent(983/999): loss=168.59525832411782, w0=0.016479252602507984, w1=-0.4107259603336426\n",
      "Gradient Descent(984/999): loss=168.25790910096288, w0=0.016324707423452503, w1=-0.41024753413827697\n",
      "Gradient Descent(985/999): loss=167.92137875385683, w0=0.016170483162388706, w1=-0.4097698738563421\n",
      "Gradient Descent(986/999): loss=167.58566470896685, w0=0.016016579153995052, w1=-0.4092929778258853\n",
      "Gradient Descent(987/999): loss=167.2507644034711, w0=0.015862994734359797, w1=-0.4088168443894263\n",
      "Gradient Descent(988/999): loss=166.91667528550002, w0=0.01570972924097821, w1=-0.40834147189394393\n",
      "Gradient Descent(989/999): loss=166.58339481407683, w0=0.015556782012749775, w1=-0.407866858690863\n",
      "Gradient Descent(990/999): loss=166.25092045906055, w0=0.01540415238997542, w1=-0.40739300313604054\n",
      "Gradient Descent(991/999): loss=165.91924970108718, w0=0.015251839714354724, w1=-0.40691990358975283\n",
      "Gradient Descent(992/999): loss=165.588380031512, w0=0.015099843328983144, w1=-0.406447558416682\n",
      "Gradient Descent(993/999): loss=165.25830895235305, w0=0.014948162578349253, w1=-0.4059759659859029\n",
      "Gradient Descent(994/999): loss=164.9290339762329, w0=0.014796796808331961, w1=-0.4055051246708699\n",
      "Gradient Descent(995/999): loss=164.6005526263237, w0=0.014645745366197763, w1=-0.4050350328494038\n",
      "Gradient Descent(996/999): loss=164.27286243628953, w0=0.014495007600597975, w1=-0.40456568890367867\n",
      "Gradient Descent(997/999): loss=163.9459609502313, w0=0.014344582861565983, w1=-0.404097091220209\n",
      "Gradient Descent(998/999): loss=163.61984572263108, w0=0.014194470500514495, w1=-0.40362923818983637\n",
      "Gradient Descent(999/999): loss=163.2945143182962, w0=0.014044669870232793, w1=-0.40316212820771685\n"
     ]
    }
   ],
   "source": [
    "gamma=0.000002\n",
    "max_iters=1000\n",
    "final_w, ws, losses=least_squares_GD(y,tX,gamma,max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights=final_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tX_test = np.delete(tX_test, del_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 19)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/op0.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
