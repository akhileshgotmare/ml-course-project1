{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "import myFunctions1 as my\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from myFunctions1 import *\n",
    "DATA_TRAIN_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "6.320208\n"
     ]
    }
   ],
   "source": [
    "# For entries with missing data, the value -999 is filled, therefore we try to figure out ...\n",
    "# ... how much of the data is missing\n",
    "\n",
    "count_miss_instances=np.zeros((len(y),1))\n",
    "for id in ids:\n",
    "    count_miss_instances[id-100000]=sum(X[id-100000] == -999.0)\n",
    "print(np.median(count_miss_instances))\n",
    "print(np.mean(count_miss_instances))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above, for every instance on an average about 6 field/attribute values are missing, we further perform a feature-wise check for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  38114.       0.       0.       0.  177457.  177457.  177457.       0.\n",
      "        0.       0.       0.       0.  177457.       0.       0.       0.\n",
      "        0.       0.       0.       0.       0.       0.       0.   99913.\n",
      "    99913.   99913.  177457.  177457.  177457.       0.]]\n"
     ]
    }
   ],
   "source": [
    "count_miss_features=np.zeros((X.shape[1],1))\n",
    "for d in range(X.shape[1]):\n",
    "    count_miss_features[d]=sum(X[:,d] == -999.0)\n",
    "print(count_miss_features.T)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we realize that only some (11 in number - although not few) of the features have missing values. We fill in these values with the median, the way we approximate this is by inserting zeros at the NaN positions in the standardized data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_badFeatures(X):\n",
    "    \n",
    "    # Function that calculate the mean and std of bad features without elements equal to -999\n",
    "    # Then, it replaces -999 values by zeros, zeros won't influence the train of the model... \n",
    "    mean_x = np.zeros((X.shape[1],))\n",
    "    std_x = np.zeros((X.shape[1],))\n",
    "    for d in range(X.shape[1]):\n",
    "        idx = np.where(X[:,d] == -999)[0]\n",
    "        mean_x[d] = np.mean(np.delete(X[:,d], (idx)))\n",
    "        std_x[d] = np.std(np.delete(X[:,d], (idx)))\n",
    "        X[:,d] = (X[:,d]-mean_x[d])/std_x[d]\n",
    "        X[idx,d] = 0\n",
    "    return X, mean_x, std_x\n",
    "\n",
    "\n",
    "def clean_data(X):\n",
    "\n",
    "    # find indices of features that have at least one value -999, we call them \"bad\" features\n",
    "    idx_badFeatures = []\n",
    "    for d in range(X.shape[1]):\n",
    "        if sum(X[:,d] == -999) > 0:\n",
    "            idx_badFeatures.append(d)\n",
    "\n",
    "    # separate \"good\" and \"bad\" features\n",
    "    X_badFeatures = X[:,idx_badFeatures]\n",
    "    X_goodFeatures = np.delete(X,(idx_badFeatures), axis=1)\n",
    "\n",
    "    # Standardize it differently (see : standardize_badFeatures(X))\n",
    "    tX, mean_x, std_x = standardize(X_goodFeatures)\n",
    "    tX2, mean_x2, std_x2 = standardize_badFeatures(X_badFeatures)\n",
    "\n",
    "    # comment the 3 next lines if you want to work only with \"good\" features\n",
    "    tX = np.hstack((tX, tX2))\n",
    "    mean_x = np.hstack((mean_x, mean_x2))\n",
    "    std_x = np.hstack((std_x, std_x2))\n",
    "    \n",
    "    return tX, mean_x, std_x\n",
    "\n",
    "# Now tX already has ones in the first column...\n",
    "tX, mean_x, std_x = clean_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Linear regression using gradient descent: \n",
    "\n",
    "least_squares_GD (y, tx, gamma, max_iters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \n",
    "    # Returns the mean squared error for a given data and ...\n",
    "    # ... set of weights with respect to the labels y\n",
    "    \n",
    "    e=y-np.dot(tx,w)\n",
    "    L= ( 1/(2*len(y)) )*np.dot(e.T,e) # Least squares error - assuming the (1/2N)*(e.T*e) form\n",
    "    return L\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \n",
    "    # Returns the gradient of the loss function with respect to weights ...\n",
    "    # ... at a given point w in the space of weights\n",
    "    \n",
    "    e=y-np.dot(tx,w)\n",
    "    grad_L = (-1/len(y))*np.dot(tx.T,e) #Using the expression gradient of Loss = (-1/N)*(X.T*e)\n",
    "    return grad_L\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma): \n",
    "    \n",
    "    # Executes the gradient descent algorithm to find optimzal weights \n",
    "    # Returns as output the set of weight vectors in the update process\n",
    "    # and the corresponding losses \n",
    "    \n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        #print(n_iter)\n",
    "        \n",
    "        # Compute Loss and Gradient\n",
    "        L = compute_loss(y, tx, w)\n",
    "        grad_L = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma*grad_L\n",
    "        \n",
    "        loss = L\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(np.copy(w))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with method 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "#X_test = np.delete(X_test, del_features, axis=1)\n",
    "tX_test, mean_tX_test, std_tX_test = standardize(X_test)\n",
    "print(tX_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.random.random(tX.shape[1],)\n",
    "max_iters=1000\n",
    "gamma=0.02\n",
    "losses,ws = gradient_descent(y, tX, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/op1.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(ws[-1], tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Linear regression using stochastic gradient descent:\n",
    "least_squares_GD (y, tx, gamma, max_iters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \n",
    "    e=y-np.dot(tx,w)\n",
    "    grad_L = (-1/(len(y)))*(np.dot(tx.T,e))\n",
    "    # TODO: implement stochastic gradient computation.It's same as the gradient descent.\n",
    "    \n",
    "    return grad_L\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, gamma, max_epochs):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    #max_epochs = 20\n",
    "    #gamma = 0.02\n",
    "    \n",
    "    \n",
    "    # Define parameters to store w and loss\n",
    "    batch_size = 1\n",
    "    initial_w = np.random.randn(tx.shape[1],)\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_epochs):\n",
    "        \n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            \n",
    "            # Compute Loss and Gradient\n",
    "            L = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            grad_L = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        \n",
    "            # update w by gradient\n",
    "            w = w - gamma*grad_L\n",
    "            loss = L\n",
    "        \n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(np.copy(w))\n",
    "            losses.append(loss)\n",
    "    \n",
    "            #print(\"Stochastic Gradient Descent({bi}): loss={l}\".format(\n",
    "            #  bi=i , l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with method 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "from myFunctions1 import *\n",
    "\n",
    "max_epochs = 400\n",
    "gamma = 0.02\n",
    "\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = stochastic_gradient_descent(y, tX,gamma, max_epochs)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "#print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gradient_losses)\n",
    "#ws[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 31)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tX_test\n",
    "\n",
    "if mean_x is None:\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "if std_x is None:\n",
    "    std_x = np.std(x, axis=0)\n",
    "\n",
    "x[:, std_x>0] = x[:, std_x>0] / std_x[std_x>0]\n",
    "\n",
    "tx_test = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "\n",
    "tx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/op2.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(gradient_ws[-1], tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Least squares regression using normal equations:\n",
    "least_squares (y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def least_squares(y,tx):\n",
    "    \n",
    "    # Explicitly determining optimal w using normal equations\n",
    "    # returns the optimzal weight vector\n",
    "    \n",
    "    weights=np.dot( np.linalg.inv(np.dot(tx.T,tx)), np.dot(tx.T,y) )\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "(91,)\n"
     ]
    }
   ],
   "source": [
    "import myFunctions1 as my\n",
    "print(tX.shape)\n",
    "weights_method_3 = least_squares(y,my.build_poly(tX,3))\n",
    "print(weights_method_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(568238, 91)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "\n",
    "#Adding the column of ones (for the bias term)\n",
    "ek = np.ones((tX_test.shape[0],1))\n",
    "tX_test = np.c_[ ek , tX_test]\n",
    "print(tX_test.shape)\n",
    "\n",
    "import myFunctions1 as my\n",
    "tX_test=my.build_poly(tX_test,3)\n",
    "tX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/Users/akhileshgotmare/Desktop/Git_Junta/data-ml-course-project1/op3.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights_method_3, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
